<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="16583" opendate="2020-3-13 00:00:00" fixdate="2020-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalid classloader during pipeline creation</summary>
      <description>The end-to-end test SQLClientKafkaITCase.testKafka failed with18:13:02.425 [ERROR] testKafka[0: kafka-version:0.10 kafka-sql-version:.*kafka-0.10.jar](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase) Time elapsed: 32.246 s &lt;&lt;&lt; ERROR!java.io.IOException: Process execution failed due error. Error output:Mar 12, 2020 6:11:46 PM org.jline.utils.Log logrWARNING: Unable to create a system terminal, creating a dumb terminal (enable debug logging for more information)Exception in thread "main" org.apache.flink.table.client.SqlClientException: Could not submit given SQL update statement to cluster. at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:131) at org.apache.flink.table.client.SqlClient.start(SqlClient.java:104) at org.apache.flink.table.client.SqlClient.main(SqlClient.java:178) at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.insertIntoAvroTable(SQLClientKafkaITCase.java:178) at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:151)18:13:02.425 [ERROR] testKafka[1: kafka-version:0.11 kafka-sql-version:.*kafka-0.11.jar](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase) Time elapsed: 34.539 s &lt;&lt;&lt; ERROR!java.io.IOException: Process execution failed due error. Error output:Mar 12, 2020 6:12:21 PM org.jline.utils.Log logrWARNING: Unable to create a system terminal, creating a dumb terminal (enable debug logging for more information)Exception in thread "main" org.apache.flink.table.client.SqlClientException: Could not submit given SQL update statement to cluster. at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:131) at org.apache.flink.table.client.SqlClient.start(SqlClient.java:104) at org.apache.flink.table.client.SqlClient.main(SqlClient.java:178) at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.insertIntoAvroTable(SQLClientKafkaITCase.java:178) at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:151)https://api.travis-ci.org/v3/job/661535183/log.txt</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="26027" opendate="2022-2-9 00:00:00" fixdate="2022-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add FLIP-33 metrics to Pulsar connector</summary>
      <description></description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.groups.UnregisteredMetricsGroup.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.PulsarSinkE2ECase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.sink.PulsarSinkTestSuiteBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.StopCursorTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.sink.writer.topic.TopicProducerRegisterTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.sink.PulsarSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarUnorderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.PulsarSourceReaderFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSourceOptions.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.SplitAssignerBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.assigner.SplitAssigner.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.config.SourceConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.topic.TopicProducerRegister.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.PulsarWriter.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.PulsarSinkOptions.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.config.SinkConfiguration.java</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.source.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.sink.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="26028" opendate="2022-2-9 00:00:00" fixdate="2022-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Write documentation for new PulsarSink</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0,1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">docs.content.docs.connectors.datastream.pulsar.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.pulsar.md</file>
    </fixedFiles>
  </bug>
  <bug id="27115" opendate="2022-4-7 00:00:00" fixdate="2022-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error while executing BLOB connection. java.io.IOException: Unknown operation 80</summary>
      <description>hi, I have a Flink SQL job running online. Every morning, I will report the following screenshot error. I have enabled the debug log to check and find nothingversion 1.12  </description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobServerConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="27757" opendate="2022-5-24 00:00:00" fixdate="2022-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elasticsearch connector should not use flink-table-planner but flink-table-planner-loader</summary>
      <description>Connectors should not rely on flink-table-planner but on flink-table-planner-loader by default. We can should change this for the Elasticsearch connector as this is being externalized at the moment</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="28759" opendate="2022-8-1 00:00:00" fixdate="2022-8-1 01:00:00" resolution="Done">
    <buginformation>
      <summary>Enable speculative execution for in AdaptiveBatchScheduler TPC-DS e2e tests</summary>
      <description>To verify the correctness of speculative execution, we can enabled it in AdaptiveBatchScheduler TPC-DS e2e tests, which runs a lot of different batch jobs and verifies the result.Note that we need to disable the blocklist (by setting block duration to 0) in such single machine e2e tests.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="28899" opendate="2022-8-10 00:00:00" fixdate="2022-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix LOOKUP hint with retry option on async lookup mode</summary>
      <description>Fix the broken path for async lookup with retry options in  LOOKUP hint.Will use a `RetryableAsyncLookupFunctionDelegator` instead of `AsyncWaitOperator`</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.join.lookup.ResultRetryStrategy.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AsyncLookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.utils.LookupJoinUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLookupJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="29000" opendate="2022-8-16 00:00:00" fixdate="2022-1-16 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support python UDF in the SQL Gateway</summary>
      <description>Currently Flink SQL Client supports python UDF, the Gateway should also support this feature if the SQL Client is able to submit SQL to the Gateway.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.session.SessionManagerTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.DefaultContext.java</file>
      <file type="M">flink-table.flink-sql-gateway.pom.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink-daemon.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink-console.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
    </fixedFiles>
  </bug>
  <bug id="29002" opendate="2022-8-16 00:00:00" fixdate="2022-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecate Datadog reporter &amp;#39;tags&amp;#39; option</summary>
      <description>The option us subsumed by the generic scope.variables.additional option.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DatadogHttpReporterFactory.java</file>
      <file type="M">docs.content.docs.deployment.metric.reporters.md</file>
      <file type="M">docs.content.zh.docs.deployment.metric.reporters.md</file>
    </fixedFiles>
  </bug>
  <bug id="29067" opendate="2022-8-22 00:00:00" fixdate="2022-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace deprecated Calcite&amp;#39;s SqlParser#configBuilder with SqlParser#config</summary>
      <description>SqlParser#configBuilder is deprecated and probably will be removed in future versions. It is better to replace it with SqlParser#config</description>
      <version>1.17.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.delegation.PlannerContext.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkDDLDataTypeTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.CreateTableLikeTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.package-info.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.package-info.java</file>
    </fixedFiles>
  </bug>
  <bug id="29198" opendate="2022-9-5 00:00:00" fixdate="2022-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RetryExtension doesn&amp;#39;t make the test fail if the retries are exhausted</summary>
      <description>FLINK-24627 introduced retry functionality for JUnit5-based tests. It appears that the retry mechanism doesn't have the desired behavior: If the retries are exhausted without the test ever succeeding will result in the test being ignored. I would expect the test to fail in that case. Otherwise, a CI run would succeed without anyone noticing the malfunctioning of the test.</description>
      <version>1.15.0,1.16.0,1.17.0</version>
      <fixedVersion>1.17.0,1.15.3,1.16.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.test.java.org.apache.flink.testutils.junit.RetryOnFailureExtensionTest.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.test.java.org.apache.flink.testutils.junit.RetryOnExceptionExtensionTest.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.testutils.junit.extensions.retry.strategy.RetryOnExceptionStrategy.java</file>
    </fixedFiles>
  </bug>
  <bug id="29253" opendate="2022-9-12 00:00:00" fixdate="2022-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DefaultJobmanagerRunnerRegistry#localCleanupAsync calls close instead of closeAsync</summary>
      <description>DefaultJobmanagerRunnerRegistry#localCleanupAsync is meant to be called from the main thread. The current implementation calls close on the JobManagerRunner instead of closeAsync. This results in a blocking call on the Dispatcher's main thread which we want to avoid.Thanks for identifying this issue, chesnay</description>
      <version>1.16.0,1.17.0,1.15.2</version>
      <fixedVersion>1.16.0,1.17.0,1.15.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerRegistryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="29337" opendate="2022-9-19 00:00:00" fixdate="2022-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix fail to query non-hive table in Hive dialect</summary>
      <description>Flink will fail for the query with non-hive table in HiveDialect.</description>
      <version>None</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserLoadSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.FromClauseASTParser.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserDMLHelper.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserUnparseTranslator.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserQBParseInfo.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserQB.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserBaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="29377" opendate="2022-9-21 00:00:00" fixdate="2022-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make RPC timeout extraction reusable</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="29386" opendate="2022-9-22 00:00:00" fixdate="2022-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix fail to compile flink-connector-hive when profile is hive3</summary>
      <description>The compile will fail in hive3. https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41238&amp;view=logs&amp;j=b1fcf054-9138-5463-c73c-a49979b9ac2a&amp;t=9291ac46-dd95-5135-b799-3839e65a8691Introduced by FLINK-29152 which introduces org.apache.hadoop.hive.metastore.MetaStoreUtils.DEFAULT_SERIALIZATION_FORMAT,  TableType.INDEX_TABLE, ErrorMsg.SHOW_CREATETABLE_INDEX.    But they don't exist in Hive3.</description>
      <version>1.16.0,1.17.0</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveShowTableUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="29408" opendate="2022-9-26 00:00:00" fixdate="2022-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveCatalogITCase failed with NPE</summary>
      <description>2022-09-25T03:41:07.4212129Z Sep 25 03:41:07 [ERROR] org.apache.flink.table.catalog.hive.HiveCatalogUdfITCase.testFlinkUdf Time elapsed: 0.098 s &lt;&lt;&lt; ERROR!2022-09-25T03:41:07.4212662Z Sep 25 03:41:07 java.lang.NullPointerException2022-09-25T03:41:07.4213189Z Sep 25 03:41:07 at org.apache.flink.table.catalog.hive.HiveCatalogUdfITCase.testFlinkUdf(HiveCatalogUdfITCase.java:109)2022-09-25T03:41:07.4213753Z Sep 25 03:41:07 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2022-09-25T03:41:07.4224643Z Sep 25 03:41:07 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2022-09-25T03:41:07.4225311Z Sep 25 03:41:07 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2022-09-25T03:41:07.4225879Z Sep 25 03:41:07 at java.lang.reflect.Method.invoke(Method.java:498)2022-09-25T03:41:07.4226405Z Sep 25 03:41:07 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)2022-09-25T03:41:07.4227201Z Sep 25 03:41:07 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2022-09-25T03:41:07.4227807Z Sep 25 03:41:07 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)2022-09-25T03:41:07.4228394Z Sep 25 03:41:07 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2022-09-25T03:41:07.4228966Z Sep 25 03:41:07 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2022-09-25T03:41:07.4229514Z Sep 25 03:41:07 at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)2022-09-25T03:41:07.4230066Z Sep 25 03:41:07 at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)2022-09-25T03:41:07.4230587Z Sep 25 03:41:07 at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)2022-09-25T03:41:07.4231258Z Sep 25 03:41:07 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)2022-09-25T03:41:07.4231823Z Sep 25 03:41:07 at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)2022-09-25T03:41:07.4232384Z Sep 25 03:41:07 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)2022-09-25T03:41:07.4232930Z Sep 25 03:41:07 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)2022-09-25T03:41:07.4233511Z Sep 25 03:41:07 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)2022-09-25T03:41:07.4234039Z Sep 25 03:41:07 at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)2022-09-25T03:41:07.4234546Z Sep 25 03:41:07 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)2022-09-25T03:41:07.4235057Z Sep 25 03:41:07 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)2022-09-25T03:41:07.4235573Z Sep 25 03:41:07 at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)2022-09-25T03:41:07.4236087Z Sep 25 03:41:07 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)2022-09-25T03:41:07.4236635Z Sep 25 03:41:07 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2022-09-25T03:41:07.4237314Z Sep 25 03:41:07 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2022-09-25T03:41:07.4238211Z Sep 25 03:41:07 at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)2022-09-25T03:41:07.4238775Z Sep 25 03:41:07 at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)2022-09-25T03:41:07.4239277Z Sep 25 03:41:07 at org.junit.rules.RunRules.evaluate(RunRules.java:20)2022-09-25T03:41:07.4239769Z Sep 25 03:41:07 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)2022-09-25T03:41:07.4240265Z Sep 25 03:41:07 at org.junit.runners.ParentRunner.run(ParentRunner.java:413)2022-09-25T03:41:07.4240731Z Sep 25 03:41:07 at org.junit.runner.JUnitCore.run(JUnitCore.java:137)2022-09-25T03:41:07.4241196Z Sep 25 03:41:07 at org.junit.runner.JUnitCore.run(JUnitCore.java:115)2022-09-25T03:41:07.4241715Z Sep 25 03:41:07 at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)2022-09-25T03:41:07.4242316Z Sep 25 03:41:07 at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)2022-09-25T03:41:07.4242904Z Sep 25 03:41:07 at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)2022-09-25T03:41:07.4243528Z Sep 25 03:41:07 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)2022-09-25T03:41:07.4244201Z Sep 25 03:41:07 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)2022-09-25T03:41:07.4244883Z Sep 25 03:41:07 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)2022-09-25T03:41:07.4245801Z Sep 25 03:41:07 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)2022-09-25T03:41:07.4246600Z Sep 25 03:41:07 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)2022-09-25T03:41:07.4247226Z Sep 25 03:41:07 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)2022-09-25T03:41:07.4247808Z Sep 25 03:41:07 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)2022-09-25T03:41:07.4248449Z Sep 25 03:41:07 at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)2022-09-25T03:41:07.4249567Z Sep 25 03:41:07 at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)2022-09-25T03:41:07.4250222Z Sep 25 03:41:07 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)2022-09-25T03:41:07.4250889Z Sep 25 03:41:07 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)2022-09-25T03:41:07.4251559Z Sep 25 03:41:07 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)2022-09-25T03:41:07.4252193Z Sep 25 03:41:07 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)2022-09-25T03:41:07.4252776Z Sep 25 03:41:07 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)2022-09-25T03:41:07.4253335Z Sep 25 03:41:07 at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)2022-09-25T03:41:07.4253884Z Sep 25 03:41:07 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41316&amp;view=logs&amp;j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&amp;t=5acec1b4-945b-59ca-34f8-168928ce5199</description>
      <version>1.16.0,1.17.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="29427" opendate="2022-9-27 00:00:00" fixdate="2022-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LookupJoinITCase failed with classloader problem</summary>
      <description>2022-09-27T02:49:20.9501313Z Sep 27 02:49:20 Caused by: org.codehaus.janino.InternalCompilerException: Compiling "KeyProjection$108341": Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.2022-09-27T02:49:20.9502654Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382)2022-09-27T02:49:20.9503366Z Sep 27 02:49:20 at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)2022-09-27T02:49:20.9504044Z Sep 27 02:49:20 at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)2022-09-27T02:49:20.9504704Z Sep 27 02:49:20 at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)2022-09-27T02:49:20.9505341Z Sep 27 02:49:20 at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)2022-09-27T02:49:20.9505965Z Sep 27 02:49:20 at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)2022-09-27T02:49:20.9506584Z Sep 27 02:49:20 at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)2022-09-27T02:49:20.9507261Z Sep 27 02:49:20 at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104)2022-09-27T02:49:20.9507883Z Sep 27 02:49:20 ... 30 more2022-09-27T02:49:20.9509266Z Sep 27 02:49:20 Caused by: java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.2022-09-27T02:49:20.9510835Z Sep 27 02:49:20 at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:184)2022-09-27T02:49:20.9511760Z Sep 27 02:49:20 at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.loadClass(FlinkUserCodeClassLoaders.java:192)2022-09-27T02:49:20.9512456Z Sep 27 02:49:20 at java.lang.Class.forName0(Native Method)2022-09-27T02:49:20.9513014Z Sep 27 02:49:20 at java.lang.Class.forName(Class.java:348)2022-09-27T02:49:20.9513649Z Sep 27 02:49:20 at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:89)2022-09-27T02:49:20.9514339Z Sep 27 02:49:20 at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:312)2022-09-27T02:49:20.9514990Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:8556)2022-09-27T02:49:20.9515659Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6749)2022-09-27T02:49:20.9516337Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594)2022-09-27T02:49:20.9516989Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573)2022-09-27T02:49:20.9517632Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215)2022-09-27T02:49:20.9518319Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481)2022-09-27T02:49:20.9519018Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476)2022-09-27T02:49:20.9519680Z Sep 27 02:49:20 at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928)2022-09-27T02:49:20.9520386Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476)2022-09-27T02:49:20.9521042Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469)2022-09-27T02:49:20.9521677Z Sep 27 02:49:20 at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927)2022-09-27T02:49:20.9522299Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)2022-09-27T02:49:20.9522929Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.hasAnnotation(UnitCompiler.java:1365)2022-09-27T02:49:20.9523658Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1349)2022-09-27T02:49:20.9524365Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)2022-09-27T02:49:20.9525030Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)2022-09-27T02:49:20.9525750Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)2022-09-27T02:49:20.9526383Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)2022-09-27T02:49:20.9527069Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)2022-09-27T02:49:20.9527832Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)2022-09-27T02:49:20.9528560Z Sep 27 02:49:20 at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)2022-09-27T02:49:20.9529217Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)2022-09-27T02:49:20.9529862Z Sep 27 02:49:20 at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)2022-09-27T02:49:20.9530427Z Sep 27 02:49:20 ... 37 more2022-09-27T02:49:20.9530852Z Sep 27 02:49:20 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41369&amp;view=logs&amp;j=0c940707-2659-5648-cbe6-a1ad63045f0a&amp;t=075c2716-8010-5565-fe08-3c4bb45824a4</description>
      <version>1.16.0,1.17.0</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.functions.table.fullcache.LookupFullCacheTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.functions.table.fullcache.inputformat.InputFormatCacheLoaderTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.table.lookup.fullcache.LookupFullCache.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.table.lookup.fullcache.CacheLoader.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.table.lookup.CachingLookupFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="29433" opendate="2022-9-27 00:00:00" fixdate="2022-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Auth through the builder pattern in Pulsar connector</summary>
      <description>Currently in order to use auth with the Flink Connector you needs to do so through the setConfig method.It would be nice if similar to the client API we can add methods inside the builder pattern.Example:builder.authentication(new AuthenticationToken(""))We can do something similar for the connector instead of having to do:PulsarSource.builder() .setConfig(PulsarOptions.PULSAR_AUTH_PLUGIN_CLASS_NAME, "org.apache.pulsar.client.impl.auth.oauth2.AuthenticationOAuth2") .setConfig(PulsarOptions.PULSAR_AUTH_PARAMS, "{"privateKey":"..."})</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSourceBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.PulsarSinkBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="29435" opendate="2022-9-28 00:00:00" fixdate="2022-10-28 01:00:00" resolution="Done">
    <buginformation>
      <summary>securityConfiguration supports dynamic configuration</summary>
      <description>when different tenants submit jobs using the same flink-conf.yaml, the same user is displayed on the Yarn page. SecurityConfiguration does not support dynamic configuration. Therefore, the user displayed on the Yarn page is the security.kerberos.login.principal in the flink-conf.yaml.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendDynamicPropertiesTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug id="29436" opendate="2022-9-28 00:00:00" fixdate="2022-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Spotless Maven Plugin to 2.27.1</summary>
      <description>This blocker is fixed by: https://github.com/diffplug/spotless/pull/1224 and https://github.com/diffplug/spotless/pull/1228.</description>
      <version>None</version>
      <fixedVersion>1.17.0,connector-parent-1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29455" opendate="2022-9-28 00:00:00" fixdate="2022-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add OperatorIdentifier</summary>
      <description>Add a class for identifying operators, that supports both uids and uidhashes, and integrate into the low-level APIs.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.WindowSavepointReader.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.SavepointWriter.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.SavepointReader.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.metadata.SavepointMetadataV2.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.EvictingWindowSavepointReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="29457" opendate="2022-9-28 00:00:00" fixdate="2022-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a uid(hash) remapping function</summary>
      <description>Expose functionality for modifying the uid&amp;#91;hash&amp;#93; of a state.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorState.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.SavepointWriter.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.OperatorIdentifier.java</file>
      <file type="M">docs.content.docs.libs.state.processor.api.md</file>
      <file type="M">docs.content.zh.docs.libs.state.processor.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="2946" opendate="2015-10-30 00:00:00" fixdate="2015-4-30 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add orderBy() to Table API</summary>
      <description>In order to implement a FLINK-2099 prototype that uses the Table APIs code generation facilities, the Table API needs a sorting feature.I would implement it the next days. Ideas how to implement such a sorting feature are very welcome. Is there any more efficient way instead of .sortPartition(...).setParallism(1)? Is it better to sort locally on the nodes first and finally sort on one node afterwards?</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.BatchScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
    </fixedFiles>
  </bug>
  <bug id="29486" opendate="2022-9-30 00:00:00" fixdate="2022-10-30 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce Client Parser to get statement type</summary>
      <description>In FLIP-24 and  FLIP-91, the SQL client was designed to has two mode: embedded and remote. Currently only embedded mode has been implemented, so we need to implement the remote mode.</description>
      <version>1.17.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlMultiLineParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParserImpl.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29511" opendate="2022-10-5 00:00:00" fixdate="2022-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sort properties/schemas in OpenAPI spec</summary>
      <description>The properties/schema order is currently based on whatever order they were looked up, which varies as the spec is being extended.Sort them by name to prevent this.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.OpenApiSpecGenerator.java</file>
      <file type="M">docs.static.generated.rest.v1.sql.gateway.yml</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
    </fixedFiles>
  </bug>
  <bug id="29513" opendate="2022-10-5 00:00:00" fixdate="2022-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Kafka version to 3.2.3</summary>
      <description>Kafka 3.2.3 contains certain security fixes (see https://downloads.apache.org/kafka/3.2.3/RELEASE_NOTES.html). We should upgrade the dependency in Flink</description>
      <version>None</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro-confluent-registry.pom.xml</file>
      <file type="M">flink-examples.flink-examples-build-helper.flink-examples-streaming-state-machine.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.confluent.schema.registry.sh</file>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29514" opendate="2022-10-5 00:00:00" fixdate="2022-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Minikdc to v3.2.4</summary>
      <description>Bump Minikdc to v3.2.4 to remove false positive scans on CVEs like CVE-2021-29425 and CVE-2020-15250</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-hbase.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29542" opendate="2022-10-8 00:00:00" fixdate="2022-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unload.md wrongly writes UNLOAD operation as LOAD operation</summary>
      <description>UNLOAD statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns ‘OK’ for a successful LOAD operation; otherwise it will throw an exception. which should be UNLOAD </description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0,1.17.1,1.16.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.unload.md</file>
    </fixedFiles>
  </bug>
  <bug id="29543" opendate="2022-10-8 00:00:00" fixdate="2022-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jar Run Rest Handler Support Flink Configuration</summary>
      <description>Flink JM Rest Api Support Flink Configuration field</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtilsTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBodyTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBody.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRequestBody.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanRequestBody.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler.java</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="2956" opendate="2015-11-3 00:00:00" fixdate="2015-11-3 01:00:00" resolution="Done">
    <buginformation>
      <summary>Migrate integration tests for Table API</summary>
      <description>Migrate integration tests of Table API from temp file to collect() as described in umbrella jira..</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.UnionITCase.scala</file>
      <file type="M">flink-staging.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.StringExpressionsITCase.scala</file>
      <file type="M">flink-staging.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.SelectITCase.scala</file>
      <file type="M">flink-staging.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.JoinITCase.scala</file>
      <file type="M">flink-staging.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.GroupedAggreagationsITCase.scala</file>
      <file type="M">flink-staging.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.FilterITCase.scala</file>
      <file type="M">flink-staging.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.ExpressionsITCase.scala</file>
      <file type="M">flink-staging.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.CastingITCase.scala</file>
      <file type="M">flink-staging.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.AsITCase.scala</file>
      <file type="M">flink-staging.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.AggregationsITCase.scala</file>
      <file type="M">flink-staging.flink-table.src.test.java.org.apache.flink.api.java.table.test.StringExpressionsITCase.java</file>
      <file type="M">flink-staging.flink-table.src.test.java.org.apache.flink.api.java.table.test.SelectITCase.java</file>
      <file type="M">flink-staging.flink-table.src.test.java.org.apache.flink.api.java.table.test.PojoGroupingITCase.java</file>
      <file type="M">flink-staging.flink-table.src.test.java.org.apache.flink.api.java.table.test.JoinITCase.java</file>
      <file type="M">flink-staging.flink-table.src.test.java.org.apache.flink.api.java.table.test.GroupedAggregationsITCase.java</file>
      <file type="M">flink-staging.flink-table.src.test.java.org.apache.flink.api.java.table.test.FilterITCase.java</file>
      <file type="M">flink-staging.flink-table.src.test.java.org.apache.flink.api.java.table.test.ExpressionsITCase.java</file>
      <file type="M">flink-staging.flink-table.src.test.java.org.apache.flink.api.java.table.test.CastingITCase.java</file>
      <file type="M">flink-staging.flink-table.src.test.java.org.apache.flink.api.java.table.test.AsITCase.java</file>
      <file type="M">flink-staging.flink-table.src.test.java.org.apache.flink.api.java.table.test.AggregationsITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="29562" opendate="2022-10-10 00:00:00" fixdate="2022-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JM/SQl gateway OpenAPI specs should have different titles</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.rest.OpenApiSpecGeneratorTest.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.SqlGatewayOpenApiSpecGenerator.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RuntimeOpenApiSpecGenerator.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.OpenApiSpecGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="29576" opendate="2022-10-11 00:00:00" fixdate="2022-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Serialized OperatorCoordinators are collected in a not thread-safe ArrayList even though serialization was parallelized in FLINK-26675</summary>
      <description>There's a build failure being caused by SourceNAryInputChainingITCase.testDirectSourcesOnlyExecution on master:Oct 11 01:45:36 [ERROR] Errors: Oct 11 01:45:36 [ERROR] SourceNAryInputChainingITCase.testDirectSourcesOnlyExecution:89 » Runtime Fail...Oct 11 01:45:36 [INFO] Oct 11 01:45:36 [ERROR] Tests run: 1931, Failures: 0, Errors: 1, Skipped: 4The actual cause might be a missing OperatorCoordinatorHolder in DefaultOperatorCoordinatorHandler (see attached Maven logs that were extracted from the linked build):01:44:28,248 [flink-akka.actor.default-dispatcher-5] WARN org.apache.flink.runtime.taskmanager.Task [] - MultipleInputOperator [Source: source-1, Source: source-2, Source: source-3] (1/4)#0 (9babb402557eb959216c28116aabddbe_1dd2eb40b0971d6d849b9e4a69494c88_0_0) switched from RUNNING to FAILED with failure cause: org.apache.flink.util.FlinkException: No coordinator registered for operator bc764cd8ddf7a0cff126f51c16239658 at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.deliverOperatorEventToCoordinator(DefaultOperatorCoordinatorHandler.java:117) at org.apache.flink.runtime.scheduler.SchedulerBase.deliverOperatorEventToCoordinator(SchedulerBase.java:1031) at org.apache.flink.runtime.jobmaster.JobMaster.sendOperatorEventToCoordinator(JobMaster.java:588) at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) at akka.actor.Actor.aroundReceive(Actor.scala:537) at akka.actor.Actor.aroundReceive$(Actor.scala:535) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) at akka.actor.ActorCell.invoke(ActorCell.scala:548) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) at akka.dispatch.Mailbox.run(Mailbox.scala:231) at akka.dispatch.Mailbox.exec(Mailbox.scala:243) at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)</description>
      <version>1.16.0,1.17.0</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="29580" opendate="2022-10-11 00:00:00" fixdate="2022-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>pulsar.consumer.autoUpdatePartitionsIntervalSeconds doesn&amp;#39;t work and should be removed</summary>
      <description></description>
      <version>1.17.0,1.15.2,1.16.1</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSourceOptions.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.config.PulsarSourceConfigUtils.java</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.consumer.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="29585" opendate="2022-10-11 00:00:00" fixdate="2022-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate TableSchema to Schema for Hive connector</summary>
      <description>`TableSchema` is deprecated, we should migrate it to Schema</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.utils.OperationConverterUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.catalog.CatalogTestUtil.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.catalog.CatalogTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.CatalogPropertiesUtil.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlChangeColumn.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlAddReplaceColumns.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogUdfITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogHiveMetadataTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogGenericMetadataTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogDataTypeTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.read.HivePartitionFetcherTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReaderITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveOutputFormatFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveLookupJoinITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDeserializeExceptionTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.ContinuousHiveSplitEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserDMLHelper.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserQB.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserBaseSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.write.HiveWriterFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HivePartitionFetcherContextBase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveCompactReaderFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSourceBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveLookupTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveDynamicTableFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="29624" opendate="2022-10-13 00:00:00" fixdate="2022-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade org.apache.commons:commons-lang3 from 3.3.2 to 3.12.0</summary>
      <description>Upgrade org.apache.commons:commons-lang3 from 3.3.2 to 3.12.0 to avoid being falsely flagged for CVEs CVE-2021-29425 and CVE-2020-15250</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.HiveDDLUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.jsonplan.JsonPlanGenerator.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plandump.PlanJSONDumpGenerator.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFileFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvCommons.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.3.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29628" opendate="2022-10-13 00:00:00" fixdate="2022-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump aws-java-sdk-s3 to 1.12.319</summary>
      <description>As reported by Dependabot in https://github.com/apache/flink/pull/20285</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2963" opendate="2015-11-4 00:00:00" fixdate="2015-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dependence on SerializationUtils#deserialize() should be avoided</summary>
      <description>There is a problem with `SerializationUtils` from Apache CommonsLang. Here is an open issue where the class will throw a`ClassNotFoundException` even if the class is in the classpath in amultiple-classloader environment:https://issues.apache.org/jira/browse/LANG-1049 state = (HashMap&lt;String, Serializable&gt;) SerializationUtils.deserialize(bais);./flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/windowing/NonKeyedWindowOperator.java state = (HashMap&lt;String, Serializable&gt;) SerializationUtils.deserialize(bais);./flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperator.java return SerializationUtils.deserialize(message);./flink-streaming-java/src/main/java/org/apache/flink/streaming/util/serialization/JavaDefaultStringSchema.java T copied = SerializationUtils.deserialize(SerializationUtils./flink-streaming-java/src/test/java/org/apache/flink/streaming/util/MockOutput.javaWe should move away from SerializationUtils.deserialize()</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.checkstyle.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29638" opendate="2022-10-14 00:00:00" fixdate="2022-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update jackson bom because of CVE-2022-42003</summary>
      <description>There is a CVE-2022-42003 fixed in 2.13.4.1 and 2.14.0-rc1https://nvd.nist.gov/vuln/detail/CVE-2022-42003P.S. It seems there will not be 2.14.0 release until end of October according to https://github.com/FasterXML/jackson-databind/issues/3590#issuecomment-1270363915</description>
      <version>1.16.0,1.17.0,1.15.2</version>
      <fixedVersion>1.16.0,1.17.0,1.15.3</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="29639" opendate="2022-10-14 00:00:00" fixdate="2022-11-14 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add ResourceId in TransportException for debugging</summary>
      <description>When the taskmanager is lost, only the host and port are shown in the exception. It is hard to find the exactly taskmanger by resourceId. Add ResourceId info will help a lot in debugging the job.</description>
      <version>None</version>
      <fixedVersion>1.17.0,1.16.1,1.15.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.NettyShuffleDescriptorBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannelBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyTestUtil.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClientTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.ClientTransportErrorHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.ShuffleDescriptorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.ResultPartitionDeploymentDescriptorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.NettyShuffleDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NetworkClientHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.ConnectionID.java</file>
    </fixedFiles>
  </bug>
  <bug id="29644" opendate="2022-10-14 00:00:00" fixdate="2022-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reference Kubernetes operator from Flink Kubernetes deploy docs</summary>
      <description>Currently the Flink deployment/resource provider docs provide some information for the Standalone and Native Kubernetes integration without any reference to the operator. We should provide a bit more visibility and value to the users by directly proposing to use the operator when considering Flink on Kubernetes. We should make the point that for most users the easiest way to use Flink on Kubernetes is probably through the operator (where they can now benefit from both standalone and native integration under the hood). This should help us avoid cases where a new user completely misses the existence of the operator when starting out based on the Flink docs.</description>
      <version>1.16.0,1.17.0,1.15.3</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.kubernetes.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug id="29664" opendate="2022-10-17 00:00:00" fixdate="2022-12-17 01:00:00" resolution="Done">
    <buginformation>
      <summary>Collect subpartition sizes of BLOCKING result partitions</summary>
      <description>In order to divide subpartition range according to the amount of data, the scheduler needs to collect the size of each subpartition produced by upstream tasks.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SchedulerTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptivebatch.SpeculativeSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismDeciderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.TaskIOMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraphDeploymentTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.SpeculativeScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismDecider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.BlockingResultInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskIOMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.IOMetrics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
    </fixedFiles>
  </bug>
  <bug id="29665" opendate="2022-10-17 00:00:00" fixdate="2022-12-17 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support flexible subpartition range divisions</summary>
      <description>Currently, the subpartition range division algorithm is fixed (data-independent), and the TaskDeploymentDescriptor(TDD) can be generated according to the number of subpartitions and downstream parallelism. In order to support dividing according to the amount of data in the subpartition range, the scheduler needs to support flexible subpartition range divisions. The scheduler will be responsible for deciding the subpartition range division infos for each job vertex, and then pass the division infos to job vertices when initializing them.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.InputGateFairnessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.SubpartitionIndexRange.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.InputGateDeploymentDescriptor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.StateTrackingMockExecutionGraph.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.ExecutingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismDeciderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.PointwisePatternTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.EdgeManagerBuildUtilTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraphConstructionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptorFactoryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SsgNetworkMemoryCalculationUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.PointwiseBlockingResultInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.BlockingResultInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.AllToAllBlockingResultInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.InternalExecutionGraphAccessor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.IntermediateResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.EdgeManagerBuildUtil.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="29666" opendate="2022-10-17 00:00:00" fixdate="2022-1-17 01:00:00" resolution="Done">
    <buginformation>
      <summary>Let adaptive batch scheduler divide subpartition range according to amount of data</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptivebatch.SpeculativeSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismDeciderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.VertexParallelismDecider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.SpeculativeScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismDecider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchSchedulerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.VertexInputInfoComputationUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismAndInputInfosDeciderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismAndInputInfosDecider.java</file>
    </fixedFiles>
  </bug>
  <bug id="29709" opendate="2022-10-20 00:00:00" fixdate="2022-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Pulsar to 2.10.2</summary>
      <description>Pulsar released a new version 2.10.2 which contains a lot of bugfix.</description>
      <version>1.17.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-pulsar.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.schema.PulsarSchemaUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.schema.PulsarSchema.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2971" opendate="2015-11-4 00:00:00" fixdate="2015-5-4 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add outer joins to the Table API</summary>
      <description>Since Flink now supports outer joins, the Table API can also support left, right and full outer joins.Given that null values are properly supported by RowSerializer etc.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.TypeConverter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.DataSetJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.logical.operators.scala</file>
      <file type="M">docs.apis.table.md</file>
    </fixedFiles>
  </bug>
  <bug id="29733" opendate="2022-10-24 00:00:00" fixdate="2022-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error Flink connector hive Test failing</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42328&amp;view=logs&amp;j=245e1f2e-ba5b-5570-d689-25ae21e5302f&amp;t=d04c9862-880c-52f5-574b-a7a79fef8e0fThis is caused by FLINK-29478reported by hxbks2ks </description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceStatisticsReportTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="29749" opendate="2022-10-25 00:00:00" fixdate="2022-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink info command support dynamic properties</summary>
      <description></description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0,1.15.3,1.16.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendInfoTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug id="29767" opendate="2022-10-26 00:00:00" fixdate="2022-12-26 01:00:00" resolution="Done">
    <buginformation>
      <summary>VertexWiseSchedulingStrategy supports hybrid shuffle</summary>
      <description>`AdaptiveBatchScheduler` using `VertexWiseSchedulingStrategy` to schedule tasks, but it only supports blocking edge,  so we should make it also support hybrid shuffle edge.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.VertexwiseSchedulingStrategyTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.VertexwiseSchedulingStrategy.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulingTopology.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulingResultPartition.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adapter.DefaultResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.ConsumerVertexGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.EdgeManagerBuildUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="29768" opendate="2022-10-26 00:00:00" fixdate="2022-1-26 01:00:00" resolution="Done">
    <buginformation>
      <summary>Hybrid shuffle supports consume partial finished subtask in speculative execution mode</summary>
      <description>At present, downstream task can be scheduled only if all upstream tasks finished in speculative execution mode. We can do some improvements to support schedule downstream tasks when the upstream's some subtask is not completely finished. After the upstream task's data is totally produced, we need to update the result partition's information of this subtask to trigger downstream task's consumption.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.UnknownInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionType.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.ExecutingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.ShuffleDescriptorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.SpeculativeScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchSchedulerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.SpeculativeExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.InternalExecutionGraphAccessor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.CachedShuffleDescriptors.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.job.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.scheduling.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.jobmanager.section.html</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkBenchmarkEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorSubmissionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.shuffle.NettyShuffleUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NettyShuffleEnvironmentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.TestingDefaultExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.RemoveCachedShuffleDescriptorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptorFactoryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.IntermediateResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.IntermediateResult.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptorFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.InputGateDeploymentDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="2977" opendate="2015-11-5 00:00:00" fixdate="2015-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot access HBase in a Kerberos secured Yarn cluster</summary>
      <description>I have created a very simple Flink topology consisting of a streaming Source (the outputs the timestamp a few times per second) and a Sink (that puts that timestamp into a single record in HBase).Running this on a non-secure Yarn cluster works fine.To run it on a secured Yarn cluster my main routine now looks like this:public static void main(String[] args) throws Exception { System.setProperty("java.security.krb5.conf", "/etc/krb5.conf"); UserGroupInformation.loginUserFromKeytab("nbasjes@xxxxxx.NET", "/home/nbasjes/.krb/nbasjes.keytab"); final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); DataStream&lt;String&gt; stream = env.addSource(new TimerTicksSource()); stream.addSink(new SetHBaseRowSink()); env.execute("Long running Flink application");}When I run this flink run -m yarn-cluster -yn 1 -yjm 1024 -ytm 4096 ./kerberos-1.0-SNAPSHOT.jarI see after the startup messages:17:13:24,466 INFO org.apache.hadoop.security.UserGroupInformation - Login successful for user nbasjes@xxxxxx.NET using keytab file /home/nbasjes/.krb/nbasjes.keytab11/03/2015 17:13:25 Job execution switched to status RUNNING.11/03/2015 17:13:25 Custom Source -&gt; Stream Sink(1/1) switched to SCHEDULED 11/03/2015 17:13:25 Custom Source -&gt; Stream Sink(1/1) switched to DEPLOYING 11/03/2015 17:13:25 Custom Source -&gt; Stream Sink(1/1) switched to RUNNING Which looks good.However ... no data goes into HBase.After some digging I found this error in the task managers log:17:13:42,677 WARN org.apache.hadoop.hbase.ipc.RpcClient - Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed &amp;#91;Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)&amp;#93;17:13:42,677 FATAL org.apache.hadoop.hbase.ipc.RpcClient - SASL authentication failed. The most likely cause is missing or invalid credentials. Consider 'kinit'.javax.security.sasl.SaslException: GSS initiate failed &amp;#91;Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)&amp;#93; at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212) at org.apache.hadoop.hbase.security.HBaseSaslRpcClient.saslConnect(HBaseSaslRpcClient.java:177) at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupSaslConnection(RpcClient.java:815) at org.apache.hadoop.hbase.ipc.RpcClient$Connection.access$800(RpcClient.java:349)First starting a yarn-session and then loading my job gives the same error.My best guess at this point is that Flink needs the same fix as described here:https://issues.apache.org/jira/browse/SPARK-6918 ( https://github.com/apache/spark/pull/5586 )</description>
      <version>None</version>
      <fixedVersion>0.10.1,1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
    </fixedFiles>
  </bug>
  <bug id="29787" opendate="2022-10-28 00:00:00" fixdate="2022-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix ci METHOD_NEW_DEFAULT issue</summary>
      <description>`org.apache.flink.api.connector.source.SourceReader` declared a new default function `pauseOrResumeSplits()`, japicmp plugin failed during ci running, we should configure the plugin to make it compatible.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.update.japicmp.configuration.sh</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29812" opendate="2022-10-31 00:00:00" fixdate="2022-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove deprecated Netty API usages</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyConnectionManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpointConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.KeepAliveWrite.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerRedirectUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.router.RouterHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.FileUploadHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyServer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyClient.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.testutils.HttpTestClient.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.utils.WebFrontendBootstrap.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.PipelineErrorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.HttpRequestHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandler.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KvStateServerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.Client.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.AbstractServerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="29859" opendate="2022-11-3 00:00:00" fixdate="2022-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TPC-DS end-to-end test with adaptive batch scheduler failed due to oo non-empty .out files.</summary>
      <description>Nov 03 02:02:12 &amp;#91;FAIL&amp;#93; 'TPC-DS end-to-end test with adaptive batch scheduler' failed after 21 minutes and 44 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out files https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42766&amp;view=logs&amp;s=ae4f8708-9994-57d3-c2d7-b892156e7812&amp;j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a</description>
      <version>1.16.0,1.17.0</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="29868" opendate="2022-11-3 00:00:00" fixdate="2022-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dependency convergence error for org.osgi:org.osgi.core:jar</summary>
      <description>While working on FLINK-29867, the following new error is popping up while running./mvnw clean install -pl flink-dist -am -DskipTests -Dflink.convergence.phase=install -Pcheck-convergence(this is also done by CI which therefore fails)[WARNING] Dependency convergence error for org.osgi:org.osgi.core:jar:4.3.0:runtime paths to dependency are:+-org.apache.flink:flink-table-planner-loader-bundle:jar:1.17-SNAPSHOT +-org.apache.flink:flink-table-planner_2.12:jar:1.17-SNAPSHOT:runtime +-org.apache.flink:flink-table-api-java-bridge:jar:1.17-SNAPSHOT:runtime +-org.apache.flink:flink-streaming-java:jar:1.17-SNAPSHOT:runtime +-org.apache.flink:flink-runtime:jar:1.17-SNAPSHOT:runtime +-org.xerial.snappy:snappy-java:jar:1.1.8.3:runtime +-org.osgi:org.osgi.core:jar:4.3.0:runtimeand+-org.apache.flink:flink-table-planner-loader-bundle:jar:1.17-SNAPSHOT +-org.apache.flink:flink-table-planner_2.12:jar:1.17-SNAPSHOT:runtime +-org.apache.flink:flink-scala_2.12:jar:1.17-SNAPSHOT:runtime +-org.apache.flink:flink-core:jar:1.17-SNAPSHOT:runtime +-org.apache.commons:commons-compress:jar:1.21:runtime +-org.osgi:org.osgi.core:jar:6.0.0:runtime</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29913" opendate="2022-11-7 00:00:00" fixdate="2022-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shared state would be discarded by mistake when maxConcurrentCheckpoint&gt;1</summary>
      <description>When maxConcurrentCheckpoint&gt;1, the shared state of Incremental rocksdb state backend would be discarded by registering the same name handle. See https://github.com/apache/flink/pull/21050#discussion_r1011061072cc roman </description>
      <version>1.15.0,1.16.0,1.17.0</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.SharedStateRegistry.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.SharedStateRegistryImpl.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StateHandleReuseITCase.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategyTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateUploaderTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateDownloaderTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksSnapshotUtil.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksNativeFullSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksDBSnapshotStrategyBase.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateUploader.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateDownloader.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBRestoreResult.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.SharedStateRegistryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.IncrementalRemoteKeyedStateHandleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SchedulerUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.metadata.CheckpointTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.PlaceholderStreamStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.IncrementalRemoteKeyedStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.IncrementalLocalKeyedStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.IncrementalKeyedStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.ChangelogStateBackendHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="29969" opendate="2022-11-10 00:00:00" fixdate="2022-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show the root cause when exceeded checkpoint tolerable failure threshold</summary>
      <description>Add the root cause when exceeded checkpoint tolerable failure threshold, it's helpful during troubleshooting.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.CheckpointFailureManagerITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="2997" opendate="2015-11-11 00:00:00" fixdate="2015-3-11 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support range partition with user customized data distribution.</summary>
      <description>This is a followup work of FLINK-7, sometime user have better knowledge of the source data, and they can build customized data distribution to do range partition more efficiently.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.utils.package.scala</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.dataproperties.MockDistribution.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.traversals.RangePartitionRewriter.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plan.Channel.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dataproperties.GlobalProperties.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dag.PartitionNode.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.utils.DataSetUtils.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.PartitionOperator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.Keys.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.PartitionOperatorBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.distributions.DataDistribution.java</file>
    </fixedFiles>
  </bug>
  <bug id="29975" opendate="2022-11-10 00:00:00" fixdate="2022-11-10 01:00:00" resolution="Done">
    <buginformation>
      <summary>Let hybrid full spilling strategy supports partition reuse</summary>
      <description>Partition reuse is a very useful optimization in some topologies. In essence, multiple downstream tasks consume the same subpartition's data. Therefore, hybrid shuffle should also enjoy the benefits it brings. After FLINK-28889, we are finally able to achieve repeated consumption at the subpartition level for hybrid full spilling strategy, so let's make it better.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.StreamExchangeMode.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionType.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy.java</file>
    </fixedFiles>
  </bug>
  <bug id="2998" opendate="2015-11-11 00:00:00" fixdate="2015-4-11 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support range partition comparison for multi input nodes.</summary>
      <description>The optimizer may have potential opportunity to optimize the DAG while it found two input range partition are equivalent, we does not support the comparison yet.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.JoinITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.CustomDistributionITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.CoGroupITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.shipping.OutputEmitter.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.operators.JoinGlobalPropertiesCompatibilityTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.operators.CoGroupGlobalPropertiesCompatibilityTest.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.operators.OperatorDescriptorDual.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.operators.CoGroupDescriptor.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.operators.AbstractJoinDescriptor.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dataproperties.GlobalProperties.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.PartitionOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="29992" opendate="2022-11-11 00:00:00" fixdate="2022-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive lookupJoin execution plan parsing error</summary>
      <description>//tableEnv.executeSql(" CREATE CATALOG hive WITH (\n" + " 'type' = 'hive',\n" + " 'default-database' = 'flinkdebug',\n" + " 'hive-conf-dir' = '/programe/hadoop/hive-3.1.2/conf'\n" + " )");tableEnv.executeSql("create table datagen_tbl (\n" + "id STRING\n" + ",name STRING\n" + ",age bigint\n" + ",ts bigint\n" + ",`par` STRING\n" + ",pro_time as PROCTIME()\n" + ") with (\n" + " 'connector'='datagen'\n" + ",'rows-per-second'='10'\n" + " \n" + ")");String dml1 = "select * " + " from datagen_tbl as p " + " join hive.flinkdebug.default_hive_src_tbl " + " FOR SYSTEM_TIME AS OF p.pro_time AS c" + " ON p.id = c.id";// Execution succeeded  System.out.println(tableEnv.explainSql(dml1));String dml2 = "select p.id " + " from datagen_tbl as p " + " join hive.flinkdebug.default_hive_src_tbl " + " FOR SYSTEM_TIME AS OF p.pro_time AS c" + " ON p.id = c.id";// Throw an exception System.out.println(tableEnv.explainSql(dml2)); org.apache.flink.table.api.TableException: Cannot generate a valid execution plan for the given query: FlinkLogicalCalc(select=[id]) +- FlinkLogicalJoin(condition=[=($0, $1)], joinType=[inner])    :- FlinkLogicalCalc(select=[id])    :  +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, datagen_tbl]], fields=[id, name, age, ts, par])    +- FlinkLogicalSnapshot(period=[$cor1.pro_time])       +- FlinkLogicalTableSourceScan(table=[[hive, flinkdebug, default_hive_src_tbl, project=[id]]], fields=[id])This exception indicates that the query uses an unsupported SQL feature. Please check the documentation for the set of currently supported SQL features.    at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:70)     at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)</description>
      <version>1.16.0,1.17.0,1.14.7,1.15.4</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveLookupJoinITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveLookupTableSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="30010" opendate="2022-11-14 00:00:00" fixdate="2022-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-quickstart-test failed due to could not resolve dependencies</summary>
      <description>Nov 13 02:10:37 [ERROR] Failed to execute goal on project flink-quickstart-test: Could not resolve dependencies for project org.apache.flink:flink-quickstart-test:jar:1.17-SNAPSHOT: Could not find artifact org.apache.flink:flink-quickstart-scala:jar:1.17-SNAPSHOT in apache.snapshots (https://repository.apache.org/snapshots) -&gt; [Help 1]Nov 13 02:10:37 [ERROR] Nov 13 02:10:37 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.Nov 13 02:10:37 [ERROR] Re-run Maven using the -X switch to enable full debug logging.Nov 13 02:10:37 [ERROR] Nov 13 02:10:37 [ERROR] For more information about the errors and possible solutions, please read the following articles:Nov 13 02:10:37 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionExceptionNov 13 02:10:37 [ERROR] Nov 13 02:10:37 [ERROR] After correcting the problems, you can resume the build with the commandNov 13 02:10:37 [ERROR] mvn &lt;goals&gt; -rf :flink-quickstart-testNov 13 02:10:38 Process exited with EXIT CODE: 1.Nov 13 02:10:38 Trying to KILL watchdog (293)./__w/1/s/tools/ci/watchdog.sh: line 100: 293 Terminated watchdogNov 13 02:10:38 ==============================================================================Nov 13 02:10:38 Compilation failure detected, skipping test execution.Nov 13 02:10:38 ==============================================================================https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43102&amp;view=logs&amp;j=298e20ef-7951-5965-0e79-ea664ddc435e&amp;t=d4c90338-c843-57b0-3232-10ae74f00347&amp;l=18363</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-quickstart-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="30056" opendate="2022-11-17 00:00:00" fixdate="2022-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make polling for metadata no more than specified timeout by using new Consumer#poll(Duration)</summary>
      <description>New Consumer#poll poll for metadata responses (counts against timeout) if no response within timeout, return an empty collection immediatelyAlso more details are at https://cwiki.apache.org/confluence/display/KAFKA/KIP-266%3A+Fix+consumer+indefinite+blocking+behavior#KIP266:Fixconsumerindefiniteblockingbehavior-Consumer#pollThe behavior was changed at https://issues.apache.org/jira/browse/KAFKA-5697</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaInternalProducerITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaConsumerThread.java</file>
    </fixedFiles>
  </bug>
  <bug id="30083" opendate="2022-11-18 00:00:00" fixdate="2022-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump maven-shade-plugin to 3.4.1</summary>
      <description>FLINK-24273 proposes to relocate the io.fabric8 dependencies of flink-kubernetes.This is not possible because of a problem with the maven shade plugin ("mvn install" doesn't work, it needs to be "mvn clean install").MSHADE-425 solves this issue, and has been released with maven-shade-plugin 3.4.0.Upgrading the shade plugin will solve the problem, unblocking the K8s relocation.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="30085" opendate="2022-11-18 00:00:00" fixdate="2022-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JVM hanging after executing YARNSessionCapacitySchedulerITCase.testNonexistingQueueWARNmessage</summary>
      <description>After some analysis I've concluded the following: the test submitted an app into a wrong queue so fails. At submit time YARN added a shutdown hook named DeploymentFailureHook. The mentioned hook is executed after the YARN cluster is down and tries to kill the application. The kill call is hanging for 15 minutes blocking the JVM to shut down.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="30089" opendate="2022-11-18 00:00:00" fixdate="2022-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove dependency promotion from kinesis connector</summary>
      <description>The shade-plugin in the kinesis connector is configured to promote transitive dependencies.This adds a special case in our shading setup, breaks the dependency tree structure (since all transitive dependencies are moved to the top) and it makes the sql-kinesis packaging overly complicated.Get rid of the dependency promotion and explicitly depend on anything that we pull in transitively but don't shade in the kinesis connector itself.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="30134" opendate="2022-11-22 00:00:00" fixdate="2022-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Annotate all the public classes for the delegation token framework</summary>
      <description></description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.NoOpDelegationTokenManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.KerberosLoginProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.KerberosDelegationTokenManagerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.KerberosDelegationTokenManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.HBaseDelegationTokenProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.DelegationTokenUpdater.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.DelegationTokenManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.DelegationTokenListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.DelegationTokenConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="30188" opendate="2022-11-24 00:00:00" fixdate="2022-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Partition finished state in ConsumedPartitionGroup is not updated correctly in dynamic graph</summary>
      <description>For dynamic graph, the downstream task's parallelism is determined only after the upstream task is finished, and then the connection relationship between the downstream and upstream `ExecutionVertex` will be established (including `ConsumedPartitionGroup`). Therefore, the upstream task did not correctly notify `ConsumedPartitionGroup` to update the partition finished status after it is finished.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulingTopology.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulingResultPartition.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulingExecutionVertex.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.DefaultInputConsumableDeciderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopologyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.IntermediateResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.ResultPartitionState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.DefaultInputConsumableDecider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.IntermediateResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptorFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraphConstructionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.EdgeManagerBuildUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="30189" opendate="2022-11-24 00:00:00" fixdate="2022-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HsSubpartitionFileReader may load data that has been consumed from memory</summary>
      <description>In order to solve the problem that data cannot be read from the disk correctly after failover, we changed the calculation logical of the buffer's readable state in FLINK-29238.  Buffers that are greater than consumingOffset and have been released can be pre-load from file. However, the update of consumingOffset is asynchronous, If it lags behind the actual consumption progress, the buffer will have a chance to be load from the disk again. IMO, we can record the consumed status of buffer by each consumer in the InternalRegion. Only the buffers that have not been consumed and have been released will be considered as readable. In the case of failover, a new consumerId will be generated, so all buffers will be considered as unconsumed and can be correctly read from the disk too.</description>
      <version>1.16.0,1.17.0</version>
      <fixedVersion>1.17.0,1.16.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionFileReaderImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSubpartitionFileReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="30197" opendate="2022-11-24 00:00:00" fixdate="2022-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update slf4j from 1.7.32 to 1.7.36</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">docs.content.docs.dev.configuration.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.configuration.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="30213" opendate="2022-11-25 00:00:00" fixdate="2022-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The edge is wrong when the vertex parallelism is changed</summary>
      <description>After FLINK-29501, flink allows overriding JobVertex parallelisms during job submission.However, the edge should be changed as well. For example, the job has 4 vertex, and all shipStrategyName of all tasks are forward.After the parallelism of the third task is changed to 1, the second and third edge should be changed from forward to rebalance. But they are still forward. And from the second picture, the subtask_1 of sink cannot receive any data. </description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamMockEnvironment.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskMailboxTestHarnessBuilder.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskFinalCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamConfigChainer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.NonChainedOutput.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="30214" opendate="2022-11-25 00:00:00" fixdate="2022-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The resource is not enough</summary>
      <description>When turn up the parallelism, the resources isn't enough for MiniCluster. I'm not sure whether the flink on yarn or k8s is right? I can test later. I guess the parallelism should be changed in a right place. And we should add more unit test or ITCase to check these cases. And it's my honor to make these improvements and bug fixes. </description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.PerJobMiniClusterFactoryTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PerJobMiniClusterFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="3023" opendate="2015-11-17 00:00:00" fixdate="2015-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show Flink version + commit id for -SNAPSHOT versions in web frontend</summary>
      <description>The old frontend was showing the Flink version and the commit id for SNAPSHOT builds.This is a helpful feature to quickly see which Flink version is running.It would be nice to add this again to the web interface.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.EnvironmentInformation.java</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.overview.html</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.overview.jade</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.ClusterOverviewHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="30237" opendate="2022-11-29 00:00:00" fixdate="2022-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Only bundle a single Zookeeper version</summary>
      <description>Way back when we added support for ZK 3.5 we started bundling 2 zookeeper clients, because of incompatibilities between 3.4 and 3.5. This is no longer required, and we could simplify things again.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.ha.per.job.cluster.datastream.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.ha.datastream.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">docs.content.docs.deployment.ha.zookeeper.ha.md</file>
      <file type="M">docs.content.zh.docs.deployment.ha.zookeeper.ha.md</file>
    </fixedFiles>
  </bug>
  <bug id="30239" opendate="2022-11-29 00:00:00" fixdate="2022-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The flame graph doesn&amp;#39;t work due to groupExecutionsByLocation has bug</summary>
      <description>The flame graph cannot be generated forever when multiple tasks in the same TM. It's caused by FLINK-26074 Root cause:A Set cannot be converted to an ImmutableSet during the aggregation of ExecutionAttemptIDs. It will cause only the first ExecutionAttemptID of the TM to be added to the set, the second ExecutionAttemptID will fail.     Exception Info: </description>
      <version>1.16.0,1.17.0</version>
      <fixedVersion>1.17.0,1.16.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTrackerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTracker.java</file>
    </fixedFiles>
  </bug>
  <bug id="30250" opendate="2022-11-30 00:00:00" fixdate="2022-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The flame graph type is wrong</summary>
      <description>When the flame graph type is switched from On-CPU to Mixed. It still show the graph of On-CPU.Root cause:When click the other types, the web frontend will call the requestFlameGraph and update the graphType. However, the graphType is the old type during requestFlameGraph. So the graph type show the new type, but the flame graph is the result of old type. code link</description>
      <version>1.15.0,1.16.0,1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.flamegraph.job-overview-drawer-flamegraph.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.flamegraph.job-overview-drawer-flamegraph.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-flamegraph.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.components.flame-graph.flame-graph.component.ts</file>
    </fixedFiles>
  </bug>
  <bug id="30289" opendate="2022-12-4 00:00:00" fixdate="2022-1-4 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>RateLimitedSourceReader uses wrong signal for checkpoint rate-limiting</summary>
      <description>The checkpoint rate limiter is notified when the checkpoint is complete, but since this signal comes at some point in the future (or not at all) it can result in no records being emitted for a checkpoint, or more records than expected being emitted.</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="30355" opendate="2022-12-9 00:00:00" fixdate="2022-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>crictl causes long wait in e2e tests</summary>
      <description>We observed strange behavior in the e2e test where the e2e test run times out: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43824&amp;view=logs&amp;j=bea52777-eaf8-5663-8482-18fbc3630e81&amp;s=ae4f8708-9994-57d3-c2d7-b892156e7812&amp;t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&amp;l=7446The issue seems to be related to crictl again because we see the following error message in multiple tests. No logs are produced afterwards for ~30mins resulting in the overall test run taking too long:Dec 09 08:55:39 crictlfatal: destination path 'cri-dockerd' already exists and is not an empty directory.fatal: a branch named 'v0.2.3' already existsmkdir: cannot create directory ‘bin’: File existsDec 09 09:26:41 fs.protected_regular = 0</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.kubernetes.sh</file>
    </fixedFiles>
  </bug>
  <bug id="30365" opendate="2022-12-12 00:00:00" fixdate="2022-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>New dynamic partition pruning strategy to support more dpp patterns</summary>
      <description>New dynamic partition pruning strategy to support more dpp patterns. Now, dpp rules is coupled with the join reorder rules, which will affect the result of join reorder. At the same time, the dpp rule don't support these patterns like union node in fact side.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.DynamicPartitionPruningRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.physical.batch.DynamicPartitionPruningRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkBatchProgram.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.physical.batch.DynamicPartitionPruningRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="30368" opendate="2022-12-12 00:00:00" fixdate="2022-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix calcite method RelMdUtil$numDistinctVals() wrongly return zero if the method input domainSize much larger than numSelected</summary>
      <description>Fix calcite method RelMdUtil$numDistinctVals() wrongly return zero if the method input domainSize much larger than numSelected. This wrong zero value will affect the selection of join type。 </description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRelMdUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRelMdUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdDistinctRowCount.scala</file>
    </fixedFiles>
  </bug>
  <bug id="30376" opendate="2022-12-12 00:00:00" fixdate="2022-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce a new flink bushy join reorder rule which based on greedy algorithm</summary>
      <description>Introducing a new Flink bushy join reorder strategy which based on the greedy algorithm. The old join reorder rule will also be the default join reorder rule and the new bushy join reorder strategy will be optional.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.JoinReorderTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.RewriteMultiJoinConditionRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.common.JoinReorderTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.JoinReorderTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.JoinReorderTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RewriteMultiJoinConditionRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.optimize.program.DynamicPartitionPruningProgramTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.JoinReorderTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.optimize.program.DynamicPartitionPruningProgramTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.utils.JavaScalaConversionUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.RewriteMultiJoinConditionRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.OptimizerConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.optimizer.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="30409" opendate="2022-12-14 00:00:00" fixdate="2022-12-14 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support reopening closed metric groups</summary>
      <description>Currently, metricGroup.close() will unregister metrics and the underlying metric groups. If the metricGroup is created again via addGroup(), it will silently fail to create metrics since the metric group is in a closed state.We need to close metric groups and reopen them because some of the metrics may reference old objects that are no longer relevant/stale and we need to re-create the metric/metric group to point to the new references. For example, we may close `KafkaSourceReader` to remove a topic partition from assignment and then recreate `KafkaSourceReader` with a different set of topic partitions. The metrics should also reflect that.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.MetricGroupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.AbstractMetricGroup.java</file>
    </fixedFiles>
  </bug>
  <bug id="30416" opendate="2022-12-14 00:00:00" fixdate="2022-1-14 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add configureSession REST API in the SQL Gateway</summary>
      <description></description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.UtilCaseITTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.TestingSqlGatewayRestEndpoint.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.SqlGatewayRestEndpointStatementITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.SqlGatewayRestEndpointITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.SessionCaseITTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.RestAPIITTestBase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.OperationCaseITTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.util.SqlGatewayRestAPIVersion.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.SqlGatewayRestEndpoint.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.header.SqlGatewayMessageHeaders.java</file>
    </fixedFiles>
  </bug>
  <bug id="3045" opendate="2015-11-19 00:00:00" fixdate="2015-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Properly expose the key of a kafka message</summary>
      <description>Currently, the flink-kafka-connector is not properly exposing the message key.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.util.serialization.TypeInformationSerializationSchema.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaITCase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.LegacyFetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.Fetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer082.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer.java</file>
    </fixedFiles>
  </bug>
  <bug id="30461" opendate="2022-12-20 00:00:00" fixdate="2022-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some rocksdb sst files will remain forever</summary>
      <description>In rocksdb incremental checkpoint mode, during file upload, if some files have been uploaded and some files have not been uploaded, the checkpoint is canceled due to checkpoint timeout at this time, and the uploaded files will remain. Impact: The shared directory of a flink job has more than 1 million files. It exceeded the hdfs upper limit, causing new files not to be written.However only 50k files are available, the other 950k files should be cleaned up.Root cause:If an exception is thrown during the checkpoint async phase, flink will clean up metaStateHandle, miscFiles and sstFiles.However, when all sst files are uploaded, they are added together to sstFiles. If some sst files have been uploaded and some sst files are still being uploaded, and  the checkpoint is canceled due to checkpoint timeout at this time, all sst files will not be added to sstFiles. The uploaded sst will remain on hdfs.code linkSolution:Using the CloseableRegistry as the tmpResourcesRegistry. If the async phase is failed, the tmpResourcesRegistry will cleanup these temporary resources. POC code:https://github.com/1996fanrui/flink/commit/86a456b2bbdad6c032bf8e0bff71c4824abb3ce1   </description>
      <version>1.16.0,1.17.0,1.15.3</version>
      <fixedVersion>1.17.0,1.15.4,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateUploaderTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksNativeFullSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksDBSnapshotStrategyBase.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateUploader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="30468" opendate="2022-12-21 00:00:00" fixdate="2022-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The SortOrder of BusyRatio should be descend by default</summary>
      <description>Currently, the sort order is ascend by default, it should be descend.The most busy subtask should be displayed on top.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="30472" opendate="2022-12-21 00:00:00" fixdate="2022-1-21 01:00:00" resolution="Done">
    <buginformation>
      <summary>Modify the default value of the max network memory config option</summary>
      <description>This issue mainly focuses on the second issue in FLIP-266, modifying the default value of taskmanager.memory.network.max</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.task.manager.memory.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.common.memory.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="30473" opendate="2022-12-21 00:00:00" fixdate="2022-1-21 01:00:00" resolution="Done">
    <buginformation>
      <summary>Optimize the InputGate network memory management for TaskManager</summary>
      <description>Based on the FLIP-266, this issue mainly focuses on the first issue.This change proposes a method to control the maximum required memory buffers in an inputGate according to parallelism size.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.benchmark.SingleInputGateBenchmarkFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.shuffle.NettyShuffleUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SsgNetworkMemoryCalculationUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NettyShuffleEnvironmentBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.NettyShuffleEnvironmentConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.TaskInputsOutputsDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.NettyShuffleUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.NettyShuffleMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SsgNetworkMemoryCalculationUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.NettyShuffleEnvironmentOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.netty.shuffle.environment.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.taskmanager.network.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="30542" opendate="2023-1-3 00:00:00" fixdate="2023-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support adaptive local hash aggregate in runtime</summary>
      <description>Introduce a new strategy to adaptively determine whether local hash aggregate is required according to the aggregation degree of local hash aggregate.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.HashAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.EnforceLocalAggRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalSortAggRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalJoinRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalHashAggRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalAggRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalLocalHashAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalHashAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ProjectionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecHashAggregate.java</file>
    </fixedFiles>
  </bug>
  <bug id="30544" opendate="2023-1-3 00:00:00" fixdate="2023-1-3 01:00:00" resolution="Done">
    <buginformation>
      <summary>Speed up finding minimum watermark across all channels by introducing heap-based algorithm</summary>
      <description>Currently, every time a task receives a watermark, it tries to update the minimum watermark.Currently, we use the traversal algorithm to find the minimum watermark across all channels(see StatusWatermarkValue#findAndOutputNewMinWatermarkAcrossAlignedChannels for details), and the time complexity is O(N), where N is the number of channels.We can optimize it by introducing a heap-based algorthim, reducing the time complexity to O(log(N))</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.watermarkstatus.StatusWatermarkValve.java</file>
    </fixedFiles>
  </bug>
  <bug id="3056" opendate="2015-11-21 00:00:00" fixdate="2015-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show bytes sent/received as MBs/GB and so on in web interface</summary>
      <description>It would be great if the web interface would round show the bytes in an appropriate (=human readable) unit.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.taskmanager.taskmanager.metrics.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.taskmanager.index.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.plan.node.subtasks.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.plan.node-list.overview.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.common.filters.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanager.taskmanager.metrics.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanager.index.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.node.subtasks.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.node-list.overview.jade</file>
    </fixedFiles>
  </bug>
  <bug id="30578" opendate="2023-1-6 00:00:00" fixdate="2023-1-6 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Publish SBOM artifacts</summary>
      <description></description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="30585" opendate="2023-1-6 00:00:00" fixdate="2023-6-6 01:00:00" resolution="Done">
    <buginformation>
      <summary>Improve flame graph performance at subtask level</summary>
      <description>After FLINK-30185 , we can view the flame graph of subtask level. However, it always collects flame graphs for all subtasks.We should collect the flame graph of single subtask instead of all subtasks.</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.stats.JobVertexStatsTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTrackerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTrackerBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTracker.java</file>
    </fixedFiles>
  </bug>
  <bug id="30618" opendate="2023-1-10 00:00:00" fixdate="2023-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-connector-pulsar not retrievable from Apache&amp;#39;s Snapshot Maven repository</summary>
      <description>The build failure was caused by flink-connector-pulsar not being retrievable from the Apache Snapshot Maven repository:https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44633&amp;view=logs&amp;j=de826397-1924-5900-0034-51895f69d4b7&amp;t=f311e913-93a2-5a37-acab-4a63e1328f94&amp;l=10132Jan 10 02:03:24 [WARNING] The requested profile "skip-webui-build" could not be activated because it does not exist.Jan 10 02:03:24 [ERROR] Failed to execute goal on project flink-python: Could not resolve dependencies for project org.apache.flink:flink-python:jar:1.17-SNAPSHOT: Could not find artifact org.apache.flink:flink-sql-connector-pulsar:jar:4.0-SNAPSHOT in apache.snapshots (https://repository.apache.org/snapshots) -&gt; [Help 1]Jan 10 02:03:24 [ERROR] Jan 10 02:03:24 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.Jan 10 02:03:24 [ERROR] Re-run Maven using the -X switch to enable full debug logging.Jan 10 02:03:24 [ERROR] Jan 10 02:03:24 [ERROR] For more information about the errors and possible solutions, please read the following articles:Jan 10 02:03:24 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionExceptionJan 10 02:03:24 [ERROR] Jan 10 02:03:24 [ERROR] After correcting the problems, you can resume the build with the commandJan 10 02:03:24 [ERROR] mvn &lt;goals&gt; -rf :flink-python</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="30640" opendate="2023-1-12 00:00:00" fixdate="2023-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unstable test in CliClientITCase</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44743&amp;view=logs&amp;j=0c940707-2659-5648-cbe6-a1ad63045f0a&amp;t=075c2716-8010-5565-fe08-3c4bb45824a4 The failed test can work normally in my local environment.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.set.q</file>
    </fixedFiles>
  </bug>
  <bug id="30661" opendate="2023-1-13 00:00:00" fixdate="2023-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce interfaces for Delete/Update</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.abilities.SupportsRowLevelModificationScan.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.RowLevelModificationScanContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="30662" opendate="2023-1-13 00:00:00" fixdate="2023-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for Delete</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.DeleteTableITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.DeleteTableITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestUpdateDeleteTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.sink.SinkAbilitySpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSourceUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSinkUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.ContextResolvedTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoSourceScanRuleBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterInCalcIntoTableSourceScanRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.SinkModifyOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="30665" opendate="2023-1-13 00:00:00" fixdate="2023-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add implementation for Update</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestUpdateDeleteTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.sink.SinkAbilitySpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.sink.RowLevelDeleteSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSinkUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="30666" opendate="2023-1-13 00:00:00" fixdate="2023-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for Delete/Update API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.abilities.SupportsRowLevelModificationScan.java</file>
      <file type="M">docs.content.docs.dev.table.sourcesSinks.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sourcesSinks.md</file>
    </fixedFiles>
  </bug>
  <bug id="30672" opendate="2023-1-13 00:00:00" fixdate="2023-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support &amp;#39;EXPLAIN PLAN_ADVICE&amp;#39; statement</summary>
      <description></description>
      <version>1.17.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.TwoStageOptimizedAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.ExplainDetail.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.dql.SqlRichExplain.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.RelTreeWriterImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.analyze.FlinkStreamPlanAnalyzers.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.table.q</file>
    </fixedFiles>
  </bug>
  <bug id="30673" opendate="2023-1-13 00:00:00" fixdate="2023-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for "EXPLAIN PLAN_ADVICE" statement</summary>
      <description></description>
      <version>1.17.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.explain.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.explain.md</file>
    </fixedFiles>
  </bug>
  <bug id="30674" opendate="2023-1-13 00:00:00" fixdate="2023-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enabling consume partial finished partition by default for speculative execution in hybrid shuffle mode</summary>
      <description>At present, if hybrid shuffle enabled speculative execution, it will only consume all finished partition by default. It is better to change this default behavior to consume partial finished upstream partition.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchSchedulerFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="30677" opendate="2023-1-13 00:00:00" fixdate="2023-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SqlGatewayServiceStatementITCase.testFlinkSqlStatements fails</summary>
      <description>We're observing a test instability with SqlGatewayServiceStatementITCase.testFlinkSqlStatements in the following builds:https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44775&amp;view=logs&amp;j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&amp;t=cdd32e0b-6047-565b-c58f-14054472f1be&amp;l=14251https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44775&amp;view=logs&amp;j=de826397-1924-5900-0034-51895f69d4b7&amp;t=f311e913-93a2-5a37-acab-4a63e1328f94&amp;l=14608Jan 13 02:46:10 [ERROR] Tests run: 9, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 27.279 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.table.gateway.service.SqlGatewayServiceStatementITCaseJan 13 02:46:10 [ERROR] org.apache.flink.table.gateway.service.SqlGatewayServiceStatementITCase.testFlinkSqlStatements(String)[5] Time elapsed: 1.573 s &lt;&lt;&lt; FAILURE!Jan 13 02:46:10 org.opentest4j.AssertionFailedError: Jan 13 02:46:10 Jan 13 02:46:10 expected: Jan 13 02:46:10 "# table.q - CREATE/DROP/SHOW/ALTER/DESCRIBE TABLEJan 13 02:46:10 #Jan 13 02:46:10 # Licensed to the Apache Software Foundation (ASF) under one or moreJan 13 02:46:10 # contributor license agreements. See the NOTICE file distributed withJan 13 02:46:10 # this work for additional information regarding copyright ownership.Jan 13 02:46:10 # The ASF licenses this file to you under the Apache License, Version 2.0Jan 13 02:46:10 # (the "License"); you may not use this file except in compliance withJan 13 02:46:10 # the License. You may obtain a copy of the License atJan 13 02:46:10 #Jan 13 02:46:10 # http://www.apache.org/licenses/LICENSE-2.0Jan 13 02:46:10 #Jan 13 02:46:10 # Unless required by applicable law or agreed to in writing, softwareJan 13 02:46:10 # distributed under the License is distributed on an "AS IS" BASIS,Jan 13 02:46:10 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.Jan 13 02:46:10 # See the License for the specific language governing permissions andJan 13 02:46:10 # limitations under the License.[...]</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.SqlGatewayRestEndpointStatementITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.AbstractSqlGatewayStatementITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.result.ResultFetcher.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="30691" opendate="2023-1-15 00:00:00" fixdate="2023-1-15 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>SQL Client supports to specify endpoint address</summary>
      <description></description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.SQLJobSubmission.java</file>
      <file type="M">flink-end-to-end-tests.flink-sql-gateway-test.src.test.java.org.apache.flink.table.gateway.SqlGatewayE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.flink-sql-gateway-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkDistribution.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.SqlClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.SqlClient.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalContextUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptionsParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="30692" opendate="2023-1-15 00:00:00" fixdate="2023-1-15 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce embedded session manager to support REMOVE JAR</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.utils.SqlGatewayServiceExtension.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.session.SessionManagerTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.SqlGateway.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.session.SessionManager.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.SessionContext.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.DefaultContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.set.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.function.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutorImplITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.SqlClient.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.DefaultContextUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.ExecutorImpl.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="30693" opendate="2023-1-15 00:00:00" fixdate="2023-1-15 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce RowFormat to SQL Gateway for result fetching</summary>
      <description>Currently, the SQL gateway only sends JSON-formatted result to REST endpoint. So this ticket introduces RowFormat to control the result format.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.resources.sql.statement.set.q</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.resources.sql.insert.q</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.resources.sql.gateway.rest.api.v2.snapshot</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.resources.sql.gateway.rest.api.v1.snapshot</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.resources.sql.begin.statement.set.q</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.resources.resultInfo.txt</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.util.SqlGatewayRestEndpointExtension.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.util.RestConfigUtils.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.StatementRelatedITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.SqlGatewayRestEndpointTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.SqlGatewayRestEndpointStatementITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.SqlGatewayRestEndpointITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.serde.ResultInfoJsonSerDeTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.RestAPIITCaseBase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.OperationRelatedITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.AbstractSqlGatewayStatementITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.result.NotReadyResult.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationManager.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.SqlGatewayRestEndpoint.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.serde.ResultInfoJsonSerializer.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.serde.ResultInfoJsonDeserializer.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.serde.ResultInfo.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.message.statement.FetchResultsTokenParameters.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.message.statement.FetchResultsResponseBody.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.header.statement.FetchResultsHeaders.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.handler.statement.FetchResultsHandler.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.results.ResultSetImpl.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.resources.endpoint.select.q</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointStatementITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="30713" opendate="2023-1-17 00:00:00" fixdate="2023-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Hadoop related k8s decorators exclude possibility</summary>
      <description>Hadoop related k8s decorators are making a lot of assumptions. There are some users who mount Hadoop specific things (like Hadoop config, Kerberos config/keytab) in a custom way and not depending on Flink. As a result of these assumptions the actual decorators making the workload fail and there is no workaround.</description>
      <version>1.17.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesTaskManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesTaskManagerFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.kubernetes.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="30749" opendate="2023-1-18 00:00:00" fixdate="2023-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Delegation token provider enabled flag documentation is wrong</summary>
      <description></description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.security.security-kerberos.md</file>
    </fixedFiles>
  </bug>
  <bug id="30752" opendate="2023-1-19 00:00:00" fixdate="2023-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support &amp;#39;EXPLAIN PLAN_ADVICE&amp;#39; statement in PyFlink</summary>
      <description>For API completeness</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.util.java.utils.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.sql.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.explain.py</file>
      <file type="M">flink-python.pyflink.table.explain.detail.py</file>
      <file type="M">flink-python.pyflink.examples.table.basic.operations.py</file>
    </fixedFiles>
  </bug>
  <bug id="30753" opendate="2023-1-19 00:00:00" fixdate="2023-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Py4J cannot acquire Table.explain() method</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45044&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=c67e71ed-6451-5d26-8920-5a8cf9651901</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.table.py</file>
      <file type="M">flink-python.pyflink.table.statement.set.py</file>
    </fixedFiles>
  </bug>
  <bug id="30761" opendate="2023-1-20 00:00:00" fixdate="2023-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove JVM asserts from leader election code</summary>
      <description>assert is not enabled in the test run. We should using Preconditions</description>
      <version>1.16.0,1.17.0,1.15.3</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingLeaderElectionService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.ManualLeaderService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService.java</file>
    </fixedFiles>
  </bug>
  <bug id="30785" opendate="2023-1-25 00:00:00" fixdate="2023-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RocksDB Memory Management end-to-end test failed due to unexpected exception</summary>
      <description>We see a test instability with RocksDB Memory Management end-to-end test. The test failed because an exception was detected in the logs:2023-01-25T02:47:38.7172354Z Jan 25 02:47:38 Checking for errors...2023-01-25T02:47:39.1661969Z Jan 25 02:47:39 No errors in log files.2023-01-25T02:47:39.1662430Z Jan 25 02:47:39 Checking for exceptions...2023-01-25T02:47:39.2893767Z Jan 25 02:47:39 Found exception in log files; printing first 500 lines; see full logs for details:[...]2023-01-25T02:47:39.5674568Z Jan 25 02:47:39 Checking for non-empty .out files...2023-01-25T02:47:39.5675055Z Jan 25 02:47:39 No non-empty .out files.2023-01-25T02:47:39.5675352Z Jan 25 02:47:39 2023-01-25T02:47:39.5676104Z Jan 25 02:47:39 [FAIL] 'RocksDB Memory Management end-to-end test' failed after 1 minutes and 50 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out filesThe only exception being reported in the Flink logs is due to a warning:2023-01-25 02:47:38,242 WARN org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger or complete checkpoint 1 for job 421e4c00ef175b3b133d63cbfe9bca8b. (0 consecutive failed attempts so far)org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint Coordinator is suspending. at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.stopCheckpointScheduler(CheckpointCoordinator.java:1970) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.checkpoint.CheckpointCoordinatorDeActivator.jobStatusChanges(CheckpointCoordinatorDeActivator.java:46) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifyJobStatusChange(DefaultExecutionGraph.java:1578) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.transitionState(DefaultExecutionGraph.java:1173) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.transitionState(DefaultExecutionGraph.java:1145) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.cancel(DefaultExecutionGraph.java:973) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.scheduler.SchedulerBase.cancel(SchedulerBase.java:671) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.jobmaster.JobMaster.cancel(JobMaster.java:461) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_352] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_352] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_352] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_352] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at akka.actor.ActorCell.invoke(ActorCell.scala:547) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_352] at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_352] at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_352] at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_352]https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45185&amp;view=logs&amp;j=bea52777-eaf8-5663-8482-18fbc3630e81&amp;t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&amp;l=5117</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="30797" opendate="2023-1-26 00:00:00" fixdate="2023-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump json5 from 1.0.1 to 1.0.2</summary>
      <description>Dependabot has created https://github.com/apache/flink/pull/21617This is the corresponding Jira ticket</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
    </fixedFiles>
  </bug>
  <bug id="30799" opendate="2023-1-27 00:00:00" fixdate="2023-1-27 01:00:00" resolution="Done">
    <buginformation>
      <summary>Make SinkFunction support speculative execution for batch jobs</summary>
      <description>In this ticket, it would make SinkFunction based sink run with speculative execution for batch jobs.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.SpeculativeSchedulerITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.LegacySinkTransformation.java</file>
    </fixedFiles>
  </bug>
  <bug id="30801" opendate="2023-1-27 00:00:00" fixdate="2023-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate LeaderElection-related unit tests to JUnit5</summary>
      <description>To prepare the merge of the MultipleComponentLeaderElectionService-related tests with the legacy test, we want to align the JUnit versin they are using.</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderretrieval.SettableLeaderRetrievalServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriverTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.StandaloneLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderChangeClusterComponentsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.DefaultMultipleComponentLeaderElectionServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.DefaultLeaderElectionServiceTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="30808" opendate="2023-1-28 00:00:00" fixdate="2023-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MultipleInputITCase failed with AdaptiveBatch Scheduler</summary>
      <description>MultipleInputITCase#testRelatedInputs failed with AdaptiveBatch Scheduler.java.lang.UnsupportedOperationException: Forward partitioning does not allow change of parallelism. Upstream operation: Calc[10]-14 parallelism: 1, downstream operation: HashJoin[15]-20 parallelism: 3 You must use another partitioning strategy, such as broadcast, rebalance, shuffle or global.</description>
      <version>1.16.0,1.17.0</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="30811" opendate="2023-1-28 00:00:00" fixdate="2023-2-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix sql gateway can not stop job correctly</summary>
      <description></description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.session.Session.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.SessionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.ExecutorImplITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="30816" opendate="2023-1-29 00:00:00" fixdate="2023-1-29 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Fix Sql Client always uses the packaged highests version to connect to gateway</summary>
      <description></description>
      <version>1.17.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.util.SqlGatewayRestAPIVersion.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.header.statement.FetchResultsHeaders.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.handler.util.GetApiVersionHandler.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutorImplITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.ExecutorImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="30819" opendate="2023-1-29 00:00:00" fixdate="2023-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix sql client print an empty line between the multi-line statements</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.util.CliClientTestUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.SqlClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectBatchResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.ChangelogCollectResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutorImplITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.ResultDescriptor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectResultBase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectBatchResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.CollectResultBase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.ChangelogCollectResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.ExecutorImpl.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.ClientResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.parser.SqlMultiLineParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="30824" opendate="2023-1-30 00:00:00" fixdate="2023-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for new introduced hive agg option</summary>
      <description></description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.hive.hive.functions.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.functions.md</file>
    </fixedFiles>
  </bug>
  <bug id="30827" opendate="2023-1-30 00:00:00" fixdate="2023-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the SQL Client code structure</summary>
      <description></description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectBatchResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.ChangelogCollectResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.BaseMaterializedResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutorImplITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliTableauResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.SqlClient.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.ResultDescriptor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.SingleSessionManager.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectResultBase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectBatchResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.DynamicResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.CollectResultBase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.ChangelogResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.ChangelogCollectResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.DefaultContextUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliTableResultView.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliTableauResultView.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliChangelogResultView.java</file>
    </fixedFiles>
  </bug>
  <bug id="30828" opendate="2023-1-30 00:00:00" fixdate="2023-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SortAggITCase.testLeadLag failed</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45389&amp;view=logs&amp;j=0c940707-2659-5648-cbe6-a1ad63045f0a&amp;t=075c2716-8010-5565-fe08-3c4bb45824a4&amp;l=12560Jan 30 11:03:32 [ERROR] Tests run: 72, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 37.42 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCaseJan 30 11:03:32 [ERROR] org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase.testLeadLag Time elapsed: 0.547 s &lt;&lt;&lt; FAILURE!Jan 30 11:03:32 java.lang.AssertionError: Jan 30 11:03:32 Jan 30 11:03:32 Results do not match for query:Jan 30 11:03:32 Jan 30 11:03:32 SELECTJan 30 11:03:32 a,Jan 30 11:03:32 b, LEAD(b, 1) over (order by a) AS bLead, LAG(b, 1) over (order by a) AS bLag,Jan 30 11:03:32 c, LEAD(c, 1) over (order by a) AS cLead, LAG(c, 1) over (order by a) AS cLag,Jan 30 11:03:32 d, LEAD(d, 1) over (order by a) AS dLead, LAG(d, 1) over (order by a) AS dLag,Jan 30 11:03:32 e, LEAD(e, 1) over (order by a) AS eLead, LAG(e, 1) over (order by a) AS eLag,Jan 30 11:03:32 f, LEAD(f, 1) over (order by a) AS fLead, LAG(f, 1) over (order by a) AS fLag,Jan 30 11:03:32 g, LEAD(g, 1) over (order by a) AS gLead, LAG(g, 1) over (order by a) AS gLag,Jan 30 11:03:32 h, LEAD(h, 1) over (order by a) AS hLead, LAG(h, 1) over (order by a) AS hLag,Jan 30 11:03:32 i, LEAD(i, 1) over (order by a) AS iLead, LAG(i, 1) over (order by a) AS iLag,Jan 30 11:03:32 j, LEAD(j, 1) over (order by a) AS jLead, LAG(j, 1) over (order by a) AS jLag,Jan 30 11:03:32 k, LEAD(k, 1) over (order by a) AS kLead, LAG(k, 1) over (order by a) AS kLag,Jan 30 11:03:32 l, LEAD(l, 1) over (order by a) AS lLead, LAG(l, 1) over (order by a) AS lLag,Jan 30 11:03:32 m, LEAD(m, 1) over (order by a) AS mLead, LAG(m, 1) over (order by a) AS mLag,Jan 30 11:03:32 n, LEAD(n, 1) over (order by a) AS nLead, LAG(n, 1) over (order by a) AS nLagJan 30 11:03:32 Jan 30 11:03:32 FROM UnnamedTable$230Jan 30 11:03:32 order by aJan 30 11:03:32 Jan 30 11:03:32 Jan 30 11:03:32 ResultsJan 30 11:03:32 == Correct Result - 3 == == Actual Result - 3 ==Jan 30 11:03:32 +I[Alice, 1, 1, null, 1, 1, null, 2, 2, null, 9223, 9223, null, -2.3, -2.3, null, 9.9, 9.9, null, true, true, null, varchar, varchar, null, char , char , null, 2021-08-03, 2021-08-03, null, 20:08:17, 20:08:17, null, 2021-08-03T20:08:29, 2021-08-03T20:08:29, null, 9.99, 9.99, null] +I[Alice, 1, 1, null, 1, 1, null, 2, 2, null, 9223, 9223, null, -2.3, -2.3, null, 9.9, 9.9, null, true, true, null, varchar, varchar, null, char , char , null, 2021-08-03, 2021-08-03, null, 20:08:17, 20:08:17, null, 2021-08-03T20:08:29, 2021-08-03T20:08:29, null, 9.99, 9.99, null]Jan 30 11:03:32 +I[Alice, 1, null, 1, 1, null, 1, 2, null, 2, 9223, null, 9223, -2.3, null, -2.3, 9.9, null, 9.9, true, null, true, varchar, null, varchar, char , null, char , 2021-08-03, null, 2021-08-03, 20:08:17, null, 20:08:17, 2021-08-03T20:08:29, null, 2021-08-03T20:08:29, 9.99, null, 9.99] +I[Alice, 1, null, 1, 1, null, 1, 2, null, 2, 9223, null, 9223, -2.3, null, -2.3, 9.9, null, 9.9, true, null, true, varchar, null, varchar, char , null, char , 2021-08-03, null, 2021-08-03, 20:08:17, null, 20:08:17, 2021-08-03T20:08:29, null, 2021-08-03T20:08:29, 9.99, null, 9.99]Jan 30 11:03:32 !+I[Alice, null, null, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99] +I[Alice, null, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99, null]Jan 30 11:03:32 Jan 30 11:03:32 Plan:Jan 30 11:03:32 == Abstract Syntax Tree ==Jan 30 11:03:32 LogicalSort(sort0=[$0], dir0=[ASC-nulls-first])Jan 30 11:03:32 +- LogicalProject(inputs=[0..1], exprs=[[LEAD($1, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($1, 1) OVER (ORDER BY $0 NULLS FIRST), $2, LEAD($2, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($2, 1) OVER (ORDER BY $0 NULLS FIRST), $3, LEAD($3, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($3, 1) OVER (ORDER BY $0 NULLS FIRST), $4, LEAD($4, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($4, 1) OVER (ORDER BY $0 NULLS FIRST), $5, LEAD($5, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($5, 1) OVER (ORDER BY $0 NULLS FIRST), $6, LEAD($6, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($6, 1) OVER (ORDER BY $0 NULLS FIRST), $7, LEAD($7, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($7, 1) OVER (ORDER BY $0 NULLS FIRST), $8, LEAD($8, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($8, 1) OVER (ORDER BY $0 NULLS FIRST), $9, LEAD($9, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($9, 1) OVER (ORDER BY $0 NULLS FIRST), $10, LEAD($10, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($10, 1) OVER (ORDER BY $0 NULLS FIRST), $11, LEAD($11, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($11, 1) OVER (ORDER BY $0 NULLS FIRST), $12, LEAD($12, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($12, 1) OVER (ORDER BY $0 NULLS FIRST), $13, LEAD($13, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($13, 1) OVER (ORDER BY $0 NULLS FIRST)]])Jan 30 11:03:32 +- LogicalUnion(all=[true])[...]</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="30846" opendate="2023-1-31 00:00:00" fixdate="2023-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SpeculativeSchedulerITCase.testSpeculativeExecutionOfInputFormatSource fails</summary>
      <description>SpeculativeSchedulerITCase.testSpeculativeExecutionOfInputFormatSource is timing outhttps://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45440&amp;view=logs&amp;j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&amp;t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&amp;l=8599Jan 31 02:02:28 "ForkJoinPool-1-worker-25" #27 daemon prio=5 os_prio=0 tid=0x00007fcf74f2b800 nid=0x5476 waiting on condition [0x00007fce2b078000]Jan 31 02:02:28 java.lang.Thread.State: WAITING (parking)Jan 31 02:02:28 at sun.misc.Unsafe.park(Native Method)Jan 31 02:02:28 - parking to wait for &lt;0x00000000a22933e0&gt; (a java.util.concurrent.CompletableFuture$Signaller)Jan 31 02:02:28 at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)Jan 31 02:02:28 at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)Jan 31 02:02:28 at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)Jan 31 02:02:28 at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)Jan 31 02:02:28 at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)Jan 31 02:02:28 at org.apache.flink.test.scheduling.SpeculativeSchedulerITCase.executeJob(SpeculativeSchedulerITCase.java:216)Jan 31 02:02:28 at org.apache.flink.test.scheduling.SpeculativeSchedulerITCase.testSpeculativeExecutionOfInputFormatSource(SpeculativeSchedulerITCase.java:162)</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactoryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="30854" opendate="2023-1-31 00:00:00" fixdate="2023-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose periodic compaction to RocksdbCompactFilterCleanupStrategy</summary>
      <description>In 6.20.3-ververica-2.0 rocksdb version flink uses from 1.17&amp;#91;1&amp;#93;, we introduce periodic compaction option&amp;#91;2&amp;#93;.it deserves to expose this into RocksdbCompactFilterCleanupStrategy or some confs to make users use it conveniently.cc Yun Tang &amp;#91;1&amp;#93; https://github.com/apache/flink/pull/21747&amp;#91;2&amp;#93; https://github.com/ververica/frocksdb/pull/57</description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.streaming.api.utils.ProtoUtilsTest.java</file>
      <file type="M">flink-python.pyflink.fn.execution.embedded.java.utils.py</file>
      <file type="M">flink-python.pyflink.datastream.state.py</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.ttl.RocksDbTtlCompactFiltersManager.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.state.StateTtlConfigTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.time.Time.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.StateTtlConfig.java</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.state.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.state.md</file>
    </fixedFiles>
  </bug>
  <bug id="30875" opendate="2023-2-2 00:00:00" fixdate="2023-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix usages of legacy AdaptiveBatchScheduler configuration</summary>
      <description>In FLINK-30686, we deprecated the JobManagerOptions's AdaptiveBatchScheduler configuration. However, these configuration items still have some calls. And we should change these calls to new configuration.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.SpeculativeSchedulerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismAndInputInfosDeciderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismAndInputInfosDecider.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
    </fixedFiles>
  </bug>
  <bug id="30876" opendate="2023-2-2 00:00:00" fixdate="2023-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ResetTransformationProcessor don&amp;#39;t reset the transformation of ExecNode in BatchExecMultiInput.rootNode</summary>
      <description>Now, ResetTransformationProcessor don't reset the transformation of ExecNode in BatchExecMultiInput.rootNode. This may cause error while creating StreamGraph for BatchExecMultiInput due to different id of rootNode and inputNode.</description>
      <version>1.17.0,1.16.1</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.processor.MultipleInputNodeCreationProcessorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecMultipleInput.java</file>
    </fixedFiles>
  </bug>
  <bug id="30878" opendate="2023-2-2 00:00:00" fixdate="2023-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KubernetesHighAvailabilityRecoverFromSavepointITCase fails due to a deadlock</summary>
      <description>We're seeing a test failure in KubernetesHighAvailabilityRecoverFromSavepointITCase due to a deadlock:2023-02-01T18:53:35.5540322Z "ForkJoinPool-1-worker-1" #14 daemon prio=5 os_prio=0 tid=0x00007f68ecb18000 nid=0x43dd1 waiting on condition [0x00007f68c1711000]2023-02-01T18:53:35.5540900Z java.lang.Thread.State: TIMED_WAITING (parking)2023-02-01T18:53:35.5541272Z at sun.misc.Unsafe.park(Native Method)2023-02-01T18:53:35.5541932Z - parking to wait for &lt;0x00000000d14d7b60&gt; (a java.util.concurrent.CompletableFuture$Signaller)2023-02-01T18:53:35.5542496Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2023-02-01T18:53:35.5543088Z at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1709)2023-02-01T18:53:35.5543672Z at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)2023-02-01T18:53:35.5544240Z at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1788)2023-02-01T18:53:35.5544801Z at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)2023-02-01T18:53:35.5545632Z at org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityRecoverFromSavepointITCase.testRecoverFromSavepoint(KubernetesHighAvailabilityRecoverFromSavepointITCase.java:113)2023-02-01T18:53:35.5546409Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45565&amp;view=logs&amp;j=bea52777-eaf8-5663-8482-18fbc3630e81&amp;t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&amp;l=61916The build failure happens on 1.16. I'm adding 1.17 and 1.15 as fixVersions as well because it might be due to some recent changes which were introduced with FLINK-30462 and/or FLINK-30474</description>
      <version>1.17.0,1.15.4,1.16.2</version>
      <fixedVersion>1.17.0,1.15.4,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.DefaultMultipleComponentLeaderElectionServiceTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.DefaultMultipleComponentLeaderElectionService.java</file>
    </fixedFiles>
  </bug>
  <bug id="30901" opendate="2023-2-6 00:00:00" fixdate="2023-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The jobVertex&amp;#39;s parallelismConfigured is incorrect when chaining with source operators</summary>
      <description>When creating OperatorChainInfo in StreamingJobGenerator, the chained source are not included in OperatorChainInfo#chainedNodes, because they are not added to OperatorChainInfo via #addNodeToChain().This will affect jobVertex which has a MultiInput operator chained with sources. The vertex's parallelismConfigured will be false even if the chained sources have a parallelism configured. </description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0,1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="30902" opendate="2023-2-6 00:00:00" fixdate="2023-2-6 01:00:00" resolution="Done">
    <buginformation>
      <summary>Partition reuse does not take effect on edges of hybrid selective type</summary>
      <description>Partition reuse only take effect for re-consumable edge, but hybrid selective result partition is not re-consumable. This optimization is very important to reduce the cost of the shuffle write phase. In the previous implementation, we will only force the broadcast edge to be of hybrid full(re-consumable) in the 'ResultPartitionTypeFactory'. As a result, for ALL_ EXCHANGE_HYBRID_SELECTIVE job, partition reuse cannot take effect for non-broadcast edges.In fact, we expected to replace all the edges that can be reused with hybrid full result partition.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.NonChainedOutput.java</file>
    </fixedFiles>
  </bug>
  <bug id="30903" opendate="2023-2-6 00:00:00" fixdate="2023-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The max parallelism used in adaptive batch scheduler doesn&amp;#39;t fallbacks to default parallelism</summary>
      <description>In FLINK-30684 we mark the vertices which use the default parallelism, and in AdaptiveBatchScheduler we allow users to use parallelism.default as the max parallelism if they don't configure the configuration item "execution.batch.adaptive.auto-parallelism.max-parallelism". This issue will fix the fallback logic. </description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismAndInputInfosDeciderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchSchedulerFactoryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismAndInputInfosDecider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchSchedulerFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="30905" opendate="2023-2-6 00:00:00" fixdate="2023-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>doc generation fails with "concurrent map read and map write"</summary>
      <description>We experience a build failure in master (but since it looks like a Hugo issue, I added already released version to the affected versions as well) with a concurrent map read and map write within hugo:https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45707&amp;view=logs&amp;j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&amp;t=ddd6d61a-af16-5d03-2b9a-76a279badf98Start building sites … fatal error: concurrent map read and map writegoroutine 233 [running]:runtime.throw(0x23054e4, 0x21) /usr/local/go/src/runtime/panic.go:1116 +0x72 fp=0xc0016ea860 sp=0xc0016ea830 pc=0x4f5ff2runtime.mapaccess1_faststr(0x1f71280, 0xc000764a20, 0xc000aa60e1, 0x18, 0xcd) /usr/local/go/src/runtime/map_faststr.go:21 +0x465 fp=0xc0016ea8d0 sp=0xc0016ea860 pc=0x4d29c5[...]</description>
      <version>1.17.0,1.15.3,1.16.1</version>
      <fixedVersion>1.17.0,1.15.4,1.16.2</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.docs.sh</file>
      <file type="M">.github.workflows.docs.sh</file>
    </fixedFiles>
  </bug>
  <bug id="30908" opendate="2023-2-6 00:00:00" fixdate="2023-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fatal error in ResourceManager caused YARNSessionFIFOSecuredITCase.testDetachedMode to fail</summary>
      <description>There's a build failure in YARNSessionFIFOSecuredITCase.testDetachedMode which is caused by a fatal error in the ResourceManager:https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45720&amp;view=logs&amp;j=245e1f2e-ba5b-5570-d689-25ae21e5302f&amp;t=d04c9862-880c-52f5-574b-a7a79fef8e0f&amp;l=29869Feb 05 02:41:58 java.io.InterruptedIOException: Interrupted waiting to send RPC request to serverFeb 05 02:41:58 java.io.InterruptedIOException: Interrupted waiting to send RPC request to serverFeb 05 02:41:58 at org.apache.hadoop.ipc.Client.call(Client.java:1480) ~[hadoop-common-3.2.3.jar:?]Feb 05 02:41:58 at org.apache.hadoop.ipc.Client.call(Client.java:1422) ~[hadoop-common-3.2.3.jar:?]Feb 05 02:41:58 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118) ~[hadoop-common-3.2.3.jar:?]Feb 05 02:41:58 at com.sun.proxy.$Proxy31.allocate(Unknown Source) ~[?:?]Feb 05 02:41:58 at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77) ~[hadoop-yarn-common-3.2.3.jar:?]Feb 05 02:41:58 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]Feb 05 02:41:58 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]Feb 05 02:41:58 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]Feb 05 02:41:58 at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]Feb 05 02:41:58 at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422) ~[hadoop-common-3.2.3.jar:?]Feb 05 02:41:58 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165) ~[hadoop-common-3.2.3.jar:?]Feb 05 02:41:58 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157) ~[hadoop-common-3.2.3.jar:?]Feb 05 02:41:58 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) ~[hadoop-common-3.2.3.jar:?]Feb 05 02:41:58 at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359) ~[hadoop-common-3.2.3.jar:?]Feb 05 02:41:58 at com.sun.proxy.$Proxy32.allocate(Unknown Source) ~[?:?]Feb 05 02:41:58 at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:325) ~[hadoop-yarn-client-3.2.3.jar:?]Feb 05 02:41:58 at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$HeartbeatThread.run(AMRMClientAsyncImpl.java:311) [hadoop-yarn-client-3.2.3.jar:?]Feb 05 02:41:58 Caused by: java.lang.InterruptedExceptionFeb 05 02:41:58 at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:404) ~[?:1.8.0_292]Feb 05 02:41:58 at java.util.concurrent.FutureTask.get(FutureTask.java:191) ~[?:1.8.0_292]Feb 05 02:41:58 at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1180) ~[hadoop-common-3.2.3.jar:?]Feb 05 02:41:58 at org.apache.hadoop.ipc.Client.call(Client.java:1475) ~[hadoop-common-3.2.3.jar:?]Feb 05 02:41:58 ... 17 more</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerDriverTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.active.ResourceManagerDriverTestBase.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManagerDriver.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="30917" opendate="2023-2-6 00:00:00" fixdate="2023-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The user configured max parallelism does not take effect when using adaptive batch scheduler</summary>
      <description>Currently, the adaptive batch scheduler only respects the global maximum parallelism(which is configured by option parallelism.default or execution.batch.adaptive.auto-parallelism.max-parallelism, see FLINK-30686 for details) when deciding parallelism for job vertices, the maximum parallelism of vertices configured by the user through setMaxParallelism will not be respected.In this ticket, we will change the behavior so that the user-configured max parallelism also be respected.</description>
      <version>1.17.0,1.16.1</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismAndInputInfosDeciderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.VertexParallelismAndInputInfosDecider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismAndInputInfosDecider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler.java</file>
    </fixedFiles>
  </bug>
  <bug id="30927" opendate="2023-2-6 00:00:00" fixdate="2023-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Several tests started generate output with two non-abstract methods have the same parameter types, declaring type and return type</summary>
      <description>e.g. org.apache.flink.table.planner.runtime.stream.sql.MatchRecognizeITCase#testUserDefinedFunctions it seems during code splitter it starts generating some methods with same signature org.codehaus.janino.InternalCompilerException: Compiling "MatchRecognizePatternProcessFunction$77": Two non-abstract methods "default void MatchRecognizePatternProcessFunction$77.processMatch_0(java.util.Map, org.apache.flink.cep.functions.PatternProcessFunction$Context, org.apache.flink.util.Collector) throws java.lang.Exception" have the same parameter types, declaring type and return type Probably could be a side effect of https://issues.apache.org/jira/browse/FLINK-27246</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.splitter.expected.TestSplitJavaCode.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.block.expected.TestWhileLoopRewrite.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.block.expected.TestWhileLoopInsideIfRewrite.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.block.expected.TestRewriteInnerClass.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.block.expected.TestIfStatementRewrite3.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.block.expected.TestIfStatementRewrite2.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.block.expected.TestIfStatementRewrite1.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.block.expected.TestIfStatementRewrite.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.block.expected.TestIfMultipleSingleLineStatementRewrite.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.block.expected.TestIfInsideWhileLoopRewrite.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.BlockStatementRewriterTest.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.main.java.org.apache.flink.table.codesplit.BlockStatementRewriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="30948" opendate="2023-2-7 00:00:00" fixdate="2023-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove flink-avro-glue-schema-registry and flink-json-glue-schema-registry from Flink main repo</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.ExecutorImplITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.ExecutorImpl.java</file>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">azure-pipelines.yml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.src.test.java.org.apache.flink.glue.schema.registry.test.GlueSchemaRegistryAvroKinesisITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.src.test.java.org.apache.flink.glue.schema.registry.test.GSRKinesisPubsubClient.java</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.src.test.resources.avro.user.avsc</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.src.test.java.org.apache.flink.glue.schema.registry.test.json.GlueSchemaRegistryJsonKinesisITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.src.test.java.org.apache.flink.glue.schema.registry.test.json.GSRKinesisPubsubClient.java</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.archunit-violations.3e77c07d-dfdb-4ec0-8e64-5fad5c651c72</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.archunit-violations.dc78e80c-3bb3-45bb-87c1-b57472d6f45b</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.archunit-violations.stored.rules</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroSchemaCoder.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroSchemaCoderProvider.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroSerializationSchema.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryInputStreamDeserializer.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryOutputStreamSerializer.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.architecture.TestCodeArchitectureTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroSchemaCoderTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryInputStreamDeserializerTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryOutputStreamSerializerTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.User.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.resources.avro.user.avsc</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.resources.archunit.properties</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.resources.META-INF.services.org.junit.jupiter.api.extension.Extension</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.archunit-violations.4703059b-4f06-41c9-9724-644e6d00584f</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.archunit-violations.b99819a4-a946-475e-883f-963de77c7e57</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.archunit-violations.stored.rules</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.main.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.main.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonSchemaCoder.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.main.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonSchemaCoderProvider.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.main.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.java.org.apache.flink.architecture.TestCodeArchitectureTest.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.java.org.apache.flink.formats.json.glue.schema.registry.Car.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonSchemaCoderTest.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.resources.archunit.properties</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.resources.META-INF.services.org.junit.jupiter.api.extension.Extension</file>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="30968" opendate="2023-2-9 00:00:00" fixdate="2023-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sql-client supports dynamic config to open session</summary>
      <description>Currently sql client will open session with configuration in flink-conf.yaml when it creates connection to gateway. For the convenience of users, it can supports dynamic config with `--conf` as `bin/sql-client.sh gateway --endpoint host:port --conf k1=v1 --conf k2=v2`</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.cli.gateway-mode-help.out</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.cli.embedded-mode-help.out</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.cli.all-mode-help.out</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.SqlClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.DefaultContextUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptionsParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptions.java</file>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="30971" opendate="2023-2-9 00:00:00" fixdate="2023-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify the default value of parameter &amp;#39;table.exec.local-hash-agg.adaptive.sampling-threshold&amp;#39;</summary>
      <description>In our test environment, we set the default parallelism to  1 and got the most appropriate default value of parameter 'table.exec.local-hash-agg.adaptive.sampling-threshold'  is 5000000. However, for these batch jobs with high parallelism in produce environment,  the amount of data in single parallelism is almost less than 5000000. Therefore, after testing, we found that set to 500000 can get better results.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="30972" opendate="2023-2-9 00:00:00" fixdate="2023-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>E2e tests always fail in phase "Prepare E2E run"</summary>
      <description>Installing required softwareReading package lists...Building dependency tree...Reading state information...bc is already the newest version (1.07.1-2build1).bc set to manually installed.libapr1 is already the newest version (1.6.5-1ubuntu1).libapr1 set to manually installed.0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.--2023-02-09 04:38:47-- http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.10_amd64.debResolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 185.125.190.36, 185.125.190.39, ...Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.HTTP request sent, awaiting response... 404 Not Found2023-02-09 04:38:47 ERROR 404: Not Found.WARNING: apt does not have a stable CLI interface. Use with caution in scripts.Reading package lists...E: Unsupported file ./libssl1.0.0_1.0.2n-1ubuntu5.10_amd64.deb given on commandline##[error]Bash exited with code '100'.Finishing: Prepare E2E run</description>
      <version>1.17.0,1.15.4,1.16.2,1.18.0</version>
      <fixedVersion>1.17.0,1.15.4,1.16.2,1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="31036" opendate="2023-2-13 00:00:00" fixdate="2023-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StateCheckpointedITCase timed out</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46023&amp;view=logs&amp;j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&amp;t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&amp;l=10608"Legacy Source Thread - Source: Custom Source -&gt; Filter (6/12)#69980" #13718026 prio=5 os_prio=0 tid=0x00007f05f44f0800 nid=0x128157 waiting on condition [0x00007f059feef000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for &lt;0x00000000f0a974e8&gt; (a java.util.concurrent.CompletableFuture$Signaller) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707) at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323) at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908) at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegmentBlocking(LocalBufferPool.java:384) at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestBufferBuilderBlocking(LocalBufferPool.java:356) at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewBufferBuilderFromPool(BufferWritingResultPartition.java:414) at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewUnicastBufferBuilder(BufferWritingResultPartition.java:390) at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.appendUnicastDataForRecordContinuation(BufferWritingResultPartition.java:328) at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.emitRecord(BufferWritingResultPartition.java:161) at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:107) at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:55) at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:105) at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:91) at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:45) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:59) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:31) at org.apache.flink.streaming.api.operators.StreamFilter.processElement(StreamFilter.java:39) at org.apache.flink.streaming.runtime.io.RecordProcessorUtils$$Lambda$1311/1256184070.accept(Unknown Source) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:75) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:50) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418) at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513) - locked &lt;0x00000000d55035c0&gt; (a java.lang.Object) at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103) at org.apache.flink.test.checkpointing.StateCheckpointedITCase$StringGeneratingSourceFunction.run(StateCheckpointedITCase.java:178) - locked &lt;0x00000000d55035c0&gt; (a java.lang.Object) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67) at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0,1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.MiniClusterResource.java</file>
    </fixedFiles>
  </bug>
  <bug id="31077" opendate="2023-2-15 00:00:00" fixdate="2023-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Trigger checkpoint failed but it were shown as COMPLETED by rest API</summary>
      <description>Currently, we can trigger a checkpoint and poll the status of the checkpoint until it is finished by rest according to FLINK-27101. However, even if the checkpoint status returned by rest is completed, it does not mean that the checkpoint is really completed. If an exception occurs after marking the pendingCheckpoint completed(here), the checkpoint is not written to the HA service and we can not failover from this checkpoint.</description>
      <version>1.17.0,1.15.3,1.16.1</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="31082" opendate="2023-2-15 00:00:00" fixdate="2023-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setting maven property &amp;#39;flink.resueForks&amp;#39; to false in table planner module</summary>
      <description>This issue is created to alleviate the OOM problem mentioned in issue: https://issues.apache.org/jira/browse/FLINK-18356Setting maven property 'flink.resueForks' to false in table planner module can only reduce the frequency of oom, but can't solve this problem. To completely solve this problem, we need to identify the specific reasons, but this is a time-consuming work.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="31092" opendate="2023-2-15 00:00:00" fixdate="2023-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive ITCases fail with OutOfMemoryError</summary>
      <description>We're experiencing an OutOfMemoryError where the heap space reaches the upper limit:https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46161&amp;view=logs&amp;j=fc5181b0-e452-5c8f-68de-1097947f6483&amp;t=995c650b-6573-581c-9ce6-7ad4cc038461&amp;l=23142Feb 15 05:05:14 [INFO] Running org.apache.flink.table.catalog.hive.HiveCatalogITCaseFeb 15 05:05:17 [INFO] java.lang.OutOfMemoryError: Java heap spaceFeb 15 05:05:17 [INFO] Dumping heap to java_pid9669.hprof ...Feb 15 05:05:28 [INFO] Heap dump file created [1957090051 bytes in 11.718 secs]java.lang.OutOfMemoryError: Java heap space at org.apache.maven.surefire.booter.ForkedBooter.cancelPingScheduler(ForkedBooter.java:209) at org.apache.maven.surefire.booter.ForkedBooter.acknowledgedExit(ForkedBooter.java:419) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:186) at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)</description>
      <version>1.17.0,1.16.1,1.18.0</version>
      <fixedVersion>1.17.0,1.16.2,1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.FactoryUtilTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.ServiceLoaderUtil.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FactoryUtil.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.operation.OperationManagerTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationManager.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.utils.ThreadUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="31113" opendate="2023-2-17 00:00:00" fixdate="2023-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support AND filter for flink-orc</summary>
      <description>Support AND filter in flink-orc</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcFileSystemFilterTest.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcFilters.java</file>
    </fixedFiles>
  </bug>
  <bug id="31120" opendate="2023-2-17 00:00:00" fixdate="2023-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ConcurrentModificationException occurred in StringFunctionsITCase.test</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46255&amp;view=logs&amp;j=0c940707-2659-5648-cbe6-a1ad63045f0a&amp;t=075c2716-8010-5565-fe08-3c4bb45824a4&amp;l=12334Feb 17 04:51:25 [ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 10.725 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.table.planner.functions.StringFunctionsITCaseFeb 17 04:51:25 [ERROR] org.apache.flink.table.planner.functions.StringFunctionsITCase.test(TestCase)[4] Time elapsed: 4.367 s &lt;&lt;&lt; ERROR!Feb 17 04:51:25 org.apache.flink.table.api.TableException: Failed to execute sqlFeb 17 04:51:25 at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:974)Feb 17 04:51:25 at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1422)Feb 17 04:51:25 at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:476)Feb 17 04:51:25 at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase$ResultTestItem.test(BuiltInFunctionTestBase.java:354)Feb 17 04:51:25 at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase$TestSetSpec.lambda$getTestCase$4(BuiltInFunctionTestBase.java:320)Feb 17 04:51:25 at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase$TestCase.execute(BuiltInFunctionTestBase.java:113)Feb 17 04:51:25 at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase.test(BuiltInFunctionTestBase.java:93)Feb 17 04:51:25 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[...]</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0,1.16.2,1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="31136" opendate="2023-2-20 00:00:00" fixdate="2023-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client Gateway mode should not read read execution config</summary>
      <description></description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.utils.SqlGatewayServiceExtension.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.SqlGateway.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.DefaultContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.SqlClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.DefaultContextUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="31137" opendate="2023-2-20 00:00:00" fixdate="2023-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client doesn&amp;#39;t print results for SHOW CREATE TABLE/DESC in hive dialect</summary>
      <description></description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0,1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveOperationExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="31157" opendate="2023-2-20 00:00:00" fixdate="2023-3-20 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Propose a pull request for website updates</summary>
      <description>The final step of building the candidate is to propose a website pull request containing the following changes: update apache/flink-web:_config.yml update FLINK_VERSION_STABLE and FLINK_VERSION_STABLE_SHORT as required update version references in quickstarts (q/ directory) as required (major only) add a new entry to flink_releases for the release binaries and sources (minor only) update the entry for the previous release in the series in flink_releases Please pay notice to the ids assigned to the download entries. They should be unique and reflect their corresponding version number. add a new entry to release_archive.flink add a blog post announcing the release in _posts add a organized release notes page under docs/content/release-notes and docs/content.zh/release-notes (like https://nightlies.apache.org/flink/flink-docs-release-1.15/release-notes/flink-1.15/). The page is based on the non-empty release notes collected from the issues, and only the issues that affect existing users should be included (e.g., instead of new functionality). It should be in a separate PR since it would be merged to the flink project. Don’t merge the PRs before finalizing the release. Expectations Website pull request proposed to list the release (major only) Check docs/config.toml to ensure that the version constants refer to the new version the baseurl does not point to flink-docs-master  but flink-docs-release-X.Y instead</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content..index.md</file>
      <file type="M">docs.content.zh..index.md</file>
    </fixedFiles>
  </bug>
  <bug id="31182" opendate="2023-2-22 00:00:00" fixdate="2023-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CompiledPlan cannot deserialize BridgingSqlFunction with MissingTypeStrategy</summary>
      <description>This issue is reported from the user mail list.The stacktrace is Unable to find source-code formatter for language: text. Available languages are: actionscript, ada, applescript, bash, c, c#, c++, cpp, css, erlang, go, groovy, haskell, html, java, javascript, js, json, lua, none, nyan, objc, perl, php, python, r, rainbow, ruby, scala, sh, sql, swift, visualbasic, xml, yamlCaused by: org.apache.flink.table.api.TableException: Could not resolve internal system function '$UNNEST_ROWS$1'. This is a bug, please file an issue.    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeInternalFunction(RexNodeJsonDeserializer.java:392)    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeSqlOperator(RexNodeJsonDeserializer.java:337)    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeCall(RexNodeJsonDeserializer.java:307)    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:146)    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:128)    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:115) The root cause is that although ModuleManager can resolve '$UNNEST_ROWS$1', the output type strategy is "Missing"; as a result, FunctionCatalogOperatorTable#convertToBridgingSqlFunction returns empty.</description>
      <version>1.17.0,1.18.0,1.17.1</version>
      <fixedVersion>1.17.0,1.16.2,1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.CorrelateJsonPlanITCase.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.strategies.SpecificTypeStrategies.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
    </fixedFiles>
  </bug>
  <bug id="31233" opendate="2023-2-27 00:00:00" fixdate="2023-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>no error should be logged when retrieving the task manager&amp;#39;s stdout if it does not exist</summary>
      <description>When running Flink on Kubernetes, the stdout logs is not redirected to files so it will not shown in WEB UI. This is as expected.But It returns “500 Internal error” in REST API and produces an error log in jobmanager.log. This is confusing and misleading. I think this API should return “404 Not Found” without any error logs, similar to how jobmanager/stdout works.  </description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.TestingChannelHandlerContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="31239" opendate="2023-2-27 00:00:00" fixdate="2023-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix sum function can&amp;#39;t get the corrected value when the argument type is string</summary>
      <description></description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0,1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.hive.hive.functions.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.functions.md</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.ExpressionBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.resources.explain.testSumAggFunctionPlan.out</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectAggITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveSumAggFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="31240" opendate="2023-2-27 00:00:00" fixdate="2023-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce the overhead of conversion between DataStream and Table</summary>
      <description>In some cases, users may need to convert the underlying DataStream to Table and then convert it back to DataStream(e.g. some Flink ML libraries accept a Table as input and convert it to DataStream for calculation.). This would cause unnecessary overhead because of data conversion between the internal data type and the external data type.We can reduce the overhead by checking if there are paired `fromDataStream`/`toDataStream` function call without any transformation, if so using the source datastream directly. The performance of Flink ML's Bucketizer algorithm&amp;#91;1&amp;#93; is used to demonstrate the impact of this optimization. The execution time is obtained by taking the median execution time across 5 runs for each setup.Before optimization: 40746msAfter optimization: 12972msThus this optimization reduces the total execution time of Flink ML's Bucketizer algorithm to about 1/3.&amp;#91;1&amp;#93; https://github.com/apache/flink-ml/blob/master/flink-ml-benchmark/src/main/resources/bucketizer-benchmark.json</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.DataStreamJavaITCase.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.bridge.java.StreamTableEnvironment.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="31273" opendate="2023-3-1 00:00:00" fixdate="2023-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Left join with IS_NULL filter be wrongly pushed down and get wrong join results</summary>
      <description>Left join with IS_NULL filter be wrongly pushed down and get wrong join results. The sql is:SELECT * FROM MyTable1 LEFT JOIN MyTable2 ON a1 = a2 WHERE a2 IS NULL AND a1 &lt; 10The wrongly plan is:LogicalProject(a1=[$0], b1=[$1], c1=[$2], b2=[$3], c2=[$4], a2=[$5])+- LogicalFilter(condition=[IS NULL($5)])   +- LogicalJoin(condition=[=($0, $5)], joinType=[left])      :- LogicalValues(tuples=[[]])      +- LogicalTableScan(table=[[default_catalog, default_database, MyTable2]])</description>
      <version>1.17.0,1.16.1</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.JoinTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkFilterJoinRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SortMergeJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.ShuffledHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.logical.FlinkFilterJoinRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.FlinkFilterJoinRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="31284" opendate="2023-3-1 00:00:00" fixdate="2023-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase KerberosLoginProvider test coverage</summary>
      <description></description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.token.hadoop.KerberosLoginProviderITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="31288" opendate="2023-3-2 00:00:00" fixdate="2023-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable overdraft buffer for batch shuffle</summary>
      <description>Only pipelined / pipelined-bounded partition needs overdraft buffer. More specifically, there is no reason to request more buffers for non-pipelined (i.e. batch) shuffle. The reasons are as follows: For BoundedBlockingShuffle, each full buffer will be directly released. For SortMergeShuffle, the maximum capacity of buffer pool is 4 * numSubpartitions. It is efficient enough to spill this part of memory to disk. For Hybrid Shuffle, the buffer pool is unbounded. If it can't get a normal buffer, it also can't get an overdraft buffer.</description>
      <version>1.17.0,1.16.1</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.NettyShuffleEnvironmentOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.netty.shuffle.environment.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.taskmanager.network.section.html</file>
      <file type="M">docs.content.docs.deployment.memory.network.mem.tuning.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.network.mem.tuning.md</file>
    </fixedFiles>
  </bug>
  <bug id="3129" opendate="2015-12-7 00:00:00" fixdate="2015-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tooling to ensure interface stability</summary>
      <description>I would like to use this maven plugin: https://github.com/siom79/japicmp to automatically ensure interface stability across minor releases.Ideally we have the plugin in place after Flink 1.0 is out, so that maven builds break if a breaking change has been made.The plugin already supports downloading a reference release, checking the build and breaking it.Not yet supported are class/method inclusions based on annotations, but I've opened a pull request for adding it.There are also issues with the resolution of the dependency with the annotations, but I'm working on resolving those issues.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.NormalizableKey.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.FloatValue.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.DoubleValue.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.RuntimeContext.java</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
      <file type="M">flink-shaded-curator.pom.xml</file>
      <file type="M">flink-quickstart.pom.xml</file>
      <file type="M">flink-java8.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
      <file type="M">flink-annotations.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="31300" opendate="2023-3-2 00:00:00" fixdate="2023-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TRY_CAST fails for constructed types</summary>
      <description>In case of problems with cast it is expected to return nullhowever for arrays, maps it failsexample of failing queriesselect try_cast(array['a'] as array&lt;int&gt;);select try_cast(map['a', '1'] as map&lt;int, int&gt;); [ERROR] Could not execute SQL statement. Reason:java.lang.NumberFormatException: For input string: 'a'. Invalid character found. at org.apache.flink.table.data.binary.BinaryStringDataUtil.numberFormatExceptionFor(BinaryStringDataUtil.java:585) at org.apache.flink.table.data.binary.BinaryStringDataUtil.toInt(BinaryStringDataUtil.java:518) at StreamExecCalc$15.processElement(Unknown Source) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418) at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513) at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103) at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67) at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)</description>
      <version>1.17.0,1.16.1</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionMiscITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.AbstractCastRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="31337" opendate="2023-3-6 00:00:00" fixdate="2023-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>EmbeddedDataStreamBatchTests.test_keyed_co_broadcast_side_output</summary>
      <description>Same build, multiple times: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=c67e71ed-6451-5d26-8920-5a8cf9651901&amp;l=24566 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&amp;view=logs&amp;j=821b528f-1eed-5598-a3b4-7f748b13f261&amp;t=6bb545dd-772d-5d8c-f258-f5085fba3295&amp;l=24235 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&amp;view=logs&amp;j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&amp;t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&amp;l=24545 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&amp;view=logs&amp;j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&amp;t=b4612f28-e3b5-5853-8a8b-610ae894217a&amp;l=24481 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&amp;view=logs&amp;j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&amp;t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&amp;l=24757Mar 04 01:21:35 pyflink/datastream/tests/test_data_stream.py:743: Mar 04 01:21:35 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ Mar 04 01:21:35 pyflink/datastream/tests/test_data_stream.py:63: in assert_equals_sortedMar 04 01:21:35 self.assertEqual(expected, actual)Mar 04 01:21:35 E AssertionError: Lists differ: ['0', '1', '2', '4', '5', '5', '6', '6'] != ['0', '1', '2', '3', '5', '5', '6', '6']Mar 04 01:21:35 E Mar 04 01:21:35 E First differing element 3:Mar 04 01:21:35 E '4'Mar 04 01:21:35 E '3'Mar 04 01:21:35 E Mar 04 01:21:35 E - ['0', '1', '2', '4', '5', '5', '6', '6']Mar 04 01:21:35 E ? ^Mar 04 01:21:35 E Mar 04 01:21:35 E + ['0', '1', '2', '3', '5', '5', '6', '6']Mar 04 01:21:35 E ?</description>
      <version>1.17.0,1.16.1</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.BatchExecutionUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.runtime.translators.python.PythonKeyedBroadcastStateTransformationTranslator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.process.ExternalPythonBatchKeyedCoBroadcastProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.EmbeddedPythonBatchKeyedCoBroadcastProcessOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="31346" opendate="2023-3-6 00:00:00" fixdate="2023-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Batch shuffle IO scheduler does not throw TimeoutException if numRequestedBuffers is greater than 0</summary>
      <description>We currently rely on throw exception to trigger downstream task failover to avoid read buffer request deadlock. But if numRequestedBuffers is greater than 0, IO scheduler does not throw TimeoutException. This will cause a deadlock.</description>
      <version>1.17.0,1.16.1</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsFileDataManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsFileDataManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="31347" opendate="2023-3-6 00:00:00" fixdate="2023-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AdaptiveSchedulerClusterITCase.testAutomaticScaleUp times out</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46850&amp;view=logs&amp;j=0da23115-68bb-5dcd-192c-bd4c8adebde1&amp;t=24c3384f-1bcb-57b3-224f-51bf973bbee8&amp;l=10451Mar 06 14:11:24 "main" #1 prio=5 os_prio=0 tid=0x00007f482800b800 nid=0x6eee waiting on condition [0x00007f48325cd000]Mar 06 14:11:24 java.lang.Thread.State: TIMED_WAITING (sleeping)Mar 06 14:11:24 at java.lang.Thread.sleep(Native Method)Mar 06 14:11:24 at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:151)Mar 06 14:11:24 at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:144)Mar 06 14:11:24 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.waitUntilParallelismForVertexReached(AdaptiveSchedulerClusterITCase.java:265)Mar 06 14:11:24 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.testAutomaticScaleUp(AdaptiveSchedulerClusterITCase.java:153)[...]</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0,1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testtasks.OnceBlockingNoOpInvokable.java</file>
    </fixedFiles>
  </bug>
  <bug id="31351" opendate="2023-3-6 00:00:00" fixdate="2023-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2EndpointITCase.testExecuteStatementInSyncModeWithRuntimeException2 times out on CI</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46872&amp;view=logs&amp;j=fc5181b0-e452-5c8f-68de-1097947f6483&amp;t=995c650b-6573-581c-9ce6-7ad4cc038461&amp;l=24908 Mar 06 18:28:56 "ForkJoinPool-1-worker-25" #27 daemon prio=5 os_prio=0 tid=0x00007ff4b1832000 nid=0x21b2 waiting on condition [0x00007ff3a8c3e000]Mar 06 18:28:56 java.lang.Thread.State: TIMED_WAITING (sleeping)Mar 06 18:28:56 at java.lang.Thread.sleep(Native Method)Mar 06 18:28:56 at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.waitUntilJobIsRunning(HiveServer2EndpointITCase.java:1004)Mar 06 18:28:56 at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.lambda$testExecuteStatementInSyncModeWithRuntimeException2$37(HiveServer2EndpointITCase.java:711)Mar 06 18:28:56 at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase$$Lambda$2018/2127600974.accept(Unknown Source)Mar 06 18:28:56 at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.runExecuteStatementInSyncModeWithRuntimeException(HiveServer2EndpointITCase.java:999)Mar 06 18:28:56 at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.testExecuteStatementInSyncModeWithRuntimeException2(HiveServer2EndpointITCase.java:709)Mar 06 18:28:56 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)Mar 06 18:28:56 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)Mar 06 18:28:56 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)Mar 06 18:28:56 at java.lang.reflect.Method.invoke(Method.java:498)</description>
      <version>1.17.0,1.16.1,1.18.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.operation.OperationManagerTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="31393" opendate="2023-3-10 00:00:00" fixdate="2023-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HsFileDataManager use an incorrect default timeout</summary>
      <description>For batch shuffle(i.e. hybrid shuffle &amp; sort-merge shuffle), If there is a fierce contention of the batch shuffle read memory, it will throw a TimeoutException to fail downstream task to release memory. But for hybrid shuffle, It uses an incorrect default timeout(5ms), this will make the job very easy to fail.</description>
      <version>1.17.0,1.16.1</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HybridShuffleConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="31401" opendate="2023-3-10 00:00:00" fixdate="2023-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>testTransformationSetParallelism fails on 10 core machines</summary>
      <description>StreamingJobGraphGenerator#testTransformationSetParallelism fails if it is run in an environment where the default parallelism is 10:org.opentest4j.AssertionFailedError: expected: 3 but was: 2Expected :3Actual :2The fix is trivial, we need to make an implicit assumption in the test about paralellisms explicit.</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="31476" opendate="2023-3-15 00:00:00" fixdate="2023-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AdaptiveScheduler should take lower bound parallelism settings into account</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultVertexParallelismStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.allocator.TestVertexInformation.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.allocator.SlotSharingSlotAllocatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.VertexParallelismInformation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultVertexParallelismStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultVertexParallelismInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.JobGraphJobInformation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.allocator.SlotSharingSlotAllocator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.allocator.JobInformation.java</file>
    </fixedFiles>
  </bug>
  <bug id="31485" opendate="2023-3-16 00:00:00" fixdate="2023-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Connecting to Kafka and Avro Schema Registry fails with ClassNotFoundException</summary>
      <description>When running the SQL Client and using flink-sql-connector-kafka, flink-sql-avro and flink-sql-avro-confluent-registry and trying to query Schema Registry, the job will fail with[ERROR] Could not execute SQL statement. Reason:java.lang.ClassNotFoundException: com.google.common.base.Ticker</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.pom.xml</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="31512" opendate="2023-3-19 00:00:00" fixdate="2023-1-19 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Move SqlAlterView conversion logic to SqlAlterViewConverter</summary>
      <description>Introduce SqlAlterViewConverter and move the conversion logic of SqlAlterView -&gt; AlterViewOperation to it.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConvertUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConverters.java</file>
    </fixedFiles>
  </bug>
  <bug id="31521" opendate="2023-3-20 00:00:00" fixdate="2023-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Initialize flink jdbc driver module in flink-table</summary>
      <description>Initialize jdbc driver module</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="31522" opendate="2023-3-20 00:00:00" fixdate="2023-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce FlinkResultSet and related classes for jdbc driver</summary>
      <description>Introduce FlinkResultSet and related classes for jdbc driver to support data iterator</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-jdbc-driver.pom.xml</file>
      <file type="M">flink-table.flink-sql-jdbc-driver-bundle.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="31541" opendate="2023-3-21 00:00:00" fixdate="2023-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Get metrics in Flink WEB UI error</summary>
      <description>When i get a metrics from a operator which name contains '&amp;#91;&amp;#39; or &amp;#39;&amp;#93;', it will be return 400 from rest response.The reason is we can not submit an GET request which params contains '&amp;#91;&amp;#39; or &amp;#39;&amp;#93;', it is invaild in REST. </description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.task-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.metrics.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job-manager.service.ts</file>
    </fixedFiles>
  </bug>
  <bug id="31568" opendate="2023-3-22 00:00:00" fixdate="2023-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update japicmp configuration</summary>
      <description>Update the japicmp reference version and wipe exclusions / enable API compatibility checks for @PublicEvolving APIs on the corresponding SNAPSHOT branch with the update_japicmp_configuration.sh script (see below).For a new major release (x.y.0), run the same command also on the master branch for updating the japicmp reference version and removing out-dated exclusions in the japicmp configuration.Make sure that all Maven artifacts are already pushed to Maven Central. Otherwise, there's a risk that CI fails due to missing reference artifacts.tools $ NEW_VERSION=$RELEASE_VERSION releasing/update_japicmp_configuration.shtools $ cd ..$ git add *$ git commit -m "Update japicmp configuration for $RELEASE_VERSION"</description>
      <version>None</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="31575" opendate="2023-3-22 00:00:00" fixdate="2023-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t swap table-planner-loader and table-planner to use hive dialect</summary>
      <description>From Flink 1.15,  to use Hive dialect, user have to swap the flink-table-planner-loader jar with flink-table-planner.jar.It really bothers some users who want to use Hive dialect like FLINK-27020, FLINK-28618Althogh we has paid much effort like FLINK-29350, FLINK-29045 to tell users to do the swap, but it'll still not convenient.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-loader.src.main.java.org.apache.flink.table.planner.loader.PlannerModule.java</file>
      <file type="M">flink-table.flink-table-planner-loader.src.main.java.org.apache.flink.table.planner.loader.DelegatePlannerFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-hive.src.test.java.org.apache.flink.tests.hive.HiveITCase.java</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.3.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.3.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveReflectionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="31588" opendate="2023-3-23 00:00:00" fixdate="2023-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The unaligned checkpoint type is wrong at subtask level</summary>
      <description>FLINK-20488 supported show checkpoint type for each subtask, and it based on received `CheckpointOptions` and it's right.However, FLINK-27251 supported timeout aligned to unaligned checkpoint barrier in the output buffers. It means the received `CheckpointOptions` can be converted from aligned checkpoint to unaligned checkpoint.So, the unaligned checkpoint type may be wrong at subtask level. For example, as shown in the figure below, Unaligned checkpoint type is false, but it is actually Unaligned checkpoint (persisted data &gt; 0).  </description>
      <version>1.16.0,1.17.0</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointMetricsBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="31612" opendate="2023-3-25 00:00:00" fixdate="2023-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClassNotFoundException when using GCS path as HA directory</summary>
      <description>Hi,When I am trying to run Flink job in HA mode with GCS path as a HA directory (eg: &amp;#91;gs://flame-poc/ha&amp;#93;) or while starting a job from checkpoints in GCS I am getting following exception:Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback not found at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688) ~[?:?] at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2712) ~[?:?] at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.Groups.&lt;init&gt;(Groups.java:107) ~[?:?] at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.Groups.&lt;init&gt;(Groups.java:102) ~[?:?] at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451) ~[?:?] at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:338) ~[?:?] at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300) ~[?:?] at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575) ~[?:?] at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getUgiUserName(GoogleHadoopFileSystemBase.java:1226) ~[?:?] at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.listStatus(GoogleHadoopFileSystemBase.java:858) ~[?:?] at org.apache.flink.fs.gs.org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.listStatus(HadoopFileSystem.java:170) ~[?:?] Observations:While using File system as a HA path and GCS as checkpointing directory the job is able to write checkpoints to GCS checkpoint path. After debugging what I found was all the org.apache.hadoop paths are shaded to org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop. Ideally the code should look for  org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback instead of  org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.I think it is not getting shaded over here due to reflection being used here:https://github.com/apache/hadoop/blob/branch-3.3.4/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/Groups.java#L108As a workaround I rebuilt flink-gs-fs-hadoop plugin removing this relocation and it worked for me.&lt;relocation&gt;&lt;pattern&gt;org.apache.hadoop&lt;/pattern&gt;&lt;shadedPattern&gt;org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop&lt;/shadedPattern&gt;&lt;/relocation&gt;</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-gs-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="31705" opendate="2023-4-3 00:00:00" fixdate="2023-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Conjars</summary>
      <description>With Conjars no longer being available (only https://conjars.wensel.net/ is there), we should remove all the notices to Conjars in Flink. We've already removed the need for Conjars because we've excluded Pentaho as part of FLINK-27640, which eliminates having any dependency that relies on Conjars.</description>
      <version>None</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.google-mirror-settings.xml</file>
      <file type="M">tools.ci.alibaba-mirror-settings.xml</file>
    </fixedFiles>
  </bug>
  <bug id="31707" opendate="2023-4-3 00:00:00" fixdate="2023-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Constant string cannot be used as input arguments of Pandas UDAF</summary>
      <description>It will throw exceptions as following when using constant strings in Pandas UDAF:E raise ValueError("field_type %s is not supported." % field_type)E ValueError: field_type type_name: CHARE char_info {E length: 3E }E is not supported.</description>
      <version>None</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udaf.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  <bug id="31711" opendate="2023-4-3 00:00:00" fixdate="2023-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>OpenAPI spec omits complete-statement request body</summary>
      <description>The OpenAPI generator omits request bodies for get requests because it is usually a bad idea.Still, the generator shouldn't omit this on it's own.</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.OpenApiSpecGenerator.java</file>
      <file type="M">docs.static.generated.rest.v2.sql.gateway.yml</file>
    </fixedFiles>
  </bug>
  <bug id="31728" opendate="2023-4-4 00:00:00" fixdate="2023-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Scala API dependencies from batch/streaming examples</summary>
      <description>The example modules have leftover Scala API dependencies and build infrastructure. Remove them, along with the scala suffix on these modules.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-fs-tests.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-examples.flink-examples-build-helper.flink-examples-streaming-state-machine.pom.xml</file>
      <file type="M">flink-examples.flink-examples-batch.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-high-parallelism-iterations-test.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3173" opendate="2015-12-15 00:00:00" fixdate="2015-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump org.apache.httpcomponents.httpclient version to 4.2.6</summary>
      <description>Currently, the TwitterStream example is not working with the TwitterSource, because we're using httpclient version 4.2. There seems to be bug in the httpclient which causes the connection to the Twitter stream to fail. With version 4.2.6 this issue is fixed. I propose, therefore, to bump the version to 4.2.6.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="31733" opendate="2023-4-5 00:00:00" fixdate="2023-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Model name clashes in OpenAPI spec</summary>
      <description>The OpenAPi spec uses simple class names for naming models. There are however several models, usually inner classes, that share simple names, like "Summary".This goes undetected and breaks the model for some API calls.</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.util.DocumentingRestEndpoint.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.rest.RestAPIDocGeneratorTest.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.rest.OpenApiSpecGeneratorTest.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.OpenApiSpecGenerator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.JobDetailsInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobVertexTaskManagersInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.TaskCheckpointStatisticsWithSubtaskDetails.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointingStatistics.java</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
    </fixedFiles>
  </bug>
  <bug id="3175" opendate="2015-12-15 00:00:00" fixdate="2015-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaITCase.testOffsetAutocommitTest</summary>
      <description>https://travis-ci.org/apache/flink/jobs/96981797Tests run: 16, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 126.278 sec &lt;&lt;&lt; FAILURE! - in org.apache.flink.streaming.connectors.kafka.KafkaITCasetestOffsetAutocommitTest(org.apache.flink.streaming.connectors.kafka.KafkaITCase) Time elapsed: 12.735 sec &lt;&lt;&lt; FAILURE!java.lang.AssertionError: Offset of o1=-915623761776 was not in range at org.junit.Assert.fail(Assert.java:88) at org.junit.Assert.assertTrue(Assert.java:41) at org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.runOffsetAutocommitTest(KafkaConsumerTestBase.java:333) at org.apache.flink.streaming.connectors.kafka.KafkaITCase.testOffsetAutocommitTest(KafkaITCase.java:56)Results :Failed tests: KafkaITCase.testOffsetAutocommitTest:56-&gt;KafkaConsumerTestBase.runOffsetAutocommitTest:333 Offset of o1=-915623761776 was not in range</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08ITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="31758" opendate="2023-4-10 00:00:00" fixdate="2023-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some external connectors sql client jar has a wrong download url in document</summary>
      <description>After FLINK-30378, we can load sql connector data from external connector's own data file. However, we did not replace $full_version, resulting in an incorrect URL in the download link. for example: https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-mongodb/$full_version/flink-sql-connector-mongodb-$full_version.jar.</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.layouts.shortcodes.sql.connector.download.table.html</file>
    </fixedFiles>
  </bug>
  <bug id="31761" opendate="2023-4-10 00:00:00" fixdate="2023-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Some Typo And Improve lambda expressions.</summary>
      <description>When reading the code, I found 2 typo errors and improved the lambada expression.</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvReaderFormat.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFileFormatFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="31767" opendate="2023-4-11 00:00:00" fixdate="2023-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the implementation for "analyze table" execution on partitioned table</summary>
      <description>Currently, for partitioned table, the "analyze table" command will generate a separate SQL statement for each partition. When there are too many partitions, the compilation/submission/execution time will be very long. This issue aims to improve it: we can combine the sql statements for each partition into one with "union all", and just need to execution one sql.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.AnalyzeTableUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="31792" opendate="2023-4-13 00:00:00" fixdate="2023-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Errors are not reported in the Web UI</summary>
      <description>After FLINK-29747, NzNotificationService can no longer be resolved by injector, and because we're using the injector directly, this is silently ignored.</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.main.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.interceptor.ts</file>
    </fixedFiles>
  </bug>
  <bug id="31797" opendate="2023-4-13 00:00:00" fixdate="2023-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move LeaderElectionService out of LeaderContender</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.util.DocumentingDispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.utils.MockResourceManagerRuntimeServices.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.TestingResourceManagerService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerJobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerHATest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingLeaderElectionService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingLeaderElection.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingContender.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.StandaloneLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.DefaultLeaderElectionServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterExecutionDeploymentReconciliationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.TestingHighAvailabilityServicesBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.TestingHighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.nonha.standalone.StandaloneHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.entrypoint.ClusterEntrypointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunnerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherCleanupITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.SessionRestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.JobRestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.StandaloneLeaderElectionService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.nonha.standalone.StandaloneHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.AbstractHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.DispatcherRunnerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunnerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.JobMasterServiceLeadershipRunnerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="31812" opendate="2023-4-14 00:00:00" fixdate="2023-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SavePoint from /jars/:jarid:/run api on body is not anymore set to null if empty</summary>
      <description>Since https://issues.apache.org/jira/browse/FLINK-29543 the savepointPath from the body is not anymore transform to null if empty: https://github.com/apache/flink/pull/21012/files#diff-c6d9a43d970eb07642a87e4bf9ec6a9dc7d363b1b5b557ed76f73d8de1cc5a54R145 This leads to issue running a flink job in release 1.17 with lyft operator which set savePoint in body to empty string: https://github.com/lyft/flinkk8soperator/blob/master/pkg/controller/flinkapplication/flink_state_machine.go#L721 Issue faced by the job as the savepointPath is setto empty string:org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.3 at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.lambda$new$0(DefaultJobMasterServiceProcess.java:97)4 at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)5 at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)6 at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)7 at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1705)8 at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)9 at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)10 at java.base/java.lang.Thread.run(Thread.java:829)11Caused by: java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: empty checkpoint pointer12 at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:314)13 at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:319)14 at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1702)15 ... 3 more16Caused by: java.lang.IllegalArgumentException: empty checkpoint pointer17 at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)18 at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorageAccess.resolveCheckpointPointer(AbstractFsCheckpointStorageAccess.java:240)19 at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorageAccess.resolveCheckpoint(AbstractFsCheckpointStorageAccess.java:136)20 at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1824)21 at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.tryRestoreExecutionGraphFromSavepoint(DefaultExecutionGraphFactory.java:223)22 at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:198)23 at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:365)24 at org.apache.flink.runtime.scheduler.SchedulerBase.&lt;init&gt;(SchedulerBase.java:210)25 at org.apache.flink.runtime.scheduler.DefaultScheduler.&lt;init&gt;(DefaultScheduler.java:136)26 at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:152)27 at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:119)28 at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:371)29 at org.apache.flink.runtime.jobmaster.JobMaster.&lt;init&gt;(JobMaster.java:348)30 at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:123)31 at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:95)32 at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112)33 at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)34 ... 3 more35</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0,1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="31818" opendate="2023-4-17 00:00:00" fixdate="2023-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>parsing error of &amp;#39;security.kerberos.access.hadoopFileSystems&amp;#39; in flink-conf.yaml</summary>
      <description>There is a parsing error when I gave two or more hdfs namenodes URI separated by commas as the value of key attribute 'security.kerberos.access.hadoopFileSystems'. For example, I set this key attribute and value like below in flink-conf.yaml,security.kerberos.access.hadoopFileSystems: hdfs://hadoop-nn1.testurl.com:8020,hdfs://hadoop-nn2.testurl.com:8020  then, the slash "/" is missing in second URI in parsed valuehdfs://hadoop-nn1.testurl.com:8020,hdfs:/hadoop-nn2.testurl.com:8020  Received error message is here.Caused by: org.apache.flink.util.FlinkRuntimeException: java.io.IOException: Incomplete HDFS URI, no host: hdfs://hadoop-nn1.testurl.com:8020,hdfs:/hadoop-nn2.testurl.com:8020   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.lambda$getFileSystemsToAccess$2(HadoopFSDelegationTokenProvider.java:168) ~[flink-dist-1.17.0.jar:1.17.0]   at java.util.ArrayList.forEach(ArrayList.java:1259) ~[?:1.8.0_362]   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.getFileSystemsToAccess(HadoopFSDelegationTokenProvider.java:157) ~[flink-dist-1.17.0.jar:1.17.0]   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.lambda$obtainDelegationTokens$1(HadoopFSDelegationTokenProvider.java:113) ~[flink-dist-1.17.0.jar:1.17.0]   at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_362]   at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_362]   at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1966) ~[hadoop-common-2.10.0-khp-20210414.jar:?]   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.obtainDelegationTokens(HadoopFSDelegationTokenProvider.java:108) ~[flink-dist-1.17.0.jar:1.17.0]   at org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.lambda$obtainDelegationTokensAndGetNextRenewal$1(DefaultDelegationTokenManager.java:228) ~[flink-dist-1.17.0.jar:1.17.0]   ... 13 moreCaused by: java.io.IOException: Incomplete HDFS URI, no host: hdfs://hadoop-nn1.testurl.com:8020,hdfs:/hadoop-nn2.testurl.com:8020   at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:156) ~[hadoop-hdfs-client-2.10.0-khp-20210414.jar:?]   at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3241) ~[hadoop-common-2.10.0-khp-20210414.jar:?]   at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:122) ~[hadoop-common-2.10.0-khp-20210414.jar:?]   at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3290) ~[hadoop-common-2.10.0-khp-20210414.jar:?]   at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3258) ~[hadoop-common-2.10.0-khp-20210414.jar:?]   at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:471) ~[hadoop-common-2.10.0-khp-20210414.jar:?]   at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356) ~[hadoop-common-2.10.0-khp-20210414.jar:?]   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.lambda$getFileSystemsToAccess$2(HadoopFSDelegationTokenProvider.java:163) ~[flink-dist-1.17.0.jar:1.17.0]   at java.util.ArrayList.forEach(ArrayList.java:1259) ~[?:1.8.0_362]   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.getFileSystemsToAccess(HadoopFSDelegationTokenProvider.java:157) ~[flink-dist-1.17.0.jar:1.17.0]   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.lambda$obtainDelegationTokens$1(HadoopFSDelegationTokenProvider.java:113) ~[flink-dist-1.17.0.jar:1.17.0]   at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_362]   at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_362]   at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1966) ~[hadoop-common-2.10.0-khp-20210414.jar:?]   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.obtainDelegationTokens(HadoopFSDelegationTokenProvider.java:108) ~[flink-dist-1.17.0.jar:1.17.0]   at org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.lambda$obtainDelegationTokensAndGetNextRenewal$1(DefaultDelegationTokenManager.java:228) ~[flink-dist-1.17.0.jar:1.17.0]   ... 13 more  </description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.security.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.security.auth.kerberos.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="31859" opendate="2023-4-19 00:00:00" fixdate="2023-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update maven cyclonedx plugin to 2.7.7</summary>
      <description>there are at least 2 related improvements1. current version depends on jackson-databind 2.14.0 and has a memory issue described at https://github.com/FasterXML/jackson-databind/issues/3665 which is fixed in later versions2. current version leads to lots of traces in logs (e.g. mvn clean verify for flink-core) which is fixed in later versions[ERROR] An error occurred attempting to read POMorg.codehaus.plexus.util.xml.pull.XmlPullParserException: UTF-8 BOM plus xml decl of ISO-8859-1 is incompatible (position: START_DOCUMENT seen &lt;?xml version="1.0" encoding="ISO-8859-1"... @1:42) at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDeclWithVersion (MXParser.java:3423) at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDecl (MXParser.java:3345) at org.codehaus.plexus.util.xml.pull.MXParser.parsePI (MXParser.java:3197) at org.codehaus.plexus.util.xml.pull.MXParser.parseProlog (MXParser.java:1828) at org.codehaus.plexus.util.xml.pull.MXParser.nextImpl (MXParser.java:1757) at org.codehaus.plexus.util.xml.pull.MXParser.next (MXParser.java:1375) at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:3940) at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:612) at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:627) at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:759) at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:746) at org.cyclonedx.maven.BaseCycloneDxMojo.retrieveParentProject (BaseCycloneDxMojo.java:694) at org.cyclonedx.maven.BaseCycloneDxMojo.getClosestMetadata (BaseCycloneDxMojo.java:524) at org.cyclonedx.maven.BaseCycloneDxMojo.convert (BaseCycloneDxMojo.java:481) at org.cyclonedx.maven.CycloneDxMojo.execute (CycloneDxMojo.java:70) at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137) at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:370) at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:351) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:215) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:171) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:163) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81) at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56) at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128) at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:294) at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192) at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105) at org.apache.maven.cli.MavenCli.execute (MavenCli.java:960) at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:293) at org.apache.maven.cli.MavenCli.main (MavenCli.java:196) at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke (Method.java:498) at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282) at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225) at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406) at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)</description>
      <version>1.17.0,1.18.0</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="31963" opendate="2023-4-28 00:00:00" fixdate="2023-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>java.lang.ArrayIndexOutOfBoundsException when scaling down with unaligned checkpoints</summary>
      <description>I'm testing Autoscaler through Kubernetes Operator and I'm facing the following issue.As you know, when a job is scaled down through the autoscaler, the job manager and task manager go down and then back up again.When this happens, an index out of bounds exception is thrown and the state is not restored from a checkpoint.gyfora told me via the Flink Slack troubleshooting channel that this is likely an issue with Unaligned Checkpoint and not an issue with the autoscaler, but I'm opening a ticket with Gyula for more clarification.Please see the attached JM and TM error logs.Thank you.</description>
      <version>1.17.0,1.16.1,1.15.4,1.18.0</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.TaskStateAssignment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="31972" opendate="2023-4-28 00:00:00" fixdate="2023-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove powermock whitebox usages</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.mock.Whitebox.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.CompactingHashTableTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.CompactingHashTable.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroOutputFormatTest.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.test.java.org.apache.flink.formats.avro.registry.confluent.CachedSchemaCoderProviderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.enumerator.KafkaEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.hybrid.HybridSourceSplitEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.hybrid.HybridSourceReaderTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="31974" opendate="2023-4-28 00:00:00" fixdate="2023-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JobManager crashes after KubernetesClientException exception with FatalExitExceptionHandler</summary>
      <description>When resource quota limit is reached JobManager will throw org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods "my-namespace-flink-cluster-taskmanager-1-2" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13. In 1.16.1 , this is handled gracefully:2023-04-28 22:07:24,631 WARN  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Failed requesting worker with resource spec WorkerResourceSpec \{cpuCores=1.0, taskHeapSize=25.600mb (26843542 bytes), taskOffHeapSize=0 bytes, networkMemSize=64.000mb (67108864 bytes), managedMemSize=230.400mb (241591914 bytes), numSlots=4}, current pending count: 0java.util.concurrent.CompletionException: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods "my-namespace-flink-cluster-taskmanager-1-138" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.        at java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]        at java.util.concurrent.CompletableFuture.completeThrowable(Unknown Source) ~[?:?]        at java.util.concurrent.CompletableFuture$AsyncRun.run(Unknown Source) ~[?:?]        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]        at java.lang.Thread.run(Unknown Source) ~[?:?]Caused by: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods "my-namespace-flink-cluster-taskmanager-1-138" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:684) ~[flink-dist-1.16.1.jar:1.16.1]        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:664) ~[flink-dist-1.16.1.jar:1.16.1]        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:613) ~[flink-dist-1.16.1.jar:1.16.1]        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:558) ~[flink-dist-1.16.1.jar:1.16.1]        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:521) ~[flink-dist-1.16.1.jar:1.16.1]        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:308) ~[flink-dist-1.16.1.jar:1.16.1]        at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:644) ~[flink-dist-1.16.1.jar:1.16.1]        at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:83) ~[flink-dist-1.16.1.jar:1.16.1]        at io.fabric8.kubernetes.client.dsl.base.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:61) ~[flink-dist-1.16.1.jar:1.16.1]        at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$createTaskManagerPod$1(Fabric8FlinkKubeClient.java:163) ~[flink-dist-1.16.1.jar:1.16.1]        ... 4 moreBut , in Flink 1.17.0 , Job Manager crashes:2023-04-28 20:50:50,534 ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.actor.default-dispatcher-15' produced an uncaught exception. Stopping the process...java.util.concurrent.CompletionException: org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods "my-namespace-flink-cluster-taskmanager-1-2" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.        at java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]        at java.util.concurrent.CompletableFuture.completeThrowable(Unknown Source) ~[?:?]        at java.util.concurrent.CompletableFuture$AsyncRun.run(Unknown Source) ~[?:?]        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]        at java.lang.Thread.run(Unknown Source) ~[?:?]Caused by: org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods "my-namespace-flink-cluster-taskmanager-1-2" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:684) ~[flink-dist-1.17.0.jar:1.17.0]        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:664) ~[flink-dist-1.17.0.jar:1.17.0]        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:613) ~[flink-dist-1.17.0.jar:1.17.0]        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:558) ~[flink-dist-1.17.0.jar:1.17.0]        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:521) ~[flink-dist-1.17.0.jar:1.17.0]        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:308) ~[flink-dist-1.17.0.jar:1.17.0]        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:644) ~[flink-dist-1.17.0.jar:1.17.0]        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:83) ~[flink-dist-1.17.0.jar:1.17.0]        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:61) ~[flink-dist-1.17.0.jar:1.17.0]        at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$createTaskManagerPod$1(Fabric8FlinkKubeClient.java:163) ~[flink-dist-1.17.0.jar:1.17.0]        ... 4 more</description>
      <version>1.17.0,1.18.0</version>
      <fixedVersion>1.18.0,1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriverTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="31996" opendate="2023-5-4 00:00:00" fixdate="2023-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Chaining operators with different max parallelism prevents rescaling</summary>
      <description>We might chain operators with different max parallelism together if they are set to have the same parallelism initially.When we decide to rescale the JobGraph vertices (using AdaptiveScheduler), we're gapped by the lowest maxParallelism of the operator chain. This is especially visible with things like CollectSink, TwoPhaseCommitSink, CDC, and a GlobalCommiter with maxParallelism set to 1. An obvious solution would be to prevent the chaining of operators with different maxParallelism, but we need to double-check this doesn't introduce a breaking change.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.StreamingScalaAPICompletenessTest.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.PipelineOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.pipeline.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="31997" opendate="2023-5-4 00:00:00" fixdate="2023-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update to Fabric8 6.5.1+ in flink-kubernetes</summary>
      <description>We should update the fabric8 version in flink-kubernetes to at least 6.5.1. Flink currently uses a very old fabric8 version. The fabric8 library dependencies have since been revised and greately improved to make them more moduler and allow eliminating securitiy vulnerabilities more easily like: https://issues.apache.org/jira/browse/FLINK-31815The newer versions especially 6.5.1 + also add some improvement stability fixes for watches and other parts.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.MixedKubernetesServerExtension.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesPodTemplateTestUtils.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesClusterDescriptorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClientTest.java</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesClusterDescriptor.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesClusterClientFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesLeaderElector.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClientFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32001" opendate="2023-5-4 00:00:00" fixdate="2023-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SupportsRowLevelUpdate does not support returning only a part of the columns.</summary>
      <description>FLIP-282 introduces the new Delete and Update API in Flink SQL. Although it is described in the documentation that in case of partial-update we only need to return the primary key columns and the updated columns.But in fact, the topology of the job  is source -&gt; cal -&gt; constraintEnforcer -&gt; sink, and the constraint check will be performed in the operator of constraintEnforcer, which is done according to index, not according to column. If only some columns are returned, the constraint check is wrong, and it is easy to generate ArrayIndexOutOfBoundsException.</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.UpdateTableITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.DeleteTableITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestUpdateDeleteTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.sink.RowLevelUpdateSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.sink.RowLevelDeleteSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSinkUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.delete.q</file>
    </fixedFiles>
  </bug>
  <bug id="32008" opendate="2023-5-5 00:00:00" fixdate="2023-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Protobuf format cannot work with FileSystem Connector</summary>
      <description>The protobuf format throws exception when working with Map data type. I uploaded a example project to reproduce the problem. Caused by: java.lang.RuntimeException: One or more fetchers have encountered exception    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:261)    at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)    at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:131)    at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:417)    at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:550)    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:839)    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:788)    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:931)    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)    at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:165)    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:114)    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)    at java.util.concurrent.FutureTask.run(FutureTask.java:266)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)    ... 1 moreCaused by: java.io.IOException: Failed to deserialize PB object.    at org.apache.flink.formats.protobuf.deserialize.PbRowDataDeserializationSchema.deserialize(PbRowDataDeserializationSchema.java:75)    at org.apache.flink.formats.protobuf.deserialize.PbRowDataDeserializationSchema.deserialize(PbRowDataDeserializationSchema.java:42)    at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82)    at org.apache.flink.connector.file.table.DeserializationSchemaAdapter$LineBytesInputFormat.readRecord(DeserializationSchemaAdapter.java:197)    at org.apache.flink.connector.file.table.DeserializationSchemaAdapter$LineBytesInputFormat.nextRecord(DeserializationSchemaAdapter.java:210)    at org.apache.flink.connector.file.table.DeserializationSchemaAdapter$Reader.readBatch(DeserializationSchemaAdapter.java:124)    at org.apache.flink.connector.file.src.util.RecordMapperWrapperRecordIterator$1.readBatch(RecordMapperWrapperRecordIterator.java:82)    at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:67)    at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:162)    ... 6 moreCaused by: java.lang.reflect.InvocationTargetException    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    at java.lang.reflect.Method.invoke(Method.java:498)    at org.apache.flink.formats.protobuf.deserialize.ProtoToRowConverter.convertProtoBinaryToRow(ProtoToRowConverter.java:129)    at org.apache.flink.formats.protobuf.deserialize.PbRowDataDeserializationSchema.deserialize(PbRowDataDeserializationSchema.java:70)    ... 15 moreCaused by: com.google.protobuf.InvalidProtocolBufferException: While parsing a protocol message, the input ended unexpectedly in the middle of a field.  This could mean either that the input has been truncated or that an embedded message misreported its own length.    at com.google.protobuf.InvalidProtocolBufferException.truncatedMessage(InvalidProtocolBufferException.java:115)    at com.google.protobuf.CodedInputStream$ArrayDecoder.pushLimit(CodedInputStream.java:1196)    at com.google.protobuf.CodedInputStream$ArrayDecoder.readMessage(CodedInputStream.java:887)    at com.example.proto.MapMessage.&lt;init&gt;(MapMessage.java:64)    at com.example.proto.MapMessage.&lt;init&gt;(MapMessage.java:9)    at com.example.proto.MapMessage$1.parsePartialFrom(MapMessage.java:756)    at com.example.proto.MapMessage$1.parsePartialFrom(MapMessage.java:750)    at com.google.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:158)    at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:191)    at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:203)    at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:208)    at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:48)    at com.example.proto.MapMessage.parseFrom(MapMessage.java:320)    ... 21 more</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-protobuf.src.test.java.org.apache.flink.formats.protobuf.ProtobufSQLITCaseTest.java</file>
      <file type="M">flink-formats.flink-protobuf.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-formats.flink-protobuf.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32027" opendate="2023-5-8 00:00:00" fixdate="2023-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Batch jobs could hang at shuffle phase when max parallelism is really large</summary>
      <description>In batch stream mode with adaptive batch schedule mode, If we set the max parallelism large as 32768 (pipeline.max-parallelism), the job could hang at the shuffle phase:It would hang for a long time and show "No bytes sent": After some time to debug, we can see the downstream operator did not receive the end-of-partition event.</description>
      <version>1.16.0,1.17.0,1.16.1</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriteReadTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFile.java</file>
    </fixedFiles>
  </bug>
  <bug id="32030" opendate="2023-5-8 00:00:00" fixdate="2023-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client gateway mode should accept URLs</summary>
      <description>Currently, the --endpoint parameter has to be specified in the InetSocketAddress  format, i.e. hostname:port. While this works fine for basic use cases, it does not support the placement of the gateway behind a proxy or using an Ingress for routing to a specific Flink cluster based on the URL path.  I.e. it expects some.hostname.com:9001  to directly serve requests on some.hostname.com:9001/v1 . Mapping to a non-root location, i.e. some.hostname.com:9001/flink-clusters/sql-preview-cluster-1/v1  is not supported. Since the client talks to the gateway via its REST endpoint, the right format for the --endpoint  parameter is URL, not InetSocketAddress .  The same --endpoint parameter can be reused if the changes are implemented in a backwards-compatible way.</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.SqlClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.ExecutorImpl.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptionsParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClient.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.NetUtilsTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.NetUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="32043" opendate="2023-5-10 00:00:00" fixdate="2023-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SqlClient session unrecoverable once one wrong setting occurred</summary>
      <description>In sql client, it can not work normally once one wrong setting occurred// wrong setting hereFlink SQL&gt; SET table.sql-dialect = flink;[INFO] Execute statement succeed.Flink SQL&gt; select '' AS f1, a from t1;[ERROR] Could not execute SQL statement. Reason:java.lang.IllegalArgumentException: No enum constant org.apache.flink.table.api.SqlDialect.FLINKFlink SQL&gt; SET table.sql-dialect = default;[ERROR] Could not execute SQL statement. Reason:java.lang.IllegalArgumentException: No enum constant org.apache.flink.table.api.SqlDialect.FLINKFlink SQL&gt; RESET table.sql-dialect;[ERROR] Could not execute SQL statement. Reason:java.lang.IllegalArgumentException: No enum constant org.apache.flink.table.api.SqlDialect.FLINKFlink SQL&gt; RESET;[ERROR] Could not execute SQL statement. Reason:java.lang.IllegalArgumentException: No enum constant org.apache.flink.table.api.SqlDialect.FLINK</description>
      <version>1.17.0,1.18.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.AbstractSqlGatewayStatementITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.SessionContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="32110" opendate="2023-5-16 00:00:00" fixdate="2023-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TM native memory leak when using time window in Pyflink ThreadMode</summary>
      <description>If job use time window in Pyflink thread mode, TM native memory will grow slowly during the job running until TM can't allocate memory from operate system.The leak rate is likely proportional to the number of key.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.EmbeddedPythonWindowOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="32112" opendate="2023-5-16 00:00:00" fixdate="2023-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the deprecated state backend sample config in Chinese document</summary>
      <description>Current Version Avaliable State Backends : HashMapStateBackend EmbeddedRocksDBStateBackend But in the Operations/State &amp; Fault Tolerance page of flink v1.17.0, a sample section in the configuration set state.backend: filesystem  in zh-doc.The correct configuration should be:  state.backend: hashmap I think it may cause misunderstandings for users.</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.ops.state.state.backends.md</file>
    </fixedFiles>
  </bug>
  <bug id="3216" opendate="2016-1-11 00:00:00" fixdate="2016-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Define pattern specification</summary>
      <description>In order to detect event patterns we first have to define the pattern. This issue tracks the progress of implementing a user facing API to define event patterns. Patterns should support the following operations next(): The given event has to follow directly after the preceding eventfollowedBy(): The given event has to follow the preceding event. There might occur other events in-between every(): In a follow-by relationship a starting event can be matched with multiple successive events. Consider the pattern a → b where → denotes the follow-by relationship. The event sequence a, b, b can be matched as a, b or a, (b), b where the first b is left out. The essential question is whether a is allowed to match multiple times or only the first time. The method every specifies exactly that. Every events in a pattern can match with multiple successive events. This makes only sense in a follow-by relationship, though. followedByEvery(): Similar to followedBy just that the specified element can be matched with multiple successive events or(): Alternative event which can be matched instead of the original event: every(“e1”).where().or(“e2”).where() within(): Defines a time interval in which the pattern has to be completed, otherwise an incomplete pattern can be emitted (timeout case)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
      <file type="M">flink-libraries.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32174" opendate="2023-5-24 00:00:00" fixdate="2023-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Cloudera product and link in doc page</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="32217" opendate="2023-5-30 00:00:00" fixdate="2023-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retain metric store can cause NPE</summary>
      <description>When metricsFetcher fetches metrics, it will update the metricsStore (here). But in this method, it can get null metricStore and cause NPE, which will lead to incorrect results of metrics retain, and we should also fix it from the perspective of stability.</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.metrics.MetricStoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.metrics.MetricStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="32220" opendate="2023-5-31 00:00:00" fixdate="2023-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improving the adaptive local hash agg code to avoid get value from RowData repeatedly</summary>
      <description></description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ProjectionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="32581" opendate="2023-7-12 00:00:00" fixdate="2023-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for atomic CTAS</summary>
      <description>add docs for atomic CTAS</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.docs.dev.table.sourcesSinks.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sourcesSinks.md</file>
    </fixedFiles>
  </bug>
  <bug id="32755" opendate="2023-8-4 00:00:00" fixdate="2023-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add quick start guide for Flink OLAP</summary>
      <description>I propose to add a new QUICKSTART.md guide that provides instructions for beginner to build a production ready Flink OLAP Service by using flink-jdbc-driver, flink-sql-gateway and flink session cluster.</description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="32771" opendate="2023-8-7 00:00:00" fixdate="2023-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the invalid config option slotmanager.request-timeout</summary>
      <description>slotmanager.request-timeout has been deprecated since 1.6 and is not used after the removal of SlotManagerImpl. We can simply remove it now.</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerConfigurationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerConfigurationBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManagerBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerConfiguration.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ResourceManagerOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="32888" opendate="2023-8-17 00:00:00" fixdate="2023-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>File upload runs into EndOfDataDecoderException</summary>
      <description>With the right request the FIleUploadHandler runs into a EndOfDataDecoderException although everything is fine.</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.FileUploadHandlerITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.FileUploadHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.MultipartUploadExtension.java</file>
    </fixedFiles>
  </bug>
  <bug id="33053" opendate="2023-9-7 00:00:00" fixdate="2023-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Watcher leak in Zookeeper HA mode</summary>
      <description>We observe a watcher leak in our OLAP stress test when enabling Zookeeper HA mode. TM's watches on the leader of JobMaster has not been stopped after job finished.Here is how we re-produce this issue: Start a session cluster and enable Zookeeper HA mode. Continuously and concurrently submit short queries, e.g. WordCount to the cluster. echo -n wchp | nc {zk host} {zk port} to get current watches.We can see a lot of watches on /flink/{cluster_name}/leader/{job_id}/connection_info.</description>
      <version>1.17.0,1.18.0,1.17.1</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
