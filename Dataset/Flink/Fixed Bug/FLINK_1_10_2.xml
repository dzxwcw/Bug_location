<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="12678" opendate="2019-5-29 00:00:00" fixdate="2019-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add AbstractCatalog to manage the common catalog name and default database name for catalogs</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.GenericInMemoryCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="12679" opendate="2019-5-29 00:00:00" fixdate="2019-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support &amp;#39;default-database&amp;#39; config for catalog entries in SQL CLI yaml file</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.descriptors.CatalogDescriptorValidator.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.descriptors.CatalogDescriptor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-catalogs.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="12683" opendate="2019-5-30 00:00:00" fixdate="2019-6-30 01:00:00" resolution="Done">
    <buginformation>
      <summary>Provide task manager&amp;#39;s location information for checkpoint coordinator specific log messages</summary>
      <description>Currently, the AcknowledgeCheckpoint does not contain the task manager's location information. When a task's snapshot task sends an ack message to the coordinator, we can only log this message:Received late message for now expired checkpoint attempt 6035 from ccd88d08bf82245f3466c9480fb5687a of job 775ef8ff0159b071da7804925bbd362f.Sometimes we need to get this sub task's location information to do the further debug work, e.g. stack trace dump. But, without the location information, It will not help to quickly locate the problem. </description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.FailoverRegionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ConcurrentFailoverStrategyExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorMasterHooksTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.LegacyScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="12685" opendate="2019-5-31 00:00:00" fixdate="2019-6-31 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Supports UNNEST query in blink planner</summary>
      <description>this issue aim to support queries with UNNEST keyword, which relate to nested fields.for example: table name: MyTableschema: a: int, b int, c array&amp;#91;int&amp;#93;sql:SELECT a, b, s FROM MyTable, UNNEST(MyTable.c) AS A (s)</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.AggregateReduceGroupingITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkDecorrelateProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkBatchProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.WindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
    </fixedFiles>
  </bug>
  <bug id="16432" opendate="2020-3-5 00:00:00" fixdate="2020-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Building Hive connector gives problems</summary>
      <description>When building the current Flink source I keep running to problems with the hive connector.The problems focus around dependencies that are not available by default: org.pentaho:pentaho-aggdesigner-algorithm javax.jms:jms</description>
      <version>1.10.2,1.11.0</version>
      <fixedVersion>1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17342" opendate="2020-4-23 00:00:00" fixdate="2020-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schedule savepoint if max-inflight-checkpoints limit is reached instead of forcing (in UC mode)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SchedulerTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.AdaptedRestartPipelinedRegionStrategyNGAbortPendingCheckpointsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreMockitoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.FailoverStrategyCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStatsTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointPropertiesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTriggeringTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorRestoringTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorMasterHooksTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.CheckpointCoordinatorConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.Checkpoints.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDecider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointProperties.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureReason.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="18035" opendate="2020-5-29 00:00:00" fixdate="2020-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Executors#newCachedThreadPool could not work as expected</summary>
      <description>In FLINK-17558, we introduce Executors#newCachedThreadPool to create dedicated thread pool for TaskManager io. However, it could not work as expected.The root cause is about the following constructor of ThreadPoolExecutor. Only when the workQueue is full, new thread will be started then. So if we set a LinkedBlockingQueue with Integer.MAX_VALUE capacity, only one thread will be started. It never grows up. public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory)</description>
      <version>1.10.2,1.11.0</version>
      <fixedVersion>1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="18902" opendate="2020-8-12 00:00:00" fixdate="2020-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot serve results of asynchronous REST operations in per-job mode</summary>
      <description>Due to the changes introduced with FLINK-18663 a Flink per-job cluster can no longer properly shut down. The problem is that we no longer serve asynchronous results (e.g. resulting from a cancel-with-savepoint operation) while the cluster waits to shut down.In order to solve this problem, Flink needs to serve REST request also while it waits for shutting itself down.</description>
      <version>1.10.2,1.11.2,1.12.0</version>
      <fixedVersion>1.10.2,1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.InFlightRequestTrackerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.InFlightRequestTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.AbstractHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="19109" opendate="2020-9-1 00:00:00" fixdate="2020-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split Reader eats chained periodic watermarks</summary>
      <description>Attempting to generate watermarks chained to the Split Reader / ContinuousFileReaderOperator, as inSingleOutputStreamOperator&lt;Event&gt; results = env .readTextFile(...) .map(...) .assignTimestampsAndWatermarks(bounded) .keyBy(...) .process(...);leads to the Watermarks failing to be produced. Breaking the chain, via disableOperatorChaining() or a rebalance, works around the bug. Using punctuated watermarks also avoids the issue.Looking at this in the debugger reveals that timer service is being prematurely quiesced.In many respects this is FLINK-7666 brought back to life.The problem is not present in 1.9.3.There's a minimal reproducible example in https://github.com/alpinegizmo/flink-question-001/tree/bug.</description>
      <version>1.10.0,1.10.1,1.10.2,1.11.0,1.11.1</version>
      <fixedVersion>1.10.3,1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImplTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="19110" opendate="2020-9-1 00:00:00" fixdate="2020-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flatten current PyFlink documentation structure</summary>
      <description>The navigation for this entire section is overly complex. I would much rather see something flatter, like this: Python API Installation Table API Tutorial Table API User's Guide DataStream API User's Guide FAQ</description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.try-flink.python.table.api.zh.md</file>
      <file type="M">docs.try-flink.python.table.api.md</file>
      <file type="M">docs.ops.python.shell.zh.md</file>
      <file type="M">docs.ops.python.shell.md</file>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.sql.alter.zh.md</file>
      <file type="M">docs.dev.table.sql.alter.md</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.python.udfs.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.index.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.types.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.types.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.config.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.config.md</file>
      <file type="M">docs.dev.python.user-guide.table.metrics.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.metrics.md</file>
      <file type="M">docs.dev.python.user-guide.table.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.index.md</file>
      <file type="M">docs.dev.python.user-guide.table.dependency.management.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.dependency.management.md</file>
      <file type="M">docs.dev.python.user-guide.table.conversion.of.pandas.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.conversion.of.pandas.md</file>
      <file type="M">docs.dev.python.user-guide.table.built.in.functions.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.built.in.functions.md</file>
      <file type="M">docs.dev.python.user-guide.table.10.minutes.to.table.api.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.10.minutes.to.table.api.md</file>
      <file type="M">docs.dev.python.user-guide.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.index.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.index.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.data.types.zh.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.data.types.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.table.api.tutorial.zh.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.table.api.tutorial.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.index.zh.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.index.md</file>
      <file type="M">docs.dev.python.getting-started.installation.zh.md</file>
      <file type="M">docs.dev.python.getting-started.installation.md</file>
      <file type="M">docs.dev.python.getting-started.index.zh.md</file>
      <file type="M">docs.dev.python.getting-started.index.md</file>
      <file type="M">docs.dev.python.faq.zh.md</file>
      <file type="M">docs.dev.python.faq.md</file>
    </fixedFiles>
  </bug>
  <bug id="19252" opendate="2020-9-16 00:00:00" fixdate="2020-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jaas file created under io.tmp.dirs - folder not created if not exists</summary>
      <description>https://issues.apache.org/jira/browse/FLINK-14433 The security module installation happens before the tmp directory check: https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskManagerServices.java#L390</description>
      <version>1.10.0,1.10.1,1.10.2</version>
      <fixedVersion>1.10.3,1.11.3,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.modules.JaasModuleTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.modules.JaasModule.java</file>
    </fixedFiles>
  </bug>
  <bug id="19259" opendate="2020-9-16 00:00:00" fixdate="2020-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use classloader release hooks with Kinesis producer to avoid metaspace leak</summary>
      <description>FLINK-17554 introduced hooks for clearing references before unloading a classloader.The Kinesis Producer library is currently preventing the usercode classloader from being unloaded because it keeps references around.This ticket is to use the hooks with the Kinesis producer.</description>
      <version>None</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisProducer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19410" opendate="2020-9-25 00:00:00" fixdate="2020-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RestAPIStabilityTest does not assert on enum changes</summary>
      <description>org.apache.flink.runtime.rest.compatibility.RestAPIStabilityTest does not assert on enum changes as happened in FLINK-16866.The consequence is that the test is not failing even though there was a REST API change.</description>
      <version>1.10.2,1.11.2</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.rest.compatibility.CompatibilityRoutines.java</file>
    </fixedFiles>
  </bug>
  <bug id="19435" opendate="2020-9-27 00:00:00" fixdate="2020-12-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deadlock when loading different driver classes concurrently using Class.forName</summary>
      <description>when we sink data to multi jdbc outputformat , protected void establishConnection() throws SQLException, ClassNotFoundException { Class.forName(drivername); if (username == null) { connection = DriverManager.getConnection(dbURL); } else { connection = DriverManager.getConnection(dbURL, username, password); }}may cause jdbc driver deadlock. it need to change to synchronized function.</description>
      <version>1.10.2</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunction.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcLookupFunction.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.JdbcInputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19449" opendate="2020-9-29 00:00:00" fixdate="2020-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LEAD/LAG cannot work correctly in streaming mode</summary>
      <description></description>
      <version>1.10.2,1.11.2</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalLocalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalGlobalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLocalWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGlobalWindowAggregate.java</file>
      <file type="M">docs.data.sql.functions.yml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.OverAggregateITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="1945" opendate="2015-4-27 00:00:00" fixdate="2015-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make python tests less verbose</summary>
      <description>Currently, the python tests print a lot of log messages to stdout. Furthermore there seems to be some println statements which clutter the console output. I think that these log messages are not required for the tests and thus should be suppressed.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.type.deduction.py</file>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.types.py</file>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.text.py</file>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.main.py</file>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.csv.py</file>
      <file type="M">flink-libraries.flink-python.src.test.java.org.apache.flink.python.api.PythonPlanBinderTest.java</file>
      <file type="M">flink-libraries.flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19450" opendate="2020-9-29 00:00:00" fixdate="2020-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize the Python CI Test</summary>
      <description>Currently, the CI test of PyFlink will run 4 versions of Python, which takes a lot of time. We will optimize CI test to run only one version of Python. And then nightly test will run all versions of Python.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19492" opendate="2020-10-2 00:00:00" fixdate="2020-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Consolidate Source Events between Source API and Split Reader API</summary>
      <description>The Source API (flink-core) and the SplitReader API (flink-connector-base) have currently separate copies of events to request splits and to signal that no splits are available.We should consolidate those in flink-core.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.lib.util.SplitRequestEvent.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.lib.util.NoSplitAvailableEvent.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.lib.util.IteratorSourceReader.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.lib.util.IteratorSourceEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.FileSourceHeavyThroughputTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.impl.StaticFileSplitEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.impl.FileSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.impl.ContinuousFileSplitEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.mocks.MockSplitEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SourceReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.event.RequestSplitEvent.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.event.NoMoreSplitsEvent.java</file>
    </fixedFiles>
  </bug>
  <bug id="19498" opendate="2020-10-3 00:00:00" fixdate="2020-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port LocatableInputSplitAssigner to new File Source API</summary>
      <description>The new File Source API needs a locality aware input split assigner.To preserve the experience, I suggest to port the existing LocatableInputSplitAssigner from the InputFormat API.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.FileSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="1956" opendate="2015-4-29 00:00:00" fixdate="2015-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Runtime context not initialized in RichWindowMapFunction</summary>
      <description>Trying to access the runtime context in a rich window map function results in an exception. The following snippet demonstrates the bug: env.generateSequence(0, 1000) .window(Count.of(10)) .mapWindow(new RichWindowMapFunction&lt;Long, Tuple2&lt;Long, Long&gt;&gt;() { @Override public void mapWindow(Iterable&lt;Long&gt; input, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out) throws Exception { long self = getRuntimeContext().getIndexOfThisSubtask(); for (long value : input) { out.collect(new Tuple2&lt;&gt;(self, value)); } } }).flatten().print();</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowMapper.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowFolder.java</file>
    </fixedFiles>
  </bug>
  <bug id="19762" opendate="2020-10-22 00:00:00" fixdate="2020-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Selecting Job-ID and TaskManager-ID in web UI covers more than the ID</summary>
      <description>Not only the ID is selected when trying to copy the Job ID from the web UI by double-clicking it. See the attached screenshot: The same thing happens for the TaskManager ID in the corresponding TaskManager Overview page.</description>
      <version>1.10.2,1.11.2</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.status.task-manager-status.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.status.task-manager-status.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.status.job-status.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.status.job-status.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="19764" opendate="2020-10-22 00:00:00" fixdate="2020-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add More Metrics to TaskManager in Web UI</summary>
      <description>update the UI since https://issues.apache.org/jira/browse/FLINK-14422 and https://issues.apache.org/jira/browse/FLINK-14406 has been fixed</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.task-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.task-manager.ts</file>
    </fixedFiles>
  </bug>
  <bug id="19767" opendate="2020-10-22 00:00:00" fixdate="2020-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add AbstractSlotPoolFactory</summary>
      <description>FLINK-19314 introduces another slot pool implementation, with the factory for it being a carbon copy of the factory for the existing slot pool factory.We can introduce an abstract factory class to reduce code duplication.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.DefaultSlotPoolFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="20018" opendate="2020-11-6 00:00:00" fixdate="2020-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>pipeline.cached-files option cannot escape &amp;#39;:&amp;#39; in path</summary>
      <description>pipeline.cached-files option cannot escape ':' in pathe.g. When setting it to `name:file1,path:oss://bucket/file1` The path of file1 is parsed as oss. And there are no way to escape the ':' in path.</description>
      <version>1.10.2,1.11.2</version>
      <fixedVersion>1.10.3,1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironmentComplexConfigurationTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.StructuredOptionsSplitterTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.StructuredOptionsSplitter.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigurationUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.cache.DistributedCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="2002" opendate="2015-5-12 00:00:00" fixdate="2015-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Iterative test fails when ran with other tests in the same environment</summary>
      <description>I run tests in the same StreamExecutionEnvironment with MultipleProgramsTestBase. One of the tests uses an iterative data stream. It fails as well as all tests after that. (When I put the iterative test in a separate environment, all tests passes.) For me it seems that it is a state-related issue but there is also some problem with the broker slots.The iterative test throws:java.lang.Exception: TaskManager sent illegal state update: CANCELING at org.apache.flink.runtime.executiongraph.ExecutionGraph.updateState(ExecutionGraph.java:618) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1$$anonfun$applyOrElse$2.apply$mcV$sp(JobManager.scala:222) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1$$anonfun$applyOrElse$2.apply(JobManager.scala:221) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1$$anonfun$applyOrElse$2.apply(JobManager.scala:221) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:401) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1.applyOrElse(JobManager.scala:314) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:36) at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:29) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118) at org.apache.flink.runtime.ActorLogMessages$$anon$1.applyOrElse(ActorLogMessages.scala:29) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:95) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.pollAndExecAll(ForkJoinPool.java:1253) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1346) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Not enough free slots available to run the job. You can decrease the operator parallelism or increase the number of slots per TaskManager in the configuration. Task to schedule: &lt; Attempt #0 (GroupedActiveDiscretizer (2/4)) @ (unassigned) - &amp;#91;SCHEDULED&amp;#93; &gt; with groupID &lt; e8f7c9c85e64403962648bc7e2aead8b &gt; in sharing group &lt; SlotSharingGroup &amp;#91;5e62f1cc5cae2c088430ef935470a8d5, 5bc227941969d1daa1ebb1ba070b55ce, d999ee6c10730775a8fef1c6f1af1dbd, 45b73caa75424d84adbb7bb92671591d, 5c94c54d9316b827c6eba6c721329549, 794d6c56bee347dcdd62ffdf189de267, 4c3b72e17a4acecde4241fe6e63355b8, f6a6028c224a7b81e4802eeaf9c8487e, 989c68790fc7c5e2f8b8c150a33fef89, db93daa1f9e5194f0079df2629b08efb, bf7dbb1fd756ce322249eb973844b375, 9ddf3bd146c21c574077c58a1f64aeaa, e888ff4653070b9c4adcbb22a8121292, 9c620fd6d784bc4f5d7e100ad1dcb442, e8f7c9c85e64403962648bc7e2aead8b, 4fa798b9eab295876fdd21aeb6c7cfec, 32851c5f48ac128f71df0ec76f5b5ccd, c3f65a51704444b676cd392fbda91872&amp;#93; &gt;. Resources available to scheduler: Number of instances=1, total number of slots=1, available slots=0 at org.apache.flink.runtime.jobmanager.scheduler.Scheduler.scheduleTask(Scheduler.java:212) at org.apache.flink.runtime.jobmanager.scheduler.Scheduler.scheduleImmediately(Scheduler.java:110) at org.apache.flink.runtime.executiongraph.Execution.scheduleForExecution(Execution.java:263) at org.apache.flink.runtime.executiongraph.ExecutionVertex.scheduleForExecution(ExecutionVertex.java:437) at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.scheduleAll(ExecutionJobVertex.java:306) at org.apache.flink.runtime.executiongraph.ExecutionGraph.scheduleForExecution(ExecutionGraph.java:447) at org.apache.flink.runtime.jobmanager.JobManager.org$apache$flink$runtime$jobmanager$JobManager$$submitJob(JobManager.scala:580) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1.applyOrElse(JobManager.scala:194) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:36) at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:29) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118) at org.apache.flink.runtime.ActorLogMessages$$anon$1.applyOrElse(ActorLogMessages.scala:29) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:95) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ... 2 morewhile the following tests throw:java.lang.Exception: Error setting up runtime environment: java.lang.RuntimeException: Could not register the given element, broker slot is already occupied. at org.apache.flink.runtime.execution.RuntimeEnvironment.&lt;init&gt;(RuntimeEnvironment.java:192) at org.apache.flink.runtime.taskmanager.TaskManager.org$apache$flink$runtime$taskmanager$TaskManager$$initializeTask(TaskManager.scala:855) at org.apache.flink.runtime.taskmanager.TaskManager$$anonfun$submitTask$1.apply$mcV$sp(TaskManager.scala:799) at org.apache.flink.runtime.taskmanager.TaskManager$$anonfun$submitTask$1.apply(TaskManager.scala:799) at org.apache.flink.runtime.taskmanager.TaskManager$$anonfun$submitTask$1.apply(TaskManager.scala:799) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:401) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.RuntimeException: java.lang.RuntimeException: Could not register the given element, broker slot is already occupied. at org.apache.flink.streaming.runtime.tasks.StreamIterationHead.setInputsOutputs(StreamIterationHead.java:65) at org.apache.flink.streaming.runtime.tasks.StreamTask.registerInputOutput(StreamTask.java:86) at org.apache.flink.runtime.execution.RuntimeEnvironment.&lt;init&gt;(RuntimeEnvironment.java:189) ... 12 moreCaused by: java.lang.RuntimeException: Could not register the given element, broker slot is already occupied. at org.apache.flink.runtime.iterative.concurrent.Broker.handIn(Broker.java:39) at org.apache.flink.streaming.runtime.tasks.StreamIterationHead.setInputsOutputs(StreamIterationHead.java:62) ... 14 more</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.complex.ComplexIntegrationTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="20496" opendate="2020-12-5 00:00:00" fixdate="2020-3-5 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>RocksDB partitioned index filter option</summary>
      <description>When using RocksDBStateBackend and enabling state.backend.rocksdb.memory.managed and state.backend.rocksdb.memory.fixed-per-slot, flink will strictly limited rocksdb memory usage which contains "write buffer" and "block cache". With these options rocksdb stores index and filters in block cache, because in default options index/filters can grows unlimited. But it's lead another issue, if high-priority cache(configure by state.backend.rocksdb.memory.high-prio-pool-ratio) can't fit all index/filters blocks, it will load all metadata from disk when cache missed, and program went extremely slow. According to Partitioned Index Filters[1], we can enable two-level index having acceptable performance when index/filters cache missed. Enable these options can get over 10x faster in my case&amp;#91;2&amp;#93;, I think we can add an option state.backend.rocksdb.partitioned-index-filters and default value is false, so we can use this feature easily.&amp;#91;1&amp;#93; Partitioned Index Filters: https://github.com/facebook/rocksdb/wiki/Partitioned-Index-Filters&amp;#91;2&amp;#93; Deduplicate scenario, state.backend.rocksdb.memory.fixed-per-slot=256M, SSD, elapsed time 4.91ms -&gt; 0.33ms.</description>
      <version>1.10.2,1.11.2,1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainerTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBMemoryControllerUtilsTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBSharedResources.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMemoryControllerUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMemoryConfiguration.java</file>
      <file type="M">docs.layouts.shortcodes.generated.state.backend.rocksdb.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.configuration.html</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBConfigurableOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.DefaultConfigurableOptionsFactory.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.configurable.configuration.html</file>
    </fixedFiles>
  </bug>
</bugrepository>
