<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="23021" opendate="2021-6-17 00:00:00" fixdate="2021-7-17 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Check for illegal modifications of JobGraph with finished operators</summary>
      <description>Users might modify the job topology before restart for external checkpoint and savepoint. To overcome this issue, we would need to check if a fully finished operator has been added after a non-fully-finished operator. If so, we would throw exception to disallow this situation or re-mark the fully finished operator as alive. </description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorRestoringTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="27246" opendate="2022-4-14 00:00:00" fixdate="2022-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Code of method "processElement(Lorg/apache/flink/streaming/runtime/streamrecord/StreamRecord;)V" of class "HashAggregateWithKeys$9211" grows beyond 64 KB</summary>
      <description>I think this bug should get fixed in https://issues.apache.org/jira/browse/FLINK-23007Unfortunately I spotted it on Flink 1.14.3java.lang.RuntimeException: Could not instantiate generated class 'HashAggregateWithKeys$9211' at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:85) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:81) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.streaming.runtime.tasks.OperatorChain.&lt;init&gt;(OperatorChain.java:198) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.&lt;init&gt;(RegularOperatorChain.java:63) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:666) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at java.lang.Thread.run(Unknown Source) ~[?:?]Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:76) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] ... 11 moreCaused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] ... 11 moreCaused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:89) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] ... 11 moreCaused by: org.codehaus.janino.InternalCompilerException: Compiling "HashAggregateWithKeys$9211": Code of method "processElement(Lorg/apache/flink/streaming/runtime/streamrecord/StreamRecord;)V" of class "HashAggregateWithKeys$9211" grows beyond 64 KB at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] ... 11 moreCaused by: org.codehaus.janino.InternalCompilerException: Code of method "processElement(Lorg/apache/flink/streaming/runtime/streamrecord/StreamRecord;)V" of class "HashAggregateWithKeys$9211" grows beyond 64 KB at org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:1048) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.CodeContext.write(CodeContext.java:940) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.writeShort(UnitCompiler.java:12282) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.load(UnitCompiler.java:11941) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.load(UnitCompiler.java:11926) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4465) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$8000(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16$1.visitLocalVariableAccess(UnitCompiler.java:4408) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16$1.visitLocalVariableAccess(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$LocalVariableAccess.accept(Java.java:4274) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4461) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$7500(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16$1.visitAmbiguousName(UnitCompiler.java:4403) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16$1.visitAmbiguousName(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:4224) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileBoolean2(UnitCompiler.java:4120) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$14.visitBinaryOperation(UnitCompiler.java:3957) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$14.visitBinaryOperation(UnitCompiler.java:3935) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$BinaryOperation.accept(Java.java:4864) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileBoolean(UnitCompiler.java:3935) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4448) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5004) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$8500(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitBinaryOperation(UnitCompiler.java:4417) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitBinaryOperation(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$BinaryOperation.accept(Java.java:4864) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5057) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$8100(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16$1.visitParenthesizedExpression(UnitCompiler.java:4409) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16$1.visitParenthesizedExpression(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$ParenthesizedExpression.accept(Java.java:4924) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4400) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$16.visitLvalue(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3792) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3754) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3734) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Assignment.accept(Java.java:4477) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Block.accept(Java.java:2779) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Block.accept(Java.java:2779) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2468) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Block.accept(Java.java:2779) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2468) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$Block.accept(Java.java:2779) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2468) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83) ~[flink-table_2.12-1.14.3-stream1.jar:1.14.3-stream1] ... 11 more</description>
      <version>1.14.3,1.15.3,1.16.1</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CodeSplitITCase.scala</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.splitter.expected.TestSplitJavaCode.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.splitter.expected.TestNotSplitJavaCode.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.splitter.code.TestNotSplitJavaCode.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.if.expected.TestRewriteInnerClass.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.if.expected.TestNotRewriteIfStatementInFunctionWithReturnValue.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.if.expected.TestIfStatementRewrite.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.if.code.TestRewriteInnerClass.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.if.code.TestNotRewriteIfStatementInFunctionWithReturnValue.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.resources.if.code.TestIfStatementRewrite.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.JavaCodeSplitterTest.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.IfStatementRewriterTest.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.CodeRewriterTestBase.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.main.java.org.apache.flink.table.codesplit.JavaCodeSplitter.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.main.java.org.apache.flink.table.codesplit.IfStatementRewriter.java</file>
      <file type="M">flink-table.flink-table-code-splitter.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="28759" opendate="2022-8-1 00:00:00" fixdate="2022-8-1 01:00:00" resolution="Done">
    <buginformation>
      <summary>Enable speculative execution for in AdaptiveBatchScheduler TPC-DS e2e tests</summary>
      <description>To verify the correctness of speculative execution, we can enabled it in AdaptiveBatchScheduler TPC-DS e2e tests, which runs a lot of different batch jobs and verifies the result.Note that we need to disable the blocklist (by setting block duration to 0) in such single machine e2e tests.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="29580" opendate="2022-10-11 00:00:00" fixdate="2022-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>pulsar.consumer.autoUpdatePartitionsIntervalSeconds doesn&amp;#39;t work and should be removed</summary>
      <description></description>
      <version>1.17.0,1.15.2,1.16.1</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSourceOptions.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.config.PulsarSourceConfigUtils.java</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.consumer.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="29852" opendate="2022-11-2 00:00:00" fixdate="2022-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adaptive Scheduler duplicates operators for each parallel instance in the Web UI</summary>
      <description>All the operators in the DAG are shown repeatedly</description>
      <version>1.16.0,1.16.1</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.CreatingExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="30286" opendate="2022-12-2 00:00:00" fixdate="2022-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run rat plugin in validate phase</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">tools.ci.compile.sh</file>
    </fixedFiles>
  </bug>
  <bug id="30370" opendate="2022-12-12 00:00:00" fixdate="2022-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move existing delegation token framework authentication to providers</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.token.KerberosDelegationTokenManagerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.token.ExceptionThrowingHadoopDelegationTokenProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.KerberosDelegationTokenManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.HBaseDelegationTokenProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.HadoopFSDelegationTokenProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.HadoopDelegationTokenProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="30424" opendate="2022-12-15 00:00:00" fixdate="2022-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add source operator restore readerState log to distinguish split is from newPartitions or split state</summary>
      <description>When a job start firstly, we can find 'assignPartitions' from log。but if source recover from state, we can not distinguish the newPartitions is from timed discover thread or from reader task state.  We can add a helper log to distinguish and confirm the reader using split state in recover situation.  it's very useful for troubleshooting.  </description>
      <version>1.16.0,1.15.3,1.16.1</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="30425" opendate="2022-12-15 00:00:00" fixdate="2022-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generalize token receive side</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunnerStartupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorSlotLifetimeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorExecutionDeploymentReconciliationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.token.hadoop.HadoopDelegationTokenUpdaterITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.token.DefaultDelegationTokenManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.hadoop.HBaseDelegationTokenProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.hadoop.HadoopDelegationTokenUpdater.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.DelegationTokenProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="30538" opendate="2023-1-2 00:00:00" fixdate="2023-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve error handling of stop job operation</summary>
      <description>Currently, the stop-job operation produces some verbose error msg and doesn't handle exceptions in stop-without-savepoint gracefully.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="30567" opendate="2023-1-5 00:00:00" fixdate="2023-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong insert overwrite behavior when the table contains uppercase character with Hive dialect</summary>
      <description>If use hive dialect, the data would be wrong if we rerun the following job multiple tables.INSERT overwrite TABLE db_name.target_tB_name partition(p_date = '20230101')select author_idfrom db_name.source_tb_nameWHERE p_date = '20230101'and author_id &lt;&gt; '0'The framework has a minor bug when determining whether an override is required if the target name contains uppercase character.</description>
      <version>1.16.0,1.16.1</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserDMLHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="30837" opendate="2023-1-31 00:00:00" fixdate="2023-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove dependency of MutableByteArrayInputStream from flink-avro-glue-schema-registry</summary>
      <description>MutableByteArrayInputStream is a class implemented in flink-avro. We want to make this @Internal if possible, so we can reduce the surface area of interface that we support. At the moment, it is only used in flink-avro and flink-avro-glue-schema-registry.As such, we explore if we can remove the use of MutableByteArrayInputStream from flink-avro-glue-schema-registry.</description>
      <version>1.16.1</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryInputStreamDeserializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="30838" opendate="2023-1-31 00:00:00" fixdate="2023-2-31 01:00:00" resolution="Done">
    <buginformation>
      <summary>Update documentation about the AdaptiveBatchScheduler</summary>
      <description>Documentation is needed to update to help users how to enable the AdaptiveBatchScheduler and properly configuring it.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.speculative.execution.md</file>
      <file type="M">docs.content.docs.deployment.elastic.scaling.md</file>
      <file type="M">docs.content.zh.docs.deployment.speculative.execution.md</file>
      <file type="M">docs.content.zh.docs.deployment.elastic.scaling.md</file>
    </fixedFiles>
  </bug>
  <bug id="30841" opendate="2023-1-31 00:00:00" fixdate="2023-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect calc merge generate wrong plan</summary>
      <description>currently we have a `FlinkCalcMergeRuleTest`, take one test as example: @Test def testCalcMergeWithNonDeterministicExpr1(): Unit = { val sqlQuery = "SELECT a, a1 FROM (SELECT a, random_udf(a) AS a1 FROM MyTable) t WHERE a1 &gt; 10" util.verifyRelPlan(sqlQuery) }the current final optimized plan will be wrong:Calc(select=[a, random_udf(b) AS a1], where=[(random_udf(b) &gt; 10)])+- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])the merged calc contains two `random_udf` call, users may encounter the result satisfied by where predicate (&gt;10) but the selected column &lt;= 10, that's counter-intuitive for usersthe expected plan is:Calc(select=[a, a1], where=[(a1 &gt; 10)])+- Calc(select=[a, random_udf(b) AS a1]) +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])</description>
      <version>1.16.1</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="30864" opendate="2023-2-1 00:00:00" fixdate="2023-2-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optional pattern at the start of a group pattern not working</summary>
      <description>The optional pattern at the start of a group pattern turns out be "not optional", e.g.Pattern.&lt;String&gt;begin("A").next(Pattern.&lt;String&gt;begin("B").optional().next("C")).next("D")cannot match sequence "a1 c1 d1".</description>
      <version>1.15.3,1.16.1</version>
      <fixedVersion>1.17.0,1.15.4,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.GroupITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.compiler.NFACompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="30876" opendate="2023-2-2 00:00:00" fixdate="2023-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ResetTransformationProcessor don&amp;#39;t reset the transformation of ExecNode in BatchExecMultiInput.rootNode</summary>
      <description>Now, ResetTransformationProcessor don't reset the transformation of ExecNode in BatchExecMultiInput.rootNode. This may cause error while creating StreamGraph for BatchExecMultiInput due to different id of rootNode and inputNode.</description>
      <version>1.17.0,1.16.1</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.processor.MultipleInputNodeCreationProcessorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecMultipleInput.java</file>
    </fixedFiles>
  </bug>
  <bug id="30905" opendate="2023-2-6 00:00:00" fixdate="2023-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>doc generation fails with "concurrent map read and map write"</summary>
      <description>We experience a build failure in master (but since it looks like a Hugo issue, I added already released version to the affected versions as well) with a concurrent map read and map write within hugo:https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45707&amp;view=logs&amp;j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&amp;t=ddd6d61a-af16-5d03-2b9a-76a279badf98Start building sites … fatal error: concurrent map read and map writegoroutine 233 [running]:runtime.throw(0x23054e4, 0x21) /usr/local/go/src/runtime/panic.go:1116 +0x72 fp=0xc0016ea860 sp=0xc0016ea830 pc=0x4f5ff2runtime.mapaccess1_faststr(0x1f71280, 0xc000764a20, 0xc000aa60e1, 0x18, 0xcd) /usr/local/go/src/runtime/map_faststr.go:21 +0x465 fp=0xc0016ea8d0 sp=0xc0016ea860 pc=0x4d29c5[...]</description>
      <version>1.17.0,1.15.3,1.16.1</version>
      <fixedVersion>1.17.0,1.15.4,1.16.2</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.docs.sh</file>
      <file type="M">.github.workflows.docs.sh</file>
    </fixedFiles>
  </bug>
  <bug id="30917" opendate="2023-2-6 00:00:00" fixdate="2023-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The user configured max parallelism does not take effect when using adaptive batch scheduler</summary>
      <description>Currently, the adaptive batch scheduler only respects the global maximum parallelism(which is configured by option parallelism.default or execution.batch.adaptive.auto-parallelism.max-parallelism, see FLINK-30686 for details) when deciding parallelism for job vertices, the maximum parallelism of vertices configured by the user through setMaxParallelism will not be respected.In this ticket, we will change the behavior so that the user-configured max parallelism also be respected.</description>
      <version>1.17.0,1.16.1</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismAndInputInfosDeciderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.VertexParallelismAndInputInfosDecider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismAndInputInfosDecider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler.java</file>
    </fixedFiles>
  </bug>
  <bug id="30948" opendate="2023-2-7 00:00:00" fixdate="2023-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove flink-avro-glue-schema-registry and flink-json-glue-schema-registry from Flink main repo</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.ExecutorImplITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.ExecutorImpl.java</file>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">azure-pipelines.yml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.src.test.java.org.apache.flink.glue.schema.registry.test.GlueSchemaRegistryAvroKinesisITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.src.test.java.org.apache.flink.glue.schema.registry.test.GSRKinesisPubsubClient.java</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.src.test.resources.avro.user.avsc</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.src.test.java.org.apache.flink.glue.schema.registry.test.json.GlueSchemaRegistryJsonKinesisITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.src.test.java.org.apache.flink.glue.schema.registry.test.json.GSRKinesisPubsubClient.java</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.archunit-violations.3e77c07d-dfdb-4ec0-8e64-5fad5c651c72</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.archunit-violations.dc78e80c-3bb3-45bb-87c1-b57472d6f45b</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.archunit-violations.stored.rules</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroSchemaCoder.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroSchemaCoderProvider.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroSerializationSchema.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryInputStreamDeserializer.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.main.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryOutputStreamSerializer.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.architecture.TestCodeArchitectureTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroSchemaCoderTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryAvroSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryInputStreamDeserializerTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.GlueSchemaRegistryOutputStreamSerializerTest.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.org.apache.flink.formats.avro.glue.schema.registry.User.java</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.java.resources.avro.user.avsc</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.resources.archunit.properties</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.src.test.resources.META-INF.services.org.junit.jupiter.api.extension.Extension</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.archunit-violations.4703059b-4f06-41c9-9724-644e6d00584f</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.archunit-violations.b99819a4-a946-475e-883f-963de77c7e57</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.archunit-violations.stored.rules</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.main.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.main.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonSchemaCoder.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.main.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonSchemaCoderProvider.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.main.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.java.org.apache.flink.architecture.TestCodeArchitectureTest.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.java.org.apache.flink.formats.json.glue.schema.registry.Car.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonSchemaCoderTest.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.java.org.apache.flink.formats.json.glue.schema.registry.GlueSchemaRegistryJsonSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.resources.archunit.properties</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.src.test.resources.META-INF.services.org.junit.jupiter.api.extension.Extension</file>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="30959" opendate="2023-2-8 00:00:00" fixdate="2023-1-8 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Improve the documentation of UNIX_TIMESTAMP for different argument format</summary>
      <description>When running the following pyflink program import pandas as pdfrom pyflink.datastream import StreamExecutionEnvironment, HashMapStateBackendfrom pyflink.table import StreamTableEnvironmentif __name__ == "__main__": input_data = pd.DataFrame( [ ["Alex", 100.0, "2022-01-01 08:00:00.001 +0800"], ["Emma", 400.0, "2022-01-01 00:00:00.003 +0000"], ["Alex", 200.0, "2022-01-01 08:00:00.005 +0800"], ["Emma", 300.0, "2022-01-01 00:00:00.007 +0000"], ["Jack", 500.0, "2022-01-01 08:00:00.009 +0800"], ["Alex", 450.0, "2022-01-01 00:00:00.011 +0000"], ], columns=["name", "avg_cost", "time"], ) env = StreamExecutionEnvironment.get_execution_environment() env.set_state_backend(HashMapStateBackend()) t_env = StreamTableEnvironment.create(env) input_table = t_env.from_pandas(input_data) t_env.create_temporary_view("input_table", input_table) time_format = "yyyy-MM-dd HH:mm:ss.SSS X" output_table = t_env.sql_query( f"SELECT *, UNIX_TIMESTAMP(`time`, '{time_format}') AS unix_time FROM input_table" ) output_table.execute().print()The actual output is +----+--------------------------------+--------------------------------+--------------------------------+----------------------+| op | name | avg_cost | time | unix_time |+----+--------------------------------+--------------------------------+--------------------------------+----------------------+| +I | Alex | 100.0 | 2022-01-01 08:00:00.001 +0800 | 1640995200 || +I | Emma | 400.0 | 2022-01-01 00:00:00.003 +0000 | 1640995200 || +I | Alex | 200.0 | 2022-01-01 08:00:00.005 +0800 | 1640995200 || +I | Emma | 300.0 | 2022-01-01 00:00:00.007 +0000 | 1640995200 || +I | Jack | 500.0 | 2022-01-01 08:00:00.009 +0800 | 1640995200 || +I | Alex | 450.0 | 2022-01-01 00:00:00.011 +0000 | 1640995200 |+----+--------------------------------+--------------------------------+--------------------------------+----------------------+While the expected result is+----+--------------------------------+--------------------------------+--------------------------------+----------------------+| op | name | avg_cost | time | unix_time |+----+--------------------------------+--------------------------------+--------------------------------+----------------------+| +I | Alex | 100.0 | 2022-01-01 08:00:00.001 +0800 | 1640995200 || +I | Emma | 400.0 | 2022-01-01 00:00:00.003 +0000 | 1640966400 || +I | Alex | 200.0 | 2022-01-01 08:00:00.005 +0800 | 1640995200 || +I | Emma | 300.0 | 2022-01-01 00:00:00.007 +0000 | 1640966400 || +I | Jack | 500.0 | 2022-01-01 08:00:00.009 +0800 | 1640995200 || +I | Alex | 450.0 | 2022-01-01 00:00:00.011 +0000 | 1640966400 |+----+--------------------------------+--------------------------------+--------------------------------+----------------------+ </description>
      <version>1.16.1</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="30966" opendate="2023-2-8 00:00:00" fixdate="2023-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink SQL IF FUNCTION logic error</summary>
      <description>my data is //{ "before": { "status": "sent" }, "after": { "status": "succeed" }, "op": "u", "ts_ms": 1671926400225, "transaction": null } my sql is  CREATE TABLE t( before ROW ( status varchar (32) ), after ROW ( status varchar (32) ), ts_ms bigint, op string, kafka_timestamp timestamp METADATA FROM 'timestamp',-- @formatter:off proctime AS PROCTIME()-- @formatter:on) WITH ( 'connector' = 'kafka',-- 'topic' = '', 'topic' = 'test', 'properties.bootstrap.servers' = ' ', 'properties.group.id' = '', 'format' = 'json', 'scan.topic-partition-discovery.interval' = '60s', 'scan.startup.mode' = 'earliest-offset', 'json.ignore-parse-errors' = 'true' );create table p( status STRING , before_status STRING , after_status STRING , metadata_operation STRING COMMENT '源记录操作类型', dt STRING)WITH ( 'connector' = 'print' );INSERT INTO pSELECT IF(op &lt;&gt; 'd', after.status, before.status), before.status, after.status, op AS metadata_operation, DATE_FORMAT(kafka_timestamp, 'yyyy-MM-dd') AS dtFROM t;  my local env output is   +I[null, sent, succeed, u, 2023-02-08]   my produtionc env output is +I[sent, sent, succeed, u, 2023-02-08] why?  This look like a bug. </description>
      <version>1.16.0,1.16.1</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.IfCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.type.NumericOrDefaultReturnTypeInference.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.type.FlinkReturnTypes.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="31055" opendate="2023-2-14 00:00:00" fixdate="2023-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The dynamic flag of stream graph does not take effect when translating the transformations</summary>
      <description>Currently, the dynamic flag of stream graph is not set when translate transformations. However, the dynamic flag will be used (here) when translating, we should set the dynamic flag before the translating.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.partitioner.ForwardForUnspecifiedPartitionerTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="31077" opendate="2023-2-15 00:00:00" fixdate="2023-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Trigger checkpoint failed but it were shown as COMPLETED by rest API</summary>
      <description>Currently, we can trigger a checkpoint and poll the status of the checkpoint until it is finished by rest according to FLINK-27101. However, even if the checkpoint status returned by rest is completed, it does not mean that the checkpoint is really completed. If an exception occurs after marking the pendingCheckpoint completed(here), the checkpoint is not written to the HA service and we can not failover from this checkpoint.</description>
      <version>1.17.0,1.15.3,1.16.1</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="31092" opendate="2023-2-15 00:00:00" fixdate="2023-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive ITCases fail with OutOfMemoryError</summary>
      <description>We're experiencing an OutOfMemoryError where the heap space reaches the upper limit:https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46161&amp;view=logs&amp;j=fc5181b0-e452-5c8f-68de-1097947f6483&amp;t=995c650b-6573-581c-9ce6-7ad4cc038461&amp;l=23142Feb 15 05:05:14 [INFO] Running org.apache.flink.table.catalog.hive.HiveCatalogITCaseFeb 15 05:05:17 [INFO] java.lang.OutOfMemoryError: Java heap spaceFeb 15 05:05:17 [INFO] Dumping heap to java_pid9669.hprof ...Feb 15 05:05:28 [INFO] Heap dump file created [1957090051 bytes in 11.718 secs]java.lang.OutOfMemoryError: Java heap space at org.apache.maven.surefire.booter.ForkedBooter.cancelPingScheduler(ForkedBooter.java:209) at org.apache.maven.surefire.booter.ForkedBooter.acknowledgedExit(ForkedBooter.java:419) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:186) at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)</description>
      <version>1.17.0,1.16.1,1.18.0</version>
      <fixedVersion>1.17.0,1.16.2,1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.FactoryUtilTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.ServiceLoaderUtil.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FactoryUtil.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.operation.OperationManagerTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationManager.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.utils.ThreadUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="31110" opendate="2023-2-16 00:00:00" fixdate="2023-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web UI shows "User Configuration" preserving lines and whitespaces</summary>
      <description>Currently one can use env.getConfig().setGlobalJobParameters(...) for setting user configurations. It will also show up in the Web UI &gt; Running Jobs &gt; Job Configuration &gt; User Configuration section. This is nice so users can confirm the user configuration (key/value pair) gets populated.However, it does not preserves whitespaces and line breaks in HTML page. For example, we have some prettified JSON configuration and sometimes formatted SQL statements in those configurations, and it's showing in a compacted HTML format - not human readable original formatted string.I propose we keep the whitespaces and lines for this "User Configuration" section in the Web UI. The implementation can be as simple as adding style="white-space: pre-wrap;" to the rows in that section.</description>
      <version>1.16.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.configuration.job-configuration.component.less</file>
    </fixedFiles>
  </bug>
  <bug id="31168" opendate="2023-2-21 00:00:00" fixdate="2023-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JobManagerHAProcessFailureRecoveryITCase failed due to job not being found</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46342&amp;view=logs&amp;j=b0a398c0-685b-599c-eb57-c8c2a771138e&amp;t=747432ad-a576-5911-1e2a-68c6bedc248a&amp;l=12706We see this build failure because a job couldn't be found:java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error while waiting for job to be initialized at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:319) at org.apache.flink.api.java.ExecutionEnvironment.executeAsync(ExecutionEnvironment.java:1061) at org.apache.flink.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:958) at org.apache.flink.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:942) at org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase.testJobManagerFailure(JobManagerHAProcessFailureRecoveryITCase.java:235) at org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase$4.run(JobManagerHAProcessFailureRecoveryITCase.java:336)Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error while waiting for job to be initialized at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999) at org.apache.flink.api.java.ExecutionEnvironment.executeAsync(ExecutionEnvironment.java:1056) ... 4 moreCaused by: java.lang.RuntimeException: Error while waiting for job to be initialized at org.apache.flink.client.ClientUtils.waitUntilJobInitializationFinished(ClientUtils.java:160) at org.apache.flink.client.deployment.executors.AbstractSessionClusterExecutor.lambda$execute$2(AbstractSessionClusterExecutor.java:82) at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedFunction$2(FunctionUtils.java:73) at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642) at java.base/java.util.concurrent.CompletableFuture$Completion.exec(CompletableFuture.java:479) at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290) at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020) at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656) at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594) at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.rest.util.RestClientException: [org.apache.flink.runtime.rest.NotFoundException: Job 865dcd87f4828dbeb3d93eb52e2636b1 not found at org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.lambda$handleRequest$1(AbstractExecutionGraphHandler.java:99) at java.base/java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:986) at java.base/java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:970) at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088) at org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCache.lambda$getExecutionGraphInternal$0(DefaultExecutionGraphCache.java:109) at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088) at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:252) at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088) at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1387) at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$1(ClassLoadingUtils.java:93) at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92) at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088) at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:45) at akka.dispatch.OnComplete.internal(Future.scala:299) at akka.dispatch.OnComplete.internal(Future.scala:297) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65) at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68) at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284) at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284) at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284) at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621) at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:25) at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23) at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532) at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29) at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63) at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12) at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81) at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48) at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290) at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020) at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656) at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594) at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)Caused by: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (865dcd87f4828dbeb3d93eb52e2636b1) at org.apache.flink.runtime.dispatcher.Dispatcher.requestExecutionGraphInfo(Dispatcher.java:840) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304) at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) at akka.actor.Actor.aroundReceive(Actor.scala:537) at akka.actor.Actor.aroundReceive$(Actor.scala:535) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) at akka.actor.ActorCell.invoke(ActorCell.scala:548) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) at akka.dispatch.Mailbox.run(Mailbox.scala:231) at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ... 5 more</description>
      <version>1.15.3,1.16.1,1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.TestJvmProcess.java</file>
    </fixedFiles>
  </bug>
  <bug id="31185" opendate="2023-2-22 00:00:00" fixdate="2023-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python BroadcastProcessFunction not support side output</summary>
      <description></description>
      <version>1.16.1</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.runtime.translators.python.PythonKeyedBroadcastStateTransformationTranslator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.runtime.translators.python.PythonBroadcastStateTransformationTranslator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.transformations.python.PythonKeyedBroadcastStateTransformation.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.transformations.python.PythonBroadcastStateTransformation.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.chain.PythonOperatorChainingOptimizer.java</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.embedded.operations.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="31214" opendate="2023-2-24 00:00:00" fixdate="2023-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for new command line option -py.pythonpath</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.cli.embedded-mode-help.out</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.cli.all-mode-help.out</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptionsParser.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.util.PythonDependencyUtilsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.PythonOptionsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.cli.PythonProgramOptionsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonDependencyUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptionsUtils.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">docs.layouts.shortcodes.generated.python.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="3123" opendate="2015-12-5 00:00:00" fixdate="2015-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow setting custom start-offsets for the Kafka consumer</summary>
      <description>Currently, the Kafka consumer only allows to start reading from the earliest available offset or the current offset.Sometimes, users want to set a specific start offset themselves.</description>
      <version>None</version>
      <fixedVersion>1.0.0,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerPartitionAssignmentTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.config.StartupMode.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09ITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08ITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010ITCase.java</file>
      <file type="M">docs.dev.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="31273" opendate="2023-3-1 00:00:00" fixdate="2023-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Left join with IS_NULL filter be wrongly pushed down and get wrong join results</summary>
      <description>Left join with IS_NULL filter be wrongly pushed down and get wrong join results. The sql is:SELECT * FROM MyTable1 LEFT JOIN MyTable2 ON a1 = a2 WHERE a2 IS NULL AND a1 &lt; 10The wrongly plan is:LogicalProject(a1=[$0], b1=[$1], c1=[$2], b2=[$3], c2=[$4], a2=[$5])+- LogicalFilter(condition=[IS NULL($5)])   +- LogicalJoin(condition=[=($0, $5)], joinType=[left])      :- LogicalValues(tuples=[[]])      +- LogicalTableScan(table=[[default_catalog, default_database, MyTable2]])</description>
      <version>1.17.0,1.16.1</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.JoinTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkFilterJoinRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SortMergeJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.ShuffledHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.logical.FlinkFilterJoinRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.FlinkFilterJoinRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="31288" opendate="2023-3-2 00:00:00" fixdate="2023-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable overdraft buffer for batch shuffle</summary>
      <description>Only pipelined / pipelined-bounded partition needs overdraft buffer. More specifically, there is no reason to request more buffers for non-pipelined (i.e. batch) shuffle. The reasons are as follows: For BoundedBlockingShuffle, each full buffer will be directly released. For SortMergeShuffle, the maximum capacity of buffer pool is 4 * numSubpartitions. It is efficient enough to spill this part of memory to disk. For Hybrid Shuffle, the buffer pool is unbounded. If it can't get a normal buffer, it also can't get an overdraft buffer.</description>
      <version>1.17.0,1.16.1</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.NettyShuffleEnvironmentOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.netty.shuffle.environment.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.taskmanager.network.section.html</file>
      <file type="M">docs.content.docs.deployment.memory.network.mem.tuning.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.network.mem.tuning.md</file>
    </fixedFiles>
  </bug>
  <bug id="3129" opendate="2015-12-7 00:00:00" fixdate="2015-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tooling to ensure interface stability</summary>
      <description>I would like to use this maven plugin: https://github.com/siom79/japicmp to automatically ensure interface stability across minor releases.Ideally we have the plugin in place after Flink 1.0 is out, so that maven builds break if a breaking change has been made.The plugin already supports downloading a reference release, checking the build and breaking it.Not yet supported are class/method inclusions based on annotations, but I've opened a pull request for adding it.There are also issues with the resolution of the dependency with the annotations, but I'm working on resolving those issues.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.NormalizableKey.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.FloatValue.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.DoubleValue.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.RuntimeContext.java</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
      <file type="M">flink-shaded-curator.pom.xml</file>
      <file type="M">flink-quickstart.pom.xml</file>
      <file type="M">flink-java8.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
      <file type="M">flink-annotations.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="31300" opendate="2023-3-2 00:00:00" fixdate="2023-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TRY_CAST fails for constructed types</summary>
      <description>In case of problems with cast it is expected to return nullhowever for arrays, maps it failsexample of failing queriesselect try_cast(array['a'] as array&lt;int&gt;);select try_cast(map['a', '1'] as map&lt;int, int&gt;); [ERROR] Could not execute SQL statement. Reason:java.lang.NumberFormatException: For input string: 'a'. Invalid character found. at org.apache.flink.table.data.binary.BinaryStringDataUtil.numberFormatExceptionFor(BinaryStringDataUtil.java:585) at org.apache.flink.table.data.binary.BinaryStringDataUtil.toInt(BinaryStringDataUtil.java:518) at StreamExecCalc$15.processElement(Unknown Source) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418) at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513) at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103) at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67) at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)</description>
      <version>1.17.0,1.16.1</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionMiscITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.AbstractCastRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="31337" opendate="2023-3-6 00:00:00" fixdate="2023-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>EmbeddedDataStreamBatchTests.test_keyed_co_broadcast_side_output</summary>
      <description>Same build, multiple times: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=c67e71ed-6451-5d26-8920-5a8cf9651901&amp;l=24566 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&amp;view=logs&amp;j=821b528f-1eed-5598-a3b4-7f748b13f261&amp;t=6bb545dd-772d-5d8c-f258-f5085fba3295&amp;l=24235 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&amp;view=logs&amp;j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&amp;t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&amp;l=24545 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&amp;view=logs&amp;j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&amp;t=b4612f28-e3b5-5853-8a8b-610ae894217a&amp;l=24481 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&amp;view=logs&amp;j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&amp;t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&amp;l=24757Mar 04 01:21:35 pyflink/datastream/tests/test_data_stream.py:743: Mar 04 01:21:35 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ Mar 04 01:21:35 pyflink/datastream/tests/test_data_stream.py:63: in assert_equals_sortedMar 04 01:21:35 self.assertEqual(expected, actual)Mar 04 01:21:35 E AssertionError: Lists differ: ['0', '1', '2', '4', '5', '5', '6', '6'] != ['0', '1', '2', '3', '5', '5', '6', '6']Mar 04 01:21:35 E Mar 04 01:21:35 E First differing element 3:Mar 04 01:21:35 E '4'Mar 04 01:21:35 E '3'Mar 04 01:21:35 E Mar 04 01:21:35 E - ['0', '1', '2', '4', '5', '5', '6', '6']Mar 04 01:21:35 E ? ^Mar 04 01:21:35 E Mar 04 01:21:35 E + ['0', '1', '2', '3', '5', '5', '6', '6']Mar 04 01:21:35 E ?</description>
      <version>1.17.0,1.16.1</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.BatchExecutionUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.runtime.translators.python.PythonKeyedBroadcastStateTransformationTranslator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.process.ExternalPythonBatchKeyedCoBroadcastProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.EmbeddedPythonBatchKeyedCoBroadcastProcessOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="31346" opendate="2023-3-6 00:00:00" fixdate="2023-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Batch shuffle IO scheduler does not throw TimeoutException if numRequestedBuffers is greater than 0</summary>
      <description>We currently rely on throw exception to trigger downstream task failover to avoid read buffer request deadlock. But if numRequestedBuffers is greater than 0, IO scheduler does not throw TimeoutException. This will cause a deadlock.</description>
      <version>1.17.0,1.16.1</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsFileDataManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsFileDataManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="31351" opendate="2023-3-6 00:00:00" fixdate="2023-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2EndpointITCase.testExecuteStatementInSyncModeWithRuntimeException2 times out on CI</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46872&amp;view=logs&amp;j=fc5181b0-e452-5c8f-68de-1097947f6483&amp;t=995c650b-6573-581c-9ce6-7ad4cc038461&amp;l=24908 Mar 06 18:28:56 "ForkJoinPool-1-worker-25" #27 daemon prio=5 os_prio=0 tid=0x00007ff4b1832000 nid=0x21b2 waiting on condition [0x00007ff3a8c3e000]Mar 06 18:28:56 java.lang.Thread.State: TIMED_WAITING (sleeping)Mar 06 18:28:56 at java.lang.Thread.sleep(Native Method)Mar 06 18:28:56 at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.waitUntilJobIsRunning(HiveServer2EndpointITCase.java:1004)Mar 06 18:28:56 at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.lambda$testExecuteStatementInSyncModeWithRuntimeException2$37(HiveServer2EndpointITCase.java:711)Mar 06 18:28:56 at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase$$Lambda$2018/2127600974.accept(Unknown Source)Mar 06 18:28:56 at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.runExecuteStatementInSyncModeWithRuntimeException(HiveServer2EndpointITCase.java:999)Mar 06 18:28:56 at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.testExecuteStatementInSyncModeWithRuntimeException2(HiveServer2EndpointITCase.java:709)Mar 06 18:28:56 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)Mar 06 18:28:56 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)Mar 06 18:28:56 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)Mar 06 18:28:56 at java.lang.reflect.Method.invoke(Method.java:498)</description>
      <version>1.17.0,1.16.1,1.18.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.operation.OperationManagerTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="31369" opendate="2023-3-8 00:00:00" fixdate="2023-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden modifiers for sql-gateway module</summary>
      <description>This is a follow up jira issue for https://github.com/apache/flink/pull/22127#discussion_r1129192778</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.SqlGatewayTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceStatementITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.session.SessionManagerImplTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.result.ResultFetcherTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.StatementRelatedITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.SqlGatewayRestEndpointStatementITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.SessionRelatedITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.OperationRelatedITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.AbstractSqlGatewayStatementITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointStatementITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="31393" opendate="2023-3-10 00:00:00" fixdate="2023-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HsFileDataManager use an incorrect default timeout</summary>
      <description>For batch shuffle(i.e. hybrid shuffle &amp; sort-merge shuffle), If there is a fierce contention of the batch shuffle read memory, it will throw a TimeoutException to fail downstream task to release memory. But for hybrid shuffle, It uses an incorrect default timeout(5ms), this will make the job very easy to fail.</description>
      <version>1.17.0,1.16.1</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HybridShuffleConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="31476" opendate="2023-3-15 00:00:00" fixdate="2023-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AdaptiveScheduler should take lower bound parallelism settings into account</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultVertexParallelismStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.allocator.TestVertexInformation.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.allocator.SlotSharingSlotAllocatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.VertexParallelismInformation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultVertexParallelismStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultVertexParallelismInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.JobGraphJobInformation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.allocator.SlotSharingSlotAllocator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.allocator.JobInformation.java</file>
    </fixedFiles>
  </bug>
  <bug id="31604" opendate="2023-3-24 00:00:00" fixdate="2023-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce usage of CatalogTableImpl in planner</summary>
      <description>The task is similar to https://issues.apache.org/jira/browse/FLINK-30896 however about CatalogTableImpl which is deprecated</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.testTableSourceSinks.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoLegacyTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlNodeToOperationConversionTestBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.catalog.CatalogStatisticsTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.catalog.CatalogConstraintTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.schema.LegacyCatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.FlinkCalciteCatalogReader.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.catalog.CatalogSchemaTable.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.TableSchemaUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.TableSchemaUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="31670" opendate="2023-3-30 00:00:00" fixdate="2023-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ElasticSearch connector&amp;#39;s document was not incorrect linked to external repo</summary>
      <description>In the doc, It still use "flink-version" for flink-connector-elastiacsearch instead of the version in the external repository.&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-elasticsearch6&lt;/artifactId&gt; &lt;version&gt;1.18-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt;</description>
      <version>1.16.1</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.docs.sh</file>
      <file type="M">docs.data.sql.connectors.yml</file>
    </fixedFiles>
  </bug>
  <bug id="31672" opendate="2023-3-30 00:00:00" fixdate="2023-3-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Requirement validation does not take user-specified or scheduler-generated maxParallelism into account</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="31673" opendate="2023-3-30 00:00:00" fixdate="2023-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add E2E tests for flink jdbc driver</summary>
      <description>Since jdbc driver will be used by third party projects, and we've introduced a bundled jar in flink-sql-jdbc-driver-bundle, we'd better to have e2e tests to verify and ensure it works fine (in case of the dependency management).</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.SqlGateway.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="31743" opendate="2023-4-6 00:00:00" fixdate="2023-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid relocating the RocksDB&amp;#39;s log failure when filename exceeds 255 characters</summary>
      <description>Since FLINK-24785 , the file name of the rocksdb LOG is generated by parsing the db path, when the db path is long and the filename exceeds 255 characters, the creation of the file will fail, so the relevant rocksdb LOG cannot be seen in the flink log dir.</description>
      <version>1.16.1,1.15.4</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="31744" opendate="2023-4-6 00:00:00" fixdate="2023-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend Adaptive Scheduler sparse EG to contain maxParallelism</summary>
      <description>When a job is WaitingForResources the adpative scheduler returns a sparse execution graph that omits many details that are only know at execution time (like subtasks).We could include all JobVertex-level information though, which would cover things like the vertex id/name and the maxParallelism.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.TestingJobMasterGatewayBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.TestingJobMasterGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="31763" opendate="2023-4-11 00:00:00" fixdate="2023-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Convert requested buffers to overdraft buffers when pool size is decreased</summary>
      <description>As we discussed in FLINK-31610, new buffers can be requested only when "numOfRequestedMemorySegments + numberOfRequestedOverdraftMemorySegments &lt; poolSize + maxOverdraftBuffersPerGate".Consider such a scenario, the CurrentPoolSize = 5, numOfRequestedMemorySegments = 7, maxOverdraftBuffersPerGate = 2. If numberOfRequestedOverdraftMemorySegments = 0, then 2 buffers can be requested now. We should convert numberOfRequestedMemorySegments to numberOfRequestedOverdraftMemorySegments when poolSize is decreased. Further more, we can changes the definition of overdraft buffer from static to dynamic:  When numberOfRequestedMemorySegments &lt;= poolSize, all buffers are ordinary buffer When numberOfRequestedMemorySegments &gt; poolSize, the `ordinary buffer size = poolSize`, and `the overdraft buffer size = numberOfRequestedMemorySegments - poolSize`This allows us to remove numberOfRequestedOverdraftMemorySegments, which helps us simplify logic and maintain consistency. </description>
      <version>1.16.1</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.LocalBufferPoolTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.LocalBufferPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="31767" opendate="2023-4-11 00:00:00" fixdate="2023-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the implementation for "analyze table" execution on partitioned table</summary>
      <description>Currently, for partitioned table, the "analyze table" command will generate a separate SQL statement for each partition. When there are too many partitions, the compilation/submission/execution time will be very long. This issue aims to improve it: we can combine the sql statements for each partition into one with "union all", and just need to execution one sql.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.AnalyzeTableUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="31774" opendate="2023-4-11 00:00:00" fixdate="2023-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for delete and update statement</summary>
      <description>I do not find the declaration about the usage of DELETE and UPDATE statement in the SQL section. </description>
      <version>None</version>
      <fixedVersion>1.18.0,1.17.2</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="31963" opendate="2023-4-28 00:00:00" fixdate="2023-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>java.lang.ArrayIndexOutOfBoundsException when scaling down with unaligned checkpoints</summary>
      <description>I'm testing Autoscaler through Kubernetes Operator and I'm facing the following issue.As you know, when a job is scaled down through the autoscaler, the job manager and task manager go down and then back up again.When this happens, an index out of bounds exception is thrown and the state is not restored from a checkpoint.gyfora told me via the Flink Slack troubleshooting channel that this is likely an issue with Unaligned Checkpoint and not an issue with the autoscaler, but I'm opening a ticket with Gyula for more clarification.Please see the attached JM and TM error logs.Thank you.</description>
      <version>1.17.0,1.16.1,1.15.4,1.18.0</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.TaskStateAssignment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="31983" opendate="2023-5-2 00:00:00" fixdate="2023-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add yarn acls capability to flink containers</summary>
      <description>Yarn provide application acls mechanism to be able to provide specific rights to other users than the one running the job (view logs through the resourcemanager/job history, kill the application)</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOSecuredITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">docs.layouts.shortcodes.generated.yarn.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="31984" opendate="2023-5-2 00:00:00" fixdate="2023-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Savepoint on S3 should be relocatable if entropy injection is not effective</summary>
      <description>We have a limitation that if we create savepoints with an injected entropy, they are not relocatable (https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/savepoints/#triggering-savepoints).FLINK-25952 improves the check by inspecting both the FileSystem extending EntropyInjectingFileSystem and FlinkS3FileSystem#getEntropyInjectionKey not returning null. We can improve this further by checking the checkpoint path is indeed using the entropy injection key. Without that, the savepoint is not relocatable even if the state.savepoints.dir does not contain the entropy.In our setting, we enable entropy injection by setting s3.entropy.key to &amp;#95;ENTROPY_KEY&amp;#95; and use the entropy key in the checkpoint path (for e.g. s3://mybuket/checkpoints/_ENTROPY_KEY_/myapp). However, in the savepoint path, we don't use the entropy key (for e.g. s3://mybuket/savepoints/myapp) because we want the savepoint to be relocatable. But the current logic still generates non-relocatable savepoint path just because the entropy injection key is non-null.</description>
      <version>1.16.1</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactoryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.fs.EntropyInjectorTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.EntropyInjector.java</file>
    </fixedFiles>
  </bug>
  <bug id="31996" opendate="2023-5-4 00:00:00" fixdate="2023-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Chaining operators with different max parallelism prevents rescaling</summary>
      <description>We might chain operators with different max parallelism together if they are set to have the same parallelism initially.When we decide to rescale the JobGraph vertices (using AdaptiveScheduler), we're gapped by the lowest maxParallelism of the operator chain. This is especially visible with things like CollectSink, TwoPhaseCommitSink, CDC, and a GlobalCommiter with maxParallelism set to 1. An obvious solution would be to prevent the chaining of operators with different maxParallelism, but we need to double-check this doesn't introduce a breaking change.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.StreamingScalaAPICompletenessTest.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.PipelineOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.pipeline.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="31997" opendate="2023-5-4 00:00:00" fixdate="2023-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update to Fabric8 6.5.1+ in flink-kubernetes</summary>
      <description>We should update the fabric8 version in flink-kubernetes to at least 6.5.1. Flink currently uses a very old fabric8 version. The fabric8 library dependencies have since been revised and greately improved to make them more moduler and allow eliminating securitiy vulnerabilities more easily like: https://issues.apache.org/jira/browse/FLINK-31815The newer versions especially 6.5.1 + also add some improvement stability fixes for watches and other parts.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.MixedKubernetesServerExtension.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesPodTemplateTestUtils.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesClusterDescriptorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClientTest.java</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesClusterDescriptor.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesClusterClientFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesLeaderElector.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClientFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32027" opendate="2023-5-8 00:00:00" fixdate="2023-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Batch jobs could hang at shuffle phase when max parallelism is really large</summary>
      <description>In batch stream mode with adaptive batch schedule mode, If we set the max parallelism large as 32768 (pipeline.max-parallelism), the job could hang at the shuffle phase:It would hang for a long time and show "No bytes sent": After some time to debug, we can see the downstream operator did not receive the end-of-partition event.</description>
      <version>1.16.0,1.17.0,1.16.1</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriteReadTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFile.java</file>
    </fixedFiles>
  </bug>
  <bug id="32132" opendate="2023-5-19 00:00:00" fixdate="2023-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cast function CODEGEN does not work as expected when nullOnFailure enabled</summary>
      <description>I am trying to generate code cast string to bigint, and got generated code like:  // code placeholderif (!isNull$14) {result$15 = org.apache.flink.table.data.binary.BinaryStringDataUtil.toLong(field$13.trim());} else {result$15 = -1L;} castRuleResult$16 = result$15; castRuleResultIsNull$17 = isNull$14; } catch (java.lang.Throwable e) { castRuleResult$16 = -1L; castRuleResultIsNull$17 = true; } // --- End cast sectionout.setLong(0, castRuleResult$16); such kind of handle does not provide a perfect solution as the default value of long is set to -1L, which can be meaningful in some case. And can cause some calculation error. And I understand the cast returns a bigint not null, But since there is a exception, we should ignore the type restriction, so I suggest to modify the CodeGenUtils.rowSetField like below: // code placeholderif (fieldType.isNullable || fieldExpr.nullTerm.startsWith("castRuleResultIsNull")) { s""" |${fieldExpr.code} |if (${fieldExpr.nullTerm}) { | $setNullField; |} else { | $writeField; |} """.stripMargin} else { s""" |${fieldExpr.code} |$writeField; """.stripMargin}</description>
      <version>1.16.1</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="32137" opendate="2023-5-20 00:00:00" fixdate="2023-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flame graph is hard to use with many task managers</summary>
      <description>In case there are many task managers executing the same operator, the flame graph becomes very hard to use. As you can see on the attached picture, it considers instances of the same lambda function as different classes, and their number seems to be equal to the number of task managers (i.e. each JVM gets its own "class" name, which is expected for lambdas I guess). This lambda function is deep within Flink's own call stack, so this kind of graph is inevitable regardless of the job's own logic, and there is nothing we can do at the job logic's level to fix it.This behavior makes evaluating the flame graph very hard, because all of the useful information gets "compressed" inside each "column" of the graph, and at the same time, it does not give any useful information since this is just an artifact of the class name generation in the JVM.</description>
      <version>1.16.1</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.ThreadInfoSampleServiceTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.VertexFlameGraphFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="32514" opendate="2023-7-3 00:00:00" fixdate="2023-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FLIP-309: Support using larger checkpointing interval when source is processing backlog</summary>
      <description>Umbrella issue for https://cwiki.apache.org/confluence/display/FLINK/FLIP-309%3A+Support+using+larger+checkpointing+interval+when+source+is+processing+backlog</description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorEventSendingCheckpointITCase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.ManuallyTriggeredScheduledExecutorService.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testutils.source.reader.TestingSplitEnumeratorContext.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.CheckpointConfig.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.MockOperatorCoordinatorContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.FailoverStrategyCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDeciderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTriggeringTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.VertexEndOfDataListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.CheckpointCoordinatorConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDecider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.concurrent.ManuallyTriggeredScheduledExecutor.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.connector.source.mocks.MockSplitEnumeratorContext.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SplitEnumeratorContext.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.hybrid.HybridSourceSplitEnumerator.java</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.checkpointing.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="32516" opendate="2023-7-3 00:00:00" fixdate="2023-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to parse [CREATE OR ] REPLACE TABLE AS statement</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug id="32517" opendate="2023-7-3 00:00:00" fixdate="2023-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to execute [CREATE OR] REPLACE TABLE AS statement</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.utils.OperationConverterUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlCreateTableConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConverters.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ModifyOperationVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlReplaceTableAs.java</file>
    </fixedFiles>
  </bug>
  <bug id="32518" opendate="2023-7-3 00:00:00" fixdate="2023-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable atomicity for [CREATE OR] REPLACE table as statement</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.utils.AtomicCtasITCaseBase.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.sink.abilities.SupportsStaging.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.StagedTable.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.StagedSinkModifyOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ReplaceTableAsOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.execution.CtasJobStatusHook.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.TableConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.table.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="32519" opendate="2023-7-3 00:00:00" fixdate="2023-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add doc for [CREATE OR] REPLACE TABLE AS statement</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.create.md</file>
    </fixedFiles>
  </bug>
  <bug id="33010" opendate="2023-8-31 00:00:00" fixdate="2023-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE when using GREATEST() in Flink SQL</summary>
      <description>Hi,I see NPEs in flink 1.14 and flink 1.16 when running queries with GREATEST() and timestamps. Below is an example to help in reproducing the issue.CREATE TEMPORARY VIEW Positions ASSELECTSecurityId,ccy1,CAST(publishTimestamp AS TIMESTAMP(3)) as publishTimestampFROM (VALUES(1, 'USD', '2022-01-01'),(2, 'GBP', '2022-02-02'),(3, 'GBX', '2022-03-03'),(4, 'GBX', '2022-04-4'))AS ccy(SecurityId, ccy1, publishTimestamp);CREATE TEMPORARY VIEW Benchmarks ASSELECTSecurityId,ccy1,CAST(publishTimestamp AS TIMESTAMP(3)) as publishTimestampFROM (VALUES(3, 'USD', '2023-01-01'),(4, 'GBP', '2023-02-02'),(5, 'GBX', '2023-03-03'),(6, 'GBX', '2023-04-4'))AS ccy(SecurityId, ccy1, publishTimestamp);SELECT *,GREATEST(IFNULL(Positions.publishTimestamp,CAST('1970-1-1' AS TIMESTAMP(3))),IFNULL(Benchmarks.publishTimestamp,CAST('1970-1-1' AS TIMESTAMP(3))))FROM PositionsFULL JOIN Benchmarks ON Positions.SecurityId = Benchmarks.SecurityId  Using "IF" is a workaround at the moment instead of using "GREATEST"  </description>
      <version>1.16.1,1.16.2</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2,1.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.GreatestLeastFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
</bugrepository>
