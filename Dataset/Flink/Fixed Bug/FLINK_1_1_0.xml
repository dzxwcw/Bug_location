<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="3155" opendate="2015-12-9 00:00:00" fixdate="2015-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Flink docker version to latest stable Flink version</summary>
      <description>It would be nice to always set the Docker Flink binary URL to point to the latest Flink version. Until then, this JIRA keeps track of the updates for releases.</description>
      <version>1.0.0,1.1.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.docker-flink.Dockerfile</file>
      <file type="M">flink-contrib.docker-flink.docker-compose.yml</file>
      <file type="M">flink-contrib.docker-flink.flink.Dockerfile</file>
      <file type="M">flink-contrib.docker-flink.base.Dockerfile</file>
    </fixedFiles>
  </bug>
  <bug id="3544" opendate="2016-2-29 00:00:00" fixdate="2016-3-29 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>ResourceManager runtime components</summary>
      <description></description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAJobGraphRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.taskmanager.TaskManagerFailsITCase.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAProcessFailureBatchRecoveryITCase.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.FlinkYarnSessionCli.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.NetUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.akka.FlinkUntypedActor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.ExecutionObserver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.AkkaActorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.Instance.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.InstanceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.scheduler.ResourceId.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.LeaderSessionMessageDecorator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.MessageDecorator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.LeaderRetrievalUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.SerializedThrowable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.yarn.AbstractFlinkYarnCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.yarn.FlinkYarnClusterStatus.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.LeaderSessionMessageFilter.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.messages.RegistrationMessages.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.messages.TaskManagerMessages.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.minicluster.FlinkMiniCluster.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.minicluster.LocalFlinkMiniCluster.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.LocalInputSplitsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.TerminalStateDeadlockTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.VertexLocationConstraintTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.instance.InstanceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.instance.InstanceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.instance.SimpleSlotTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobSubmitTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.SchedulerTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerComponentsStartupShutdownTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerProcessReapingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerRegistrationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerStartupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.TaskManagerProcess.java</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.JobManagerRegistrationTest.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.TaskManagerFailsWithSlotSharingITCase.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingCluster.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingJobManagerLike.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingJobManagerMessages.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingTaskManager.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingTaskManagerLike.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingTaskManagerMessages.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingUtils.scala</file>
      <file type="M">flink-test-utils.src.main.scala.org.apache.flink.test.util.ForkableFlinkMiniCluster.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHACheckpointRecoveryITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="3545" opendate="2016-2-29 00:00:00" fixdate="2016-3-29 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>ResourceManager: YARN integration</summary>
      <description>This integrates YARN support with the ResourceManager abstraction.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.YarnTaskManager.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.YarnProcessShutDownThread.java</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.YarnMessages.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.YarnJobManager.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMasterBase.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMaster.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationClient.scala</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnTaskManagerRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnCluster.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClientBase.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClient.java</file>
      <file type="M">flink-yarn-tests.src.main.scala.org.apache.flink.yarn.TestingYarnTaskManager.scala</file>
      <file type="M">flink-yarn-tests.src.main.scala.org.apache.flink.yarn.TestingYarnJobManager.scala</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.UtilsTest.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.TestingApplicationMaster.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="3547" opendate="2016-2-29 00:00:00" fixdate="2016-3-29 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add support for streaming projection, selection, and union</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.SelectITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.TranslationContext.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.schema.DataSetTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.TableConversions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.package.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3573" opendate="2016-3-3 00:00:00" fixdate="2016-3-3 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement more String functions for Table API</summary>
      <description>Implement the remaining string functions:CHARACTER_LENGTHCONCATUPPERLOWERINITCAPLIKESIMILAR</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.test.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.RexNodeTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.call.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.TrimCallGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.MethodCallGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.CallGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
    </fixedFiles>
  </bug>
  <bug id="3574" opendate="2016-3-3 00:00:00" fixdate="2016-3-3 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement math functions for Table API</summary>
      <description>MODEXPPOWERLNLOG10ABS</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.test.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.RexNodeTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.parser.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.call.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.arithmetic.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
    </fixedFiles>
  </bug>
  <bug id="3577" opendate="2016-3-4 00:00:00" fixdate="2016-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Display anchor links when hovering over headers.</summary>
      <description>This is useful to share the document url if display anchor links when hovering over headers. Currently we must scroll up to the TOC, find the section,click it, then copy the url.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">LICENSE</file>
      <file type="M">docs..layouts.base.html</file>
      <file type="M">docs.page.js.codetabs.js</file>
      <file type="M">docs.page.css.flink.css</file>
    </fixedFiles>
  </bug>
  <bug id="3585" opendate="2016-3-7 00:00:00" fixdate="2016-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deploy scripts don&amp;#39;t support spaces in paths</summary>
      <description></description>
      <version>1.0.0,1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.generate.specific.pom.sh</file>
      <file type="M">tools.deploy.to.maven.sh</file>
    </fixedFiles>
  </bug>
  <bug id="3586" opendate="2016-3-8 00:00:00" fixdate="2016-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Risk of data overflow while use sum/count to calculate AVG value</summary>
      <description>Now, we use (sum: Long, count: Long to store AVG partial aggregate data, which may have data overflow risk, we should use unbounded data type(such as BigInteger) to store them for necessary data types.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.SumAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.MinAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.MaxAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.CountAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AvgAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.Aggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="3587" opendate="2016-3-8 00:00:00" fixdate="2016-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Calcite version to 1.7.0</summary>
      <description>We currently depend on Calcite 1.5.0. The latest stable release is 1.6.0, but I propose we bump the version to 1.7.0-SNAPSHOT to benefit from latest features. If we do that, we can also get rid of the custom FlinkJoinUnionTransposeRule.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.FlinkFilterAggregateTransposeRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.DataSetUnionRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.DataSetScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.DataSetJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.DataSetCalcRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.DataSetAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.datastream.DataStreamUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.datastream.DataStreamSource.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.datastream.DataStreamRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.datastream.DataStreamCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetSource.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.BatchTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.BatchTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.BatchTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3596" opendate="2016-3-9 00:00:00" fixdate="2016-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DataSet RelNode refactoring</summary>
      <description>After discussion with fhueske, chengxiang li, and twalthr, we have decided to make the following refactoring: Make the DataSet RelNodes correspond to logical relational operators. Move the code generation from the rules into the DataSet RelNodes. Remove the Flink RelNode layer and have a 1-pass translation instead of a 2-pass translation as currently.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.GroupedAggregationsITCase.scala.orig</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.logical.FlinkUnionRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.logical.FlinkScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.logical.FlinkJoinUnionTransposeRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.logical.FlinkJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.logical.FlinkCalcRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.logical.FlinkAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataset.DataSetUnionRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataset.DataSetScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataset.DataSetJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataset.DataSetCalcRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataset.DataSetAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.logical.FlinkUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.logical.FlinkScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.logical.FlinkRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.logical.FlinkJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.logical.FlinkConvention.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.logical.FlinkCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.logical.FlinkAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetSort.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetReduce.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetMap.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetGroupReduce.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetFlatMap.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetExchange.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.JavaBatchTranslator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="3597" opendate="2016-3-9 00:00:00" fixdate="2016-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table API operator names should reflect relational expression</summary>
      <description>The names of the DataSet operators generated by the Table API do not reflect the relational expression they represent.This should be changed to make the Flink plan (which is visualized in the web dashboard) more readable.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetSource.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="3622" opendate="2016-3-16 00:00:00" fixdate="2016-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve error messages for invalid joins</summary>
      <description>Invalid Table API joins do not show descriptive error messages: Joins without a equality join condition Joins with non-matching equality join predicates</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.DataSetJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetSource.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="3632" opendate="2016-3-17 00:00:00" fixdate="2016-5-17 01:00:00" resolution="Done">
    <buginformation>
      <summary>Clean up Table API exceptions</summary>
      <description>The Table API throws many different exception types including: IllegalArgumentException TableException CodeGenException PlanGenException ExpressionParserExceptionfrom various places of the query translation code. This needs to be cleaned up.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.table.SelectITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.ToTableITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.table.FromDataSetITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.RowTypeInfo.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.trees.TreeNode.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.sources.CsvTableSource.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.schema.FlinkTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.TableConversions.scala</file>
    </fixedFiles>
  </bug>
  <bug id="3634" opendate="2016-3-17 00:00:00" fixdate="2016-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix documentation for DataSetUtils.zipWithUniqueId()</summary>
      <description>Under FLINK-2590 the assignment and testing of unique IDs was improved but the documentation looks to still reference the old implementation.With parallelism=1 there is no difference between zipWithUniqueID and zipWithIndex. With greater parallelism the results of zipWithUniqueID are dependent on the partitioning.The documentation should demonstrate a possible result that is different from the incremental sequence of zipWithIndex while noting that results are dependent on the parallelism and partitioning.</description>
      <version>1.1.0</version>
      <fixedVersion>1.0.1,1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.batch.zip.elements.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="3636" opendate="2016-3-18 00:00:00" fixdate="2016-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NoClassDefFoundError while running WindowJoin example</summary>
      <description>Example ran with:bin/flink run examples/streaming/WindowJoin.jarorbin/flink run examples/streaming/WindowJoin.jar --windowSize 1000 --rate 2------------------------------------------------------------ The program finished with the following exception:java.lang.NoClassDefFoundError: org/apache/flink/streaming/examples/utils/ThrottledIterator at org.apache.flink.streaming.examples.join.WindowJoinSampleData$GradeSource.getSource(WindowJoinSampleData.java:65) at org.apache.flink.streaming.examples.join.WindowJoin.main(WindowJoin.java:67) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:505) at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:403) at org.apache.flink.client.program.Client.runBlocking(Client.java:248) at org.apache.flink.client.CliFrontend.executeProgramBlocking(CliFrontend.java:866) at org.apache.flink.client.CliFrontend.run(CliFrontend.java:333) at org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:1189) at org.apache.flink.client.CliFrontend.main(CliFrontend.java:1239)Caused by: java.lang.ClassNotFoundException: org.apache.flink.streaming.examples.utils.ThrottledIterator at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 13 moreEncountered after hand-building the current master.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3640" opendate="2016-3-21 00:00:00" fixdate="2016-4-21 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add support for SQL queries in DataSet programs</summary>
      <description>This issue covers the task of supporting SQL queries embedded in DataSet programs. In this mode, the input and output of a SQL query is a Table. For this issue, we need to make the following additions to the Table API: add a tEnv.sql(query: String): Table method for converting a query result into a Table integrate Calcite's SQL parser into the batch Table API translation process.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.batch.libs.table.md</file>
      <file type="M">docs.apis.batch.libs.index.md</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.FilterITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.TranslationContext.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.PlanTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.AbstractTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.JavaBatchTranslator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="3641" opendate="2016-3-21 00:00:00" fixdate="2016-6-21 01:00:00" resolution="Done">
    <buginformation>
      <summary>Document registerCachedFile API call</summary>
      <description>Flink's stable API supports the registerCachedFile API call at the ExecutionEnvironment. However, it is nowhere mentioned in the online documentation. Furthermore, the DistributedCache is also not explained.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.batch.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="3649" opendate="2016-3-22 00:00:00" fixdate="2016-6-22 01:00:00" resolution="Done">
    <buginformation>
      <summary>Document stable API methods maxBy/minBy</summary>
      <description>The Java DataSet API contains the stable API methods maxBy and minBy which are nowhere mentioned in our documentation.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.batch.index.md</file>
      <file type="M">docs.apis.batch.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  <bug id="3676" opendate="2016-3-29 00:00:00" fixdate="2016-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebClient hasn&amp;#39;t been removed from the docs</summary>
      <description></description>
      <version>1.0.0,1.1.0</version>
      <fixedVersion>1.0.0,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.docker-flink.flink.conf.flink-conf.yaml</file>
      <file type="M">flink-contrib.docker-flink.flink.config-flink.sh</file>
      <file type="M">docs.apis.common.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="3695" opendate="2016-4-4 00:00:00" fixdate="2016-3-4 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>ValueArray types</summary>
      <description>Flink provides mutable Value type implementations of Java primitives along with efficient serializers and comparators. It would be useful to have corresponding ValueArray implementations backed by primitive rather than object arrays, along with an ArrayableValue interface tying a Value to its ValueArray.</description>
      <version>1.1.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3701" opendate="2016-4-5 00:00:00" fixdate="2016-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cant call execute after first execution</summary>
      <description>in the scala shell, local mode, version 1.0 this works:Scala-Flink&gt; var b = env.fromElements("a","b")Scala-Flink&gt; b.printScala-Flink&gt; var c = env.fromElements("c","d")Scala-Flink&gt; c.printin the current master (after c.print) this leads to :java.lang.NullPointerException at org.apache.flink.api.java.ExecutionEnvironment.createProgramPlan(ExecutionEnvironment.java:1031) at org.apache.flink.api.java.ExecutionEnvironment.createProgramPlan(ExecutionEnvironment.java:961) at org.apache.flink.api.java.ScalaShellRemoteEnvironment.execute(ScalaShellRemoteEnvironment.java:70) at org.apache.flink.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:855) at org.apache.flink.api.java.DataSet.collect(DataSet.java:410) at org.apache.flink.api.java.DataSet.print(DataSet.java:1605) at org.apache.flink.api.scala.DataSet.print(DataSet.scala:1615) at .&lt;init&gt;(&lt;console&gt;:56) at .&lt;clinit&gt;(&lt;console&gt;) at .&lt;init&gt;(&lt;console&gt;:7) at .&lt;clinit&gt;(&lt;console&gt;) at $print(&lt;console&gt;) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:734) at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:983) at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573) at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:604) at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:568) at scala.tools.nsc.interpreter.ILoop.reallyInterpret$1(ILoop.scala:760) at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:805) at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:717) at scala.tools.nsc.interpreter.ILoop.processLine$1(ILoop.scala:581) at scala.tools.nsc.interpreter.ILoop.innerLoop$1(ILoop.scala:588) at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:591) at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply$mcZ$sp(ILoop.scala:882) at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:837) at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:837) at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135) at scala.tools.nsc.interpreter.ILoop.process(ILoop.scala:837) at org.apache.flink.api.scala.FlinkShell$.startShell(FlinkShell.scala:199) at org.apache.flink.api.scala.FlinkShell$.main(FlinkShell.scala:127) at org.apache.flink.api.scala.FlinkShell.main(FlinkShell.scala)</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.TaskManagerFailsWithSlotSharingITCase.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.taskmanager.TaskManagerFailsITCase.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.jobmanager.JobManagerLeaderSessionIDITSuite.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.jobmanager.JobManagerFailsITCase.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.web.WebFrontendITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.NetworkStackThroughputITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAJobGraphRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHACheckpointRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.failingPrograms.JobSubmissionFailsITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.partitioner.RescalePartitionerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.RestartStrategyTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.ExecutionConfigTest.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plantranslate.JobGraphGenerator.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobConfigHandler.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.BackPressureStatsTrackerITCase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.StackTraceSampleCoordinatorITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CoordinatorShutdownTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ExecutionGraphCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.client.JobClientActorRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.client.JobClientActorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphConstructionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphDeploymentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphRestartTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphSignalsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionStateProgressTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.LocalInputSplitsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.PointwisePatternTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.TerminalStateDeadlockTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.VertexLocationConstraintTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.VertexSlotSharingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartialConsumePipelinedResultTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobgraph.JobGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobgraph.jsonplan.JsonGeneratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobSubmitTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.ScheduleOrUpdateConsumersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.SlotCountExceedingParallelismTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.StandaloneSubmittedJobGraphStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.ZooKeeperSubmittedJobGraphsStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderChangeJobRecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderChangeStateCleanupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskAsyncCallTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskCancelAsyncProducerConsumerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskCancelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskStopTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskTest.java</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.executiongraph.TaskManagerLossFailsTasksTest.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.CoLocationConstraintITCase.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.JobManagerITCase.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.RecoveryITCase.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.SlotSharingITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="3708" opendate="2016-4-6 00:00:00" fixdate="2016-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala API for CEP</summary>
      <description>Currently, The CEP library does not support Scala case classes, because the TypeExtractor cannot handle them. In order to support them, it would be necessary to offer a Scala API for the CEP library.</description>
      <version>1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.completeness.ScalaAPICompletenessTestBase.scala</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-libraries.pom.xml</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.Pattern.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.AndFilterFunction.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.PatternStream.java</file>
      <file type="M">flink-libraries.flink-cep.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3709" opendate="2016-4-6 00:00:00" fixdate="2016-1-6 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>[streaming] Graph event rates over time</summary>
      <description>The streaming server job page displays bytes and records sent and received, which answers the question "is data moving?" The next obvious question is "is data moving over time?" That could be answered by a chart displaying bytes/events rates. This would be a great chart to add to this display.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.ExecutionGraphHolder.java</file>
    </fixedFiles>
  </bug>
  <bug id="3727" opendate="2016-4-11 00:00:00" fixdate="2016-4-11 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add support for embedded streaming SQL (projection, filter, union)</summary>
      <description>Similar to the support for SQL embedded in batch Table API programs, this issue tracks the support for SQL embedded in stream Table API programs. The only currently supported operations on streaming Tables are projection, filtering, and union.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.streaming.test.utils.StreamTestData.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.streaming.test.utils.StreamITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.streaming.test.SelectITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.streaming.test.FilterITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.schema.DataStreamTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.EnumerableToLogicalTableScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.StreamTableEnvironment.scala</file>
      <file type="M">docs.apis.batch.libs.table.md</file>
    </fixedFiles>
  </bug>
  <bug id="3728" opendate="2016-4-11 00:00:00" fixdate="2016-5-11 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Throw meaningful exceptions for unsupported SQL features</summary>
      <description>We must explicitly exclude unsupported SQL features such as Grouping Sets from being translated to Flink programs. Otherwise, the resulting program will compute invalid results.For that we must restrict the Calcite rules that translate Logical RelNodes into DataSetRel or DataStreamRel nodes.We may only translate to DataSetRel or DataStreamRel nodes if these support the semantics of the RelNode.Not translating a RelNode will yield a Calcite CannotPlanException that we should catch and enrich with a meaningful error message.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.PlanGenException.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.BatchTableEnvironment.scala</file>
      <file type="M">docs.apis.table.md</file>
    </fixedFiles>
  </bug>
  <bug id="3735" opendate="2016-4-11 00:00:00" fixdate="2016-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Embedded SQL union should fail during translation</summary>
      <description>The Table API currently only supports union all and embedded SQL queries translate both union and union all to union all. We should only generate a valid plan for union all.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.DataSetUnionRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="3739" opendate="2016-4-12 00:00:00" fixdate="2016-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a null literal to Table API</summary>
      <description>The Table API needs support for the null literal e.g. for passing null values to functions.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.ExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.sql.test.FilterITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.ExpressionsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.TypeConverter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.package.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.literals.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
      <file type="M">docs.apis.batch.libs.table.md</file>
    </fixedFiles>
  </bug>
  <bug id="3743" opendate="2016-4-12 00:00:00" fixdate="2016-4-12 01:00:00" resolution="Done">
    <buginformation>
      <summary>Upgrade breeze from 0.11.2 to 0.12</summary>
      <description>Upgrade to the a new version of breeze that is available.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-ml.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3747" opendate="2016-4-13 00:00:00" fixdate="2016-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Consolidate TimestampAssigner Methods in Kafka Consumer</summary>
      <description>On DataStream the methods for setting a TimestampAssigner/WatermarkEmitter are called assignTimestampsAndWatermarks() while on FlinkKafkaConsumer* they are called setPunctuatedWatermarkEmitter() and setPeriodicWatermarkEmitter().I think these names should be matched, also the name setWatermarkEmitter does not hint at the fact that the assigner primarily assigns timestamps.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="3748" opendate="2016-4-13 00:00:00" fixdate="2016-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CASE function to Table API</summary>
      <description>Add a CASE/WHEN functionality to Java/Scala Table API and add support for SQL API.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.ExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.sql.test.ExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.ExpressionsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.RexNodeTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.logic.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.call.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.ScalarOperators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
    </fixedFiles>
  </bug>
  <bug id="3755" opendate="2016-4-14 00:00:00" fixdate="2016-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce key groups for key-value state to support dynamic scaling</summary>
      <description>In order to support dynamic scaling, it is necessary to sub-partition the key-value states of each operator. This sub-partitioning, which produces a set of key groups, allows to easily scale in and out Flink jobs by simply reassigning the different key groups to the new set of sub tasks. The idea of key groups is described in this design document &amp;#91;1&amp;#93;. &amp;#91;1&amp;#93; https://docs.google.com/document/d/1G1OS1z3xEBOrYD4wSu-LuBCyPUWyFd9l3T9WyssQ63w/edit?usp=sharing</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase.java</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.translation.DeltaIterationTranslationTest.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.translation.CustomPartitioningTest.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.IterateITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.WindowCheckpointingITCase.java</file>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.StreamingOperatorsITCase.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.KeyedOneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBAsyncSnapshotTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.query.QueryableStateITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="3757" opendate="2016-4-14 00:00:00" fixdate="2016-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>addAccumulator does not throw Exception on duplicate accumulator name</summary>
      <description>Works if tasks are chained, but in many situations it does not, seeSee https://gist.github.com/knaufk/a0026eff9792aa905a2f586f3b0cb112Is this an undocumented feature or a valid bug?</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.RuntimeContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="3759" opendate="2016-4-14 00:00:00" fixdate="2016-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table API should throw exception is null value is encountered in non-null mode.</summary>
      <description>The Table API can be configured to omit null-checks in generated code to speed up processing. Currently, the generated code replaces a null value with a data type specific default value if it is encountered in non-null-check mode. This can silently cause wrong results and should be changed such that an exception is thrown if a null value is encountered in non-null-check mode.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.ExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.sql.test.ExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.ExpressionsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="3768" opendate="2016-4-15 00:00:00" fixdate="2016-5-15 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Clustering Coefficient</summary>
      <description>The local clustering coefficient measures the connectedness of each vertex's neighborhood. Values range from 0.0 (no edges between neighbors) to 1.0 (neighborhood is a clique).</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.batch.libs.gelly.md</file>
    </fixedFiles>
  </bug>
  <bug id="3772" opendate="2016-4-16 00:00:00" fixdate="2016-5-16 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Graph algorithms for vertex and edge degree</summary>
      <description>Many graph algorithms require vertices or edges to be marked with the degree. This ticket provides algorithms for annotating vertex degree for undirected graphs vertex out-, in-, and out- and in-degree for directed graphs edge source, target, and source and target degree for undirected graphs</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.batch.libs.gelly.md</file>
    </fixedFiles>
  </bug>
  <bug id="3775" opendate="2016-4-18 00:00:00" fixdate="2016-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink Scala shell does not respect Flink configuration</summary>
      <description>The Flink Scala shell does not load Flink's configuration properly. This makes it impossible to connect to a remote HA cluster, for example. The reason is that the GlobalConfiguration is never properly loaded. I think the Scala shell should respect the Flink settings specified in flink-conf.yaml if available.</description>
      <version>1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellLocalStartupITCase.scala</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
    </fixedFiles>
  </bug>
  <bug id="3776" opendate="2016-4-18 00:00:00" fixdate="2016-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink Scala shell does not allow to set configuration for local execution</summary>
      <description>Flink's Scala shell starts a LocalFlinkMiniCluster with an empty configuration when the shell is started in local mode. In order to allow the user to configure the mini cluster, e.g., number of slots, size of memory, it would be good to forward a user specified configuration.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
    </fixedFiles>
  </bug>
  <bug id="3804" opendate="2016-4-22 00:00:00" fixdate="2016-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update YARN documentation</summary>
      <description>The YARN documentation should be reviewed and updated so that it reflects the current state. It contains parts which are no longer valid. E.g., the statement "The ports Flink is using for its services are the standard ports configured by the user + the application id as an offset" is outdated.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.yarn.setup.md</file>
      <file type="M">docs.apis.streaming.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="3824" opendate="2016-4-26 00:00:00" fixdate="2016-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ResourceManager may repeatedly connect to outdated JobManager in HA mode</summary>
      <description>When the ResourceManager receives a new leading JobManager via the LeaderRetrievalService it tries to register with this JobManager until connected. If during registration a new leader gets elected, the ResourceManager may still repeatedly try to register with the old one. This doesn't affect the registration with the new JobManager but leaves error messages in the log file and may process unnecessary messages.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.FlinkResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="3825" opendate="2016-4-26 00:00:00" fixdate="2016-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update CEP documentation to include Scala API</summary>
      <description>After adding the Scala CEP API FLINK-3708, we should update the online documentation to also contain the Scala API. This can be done similarly to the DataSet and DataStream API by providing Java and Scala code for all examples.</description>
      <version>1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.streaming.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug id="3835" opendate="2016-4-27 00:00:00" fixdate="2016-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JSON execution plan not helpful to debug plans with KeySelectors</summary>
      <description>The JSON execution plans are not helpful when debugging plans that include join operators with key selectors. For joins with hash join strategy, the driver strategy shows: "Hybrid Hash (build: Key Extractor)" where (build: Key Extractor) shall help to identify the build side of the join. However, if both inputs use KeySelectors, the build side cannot be identified.I propose to add the operator id to the build side information. The same issue applied for cross driver strategies.</description>
      <version>1.0.2,1.1.0</version>
      <fixedVersion>1.0.3,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plandump.PlanJSONDumpGenerator.java</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testJoin1.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testJoin0.out</file>
    </fixedFiles>
  </bug>
  <bug id="3840" opendate="2016-4-28 00:00:00" fixdate="2016-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RocksDB local parent dir is polluted with empty folders with random names</summary>
      <description>For some reason when the job starts the rocksdb root dir filled with hundreds of empty folders with random names like:041da1c-5fec-42ed-a69c-298240f1a065 4e6061aa-0c69-4755-a1ad-5ac4dec1d3f0 a7004bd1-778c-4a0f-96d4-9941208d188800db8406-6cb4-46ad-aac9-beeaa3247d16</description>
      <version>1.0.0,1.0.1,1.0.2,1.1.0</version>
      <fixedVersion>1.0.3,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="3847" opendate="2016-4-28 00:00:00" fixdate="2016-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reorganize package structure of flink-table</summary>
      <description>The flink-table module has tests for the matrix of Java and Scala Batch and Streaming Table API and SQLRight now, there is no consistent package structure for the tests. I propose to structure the test packages as follows:test/java/o/a/f/api/table/batch/tabletest/java/o/a/f/api/table/batch/sqltest/java/o/a/f/api/table/stream/tabletest/java/o/a/f/api/table/stream/sqltest/scala/o/a/f/api/table/batch/tabletest/scala/o/a/f/api/table/batch/sqltest/scala/o/a/f/api/table/stream/tabletest/scala/o/a/f/api/table/stream/sql</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.test.utils.ExpressionEvaluator.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.typeutils.RowSerializerTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.typeutils.RowComparatorTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.test.utils.TableProgramsTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.sql.test.BatchSQLITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.sql.test.StreamingSQLITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.AggregationsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.CastingITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.DistinctITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.ExpressionsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.FilterITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.FromDataSetITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.GroupedAggregationsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.JoinITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.PojoGroupingITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.SelectITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.SqlExplainTest.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.StringExpressionsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.TableEnvironmentITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.TableSourceITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.UnionITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.utils.StreamTestData.java</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.sql.streaming.test.StreamSQLITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.sql.test.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.sql.test.ExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.sql.test.FilterITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.sql.test.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.sql.test.SelectITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.sql.test.TableWithSQLITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.sql.test.UnionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.streaming.test.FilterITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.streaming.test.SelectITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.streaming.test.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.streaming.test.UnionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.streaming.test.UnsupportedOpsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.streaming.test.utils.StreamITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.streaming.test.utils.StreamTestData.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.CastingITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.DistinctITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.ExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.FilterITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.GroupedAggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.SelectITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.SqlExplainTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.StringExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.TableEnvironmentITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.ToTableITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.UnionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.test.ScalarFunctionsTest.scala</file>
    </fixedFiles>
  </bug>
  <bug id="3853" opendate="2016-4-29 00:00:00" fixdate="2016-5-29 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Reduce object creation in Gelly utility mappers</summary>
      <description>Gelly contains a set of MapFunction between Vertex and Tuple2 and between Edge and Tuple3. A Vertex is a Tuple2 and an Edge is a Tuple3, and conversion in the opposite direction can be performed with a single object per MapFunction.This only applies to the Gelly Java API. Scala tuples are not related to Vertex or Edge.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.GraphMutationsITCase.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.utils.VertexToTuple2Map.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.utils.Tuple3ToEdgeMap.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.utils.Tuple2ToVertexMap.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.utils.EdgeToTuple3Map.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.Graph.java</file>
      <file type="M">flink-libraries.flink-gelly-scala.src.test.scala.org.apache.flink.graph.scala.test.operations.GraphMutationsITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="3859" opendate="2016-5-2 00:00:00" fixdate="2016-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add BigDecimal/BigInteger support to Table API</summary>
      <description>Since FLINK-3786 has been solved, we can now start integrating BigDecimal/BigInteger into the Table API.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.SumAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.MinAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.MaxAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.AvgAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.AggregateTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.utils.ExpressionEvaluator.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.ExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.TypeConverter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.TypeCoercion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.TypeCheckUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.SumAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.MinAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.MaxAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AvgAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.literals.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.comparison.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.arithmetic.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.ScalarOperators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.FloorCeilCallGen.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
      <file type="M">docs.apis.table.md</file>
    </fixedFiles>
  </bug>
  <bug id="386" opendate="2014-6-9 00:00:00" fixdate="2014-7-9 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Duplicate dependencies in lib folder</summary>
      <description>There are some dependencies for which two different versions end up in the lib folder```servlet-api-2.5-6.1.14.jarservlet-api-3.0.20100224.jarstax-api-1.0.1.jarstax-api-1.0-2.jar```As far as I see it, these are transitive dependencies.Teh problem is that what classes are available at runtime depends on which jar is loaded first by the classloader. This is somewhat unpredictable and can lead to weird errors in the components that have these dependencies. ---------------- Imported from GitHub ----------------Url: https://github.com/stratosphere/stratosphere/issues/386Created by: StephanEwenLabels: Created at: Thu Jan 09 19:44:07 CET 2014State: open</description>
      <version>None</version>
      <fixedVersion>pre-apache</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CombineTaskExternalITCase.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.util.FailingTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.MapITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.BulkIterationWithAllReducerITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.aggregators.ConnectedComponentsWithParametrizableAggregatorITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.aggregators.AggregatorsITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.cancelling.CancellingTestBase.java</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-test-utils.pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.RecordOutputEmitterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.OutputEmitterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.TaskCancelThread.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DriverTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.ExternalSortITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.CombiningUnilateralSortMergerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.AsynchonousPartialSorterITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.resettable.SpillingResettableMutableObjectIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.resettable.SpillingResettableIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.resettable.BlockResettableMutableObjectIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.resettable.BlockResettableIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.ReduceTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.ReduceTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.MatchTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.DataSourceTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.DataSinkTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CrossTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CrossTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CombineTaskTest.java</file>
      <file type="M">flink-addons.flink-avro.pom.xml</file>
      <file type="M">flink-addons.flink-avro.src.test.java.org.apache.flink.api.java.record.io.avro.AvroRecordInputFormatTest.java</file>
      <file type="M">flink-addons.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCInputFormatTest.java</file>
      <file type="M">flink-addons.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCOutputFormatTest.java</file>
      <file type="M">flink-addons.flink-jdbc.src.test.java.org.apache.flink.api.java.record.io.jdbc.JDBCInputFormatTest.java</file>
      <file type="M">flink-addons.flink-jdbc.src.test.java.org.apache.flink.api.java.record.io.jdbc.JDBCOutputFormatTest.java</file>
      <file type="M">flink-clients.pom.xml</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.DOPChangeTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.Configuration.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.io.FileOutputFormatTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.SerializerTestBase.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.distributions.SimpleDataDistributionTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.types.CollectionsDataTypeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.types.NormalizableKeyTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.types.PrimitiveDataTypeTest.java</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.functions.SemanticPropUtilTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.AggregateOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.CoGroupOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.CrossOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.DistinctOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.GroupingTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.JoinOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.ProjectionOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.record.io.CsvInputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.record.io.CsvOutputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.record.io.ExternalProcessFixedLengthInputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.record.io.ExternalProcessInputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.tuple.Tuple2Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.TypeInfoParserTest.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.filecache.FileCacheDeleteValidationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.instance.DefaultInstanceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.instance.HostInClusterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.instance.local.LocalInstanceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.ChannelViewsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.iomanager.IOManagerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.iomanager.IOManagerPerformanceBenchmark.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.iomanager.IOManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.SpillingBufferTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.InboundEnvelopeDecoderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyConnectionManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.OutboundEnvelopeEncoderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.serialization.DataInputOutputSerializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.serialization.SpanningRecordSerializationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.serialization.SpanningRecordSerializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.memory.DefaultMemoryManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.memory.MemorySegmentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CoGroupTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CoGroupTaskTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="3871" opendate="2016-5-4 00:00:00" fixdate="2016-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Kafka TableSource with Avro serialization</summary>
      <description>Add a Kafka TableSource which supports Avro serialized data.The KafkaAvroTableSource should support two modes: SpecificRecord Mode: In this case the user specifies a class which was code-generated by Avro depending on a schema. Flink treats these classes as regular POJOs. Hence, they are also natively supported by the Table API and SQL. Classes generated by Avro contain their Schema in a static field. The schema should be used to automatically derive field names and types. Hence, there is no additional information required than the name of the class. GenericRecord Mode: In this case the user specifies an Avro Schema. The schema is used to deserialize the data into a GenericRecord which must be translated into possibly nested Row based on the schema information. Again, the Avro Schema is used to automatically derive the field names and types. This mode is less efficient than the SpecificRecord mode because the GenericRecord needs to be converted into Row.This feature depends on FLINK-5280, i.e., support for nested data in TableSource.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.Types.scala</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.RowTypeInfo.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeinfo.Types.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSinkTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.JsonRowSerializationSchemaTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.JsonRowDeserializationSchemaTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.JsonRowDeserializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaJsonTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.TypeUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010JsonTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="3872" opendate="2016-5-4 00:00:00" fixdate="2016-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Kafka TableSource with JSON serialization</summary>
      <description>Add a Kafka TableSource which reads JSON serialized data.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09ITCase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08ITCase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.pom.xml</file>
      <file type="M">docs.apis.table.md</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3875" opendate="2016-5-4 00:00:00" fixdate="2016-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a TableSink for Elasticsearch</summary>
      <description>Add a TableSink that writes data to Elasticsearch</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.typeutils.TypeCheckUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.sinks.UpsertStreamTableSink.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.StreamTableDescriptorValidator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.StreamTableDescriptor.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.DescriptorProperties.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.elasticsearch.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.elasticsearch-common.sh</file>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
      <file type="M">docs.dev.table.connect.md</file>
    </fixedFiles>
  </bug>
  <bug id="3879" opendate="2016-5-6 00:00:00" fixdate="2016-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Native implementation of HITS algorithm</summary>
      <description>Hyperlink-Induced Topic Search (HITS, also "hubs and authorities") is presented in &amp;#91;0&amp;#93; and described in &amp;#91;1&amp;#93;."&amp;#91;HITS&amp;#93; is a very popular and effective algorithm to rank documents based on the link information among a set of documents. The algorithm presumes that a good hub is a document that points to many others, and a good authority is a document that many documents point to." https://pdfs.semanticscholar.org/a8d7/c7a4c53a9102c4239356f9072ec62ca5e62f.pdfThis implementation differs from FLINK-2044 by providing for convergence, outputting both hub and authority scores, and completing in half the number of iterations.&amp;#91;0&amp;#93; http://www.cs.cornell.edu/home/kleinber/auth.pdf&amp;#91;1&amp;#93; https://en.wikipedia.org/wiki/HITS_algorithm</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.library.clustering.undirected.LocalClusteringCoefficientTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.library.clustering.directed.LocalClusteringCoefficientTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.HITSAlgorithm.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.test.java.org.apache.flink.graph.library.HITSAlgorithmITCase.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.examples.JaccardIndex.java</file>
      <file type="M">docs.apis.batch.libs.gelly.md</file>
    </fixedFiles>
  </bug>
  <bug id="3880" opendate="2016-5-6 00:00:00" fixdate="2016-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve performance of Accumulator map</summary>
      <description>I was looking at improving DataSet performance - this is for a job created using the Cascading-Flink planner for Cascading 3.1.While doing a quick "poor man's profiler" session with one of the TaskManager processes, I noticed that many (most?) of the threads that were actually running were in this state:"DataSource (/working1/terms) (8/20)" daemon prio=10 tid=0x00007f55673e0800 nid=0x666a runnable [0x00007f556abcf000] java.lang.Thread.State: RUNNABLE at java.util.Collections$SynchronizedMap.get(Collections.java:2037) - locked &lt;0x00000006e73fe718&gt; (a java.util.Collections$SynchronizedMap) at org.apache.flink.api.common.functions.util.AbstractRuntimeUDFContext.getAccumulator(AbstractRuntimeUDFContext.java:162) at org.apache.flink.api.common.functions.util.AbstractRuntimeUDFContext.getLongCounter(AbstractRuntimeUDFContext.java:113) at com.dataartisans.flink.cascading.runtime.util.FlinkFlowProcess.getOrInitCounter(FlinkFlowProcess.java:245) at com.dataartisans.flink.cascading.runtime.util.FlinkFlowProcess.increment(FlinkFlowProcess.java:128) at com.dataartisans.flink.cascading.runtime.util.FlinkFlowProcess.increment(FlinkFlowProcess.java:122) at cascading.tap.hadoop.util.MeasuredRecordReader.next(MeasuredRecordReader.java:65) at cascading.scheme.hadoop.SequenceFile.source(SequenceFile.java:97) at cascading.tuple.TupleEntrySchemeIterator.getNext(TupleEntrySchemeIterator.java:166) at cascading.tuple.TupleEntrySchemeIterator.hasNext(TupleEntrySchemeIterator.java:139) at com.dataartisans.flink.cascading.runtime.source.TapSourceStage.readNextRecord(TapSourceStage.java:70) at com.dataartisans.flink.cascading.runtime.source.TapInputFormat.reachedEnd(TapInputFormat.java:175) at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:173) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:559) at java.lang.Thread.run(Thread.java:745)}}}It looks like Cascading is asking Flink to increment a counter with each Tuple read, and that in turn is often blocked on getting access to the Accumulator object in a map. It looks like this is a SynchronizedMap, but using a ConcurrentHashMap (for example) would reduce this contention.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.accumulators.AccumulatorRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="3883" opendate="2016-5-8 00:00:00" fixdate="2016-8-8 01:00:00" resolution="Abandoned">
    <buginformation>
      <summary>Rename flink clients for inclusion on system path</summary>
      <description>I ran into some trouble in preparing a Homebrew formula to install Flink on Mac (homebrew-core#968). Homebrew can install bin scripts into the system path, e.g. `/usr/local/bin/flink`:$ cat /usr/local/bin/flink#!/bin/bashexec "/usr/local/Cellar/apache-flink/1.0.2/libexec/bin/flink" "$@"It would be nice to install the various Flink shells too, but their names don't seem ideal for inclusion on the system path. I propose that they be renamed or aliased to have the following names (old -&gt; new):"pyflink2.sh" -&gt; "pyflink2""pyflink3.sh" -&gt; "pyflink3""start-scala-shell.sh" -&gt; "flink-scala-shell" (note: updated based on feedback)A related issue is, the shell scripts don't correctly determine their install location when called via a symlink.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.deploy.to.maven.sh</file>
    </fixedFiles>
  </bug>
  <bug id="3898" opendate="2016-5-11 00:00:00" fixdate="2016-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adamic-Adar Similarity</summary>
      <description>The implementation of Adamic-Adar Similarity &amp;#91;0&amp;#93; is very close to Jaccard Similarity. Whereas Jaccard Similarity counts common neighbors, Adamic-Adar Similarity sums the inverse logarithm of the degree of common neighbors.Consideration will be given to the computation of the inverse logarithm, in particular whether to pre-compute a small array of values.&amp;#91;0&amp;#93; http://social.cs.uiuc.edu/class/cs591kgk/friendsadamic.pdf</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.utils.Murmur3.32.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.similarity.JaccardIndex.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.clustering.directed.LocalClusteringCoefficient.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.generator.SingletonEdgeGraph.java</file>
      <file type="M">docs.apis.batch.libs.gelly.md</file>
    </fixedFiles>
  </bug>
  <bug id="3907" opendate="2016-5-13 00:00:00" fixdate="2016-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Directed Clustering Coefficient</summary>
      <description>A directed clustering coefficient algorithm can be implemented using an efficient triangle listing implementation which emits not only the three vertex IDs forming the triangle but also a bitmask indicating which edges form the triangle. A triangle can be formed with a minimum of three or maximum of six directed edges. Directed clustering coefficient can then shatter the triangles and emit a score of either 1 or 2 for each vertex.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.library.similarity.JaccardIndexTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.library.clustering.undirected.LocalClusteringCoefficientTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.asm.degree.annotate.directed.VertexOutDegreeTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.asm.degree.annotate.directed.VertexInDegreeTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.asm.degree.annotate.directed.VertexDegreesTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.asm.degree.annotate.directed.EdgeTargetDegreesTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.asm.degree.annotate.directed.EdgeSourceDegreesTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.asm.degree.annotate.directed.EdgeDegreesPairTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.asm.AsmTestBase.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.utils.Murmur3.32.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.metric.undirected.VertexMetrics.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.clustering.undirected.TriangleListing.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.clustering.undirected.TriangleCount.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.clustering.undirected.LocalClusteringCoefficient.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.clustering.undirected.GlobalClusteringCoefficient.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.GraphAnalytic.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.Graph.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.degree.annotate.directed.VertexDegrees.java</file>
      <file type="M">flink-libraries.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.Graph.scala</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.examples.TriangleListing.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.examples.LocalClusteringCoefficient.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.examples.JaccardIndex.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.Utils.java</file>
      <file type="M">docs.apis.batch.libs.gelly.md</file>
    </fixedFiles>
  </bug>
  <bug id="3909" opendate="2016-5-13 00:00:00" fixdate="2016-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Maven Failsafe plugin may report SUCCESS on failed tests</summary>
      <description>The following build completed successfully on Travis but there are actually test failures: https://travis-ci.org/apache/flink/jobs/129943398#L5402</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-nifi.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-elasticsearch.pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-java8.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3913" opendate="2016-5-15 00:00:00" fixdate="2016-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up documentation typos</summary>
      <description>While reading through the docs I noticed a few spelling mistakes. We should do a quick review of the docs and fix up any mistakes found.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.yarn.setup.md</file>
      <file type="M">docs.setup.local.setup.md</file>
      <file type="M">docs.setup.config.md</file>
      <file type="M">docs.internals.monitoring.rest.api.md</file>
      <file type="M">docs.internals.job.scheduling.md</file>
      <file type="M">docs.internals.general.arch.md</file>
      <file type="M">docs.internals.back.pressure.monitoring.md</file>
      <file type="M">docs.apis.table.md</file>
      <file type="M">docs.apis.streaming.storm.compatibility.md</file>
      <file type="M">docs.apis.streaming.index.md</file>
      <file type="M">docs.apis.streaming.fault.tolerance.md</file>
      <file type="M">docs.apis.streaming.event.timestamps.watermarks.md</file>
      <file type="M">docs.apis.streaming.event.time.md</file>
      <file type="M">docs.apis.streaming.connectors.rabbitmq.md</file>
      <file type="M">docs.apis.scala.shell.md</file>
      <file type="M">docs.apis.common.index.md</file>
      <file type="M">docs.apis.best.practices.md</file>
      <file type="M">docs.apis.batch.python.md</file>
      <file type="M">docs.apis.batch.libs.ml.pipelines.md</file>
      <file type="M">docs.apis.batch.libs.ml.multiple.linear.regression.md</file>
      <file type="M">docs.apis.batch.libs.gelly.md</file>
      <file type="M">docs.apis.batch.iterations.md</file>
      <file type="M">docs.apis.batch.index.md</file>
      <file type="M">docs.apis.batch.examples.md</file>
      <file type="M">docs.apis.batch.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  <bug id="3923" opendate="2016-5-18 00:00:00" fixdate="2016-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unify configuration conventions of the Kinesis producer to the same as the consumer</summary>
      <description>Currently, the Kinesis consumer and producer are configured differently.The producer expects a list of arguments for the access key, secret, region, stream. The consumer is accepting properties (similar to the Kafka connector).The objective of this issue is to change the producer so that it is also using a properties-based configuration (including an input validation step)</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualProducerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualExactlyOnceTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualConsumerProducerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.model.KinesisStreamShard.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumerThread.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.examples.ProduceIntoKinesis.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.KinesisConfigConstants.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.pom.xml</file>
      <file type="M">tools.create.release.files.sh</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtil.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisProducer.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.util.KafkaUtils.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.SimpleConsumerThread.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer08.java</file>
      <file type="M">docs.apis.streaming.connectors.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug id="3925" opendate="2016-5-18 00:00:00" fixdate="2016-6-18 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>GraphAlgorithm to filter by maximum degree</summary>
      <description>Filtering by minimum degree is K-Core which is iterative. Filtering by maximum degree can be performed in constant time by filtering the set of high-degree vertices then doing an anti-join against the original vertex set and two anti-joins against the original edge set.Two reasons to remove high-degree vertices: 1) they may simply be noise in the input data, and 2) speedup algorithms such as Adamic-Adar and Jaccard Index which run quadratic in the vertex degree.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.batch.libs.gelly.md</file>
    </fixedFiles>
  </bug>
  <bug id="3927" opendate="2016-5-18 00:00:00" fixdate="2016-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TaskManager registration may fail if Yarn versions don&amp;#39;t match</summary>
      <description>Flink's ResourceManager uses the Yarn container ids to identify connecting task managers. Yarn's stringified container id may not be consistent across different Hadoop versions, e.g. Hadoop 2.3.0 and Hadoop 2.7.1. The ResourceManager gets it from the Yarn reports while the TaskManager infers it from the Yarn environment variables. The ResourceManager may use Hadoop 2.3.0 version while the cluster runs Hadoop 2.7.1. The solution is to pass the ID through a custom environment variable which is set by the ResourceManager before launching the TaskManager in the container. That way we will always use the Hadoop client's id generation method.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnTaskManagerRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnFlinkResourceManager.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnContainerInLaunch.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.RegisteredYarnWorkerNode.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.ResourceID.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.FlinkResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="3929" opendate="2016-5-18 00:00:00" fixdate="2016-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support for Kerberos Authentication with Keytab Credential</summary>
      <description>This issue is part of a series of improvements detailed in the Secure Data Access design doc.Add support for a keytab credential to be associated with the Flink cluster, to facilitate: Kerberos-authenticated data access for connectors Kerberos-authenticated ZooKeeper accessSupport both the standalone and YARN deployment modes.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.SecureTestEnvironment.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09SecureRunITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.SecurityContextTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.SecurityContext.java</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.flink-jaas.conf</file>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.RollingSinkSecuredITCase.java</file>
      <file type="M">tools.log4j-travis.properties</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnTaskManagerRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnConfigKeys.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnApplicationMasterRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-yarn-tests.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNHighAvailabilityITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.FlinkYarnSessionCliTest.java</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.StreamingMultipleProgramsTestBase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.DataGenerators.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaShortRetentionTestBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09ProducerITCase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09ITCase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08ProducerITCase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.RollingSinkITCase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.pom.xml</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.java</file>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.Preconditions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="3932" opendate="2016-5-18 00:00:00" fixdate="2016-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement State Backend Security</summary>
      <description>This issue is part of a series of improvements detailed in the Secure Data Access design doc.Flink should protect its HA, checkpoint, and savepoint state against unauthorized access.As described in the design doc, implement: ZooKeeper authentication w/ Kerberos ZooKeeper authorization (i.e. znode ACLs) Checkpoint/savepoint data protection</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="3936" opendate="2016-5-19 00:00:00" fixdate="2016-5-19 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add MIN / MAX aggregations function for BOOLEAN types</summary>
      <description>When executing TPC-H Q4, I observed that Calcite generates a MIN aggregate on Boolean literals to translate a decorrelate subquery in an EXIST predicate.MIN and MAX aggregates on Boolean data types are currently not supported and should be added.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.MinAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.MaxAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.MinAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.MaxAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateUtil.scala</file>
    </fixedFiles>
  </bug>
  <bug id="3938" opendate="2016-5-19 00:00:00" fixdate="2016-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn tests don&amp;#39;t run on the current master</summary>
      <description>Independently of FLINK-3909, I just discovered that the Yarn tests don't run on the current master (09b428b).[INFO] ------------------------------------------------------------------------[INFO] Building flink-yarn-tests 1.1-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] [INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ flink-yarn-tests_2.10 ---[INFO] [INFO] --- maven-checkstyle-plugin:2.16:check (validate) @ flink-yarn-tests_2.10 ---[INFO] [INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-maven) @ flink-yarn-tests_2.10 ---[INFO] [INFO] --- build-helper-maven-plugin:1.7:add-source (add-source) @ flink-yarn-tests_2.10 ---[INFO] Source directory: /home/travis/build/apache/flink/flink-yarn-tests/src/main/scala added.[INFO] [INFO] --- maven-remote-resources-plugin:1.5:process (default) @ flink-yarn-tests_2.10 ---[INFO] [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ flink-yarn-tests_2.10 ---[INFO] Using 'UTF-8' encoding to copy filtered resources.[INFO] skip non existing resourceDirectory /home/travis/build/apache/flink/flink-yarn-tests/src/main/resources[INFO] Copying 3 resources[INFO] [INFO] --- scala-maven-plugin:3.1.4:compile (scala-compile-first) @ flink-yarn-tests_2.10 ---[INFO] No sources to compile[INFO] [INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ flink-yarn-tests_2.10 ---[INFO] No sources to compile[INFO] [INFO] --- build-helper-maven-plugin:1.7:add-test-source (add-test-source) @ flink-yarn-tests_2.10 ---[INFO] Test Source directory: /home/travis/build/apache/flink/flink-yarn-tests/src/test/scala added.[INFO] [INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ flink-yarn-tests_2.10 ---[INFO] Using 'UTF-8' encoding to copy filtered resources.[INFO] Copying 1 resource[INFO] Copying 3 resources[INFO] [INFO] --- scala-maven-plugin:3.1.4:testCompile (scala-test-compile) @ flink-yarn-tests_2.10 ---[INFO] /home/travis/build/apache/flink/flink-yarn-tests/src/test/scala:-1: info: compiling[INFO] Compiling 2 source files to /home/travis/build/apache/flink/flink-yarn-tests/target/test-classes at 1463615798796[INFO] prepare-compile in 0 s[INFO] compile in 9 s[INFO] [INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ flink-yarn-tests_2.10 ---[INFO] Nothing to compile - all classes are up to date[INFO] [INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ flink-yarn-tests_2.10 ---[INFO] Surefire report directory: /home/travis/build/apache/flink/flink-yarn-tests/target/surefire-reports[WARNING] The system property log4j.configuration is configured twice! The property appears in &lt;argLine/&gt; and any of &lt;systemPropertyVariables/&gt;, &lt;systemProperties/&gt; or user property.------------------------------------------------------- T E S T S-------------------------------------------------------Results :Tests run: 0, Failures: 0, Errors: 0, Skipped: 0</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3942" opendate="2016-5-20 00:00:00" fixdate="2016-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for INTERSECT</summary>
      <description>Currently, the Table API and SQL do not support INTERSECT.INTERSECT can be executed as join on all fields.In order to add support for INTERSECT to the Table API and SQL we need to: Implement a DataSetIntersect class that translates an INTERSECT into a DataSet API program using a join on all fields. Implement a DataSetIntersectRule that translates a Calcite LogicalIntersect into a DataSetIntersect. Extend the Table API (and validation phase) to provide an intersect() method.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.UnionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.UnionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetSort.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.logical.operators.scala</file>
      <file type="M">docs.apis.table.md</file>
    </fixedFiles>
  </bug>
  <bug id="3943" opendate="2016-5-20 00:00:00" fixdate="2016-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for EXCEPT (set minus)</summary>
      <description>Currently, the Table API and SQL do not support EXCEPT.EXCEPT can be executed as a coGroup on all fields that forwards records of the first input if the second input is empty.In order to add support for EXCEPT to the Table API and SQL we need to: Implement a DataSetMinus class that translates an EXCEPT into a DataSet API program using a coGroup on all fields. Implement a DataSetMinusRule that translates a Calcite LogicalMinus into a DataSetMinus. Extend the Table API (and validation phase) to provide an except() method.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.util.CollectionDataSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.table.UnsupportedOpsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.IntersectCoGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.DataSetIntersectRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetIntersect.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.logical.operators.scala</file>
      <file type="M">docs.apis.table.md</file>
    </fixedFiles>
  </bug>
  <bug id="3945" opendate="2016-5-20 00:00:00" fixdate="2016-6-20 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Degree annotation for directed graphs</summary>
      <description>There is a third degree count for vertices in directed graphs which is the distinct count of out- and in-neighbors. This also adds edge annotation of the vertex degrees for directed graphs.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.asm.degree.annotate.directed.VertexOutDegreeTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.asm.degree.annotate.directed.VertexInDegreeTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.asm.degree.annotate.directed.VertexDegreePairTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.degree.annotate.undirected.VertexDegree.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.degree.annotate.undirected.EdgeTargetDegree.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.degree.annotate.undirected.EdgeSourceDegree.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.degree.annotate.undirected.EdgeDegreePair.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.degree.annotate.package-info.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.degree.annotate.directed.VertexOutDegree.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.degree.annotate.directed.VertexInDegree.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.degree.annotate.directed.VertexDegreePair.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.asm.degree.annotate.DegreeAnnotationFunctions.java</file>
      <file type="M">docs.apis.batch.libs.gelly.md</file>
    </fixedFiles>
  </bug>
  <bug id="3948" opendate="2016-5-21 00:00:00" fixdate="2016-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>EventTimeWindowCheckpointingITCase Fails with Core Dump</summary>
      <description>It fails because of a core dump in RocksDB.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3949" opendate="2016-5-22 00:00:00" fixdate="2016-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Collect Metrics in Runtime Operators</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.UnregisteredTaskMetricsGroup.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamIterationTail.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DataSourceTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.source.InputFormatSourceFunctionTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.groups.IOMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.reader.BufferReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.reader.ReaderBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.AdaptiveSpanningRecordDeserializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.RecordDeserializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.UnknownInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AbstractCachedBuildSideJoinDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AbstractOuterJoinDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AllGroupCombineDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AllReduceDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.BatchTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.ChainedAllReduceDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.ChainedDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.ChainedFlatMapDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.ChainedMapDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.ChainedTerminationCriterionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.GroupCombineChainedDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.chaining.SynchronousChainedCombineDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.CoGroupDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.CrossDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DataSinkTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.FlatMapDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.GroupReduceDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.JoinDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.MapDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.MapPartitionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.NoOpChainedDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.NoOpDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.ReduceCombineDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.ReduceDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.UnionWithTempOperator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.reader.AbstractReaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.TestSingleInputGate.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.UnionInputGateTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="3950" opendate="2016-5-22 00:00:00" fixdate="2016-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Meter Metric Type</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.ProxyMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.AbstractMetricGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-statsd.src.test.java.org.apache.flink.metrics.statsd.StatsDReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-statsd.src.main.java.org.apache.flink.metrics.statsd.StatsDReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.metrics.jmx.JMXReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.main.java.org.apache.flink.metrics.jmx.JMXReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.test.java.org.apache.flink.dropwizard.ScheduledDropwizardReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.main.java.org.apache.flink.dropwizard.ScheduledDropwizardReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.main.java.org.apache.flink.dropwizard.metrics.DropwizardHistogramWrapper.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.reporter.AbstractReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.MetricGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.groups.UnregisteredMetricsGroup.java</file>
      <file type="M">docs.monitoring.metrics.md</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="3951" opendate="2016-5-22 00:00:00" fixdate="2016-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Histogram Metric Type</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-metric-reporters.pom.xml</file>
      <file type="M">flink-metric-reporters.flink-metrics-statsd.src.main.java.org.apache.flink.metrics.statsd.StatsDReporter.java</file>
      <file type="M">flink-metric-reporters.flink-metrics-statsd.pom.xml</file>
      <file type="M">flink-metric-reporters.flink-metrics-graphite.src.main.java.org.apache.flink.metrics.graphite.GraphiteReporter.java</file>
      <file type="M">flink-metric-reporters.flink-metrics-graphite.pom.xml</file>
      <file type="M">flink-metric-reporters.flink-metrics-ganglia.src.main.java.org.apache.flink.metrics.ganglia.GangliaReporter.java</file>
      <file type="M">flink-metric-reporters.flink-metrics-ganglia.pom.xml</file>
      <file type="M">flink-metric-reporters.flink-metrics-dropwizard.src.main.java.org.apache.flink.dropwizard.ScheduledDropwizardReporter.java</file>
      <file type="M">flink-metric-reporters.flink-metrics-dropwizard.src.main.java.org.apache.flink.dropwizard.metrics.GaugeWrapper.java</file>
      <file type="M">flink-metric-reporters.flink-metrics-dropwizard.src.main.java.org.apache.flink.dropwizard.metrics.CounterWrapper.java</file>
      <file type="M">flink-metric-reporters.flink-metrics-dropwizard.pom.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.reporter.JMXReporterTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.MetricRegistryTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.groups.MetricGroupRegistrationTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.reporter.JMXReporter.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.reporter.AbstractReporter.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.MetricGroup.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.groups.UnregisteredMetricsGroup.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.groups.AbstractMetricGroup.java</file>
    </fixedFiles>
  </bug>
  <bug id="3952" opendate="2016-5-22 00:00:00" fixdate="2016-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Netty to 4.1</summary>
      <description>Netty 4.1 is about to release final. This release has a number of significant enhancements, and in particular I find HTTP/2 codecs to be incredibly desirable to have. Additionally, hopefully, the Hadoop patches for Netty 4.1 get some tests and get merged, &amp; I believe if/when that happens it'll be important for Flink to also be using the new Netty minor version.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.minicluster.LocalFlinkMiniClusterITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyMessageSerializationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.ReadOnlySlicedBufferTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.BufferTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.AbstractByteBufTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.router.RouterHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.router.RoutedRequest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.FileUploadHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.AbstractHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.ReadOnlySlicedNetworkBuffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.NetworkBuffer.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.HttpRequestHandler.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.NettyBufferPool.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.ChunkedByteBuf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3953" opendate="2016-5-23 00:00:00" fixdate="2016-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Surefire plugin executes unit tests twice</summary>
      <description>After FLINK-3909 the unit tests are executed twice. There are now two executions defined for the Surefire plugin: unit-tests and integration-tests. In addition, there is a default execution called default-test. This leads to the unit tests to be executed twice. Either renaming unit-tests to default-test or skipping default-test would fix the problem.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3955" opendate="2016-5-23 00:00:00" fixdate="2016-5-23 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Change Table.toSink() to Table.writeToSink()</summary>
      <description>Currently, a Table can be emitted to a TableSink using the Table.toSink() method.However, the name of the method indicates that the Table is converted into a Sink.Therefore, I propose to change the method to Table.writeToSink().</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.TableSinkITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.TableSinkITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.BatchTableEnvironment.scala</file>
      <file type="M">docs.apis.table.md</file>
    </fixedFiles>
  </bug>
  <bug id="3971" opendate="2016-5-25 00:00:00" fixdate="2016-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Aggregates handle null values incorrectly.</summary>
      <description>Table API and SQL aggregates are supposed to ignore null values, e.g., sum(1,2,null,4) is supposed to return 7. There current implementation is correct if at least one valid value is present however, is incorrect if only null values are aggregated. sum(null, null, null) should return null instead of 0Currently only the Count aggregate handles the case of null-values-only correctly.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.SumAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.MinAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.MaxAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.CountAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.AvgAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.SumAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.MinAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.MaxAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AvgAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="3973" opendate="2016-5-25 00:00:00" fixdate="2016-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table API documentation is "hidden" in Programming Guide menu list</summary>
      <description>The Table API / SQL documentation is hard to find in the drop down list of the "Programming Guide" menu entry.We should either move it into the "Libraries" menu or move it up in the "Programming Guide" entry and bold like the other top entries.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.libs.table.md</file>
      <file type="M">docs.apis.table.md</file>
      <file type="M">docs.apis.best.practices.md</file>
    </fixedFiles>
  </bug>
  <bug id="3981" opendate="2016-5-27 00:00:00" fixdate="2016-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t log duplicate TaskManager registrations as exceptions</summary>
      <description>Duplicate TaskManager registrations shouldn't be logged with Exceptions in the ResourceManager. Duplicate registrations can happen if the TaskManager sends out registration messages too fast when the actual reply is not lost but still in transit.The ResourceManager should simply acknowledge the duplicate registrations, leaving it up to the JobManager to decide how to treat the duplicate registrations (currently it will send an AlreadyRegistered to the TaskManager).This change also affects our test stability because the Yarn tests check the logs for exceptions.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.FlinkResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="3982" opendate="2016-5-27 00:00:00" fixdate="2016-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multiple ResourceManagers register at JobManager in standalone HA mode</summary>
      <description>In HA mode, multiple ResourceManagers may register at the leading JobManager. They register one after another at the JobManager. The last registering ResourceManager stays registered with the JobManager. This only applies to Standalone mode and doesn't affect functionality.To prevent duplicate registration for the standalone ResourceManager, the easiest solution is to only start registration when the leading JobManager runs in the same ActorSystem as the ResourceManager. Other ResourceManager implementations may also run independently of the JobManager.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.standalone.StandaloneResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.FlinkResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="3997" opendate="2016-5-31 00:00:00" fixdate="2016-10-31 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>PRNG Skip-ahead</summary>
      <description>The current sources of randomness for Gelly Graph Generators use fixed-size blocks of work which include an initial seed. There are two issues with this approach. First, the size of the collection of blocks can exceed the Akka limit and cause the job to silently fail. Second, as the block seeds are randomly chosen, the likelihood of blocks overlapping and producing the same sequence increases with the size of the graph.The random generators will be reimplemented using SplittableIterator and PRNGs supporting skip-ahead.This ticket will implement skip-ahead with LCGs &amp;#91;0&amp;#93;. Future work may add support for xorshift generators (&amp;#91;1&amp;#93;, section 5 "Jumping Ahead").&amp;#91;0&amp;#93; https://mit-crpg.github.io/openmc/methods/random_numbers.html#skip-ahead-capability&amp;#91;1&amp;#93; https://arxiv.org/pdf/1404.0390.pdf</description>
      <version>1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.generator.random.AbstractGeneratorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="4009" opendate="2016-6-2 00:00:00" fixdate="2016-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala Shell fails to find library for inclusion in test</summary>
      <description>The Scala Shell test fails to find the flink-ml library jar in the target folder when executing with Intellij. This is due to its working directory being expected in "flink-scala-shell/target" when it is in fact "flink-scala-shell". When executed with Maven, this works fine because the Shade plugin changes the basedir from the project root to the /target folder*. As per till.rohrmann and greghogan suggestions we could simply add flink-ml as a test dependency and look for the jar path in the classpath.&amp;#42; Because we have the dependencyReducedPomLocation set to /target/.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-scala-shell.src.test.resources.log4j-test.properties</file>
    </fixedFiles>
  </bug>
  <bug id="4024" opendate="2016-6-5 00:00:00" fixdate="2016-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FileSourceFunction not adjusted to new IF lifecycle</summary>
      <description>The InputFormat lifecycle was extended in ac2137cfa5e63bd4f53a4b7669dc591ab210093f, adding additional open-/closeInputFormat() methods.The streaming FileSourceFunction was not adjusted for this change, and thus will fail for every InputFormat that leverages these new methods.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.InputFormatSource.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="4030" opendate="2016-6-8 00:00:00" fixdate="2016-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ScalaShellITCase gets stuck</summary>
      <description>The ScalaShellITCase fails regularly on Travis:[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.19.1:test (integration-tests) on project flink-scala-shell_2.10: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?[ERROR] Command was /bin/sh -c cd /home/travis/build/apache/flink/flink-scala-shell/target &amp;&amp; /usr/lib/jvm/java-8-oracle/jre/bin/java -Xms256m -Xmx800m -Dmvn.forkNumber=1 -XX:-UseGCOverheadLimit -jar /home/travis/build/apache/flink/flink-scala-shell/target/surefire/surefirebooter5669599672364114854.jar /home/travis/build/apache/flink/flink-scala-shell/target/surefire/surefire854521958557782961tmp /home/travis/build/apache/flink/flink-scala-shell/target/surefire/surefire_7186137661441589930637tmp</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4038" opendate="2016-6-9 00:00:00" fixdate="2016-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Impossible to set more than 1 JVM argument in env.java.opts</summary>
      <description>The Taskmanager start scripts fail when env.java.opts contains more than 1 jvm opts due to:if [[ $FLINK_TM_MEM_PRE_ALLOCATE == "false" ]] &amp;&amp; [ -z $FLINK_ENV_JAVA_OPTS ]; then-z checks the length of the first argument but it fails if it has more than 1 argument</description>
      <version>1.0.0,1.1.0</version>
      <fixedVersion>1.0.4,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.taskmanager.sh</file>
    </fixedFiles>
  </bug>
  <bug id="4062" opendate="2016-6-13 00:00:00" fixdate="2016-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Windowing Documentation</summary>
      <description>The window documentation could be a bit more principled and also needs updating with the new allowed lateness setting.There is also essentially no documentation about how to write a custom trigger.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.streaming.windows.md</file>
    </fixedFiles>
  </bug>
  <bug id="4068" opendate="2016-6-13 00:00:00" fixdate="2016-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move constant computations out of code-generated `flatMap` functions.</summary>
      <description>The generated functions for expressions of the Table API or SQL include constant computations.For instance the code generated for a predicate like:myInt &lt; (10 + 20)looks roughly like:public void flatMap(Row in, Collector&lt;Row&gt; out) { Integer in1 = in.productElement(1); int temp = 10 + 20; if (in1 &lt; temp) { out.collect(in) }}In this example the computation of temp is constant and could be moved out of the flatMap() method.The same might apply for generated function other than FlatMap as well.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.BatchTableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.FlinkTypeSystem.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.FlinkTypeFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testJoin1.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testJoin0.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.ExplainTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.FlinkCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.FlinkRelBuilder.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.BatchTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug id="4074" opendate="2016-6-15 00:00:00" fixdate="2016-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reporter can block TaskManager shutdown</summary>
      <description>If a report is being submitted while a TaskManager is shutting down the reporter can cause the shutdown to be delayed since it submits the complete report.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metric-reporters.flink-metrics-statsd.src.main.java.org.apache.flink.metrics.statsd.StatsDReporter.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.MetricRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="4077" opendate="2016-6-15 00:00:00" fixdate="2016-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Register Pojo DataSet/DataStream as Table requires alias expression.</summary>
      <description>Registering a Pojo DataSet / DataStream as Table requires alias expressions and does not work with simple field references. However, alias expressions would only be necessary if the fields of the Pojo should be renamed. DataStream&lt;Person&gt; persons = ...// DOES NOT WORKtEnv.registerDataStream( "Persons", persons, "name, age, address");// DOES WORKtEnv.registerDataStream( "Persons", persons, "name AS name, age AS age, address AS address");We should also allow simple field name references in addition to alias expressions to rename fields.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug id="4085" opendate="2016-6-17 00:00:00" fixdate="2016-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set Kinesis Consumer Agent to Flink</summary>
      <description>Currently, we use the default Kinesis Agent name.I was asked by Amazon to set it to something containing Flink.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.java</file>
    </fixedFiles>
  </bug>
  <bug id="4091" opendate="2016-6-17 00:00:00" fixdate="2016-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-connector-cassandra has conflicting guava version</summary>
      <description>The newly merged cassandra streaming connector has an issue with its guava dependency.The build-process for flink-connector-cassandra creates shaded JAR file which contains the connector, the datastax cassandra driver plus in org.apache.flink.shaded a shaded copy of guava. The datastax cassandra driver calls into Futures.withFallback (&amp;#91;1&amp;#93;) which is present in this guava version. This also works inside the flink-connector-cassandra jar.Now the actual build-process for Flink happens and builds another shaded JAR and creates the flink-dist.jar. Inside this JAR, there is also a shaded version of guava inside org.apache.flink.shaded.Now the issue: The guava version which is in the flink-dist.jar is not compatible and doesn't contain the Futures.withFallback which the datastax driver is using.This leads into the following issue: You can without any problems launch a flink task which uses the casandra driver locally (so through the mini-cluster) because that is never using the flink-dist.jar. BUT: As soon as you are trying to start this job on a flink cluster (which uses the flink-dist.jar), the job breaks with the following exception:https://gist.github.com/theomega/5ab9b14ffb516b15814de28e499b040dYou can inspect this by opening the flink-connector-cassandra_2.11-1.1-SNAPSHOT.jar and the flink-dist_2.11-1.1-SNAPSHOT.jar in a java decompiler.I don't know a good solution here: Perhaps it would be one solution to shade the guava for the cassandra-driver somewhere else than at org.apache.flink.shaded.&amp;#91;1&amp;#93;: https://google.github.io/guava/releases/19.0/api/docs/com/google/common/util/concurrent/Futures.html#withFallback(com.google.common.util.concurrent.ListenableFuture, com.google.common.util.concurrent.FutureFallback, java.util.concurrent.Executor)</description>
      <version>1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4093" opendate="2016-6-20 00:00:00" fixdate="2016-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose metric interfaces</summary>
      <description></description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannelTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.MetricGroup.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.groups.UnregisteredMetricsGroup.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.groups.AbstractMetricGroup.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.Gauge.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.Counter.java</file>
    </fixedFiles>
  </bug>
  <bug id="4094" opendate="2016-6-20 00:00:00" fixdate="2016-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Off heap memory deallocation might not properly work</summary>
      <description>A user reported that off-heap memory is not properly deallocated when setting taskmanager.memory.preallocate:false (per default) &amp;#91;1&amp;#93;. This can cause the TaskManager process being killed by the OS.It should be possible to execute multiple batch jobs with preallocation turned off. No longer used direct memory buffers should be properly garbage collected so that the JVM process does not exceed it's maximum memory bounds.&amp;#91;1&amp;#93; http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/offheap-memory-allocation-and-memory-leak-bug-td12154.html</description>
      <version>1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.memory.MemoryManager.java</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="4104" opendate="2016-6-22 00:00:00" fixdate="2016-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Restructure Gelly docs</summary>
      <description>The Gelly documentation has grown sufficiently long to suggest dividing into sub-pages. Leave "Using Gelly" on the main page and link to the following topics as sub-pages: Graph API Iterative Graph Processing Library Methods Graph Algorithms Graph Generators</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.batch.libs.index.md</file>
      <file type="M">docs.apis.batch.libs.gelly.md</file>
    </fixedFiles>
  </bug>
  <bug id="4108" opendate="2016-6-22 00:00:00" fixdate="2016-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE in Row.productArity</summary>
      <description>&amp;#91;this is my first issue request here, please apologize if something is missing&amp;#93; JDBCInputFormat of flink 1.1-SNAPSHOT fails with an NPE in Row.productArity:java.io.IOException: Couldn't access resultSet at org.apache.flink.api.java.io.jdbc.JDBCInputFormat.nextRecord(JDBCInputFormat.java:288) at org.apache.flink.api.java.io.jdbc.JDBCInputFormat.nextRecord(JDBCInputFormat.java:98) at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:162) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:588) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.NullPointerException at org.apache.flink.api.table.Row.productArity(Row.scala:28) at org.apache.flink.api.java.io.jdbc.JDBCInputFormat.nextRecord(JDBCInputFormat.java:279) ... 4 moreThe code reproduce this can be found in this gist: https://gist.github.com/zeitgeist/b91a60460661618ca4585e082895c616The reason for the NPE, I believe, is the way through which Flink creates Row instances through Kryo. As Row expects the number of fields to allocate as a parameter, which Kryo does not provide, the fields member of Row ends up being null. As Im neither a reflection nor a Kryo expert, I rather leave a true analysis to more knowledgable programmers.Part of the aforementioned example is a not very elegant workaround though a custom type and a cast (function jdbcNoIssue + custom Row type MyRow) to serve as a further hint towards my theory.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.typeutils.TypeInfoFactoryTest.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.package.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug id="4109" opendate="2016-6-23 00:00:00" fixdate="2016-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change the name of ternary condition operator &amp;#39;eval&amp;#39; to &amp;#39;?&amp;#39;</summary>
      <description>The ternary condition operator in Table API is named eval, for example: (42 &gt; 5).eval("A", "B") leads to "A". IMO, the eval function is not well understood. Instead the "?" is a better choice I think, which is used in Java for condition operator. It will be clearer and more literal understood, e.g.(42 &gt; 5).?("A", "B") or (42 &gt; 5) ? ("A", "B")If it make sense, I will pull a request.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.ExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.table.ExpressionsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.logic.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
      <file type="M">docs.apis.table.md</file>
    </fixedFiles>
  </bug>
  <bug id="4116" opendate="2016-6-24 00:00:00" fixdate="2016-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document metrics</summary>
      <description>The metric system is currently not documented, which should be fixed before the 1.1 release.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerMetricTest.java</file>
      <file type="M">flink-metrics.flink-metrics-statsd.src.test.java.org.apache.flink.metrics.statsd.StatsDReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.test.java.org.apache.flink.dropwizard.metrics.DropwizardFlinkHistogramWrapperTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.reporter.JMXReporterTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.MetricRegistryTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.groups.MetricGroupRegistrationTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.reporter.JMXReporter.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.MetricRegistry.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.setup.config.md</file>
      <file type="M">docs.apis.common.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="4119" opendate="2016-6-25 00:00:00" fixdate="2016-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Null checks in close() for Cassandra Input/Output Formats, checking arguments via Flink Preconditions</summary>
      <description>Add null checks for session and cluster to align the behaviour with Cassandra Streaming Connector, refactor check arguments using Flink Preconditions</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.batch.connectors.cassandra.CassandraOutputFormat.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.batch.connectors.cassandra.CassandraInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="4122" opendate="2016-6-27 00:00:00" fixdate="2016-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cassandra jar contains 2 guava versions</summary>
      <description></description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4123" opendate="2016-6-27 00:00:00" fixdate="2016-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CassandraWriteAheadSink can hang on cassandra failure</summary>
      <description>The CassandraWriteAheadSink verifies that all writes send to cassandra have been applied by counting how many were sent and how many callbacks were activated. Once all writes were sent the sink enters into a loop that is only exited once both counts are equal.Thus, should cassandra crash after all writes were sent, without having acknowledged all writes, the sink will deadlock in the loop.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraConnectorUnitTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.GenericWriteAheadSinkTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.GenericWriteAheadSink.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-cassandra.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-streaming-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.streaming.connectors.cassandra.CassandraTupleWriteAheadSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="4128" opendate="2016-6-28 00:00:00" fixdate="2016-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>compile error about git-commit-id-plugin</summary>
      <description>When I build with latest flink code, I got following error:&amp;#91;INFO&amp;#93; ------------------------------------------------------------------------&amp;#91;INFO&amp;#93; BUILD FAILURE&amp;#91;INFO&amp;#93; ------------------------------------------------------------------------&amp;#91;INFO&amp;#93; Total time: 01:06 h&amp;#91;INFO&amp;#93; Finished at: 2016-06-28T22:11:58+08:00&amp;#91;INFO&amp;#93; Final Memory: 104M/3186M&amp;#91;INFO&amp;#93; ------------------------------------------------------------------------&amp;#91;ERROR&amp;#93; Failed to execute goal pl.project13.maven:git-commit-id-plugin:2.1.5:revision (default) on project flink-runtime_2.11: Execution default of goal pl.project13.maven:git-commit-id-plugin:2.1.5:revision failed. NullPointerException -&gt; &amp;#91;Help 1&amp;#93;&amp;#91;ERROR&amp;#93;&amp;#91;ERROR&amp;#93; To see the full stack trace of the errors, re-run Maven with the -e switch.&amp;#91;ERROR&amp;#93; Re-run Maven using the -X switch to enable full debug logging.&amp;#91;ERROR&amp;#93;&amp;#91;ERROR&amp;#93; For more information about the errors and possible solutions, please read the following articles:&amp;#91;ERROR&amp;#93; &amp;#91;Help 1&amp;#93; http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException&amp;#91;ERROR&amp;#93;&amp;#91;ERROR&amp;#93; After correcting the problems, you can resume the build with the command&amp;#91;ERROR&amp;#93; mvn &lt;goals&gt; -rf :flink-runtime_2.11I think it's because wrong `doGetDirectory` value is provided.And another question is if we should upgrade the version of this plugin, so that we can got more meaningful error message instead of NPE. Eg:Could not get HEAD Ref, are you sure you have some commits in the dotGitDirectory?Current stable version is 2.2.1, but the disadvantage is that Java 1.6 is no longer supported with new version.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4133" opendate="2016-6-29 00:00:00" fixdate="2016-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reflect streaming file source changes in documentation</summary>
      <description>In FLINK-2314 the file sources for the DataStream API were reworked.The documentation doesn't explain the (new?) semantics of the file sources.In which order are files read?How are file modifications treated? (appends, in place modifications?)I suspect this table is also not up-to-date: https://ci.apache.org/projects/flink/flink-docs-master/apis/streaming/fault_tolerance.html#fault-tolerance-guarantees-of-data-sources-and-sinks</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator.java</file>
      <file type="M">docs.apis.streaming.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="4137" opendate="2016-6-30 00:00:00" fixdate="2016-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JobManager web frontend does not shut down on OOM exception on JM</summary>
      <description>After the following Exception on the JobManager.2016-06-30 14:45:06,642 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Completed checkpoint 379 (in 7017 ms)2016-06-30 14:45:06,642 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Triggering checkpoint 380 @ 14672979066422016-06-30 14:45:17,902 ERROR akka.actor.ActorSystemImpl - Uncaught fatal error from thread [flink-akka.remote.default-remote-dispatcher-6] shutting down ActorSystem [flink]java.lang.OutOfMemoryError: Java heap space at com.google.protobuf.ByteString.copyFrom(ByteString.java:192) at com.google.protobuf.CodedInputStream.readBytes(CodedInputStream.java:324) at akka.remote.WireFormats$SerializedMessage.&lt;init&gt;(WireFormats.java:3030) at akka.remote.WireFormats$SerializedMessage.&lt;init&gt;(WireFormats.java:2980) at akka.remote.WireFormats$SerializedMessage$1.parsePartialFrom(WireFormats.java:3073) at akka.remote.WireFormats$SerializedMessage$1.parsePartialFrom(WireFormats.java:3068) at com.google.protobuf.CodedInputStream.readMessage(CodedInputStream.java:309) at akka.remote.WireFormats$RemoteEnvelope.&lt;init&gt;(WireFormats.java:993) at akka.remote.WireFormats$RemoteEnvelope.&lt;init&gt;(WireFormats.java:927) at akka.remote.WireFormats$RemoteEnvelope$1.parsePartialFrom(WireFormats.java:1049) at akka.remote.WireFormats$RemoteEnvelope$1.parsePartialFrom(WireFormats.java:1044) at com.google.protobuf.CodedInputStream.readMessage(CodedInputStream.java:309) at akka.remote.WireFormats$AckAndEnvelopeContainer.&lt;init&gt;(WireFormats.java:241) at akka.remote.WireFormats$AckAndEnvelopeContainer.&lt;init&gt;(WireFormats.java:175) at akka.remote.WireFormats$AckAndEnvelopeContainer$1.parsePartialFrom(WireFormats.java:279) at akka.remote.WireFormats$AckAndEnvelopeContainer$1.parsePartialFrom(WireFormats.java:274) at com.google.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:141) at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:176) at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:188) at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:193) at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:49) at akka.remote.WireFormats$AckAndEnvelopeContainer.parseFrom(WireFormats.java:409) at akka.remote.transport.AkkaPduProtobufCodec$.decodeMessage(AkkaPduCodec.scala:181) at akka.remote.EndpointReader.akka$remote$EndpointReader$$tryDecodeMessageAndAck(Endpoint.scala:995) at akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:928) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:415) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231)2016-06-30 14:45:18,502 INFO org.apache.flink.yarn.YarnJobManager - Stopping JobManager akka.tcp://flink@172.31.23.121:45569/user/jobmanager.2016-06-30 14:45:18,533 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom File Source (1/1) (5f2a1062c796ec6098a0a88227b9eab4) switched from RUNNING to CANCELINGThe JobManager JVM keeps running (keeping the YARN session alive) because the web monitor is not stopped on such errors.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug id="4139" opendate="2016-7-1 00:00:00" fixdate="2016-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn: Adjust parallelism and task slots correctly</summary>
      <description>The Yarn CLI should handle the following situations correctly: The user specifies no parallelism -&gt; parallelism is adjusted to #taskSlots * #nodes. The user specifies parallelism but no #taskSlots or too few slots -&gt; #taskSlots are set such that they meet the parallelismThese functionality has been present in Flink 1.0.x but there were some glitches in the implementation.</description>
      <version>1.0.3,1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.FlinkYarnSessionCliTest.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.CliFrontendYarnAddressConfigurationTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.CliFrontendAddressConfigurationTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="4143" opendate="2016-7-1 00:00:00" fixdate="2016-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configurable delimiter for metric identifier</summary>
      <description>The metric identifier is currently hard-coded to separate components with a dot.We should make this configurable.</description>
      <version>1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-statsd.src.test.java.org.apache.flink.metrics.statsd.StatsDReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.test.java.org.apache.flink.dropwizard.metrics.DropwizardFlinkHistogramWrapperTest.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.main.java.org.apache.flink.dropwizard.ScheduledDropwizardReporter.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.MetricRegistryTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.groups.TaskManagerJobGroupTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.groups.TaskManagerGroupTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.groups.TaskGroupTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.groups.OperatorGroupTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.groups.JobManagerJobGroupTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.groups.JobManagerGroupTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.reporter.AbstractReporter.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.MetricRegistry.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.groups.scope.ScopeFormat.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.groups.AbstractMetricGroup.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.apis.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="4150" opendate="2016-7-4 00:00:00" fixdate="2016-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Problem with Blobstore in Yarn HA setting on recovery after cluster shutdown</summary>
      <description>Submitting a job in Yarn with HA can lead to the following exception:org.apache.flink.streaming.runtime.tasks.StreamTaskException: Cannot load user class: org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09ClassLoader info: URL ClassLoader: file: '/tmp/blobStore-ccec0f4a-3e07-455f-945b-4fcd08f5bac1/cache/blob_7fafffe9595cd06aff213b81b5da7b1682e1d6b0' (invalid JAR: zip file is empty)Class not resolvable through given classloader. at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperator(StreamConfig.java:207) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:222) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:588) at java.lang.Thread.run(Thread.java:745)Some job information, including the Blob ids, are stored in Zookeeper. The actual Blobs are stored in a dedicated BlobStore, if the recovery mode is set to Zookeeper. This BlobStore is typically located in a FS like HDFS. When the cluster is shut down, the path for the BlobStore is deleted. When the cluster is then restarted, recovering jobs cannot restore because it's Blob ids stored in Zookeeper now point to deleted files.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingLeaderElectionService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.FileSystemBlobStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobServerConnection.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="4151" opendate="2016-7-5 00:00:00" fixdate="2016-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Address Travis CI build time: We are exceeding the 2 hours limit</summary>
      <description>We've recently started hitting the two hours limit for Travis CI.I'll look into some approaches to get our build stable again.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="4154" opendate="2016-7-5 00:00:00" fixdate="2016-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correction of murmur hash breaks backwards compatibility</summary>
      <description>The correction of Flink's murmur hash with commit &amp;#91;1&amp;#93;, breaks Flink's backwards compatibility with respect to savepoints. The reason is that the changed murmur hash which is used to partition elements in a KeyedStream changes the mapping from keys to sub tasks. This changes the assigned key spaces for a sub task. Consequently, an old savepoint (version 1.0) assigns states with a different key space to the sub tasks.I think that this must be fixed for the upcoming 1.1 release. I see two options to solve the problem: revert the changes, but then we don't know how the flawed murmur hash performs develop tooling to repartition state of old savepoints. This is probably not trivial since a keyed stream can also contain non-partitioned state which is not partitionable in all cases. And even if only partitioned state is used, we would need some kind of special operator which can repartition the state wrt the key.I think that the latter option requires some more thoughts and is thus unlikely to be done before the release 1.1. Therefore, as a workaround, I think that we should revert the murmur hash changes.&amp;#91;1&amp;#93; https://github.com/apache/flink/commit/641a0d436c9b7a34ff33ceb370cf29962cac4dee</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.api.StreamingOperatorsITCase.java</file>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.StreamingOperatorsITCase.scala</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.MathUtils.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.StormFieldsGroupingITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="4155" opendate="2016-7-5 00:00:00" fixdate="2016-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Get Kafka producer partition info in open method instead of constructor</summary>
      <description>Currently the Flink Kafka producer does not really do any error handling if something is wrong with the partition metadata as it is serialized with the user function.This means that in some cases the job can go into an error loop when using the checkpoints. Getting the partition info in the open method would solve this problem (like restarting from a savepoint which re-runs the constructor).</description>
      <version>1.0.3,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSinkTestBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.AtLeastOnceProducerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSinkTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSinkTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="4160" opendate="2016-7-6 00:00:00" fixdate="2016-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>YARN session doesn&amp;#39;t show input validation errors</summary>
      <description>Setting a jobmanager size below 768 mb causes this error:~/flink/build-target$ ./bin/yarn-session.sh -n 5 -s 4 -jm 512Error while starting the YARN Client. Please check log output!The problem is that the logs don't contain any information.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
    </fixedFiles>
  </bug>
  <bug id="4169" opendate="2016-7-7 00:00:00" fixdate="2016-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CEP Does Not Work with RocksDB StateBackend</summary>
      <description>A job will never match any patterns because ValueState.update() is not called in the keyed CEP operators for updating the NFA state and the priority queue state.The reason why it works for other state backends is that they are very lax in their handling of state: if the object returned from ValueState.value()) is mutable changes to this will be reflected in checkpoints even if ValueState.update() is not called. RocksDB, on the other hand, does always deserialize/serialize state values when accessing/updating them, so changes to the returned object will not be reflected in the state unless update() is called.We should fix this and also add a test for it. This might be tricky because we have to pull together RocksDB and CEP.</description>
      <version>1.0.0,1.0.1,1.0.2,1.0.3,1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractCEPBasePatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4172" opendate="2016-7-7 00:00:00" fixdate="2016-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t proxy a ProxiedObject</summary>
      <description>Some graph algorithms pass through a DataSet unmodified (at least, until we have VertexSet and EdgeSet). We need to prevent a DataSet from being proxied twice. Allowing two methods to own a single object sounds brittle, so we can instead provide access to the original DataSet which can be wrapped in a new proxy.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.utils.proxy.Delegate.java</file>
    </fixedFiles>
  </bug>
  <bug id="4173" opendate="2016-7-7 00:00:00" fixdate="2016-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace maven-assembly-plugin by maven-shade-plugin in flink-metrics</summary>
      <description>The modules flink-metrics-dropwizard, flink-metrics-ganglia and flink-metrics-graphite use the maven-assembly-plugin to build a fat jar. The resulting fat jar has the suffix jar-with-dependencies. In order to make the naming consistent with the rest of the system we should create a fat-jar without this suffix.Additionally we could replace the maven-assembly-plugin with the maven-shade-plugin to make it consistent with the rest of the system.</description>
      <version>1.1.0</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-graphite.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4180" opendate="2016-7-8 00:00:00" fixdate="2016-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a batch SQL example</summary>
      <description>Currently there is no runnable code example in `flink-table` showing a working batch SQL query with the Table API.A Scala and Java example should be added.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.table.AggregationsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.examples.scala.WordCountTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.examples.scala.WordCountSQL.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.examples.scala.TPCHQuery3Table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.examples.scala.StreamTableExample.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.examples.scala.StreamSQLExample.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.examples.java.JavaTableExample.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.examples.java.JavaSQLExample.java</file>
    </fixedFiles>
  </bug>
  <bug id="4186" opendate="2016-7-10 00:00:00" fixdate="2016-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose Kafka metrics through Flink metrics</summary>
      <description>Currently, we expose the Kafka metrics through Flink's accumulators.We can now use the metrics system in Flink to report Kafka metrics.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.MockRuntimeContext.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcherTimestampsTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.metrics.MinKafkaMetricAccumulator.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.metrics.MaxKafkaMetricAccumulator.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.metrics.DefaultKafkaMetricAccumulator.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.metrics.AvgKafkaMetricAccumulator.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09ITCase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08ITCase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.Kafka08Fetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer08.java</file>
    </fixedFiles>
  </bug>
  <bug id="4189" opendate="2016-7-11 00:00:00" fixdate="2016-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce symbols for internal use</summary>
      <description>Currently we are using integer values to pass Calcite SQL symbols down to code generation. This causes problems like in FLINK-4068. We should support symbols internally.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.table.ExpressionsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.RexNodeTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.GeneratedFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.GeneratedExpression.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.TrimCallGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.ScalarOperators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.MethodCallGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
      <file type="M">docs.apis.table.md</file>
    </fixedFiles>
  </bug>
  <bug id="4199" opendate="2016-7-12 00:00:00" fixdate="2016-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Misleading messages by CLI upon job submission</summary>
      <description>Trying to submit a job jar from the client to a non-existing cluster gives the following messages. In particular the first and last lines: "Cluster retrieved: Standalone cluster with JobManager at localhost/127.0.0.1:6123" and "Job has been submitted with" are totally misleading.Cluster retrieved: Standalone cluster with JobManager at localhost/127.0.0.1:6123Using address localhost:6123 to connect to JobManager.JobManager web interface address http://localhost:8081Starting execution of programSubmitting job with JobID: 9c7120e5cc55b2a9157a7e2bc5a12c9d. Waiting for job completion.org.apache.flink.client.program.ProgramInvocationException: The program execution failed: Communication with JobManager failed: Lost connection to the JobManager.Job has been submitted with JobID 9c7120e5cc55b2a9157a7e2bc5a12c9d</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.accumulators.AccumulatorHelper.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug id="4200" opendate="2016-7-12 00:00:00" fixdate="2016-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka consumers should log the offset from which they restore</summary>
      <description>Kafka consumers should log the offset from which they restore so that it is easier to investigate problems with recovery.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="4201" opendate="2016-7-12 00:00:00" fixdate="2016-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkpoints for jobs in non-terminal state (e.g. suspended) get deleted</summary>
      <description>For example, when shutting down a Yarn session, according to the logs checkpoints for jobs that did not terminate are deleted. In the shutdown hook, removeAllCheckpoints is called and removes checkpoints that should still be kept.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerHARecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.SavepointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ExecutionGraphCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointIDCounterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointIDCounter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StandaloneCheckpointIDCounter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.SavepointCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointIDCounter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4203" opendate="2016-7-12 00:00:00" fixdate="2016-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Table API documentation</summary>
      <description>Some ideas: Add a list of all supported scalar functions and a description Add a more advanced example Describe supported data types</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
      <file type="M">docs.apis.table.md</file>
    </fixedFiles>
  </bug>
  <bug id="4206" opendate="2016-7-13 00:00:00" fixdate="2016-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metric names should allow special characters</summary>
      <description>Currently, the name of the metric is restricted to alphanumeric characters. This restriction was originally put in place to circumvent issues due to systems not supporting certain characters.However, this restriction does not make a lot of sense since for group names we don't enforce such a restriction.This also affects the integration of the Kafka metrics, so i suggest removing the restriction.From now on it will be the responsibility of the reporter to make sure that the metric identifier is supported by the external system.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.groups.MetricGroupTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.groups.MetricGroupRegistrationTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.groups.AbstractMetricGroup.java</file>
    </fixedFiles>
  </bug>
  <bug id="4212" opendate="2016-7-13 00:00:00" fixdate="2016-8-13 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Lock PID file when starting daemons</summary>
      <description>As noted on the mailing list (0), when multiple TaskManagers are started in parallel (using pdsh) there is a race condition on updating the pid: 1) the pid file is first read to parse the process' index, 2) the process is started, and 3) on success the daemon pid is appended to the pid file.We could use a tool such as flock to lock on the pid file while starting the Flink daemon.0: http://mail-archives.apache.org/mod_mbox/flink-user/201607.mbox/%3CCA%2BssbKXw954Bz_sBRwP6db0FntWyGWzTyP7wJZ5nhOeQnof3kg%40mail.gmail.com%3E</description>
      <version>1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink-daemon.sh</file>
    </fixedFiles>
  </bug>
  <bug id="4242" opendate="2016-7-21 00:00:00" fixdate="2016-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve validation exception messages</summary>
      <description>The Table API's validation exceptions could be improved to be more meaningful for users. For example, the following code snippet:Table inputTable = tableEnv.fromDataStream(env.fromElements( Tuple3.of(1, "a", 1.0), Tuple3.of(2, "b", 2.0), Tuple3.of(3, "c", 3.0)), "a, b, c");inputTable.select("a").where("!a");fails correctly. However, the validation exception message says "Expression !('a) failed on input check: Not only accepts child of Boolean Type, get Integer". I think it could be changed such that it says: "The not operator requires a boolean input but "a" is of type integer." or something similar.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.logic.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.arithmetic.scala</file>
    </fixedFiles>
  </bug>
  <bug id="4245" opendate="2016-7-21 00:00:00" fixdate="2016-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metric naming improvements</summary>
      <description>A metric currently has two parts to it: The name of that particular metric The "scope" (or namespace), defined by the group that contains the metric.A metric group actually always implicitly has a map of naming "tags", like: taskmanager_host : &lt;some-hostname&gt; taskmanager_id : &lt;id&gt; task_name : "map() -&gt; filter()"We derive the scope from that map, following the defined scope formats.For JMX (and some users that use JMX), it would be natural to expose that map of tags. Some users reconstruct that map by parsing the metric scope. JMX, we can expose a metric like: domain: "taskmanager.task.operator.io" name: "numRecordsIn" tags: { "hostname" -&gt; "localhost", "operator_name" -&gt; "map() at X.java:123", ... }For many other reporters, the formatted scope makes a lot of sense, since they think only in terms of (scope, metric-name).We may even have the formatted scope in JMX as well (in the domain), if we want to go that route. jgrier and Zentol - what do you think about that?mdaxini Does that match your use of the metrics?</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.OperatorGroupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskManagerJobMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.ProxyMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.JobManagerJobMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.ComponentMetricGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.MetricGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.groups.UnregisteredMetricsGroup.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.MetricGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.AbstractMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskManagerMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.OperatorMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.JobMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.JobManagerMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.GenericMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.FrontMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.AbstractMetricGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.runtime.jobmanager.JMXJobManagerMetricTest.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.metrics.jmx.JMXReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.main.java.org.apache.flink.metrics.jmx.JMXReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4248" opendate="2016-7-21 00:00:00" fixdate="2016-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CsvTableSource does not support reading SqlTimeTypeInfo types</summary>
      <description>The Table API's CsvTableSource does not support to read all Table API supported data types. For example, it is not possible to read SqlTimeTypeInfo types via the CsvTableSource.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.io.RowCsvInputFormatTest.scala</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.SqlTimestampSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.SqlTimestampComparatorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.SqlTimeSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.SqlTimeComparatorTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.parser.FloatValueParser.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.parser.FloatParser.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.parser.FieldParser.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.parser.DoubleValueParser.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.parser.DoubleParser.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.parser.BigIntParser.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.parser.BigDecParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="4251" opendate="2016-7-21 00:00:00" fixdate="2016-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add possiblity for the RMQ Streaming Sink to customize the queue</summary>
      <description>This patch adds the possibilty for the user of the RabbitMQStreaming Sink to customize the queue which is used. This adopts the behavior of FLINK-4025 for the sink.The commit doesn't change the actual behaviour but makes itpossible for users to override the `setupQueue`method and customize their implementation. This was only possible for the RMQSource before. The Sink and the Source offer now both the same functionality, so this should increase usability. FLINK-4025 = https://issues.apache.org/jira/browse/FLINK-4025</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.1,1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-rabbitmq.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="4252" opendate="2016-7-22 00:00:00" fixdate="2016-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table program cannot be compiled</summary>
      <description>I'm trying the table apis.I got some errors like thisMy code is in the attachments------------------------------------------------------------ The program finished with the following exception:org.apache.flink.client.program.ProgramInvocationException: The program execution failed: Job execution failed. at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:413) at org.apache.flink.client.program.StandaloneClusterClient.submitJob(StandaloneClusterClient.java:92) at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:389) at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:376) at org.apache.flink.client.program.ContextEnvironment.execute(ContextEnvironment.java:61) at org.apache.flink.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:896) at org.apache.flink.api.java.DataSet.collect(DataSet.java:410) at org.apache.flink.api.java.DataSet.print(DataSet.java:1605) at org.apache.flink.api.scala.DataSet.print(DataSet.scala:1672) at TestMain$.main(TestMain.scala:31) at TestMain.main(TestMain.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:509) at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:403) at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:331) at org.apache.flink.client.CliFrontend.executeProgram(CliFrontend.java:777) at org.apache.flink.client.CliFrontend.run(CliFrontend.java:253) at org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:1005) at org.apache.flink.client.CliFrontend.main(CliFrontend.java:1048)Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply$mcV$sp(JobManager.scala:853) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply(JobManager.scala:799) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply(JobManager.scala:799) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:401) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.pollAndExecAll(ForkJoinPool.java:1253) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1346) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.Exception: The user defined 'open(Configuration)' method in class org.apache.flink.api.table.runtime.FlatMapRunner caused an exception: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.runtime.operators.BatchTask.openUserCode(BatchTask.java:1337) at org.apache.flink.runtime.operators.chaining.ChainedFlatMapDriver.openTask(ChainedFlatMapDriver.java:47) at org.apache.flink.runtime.operators.BatchTask.openChainedTasks(BatchTask.java:1377) at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:471) at org.apache.flink.runtime.operators.BatchTask.invoke(BatchTask.java:351) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:584) at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.api.table.runtime.FunctionCompiler$class.compile(FunctionCompiler.scala:37) at org.apache.flink.api.table.runtime.FlatMapRunner.compile(FlatMapRunner.scala:28) at org.apache.flink.api.table.runtime.FlatMapRunner.open(FlatMapRunner.scala:42) at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:38) at org.apache.flink.runtime.operators.BatchTask.openUserCode(BatchTask.java:1335) ... 6 moreCaused by: org.codehaus.commons.compiler.CompileException: Line 46, Column 11: Expression "null" is not a type at org.codehaus.janino.Java$Located.throwCompileException(Java.java:111) at org.codehaus.janino.Java$Atom.toTypeOrCompileException(Java.java:2684) at org.codehaus.janino.Parser.parseBlockStatement(Parser.java:1162) at org.codehaus.janino.Parser.parseBlockStatements(Parser.java:1090) at org.codehaus.janino.Parser.parseMethodDeclarationRest(Parser.java:943) at org.codehaus.janino.Parser.parseClassBodyDeclaration(Parser.java:569) at org.codehaus.janino.Parser.parseClassBody(Parser.java:519) at org.codehaus.janino.Parser.parseClassDeclarationRest(Parser.java:485) at org.codehaus.janino.Parser.parsePackageMemberTypeDeclaration(Parser.java:273) at org.codehaus.janino.Parser.parseCompilationUnit(Parser.java:168) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:201) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:77) at org.apache.flink.api.table.runtime.FunctionCompiler$class.compile(FunctionCompiler.scala:34) ... 10 more-----------------</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.table.FromDataSetITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.FlinkRelBuilder.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.BatchTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.BatchTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug id="4256" opendate="2016-7-22 00:00:00" fixdate="2016-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fine-grained recovery</summary>
      <description>When a task fails during execution, Flink currently resets the entire execution graph and triggers complete re-execution from the last completed checkpoint. This is more expensive than just re-executing the failed tasks.In many cases, more fine-grained recovery is possible.The full description and design is in the corresponding FLIP.https://cwiki.apache.org/confluence/display/FLINK/FLIP-1+%3A+Fine+Grained+Recovery+from+Task+FailuresThe detail desgin for version1 is https://docs.google.com/document/d/1_PqPLA1TJgjlqz8fqnVE3YSisYBDdFsrRX_URgRSj74/edit#</description>
      <version>1.1.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
    </fixedFiles>
  </bug>
  <bug id="4260" opendate="2016-7-25 00:00:00" fixdate="2016-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow SQL&amp;#39;s LIKE ESCAPE</summary>
      <description>Currently, the SQL API does not support specifying an ESCAPE character in a LIKE expression. The SIMILAR TO should also support that.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="4279" opendate="2016-7-29 00:00:00" fixdate="2016-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Set flink dependencies to provided</summary>
      <description></description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4280" opendate="2016-7-29 00:00:00" fixdate="2016-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>New Flink-specific option to set starting position of Kafka consumer without respecting external offsets in ZK / Broker</summary>
      <description>Currently, to start reading from the "earliest" and "latest" position in topics for the Flink Kafka consumer, users set the Kafka config auto.offset.reset in the provided properties configuration.However, the way this config actually works might be a bit misleading if users were trying to find a way to "read topics from a starting position". The way the auto.offset.reset config works in the Flink Kafka consumer resembles Kafka's original intent for the setting: first, existing external offsets committed to the ZK / brokers will be checked; if none exists, then will auto.offset.reset be respected.I propose to add Flink-specific ways to define the starting position, without taking into account the external offsets. The original behaviour (reference external offsets first) can be changed to be a user option, so that the behaviour can be retained for frequent Kafka users that may need some collaboration with existing non-Flink Kafka consumer applications.How users will interact with the Flink Kafka consumer after this is added, with a newly introduced flink.starting-position config:Properties props = new Properties();props.setProperty("flink.starting-position", "earliest/latest");props.setProperty("auto.offset.reset", "..."); // this will be ignored (log a warning)props.setProperty("group.id", "...") // this won't have effect on the starting position anymore (may still be used in external offset committing)...Or, reference external offsets in ZK / broker:Properties props = new Properties();props.setProperty("flink.starting-position", "external-offsets");props.setProperty("auto.offset.reset", "earliest/latest"); // default will be latestprops.setProperty("group.id", "..."); // will be used to lookup external offsets in ZK / broker on startup...A thing we would need to decide on is what would the default value be for flink.starting-position.Two merits I see in adding this:1. This compensates the way users generally interpret "read from a starting position". As the Flink Kafka connector is somewhat essentially a "high-level" Kafka consumer for Flink users, I think it is reasonable to add Flink-specific functionality that users will find useful, although it wasn't supported in Kafka's original consumer designs.2. By adding this, the idea that "the Kafka offset store (ZK / brokers) is used only to expose progress to the outside world, and not used to manipulate how Kafka topics are read in Flink (unless users opt to do so)" is even more definite and solid. There was some discussion in this PR (https://github.com/apache/flink/pull/1690, FLINK-3398) on this aspect. I think adding this "decouples" more Flink's internal offset checkpointing from the external Kafka's offset store.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.JobManagerCommunicationUtils.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaShortRetentionTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcherTimestampsTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09ITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09FetcherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08ITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.SimpleConsumerThread.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.Kafka08Fetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer08.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010ITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010FetcherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerCallBridge010.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer010.java</file>
      <file type="M">docs.dev.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="4291" opendate="2016-8-1 00:00:00" fixdate="2016-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>No log entry for unscheduled reporters</summary>
      <description>When a non-Scheduled reporter is configured no log message is printed. It would be nice if we would print a log message for every instantiated reporter, not just Scheduled ones.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="4296" opendate="2016-8-1 00:00:00" fixdate="2016-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scheduler accepts more tasks than it has task slots available</summary>
      <description>Flink's scheduler doesn't support queued scheduling but expects to find all necessary task slots upon scheduling. If it does not it throws an error. Due to some changes in the latest master, this seems to be broken.Flink accepts jobs with parallelism &gt; total number of task slots, schedules and deploys tasks in all available task slots, and leaves the remaining tasks lingering forever.Easy to reproduce: ./bin/flink run -p TASK_SLOTS+n where TASK_SLOTS is the number of total task slots of the cluster and n&gt;=1.Here, p=11, TASK_SLOTS=10:bin/flink run -p 11 examples/batch/EnumTriangles.jarCluster configuration: Standalone cluster with JobManager at localhost/127.0.0.1:6123Using address localhost:6123 to connect to JobManager.JobManager web interface address http://localhost:8081Starting execution of programExecuting EnumTriangles example with default edges data set.Use --edges to specify file input.Printing result to stdout. Use --output to specify output path.Submitting job with JobID: cd0c0b4cbe25643d8d92558168cfc045. Waiting for job completion.08/01/2016 12:12:12 Job execution switched to status RUNNING.08/01/2016 12:12:12 CHAIN DataSource (at getDefaultEdgeDataSet(EnumTrianglesData.java:57) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; Map (Map at main(EnumTriangles.java:108))(1/1) switched to SCHEDULED08/01/2016 12:12:12 CHAIN DataSource (at getDefaultEdgeDataSet(EnumTrianglesData.java:57) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; Map (Map at main(EnumTriangles.java:108))(1/1) switched to DEPLOYING08/01/2016 12:12:12 CHAIN DataSource (at getDefaultEdgeDataSet(EnumTrianglesData.java:57) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; Map (Map at main(EnumTriangles.java:108))(1/1) switched to RUNNING08/01/2016 12:12:12 CHAIN DataSource (at getDefaultEdgeDataSet(EnumTrianglesData.java:57) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; Map (Map at main(EnumTriangles.java:108))(1/1) switched to FINISHED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(1/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(3/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(2/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(7/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(7/11) switched to DEPLOYING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(6/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(4/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(5/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(4/11) switched to DEPLOYING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(3/11) switched to DEPLOYING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(9/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(9/11) switched to DEPLOYING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(5/11) switched to DEPLOYING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(1/11) switched to DEPLOYING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(1/11) switched to SCHEDULED08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(1/11) switched to DEPLOYING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(2/11) switched to SCHEDULED08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(2/11) switched to DEPLOYING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(3/11) switched to SCHEDULED08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(3/11) switched to DEPLOYING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(4/11) switched to SCHEDULED08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(4/11) switched to DEPLOYING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(5/11) switched to SCHEDULED08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(5/11) switched to DEPLOYING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(6/11) switched to SCHEDULED08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(6/11) switched to DEPLOYING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(7/11) switched to SCHEDULED08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(7/11) switched to DEPLOYING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(8/11) switched to SCHEDULED08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(8/11) switched to DEPLOYING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(9/11) switched to SCHEDULED08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(9/11) switched to DEPLOYING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(10/11) switched to SCHEDULED08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(10/11) switched to DEPLOYING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(11/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(10/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(11/11) switched to DEPLOYING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(10/11) switched to DEPLOYING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(8/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(6/11) switched to DEPLOYING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(2/11) switched to DEPLOYING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(3/11) switched to RUNNING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(11/11) switched to SCHEDULED08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(1/11) switched to RUNNING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(1/11) switched to RUNNING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(2/11) switched to RUNNING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(3/11) switched to RUNNING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(9/11) switched to RUNNING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(4/11) switched to RUNNING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(5/11) switched to RUNNING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(7/11) switched to RUNNING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(6/11) switched to RUNNING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(8/11) switched to RUNNING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(9/11) switched to RUNNING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(10/11) switched to RUNNING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(10/11) switched to RUNNING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(11/11) switched to RUNNING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(4/11) switched to RUNNING08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(5/11) switched to RUNNING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(7/11) switched to RUNNING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(2/11) switched to RUNNING08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(6/11) switched to RUNNING08/01/2016 12:12:13 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(1/11) switched to FINISHED08/01/2016 12:12:13 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(2/11) switched to FINISHED08/01/2016 12:12:13 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(7/11) switched to FINISHED08/01/2016 12:12:13 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(6/11) switched to FINISHED08/01/2016 12:12:13 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(3/11) switched to FINISHED08/01/2016 12:12:13 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(9/11) switched to FINISHED08/01/2016 12:12:13 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(11/11) switched to FINISHED08/01/2016 12:12:13 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(5/11) switched to FINISHED08/01/2016 12:12:13 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(10/11) switched to FINISHED08/01/2016 12:12:13 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(4/11) switched to FINISHEDFor 8/11, the Join task switches to RUNNING, but the GroupReduce does not:08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(8/11) switched to SCHEDULED08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(8/11) switched to DEPLOYING....08/01/2016 12:12:12 GroupReduce (GroupReduce at main(EnumTriangles.java:112))(8/11) switched to SCHEDULED....{08/01/2016 12:12:12 Join(Join at main(EnumTriangles.java:114))(8/11) switched to RUNNING}}</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphDeploymentTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
    </fixedFiles>
  </bug>
  <bug id="4297" opendate="2016-8-1 00:00:00" fixdate="2016-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn client can&amp;#39;t determine fat jar location if path contains spaces</summary>
      <description>The code that automatically determines the fat jar path through the ProtectionDomain of the Yarn class, receives a possibly URL encoded path string. We need to decode using the system locale encoding, otherwise we can receive errors of the following when spaces are in the file path: Caused by: java.io.FileNotFoundException: File file:/Users/max/Downloads/release-testing/flink-1.1.0-rc1/flink-1.1.0/build%20target/lib/flink-dist_2.11-1.1.0.jar does not exist at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:511) at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:724) at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:501) at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:397) at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337) at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289) at org.apache.hadoop.fs.LocalFileSystem.copyFromLocalFile(LocalFileSystem.java:82) at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1836) at org.apache.flink.yarn.Utils.setupLocalResource(Utils.java:129) at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deployInternal(AbstractYarnClusterDescriptor.java:616) at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deploy(AbstractYarnClusterDescriptor.java:365) ... 6 more</description>
      <version>None</version>
      <fixedVersion>1.1.1,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-dist.src.main.flink-bin.yarn-bin.yarn-session.sh</file>
    </fixedFiles>
  </bug>
  <bug id="4298" opendate="2016-8-1 00:00:00" fixdate="2016-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up Storm Compatibility Dependencies</summary>
      <description>The flink-storm project contains unnecessary (transitive) dependencies Google Guava Ring Closure Servlet BindingsParticularly the last one is frequently troublesome in Maven builds (unstable downloads) and is not required, because the Storm Compatibility layer does not start a web UI.</description>
      <version>1.0.3,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.WrapperSetupHelperTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.SpoutWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.MergedInputsBoltWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.FlinkTopologyContext.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.BoltWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkClient.java</file>
      <file type="M">flink-contrib.flink-storm.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4299" opendate="2016-8-1 00:00:00" fixdate="2016-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show loss of job manager in Client</summary>
      <description>If the client looses the connection to a job manager and the job recovers from this, the client will only print the job status as RUNNING again. It is hard to actually notice that something went wrong and a job manager was lost....08/01/2016 14:35:43 Flat Map -&gt; Sink: Unnamed(8/8) switched to RUNNING08/01/2016 14:35:43 Source: Custom Source(6/8) switched to RUNNING&lt;------ EVERYTHING'S RUNNING ------&gt;08/01/2016 14:40:40 Job execution switched to status RUNNING &lt;--- JOB MANAGER FAIL OVER08/01/2016 14:40:40 Source: Custom Source(1/8) switched to SCHEDULED08/01/2016 14:40:40 Source: Custom Source(1/8) switched to DEPLOYING08/01/2016 14:40:40 Source: Custom Source(2/8) switched to SCHEDULED...After 14:35:43 everything is running and the client does not print any execution state updates. When the job manager fails, the job will be recovered and enter the running state again eventually (at 14:40:40), but the user might never notice this.I would like to improve on this by printing some messages about the state of the job manager connection. For example, between 14:35:43 and 14:40:40 it might say that the job manager connection was lost, a new one established, etc.</description>
      <version>None</version>
      <fixedVersion>1.1.0,1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.client.JobClientActor.java</file>
    </fixedFiles>
  </bug>
  <bug id="4304" opendate="2016-8-2 00:00:00" fixdate="2016-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jar names that contain whitespace cause problems in web client</summary>
      <description>If the Jar file name contains whitespaces the web client can not start or delete a job:org.apache.flink.client.program.ProgramInvocationException: JAR file does not exist '/var/folders/w8/k702f8s1017dfbfx_qlv2p240000gn/T/flink-web-upload-4c52b922-8307-4098-b196-58b971864c51/980cad63-304c-48bb-a403-4756aea26ab4_Word%20Count.jar' at org.apache.flink.client.program.PackagedProgram.checkJarFile(PackagedProgram.java:755) at org.apache.flink.client.program.PackagedProgram.&lt;init&gt;(PackagedProgram.java:181) at org.apache.flink.client.program.PackagedProgram.&lt;init&gt;(PackagedProgram.java:147) at org.apache.flink.runtime.webmonitor.handlers.JarActionHandler.getJobGraphAndClassLoader(JarActionHandler.java:91) at org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.handleRequest(JarRunHandler.java:50)</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.js.vendor.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.submit.submit.svc.coffee</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.RuntimeMonitorHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="4306" opendate="2016-8-2 00:00:00" fixdate="2016-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Flink and Storm dependencies in flink-storm and flink-storm-examples</summary>
      <description>Flink dependencies should be in scope provided, like in the other libraries. flink-storm-examples should not draw storm-core directly, but only via flink-storm, so it gets the proper transitive dependency exclusions flink-storm-examples should have the clojure jar repository as an additional maven repository</description>
      <version>1.0.3,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm.pom.xml</file>
      <file type="M">flink-contrib.flink-storm-examples.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4307" opendate="2016-8-2 00:00:00" fixdate="2016-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Broken user-facing API for ListState</summary>
      <description>The user-facing ListState is supposed to return an empty list when no element is contained in the state.A previous change altered that behavior to make it in the runtime classes accessible whether a ListState is empty.To not break the user-facing API, we need to restore the behavior for ListState exposed to the users via the RuntimeContext.</description>
      <version>None</version>
      <fixedVersion>1.1.0,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContextTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="4310" opendate="2016-8-3 00:00:00" fixdate="2016-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move BinaryCompatibility Check plugin to relevant projects</summary>
      <description>The Maven plugin that checks binary compatibility is currently run for every project, rather than only the once where we have public API classes.Since the plugin contributes to build instability in some cases, we can improve stability by running it only in the relevant projects.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-streaming-java.pom.xml</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
      <file type="M">flink-shaded-curator.pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-core.pom.xml</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">flink-batch-connectors.flink-hadoop-compatibility.pom.xml</file>
      <file type="M">flink-annotations.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4315" opendate="2016-8-5 00:00:00" fixdate="2016-11-5 01:00:00" resolution="Done">
    <buginformation>
      <summary>Deprecate Hadoop dependent methods in flink-java</summary>
      <description>The API projects should be independent of Hadoop, because Hadoop is not an integral part of the Flink stack, and we should have the option to offer Flink without Hadoop dependencies.The current batch APIs have a hard dependency on Hadoop, mainly because the API has utility methods like `readHadoopFile(...)`.I suggest to deprecate those methods and add helpers in the `flink-hadoop-compatibility` project.FLINK-4048 will later remove the deprecated methods.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.hadoop.mapred.WordCountMapredITCase.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.hadoop.mapreduce.WordCountMapreduceITCase.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.hadoop.mapred.WordCountMapredITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.hadoop.mapreduce.WordCountMapreduceITCase.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.utils.ParameterToolTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.utils.ParameterTool.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">flink-batch-connectors.flink-hadoop-compatibility.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4318" opendate="2016-8-5 00:00:00" fixdate="2016-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make master docs build target version-specific</summary>
      <description>Snapshots docs are found at .../flink-docs-master. This is relative to when the link is posted. We often do this on the mailing lists, but the docs evolve over time. This can lead to dead links or page content which don't match the original content (when the link was posted).I would like to move the snapshot docs to their respective version, e.g. .../flink-docs-release-1.2 before we do the actual release. This way, we will hopefully have less broken links and archived emails will make more sense. Furthermore, we will have more freedom when re-organizing pages.The current master could point to .../flink-docs-release-1.1.mxm what do you think?</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..config.yml</file>
    </fixedFiles>
  </bug>
  <bug id="4337" opendate="2016-8-9 00:00:00" fixdate="2016-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary Scala suffix from Hadoop1 artifact</summary>
      <description>The hadoop1 artifacts have a "_2.10" Scala suffix, but have no Scala dependency.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop1.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4339" opendate="2016-8-9 00:00:00" fixdate="2016-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement Slot Pool Core</summary>
      <description>Impements the core slot structures and behavior of the SlotPool: pool of available slots request slots and response if slot is available in pool return / deallocate slotsDetail design in here: https://docs.google.com/document/d/1y4D-0KGiMNDFYOLRkJy-C04nl8fwJNdm9hoUfxce6zY/</description>
      <version>1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.SlotID.java</file>
    </fixedFiles>
  </bug>
  <bug id="4341" opendate="2016-8-9 00:00:00" fixdate="2016-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kinesis connector does not emit maximum watermark properly</summary>
      <description>*Prevously reported as "Checkpoint state size grows unbounded when task parallelism not uniform"*This issue was first encountered with Flink release 1.1.0 (commit 45f7825). I was previously using a 1.1.0 snapshot (commit 18995c8) which performed as expected. This issue was introduced somewhere between those commits.I've got a Flink application that uses the Kinesis Stream Consumer to read from a Kinesis stream with 2 shards. I've got 2 task managers with 2 slots each, providing a total of 4 slots. When running the application with a parallelism of 4, the Kinesis consumer uses 2 slots (one per Kinesis shard) and 4 slots for subsequent tasks that process the Kinesis stream data. I use an in-memory store for checkpoint data.Yesterday I upgraded to Flink 1.1.0 (45f7825) and noticed that checkpoint states were growing unbounded when running with a parallelism of 4, checkpoint interval of 10 seconds:ID State Size1 11.3 MB2 20.9 MB3 30.6 MB4 41.4 MB5 52.6 MB6 62.5 MB7 71.5 MB8 83.3 MB9 93.5 MBThe first 4 checkpoints generally succeed, but then fail with an exception like the following:java.lang.RuntimeException: Error triggering a checkpoint as the result of receiving checkpoint barrier at org.apache.flink.streaming.runtime.tasks.StreamTask$2.onEvent(StreamTask.java:768) at org.apache.flink.streaming.runtime.tasks.StreamTask$2.onEvent(StreamTask.java:758) at org.apache.flink.streaming.runtime.io.BarrierBuffer.processBarrier(BarrierBuffer.java:203) at org.apache.flink.streaming.runtime.io.BarrierBuffer.getNextNonBlocked(BarrierBuffer.java:129) at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:183) at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:66) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:266) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:584) at java.lang.Thread.run(Thread.java:745)Caused by: java.io.IOException: Size of the state is larger than the maximum permitted memory-backed state. Size=12105407 , maxSize=5242880 . Consider using a different state backend, like the File System State backend. at org.apache.flink.runtime.state.memory.MemoryStateBackend.checkSize(MemoryStateBackend.java:146) at org.apache.flink.runtime.state.memory.MemoryStateBackend$MemoryCheckpointOutputStream.closeAndGetBytes(MemoryStateBackend.java:200) at org.apache.flink.runtime.state.memory.MemoryStateBackend$MemoryCheckpointOutputStream.closeAndGetHandle(MemoryStateBackend.java:190) at org.apache.flink.runtime.state.AbstractStateBackend$CheckpointStateOutputView.closeAndGetHandle(AbstractStateBackend.java:447) at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.snapshotOperatorState(WindowOperator.java:879) at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:598) at org.apache.flink.streaming.runtime.tasks.StreamTask$2.onEvent(StreamTask.java:762) ... 8 moreOr:2016-08-09 17:44:43,626 INFO org.apache.flink.streaming.runtime.tasks.StreamTask - Restoring checkpointed state to task Fold: property_id, player -&gt; 10-minute Sliding-Window Percentile Aggregation -&gt; Sink: InfluxDB (2/4)2016-08-09 17:44:51,236 ERROR akka.remote.EndpointWriter - Transient association error (association remains live) akka.remote.OversizedPayloadException: Discarding oversized payload sent to Actor[akka.tcp://flink@10.55.2.212:6123/user/jobmanager#510517238]: max allowed size 10485760 bytes, actual size of encoded class org.apache.flink.runtime.messages.checkpoint.AcknowledgeCheckpoint was 10891825 bytes.This can be fixed by simply submitting the job with a parallelism of 2. I suspect there was a regression introduced relating to assumptions about the number of sub-tasks associated with a job stage (e.g. assuming 4 instead of a value ranging from 1-4). This is currently preventing me from using all available Task Manager slots.</description>
      <version>1.1.0,1.1.1</version>
      <fixedVersion>1.1.2,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer.java</file>
      <file type="M">docs.dev.connectors.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug id="4342" opendate="2016-8-9 00:00:00" fixdate="2016-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix dependencies of flink-connector-filesystem</summary>
      <description>The flink-connector-filesystem has inconsistent dependencies The Guava dependency is unused and can be removed The hadoop-shaded dependency is in 'compile' scope, but should be in 'provided' scope, because it must not go into the user code jar.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4346" opendate="2016-8-9 00:00:00" fixdate="2016-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement basic RPC abstraction</summary>
      <description>As part of refactoring of the cluster management, we can introduce a new RPC abstraction on top of our Akka-based distributed coordination.It should address the following issues: Add type safety to the sender and receiver of messages. We want proper types methods to be called, rather than haveing generic message types and pattern matching everywhere. This is similar to typed actors. Make the message receivers testable without involving actors, i.e. the methods should be callable directly. When used with other component, the receiver will be wrapped in an actor that calls the methods based on received messages. We want to keep the paradigm of single-threaded execution per "actor"There is some basic code layout in the following branch and commit:https://github.com/apache/flink/tree/flip-6/flink-runtime/src/main/java/org/apache/flink/runtime/rpc</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4351" opendate="2016-8-10 00:00:00" fixdate="2016-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JobManager handle TaskManager&amp;#39;s registration</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.JobLeaderService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="4359" opendate="2016-8-10 00:00:00" fixdate="2016-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add INTERVAL type</summary>
      <description>In order to start with StreamSQL windows we need a way to define intervals in time.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.expression.TimeTypesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.table.ExpressionsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.TypeCoercion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.TypeCheckUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.Types.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.FlinkTypeFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.literals.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.cast.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.arithmetic.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.ScalarOperators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
      <file type="M">docs.apis.table.md</file>
    </fixedFiles>
  </bug>
  <bug id="4362" opendate="2016-8-10 00:00:00" fixdate="2016-8-10 01:00:00" resolution="Done">
    <buginformation>
      <summary>Auto generate message sender classes via Java Proxies</summary>
      <description>The first version of the RPC service needs to manually create the sender classes, which turn method calls into messages.This can be automated by using Java Proxies.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.RpcCompletenessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.MainThreadExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.taskexecutor.TaskExecutorAkkaGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.taskexecutor.TaskExecutorAkkaActor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.resourcemanager.ResourceManagerAkkaGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.resourcemanager.ResourceManagerAkkaActor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.messages.UpdateTaskExecutionState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.messages.RunnableMessage.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.messages.RequestSlot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.messages.RegisterJobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.messages.RegisterAtResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.messages.ExecuteTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.messages.CancelTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.messages.CallableMessage.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.jobmaster.JobMasterAkkaGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.jobmaster.JobMasterAkkaActor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.BaseAkkaGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.BaseAkkaActor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaGateway.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.ReflectionUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="4363" opendate="2016-8-10 00:00:00" fixdate="2016-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement TaskManager basic startup of all components in java</summary>
      <description>Similar with current TaskManager,but implement initialization and startup all components in java instead of scala.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.taskexecutor.TaskExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="4364" opendate="2016-8-10 00:00:00" fixdate="2016-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement heartbeat logic between TaskManager and JobManager</summary>
      <description>It is part of work for FLIP-6.The HeartbeatManager is mainly used for monitoring heartbeat target and reporting payloads.For JobManager side, it would trigger monitoring the HeartbeatTarget when receive registration from TaskManager, and schedule a task to requestHeartbeat at interval time. If not receive heartbeat response within duration time, the HeartbeatListener will notify heartbeat timeout, then the JobManager should remove the internal registered TaskManager.For TaskManger side, it would trigger monitoring the HeartbeatTarget when receive registration acknowledgement from JobManager. An it will also be notified heartbeat timeout if not receive heartbeat request from JobManager within duration time.The current implementation will not interact payloads via heartbeat, and it can be added if needed future.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnTaskExecutorRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnFlinkApplicationMasterRunner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobManagerRunnerMockTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterJobDispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.heartbeat.TestingHeartbeatManagerSenderImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.heartbeat.TestingHeartbeatManagerImpl.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HeartbeatManagerOptions.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.heartbeat.HeartbeatManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.JobManagerConnection.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.heartbeat.HeartbeatManagerSenderImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.heartbeat.HeartbeatManagerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="4368" opendate="2016-8-10 00:00:00" fixdate="2016-8-10 01:00:00" resolution="Done">
    <buginformation>
      <summary>Eagerly initialize RrcProtocol members</summary>
      <description>The members of the RPC endpoint (RpcProtocol) are lazily created upon the start() call.I suggest to initialize them eagerly as they seem to be integral parts without which several functions cannot work properly.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.MainThreadExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="4375" opendate="2016-8-11 00:00:00" fixdate="2016-8-11 01:00:00" resolution="Invalid">
    <buginformation>
      <summary>Introduce rpc protocols implemented by job manager</summary>
      <description>job manager RPC server needs to implement a job control protocol, resource user protocol, task control protocol, 1. job controller: cancelJob, suspendJob, etc. 2. resource user: slotFailed(notify slot failure), slotAvailable(offer slot), etc. 3. task controller: updateTaskState, updateResultPartitionInfo, etc.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobManagerRunnerMockTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.TestingHighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.rpc.RpcResultPartitionConsumableNotifier.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.rpc.RpcPartitionStateChecker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.rpc.RpcInputSplitProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.JobManagerConnection.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.MiniClusterJobDispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.message.TriggerSavepointResponse.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.message.DisposeSavepointResponse.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.message.ClassloadingProps.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobManagerServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.OnCompletionActions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.ZookeeperHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.NonHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.VoidBlobStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.FileSystemBlobStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobServer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.StringUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="4379" opendate="2016-8-11 00:00:00" fixdate="2016-9-11 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add Rescalable Non-Partitioned State</summary>
      <description>This issue is associated with FLIP-8.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.stats.SimpleCheckpointStatsTrackerTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.StateBackendITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UdfStreamOperatorCheckpointingITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.KeyedOneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamMockEnvironment.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.InterruptSensitiveRestoreTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamOperatorChainingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierTrackerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierBufferTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContextTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.EvictingWindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.GenericWriteAheadSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContext.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.checkpoint.Checkpointed.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.MockRuntimeContext.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.AtLeastOnceProducerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumer08Test.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer08.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskAsyncCallTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.MemoryStateBackendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStateOutputStreamTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.FileStateBackendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.AbstractCloseableHandleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.query.QueryableStateClientTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.query.netty.KvStateServerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.query.netty.KvStateServerHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.query.netty.KvStateClientTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DummyEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.messages.CheckpointMessagesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerHARecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreITCase.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.AbstractRocksDBState.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBAsyncSnapshotTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.RuntimeContext.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.util.AbstractRuntimeUDFContext.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.OperatorState.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.ValueState.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.FileStateBackendTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractCEPBasePatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointV1Serializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.SubtaskState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.TaskState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.Environment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.StatefulTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.checkpoint.AcknowledgeCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractCloseableHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ChainedStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FileStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyGroupRangeOffsets.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyGroupsStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.ByteStreamStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.MemoryStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.RetrievableStreamStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateObject.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateUtil.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayCheckpointResponder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.CheckpointResponder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.RuntimeEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingSavepointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointV1Test.java</file>
    </fixedFiles>
  </bug>
  <bug id="4385" opendate="2016-8-11 00:00:00" fixdate="2016-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Union on Timestamp fields does not work</summary>
      <description>The following does not work:public static class SDF { public Timestamp t = Timestamp.valueOf("1990-10-10 12:10:10");}ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();DataSet&lt;SDF&gt; dataSet1 = env.fromElements(new SDF());DataSet&lt;SDF&gt; dataSet2 = env.fromElements(new SDF());BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);tableEnv.registerDataSet( "table0", dataSet1 );tableEnv.registerDataSet( "table1", dataSet2 );Table table = tableEnv.sql( "select t from table0 union select t from table1" );DataSet&lt;Row&gt; d = tableEnv.toDataSet(table, Row.class);d.print();</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.IntervalTypeInfo.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetRel.scala</file>
    </fixedFiles>
  </bug>
  <bug id="4389" opendate="2016-8-12 00:00:00" fixdate="2016-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose metrics to Webfrontend</summary>
      <description>https://cwiki.apache.org/confluence/display/FLINK/FLIP-7%3A+Expose+metrics+to+WebInterface</description>
      <version>1.1.0</version>
      <fixedVersion>pre-apache,1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.YarnTaskManager.scala</file>
      <file type="M">flink-yarn-tests.src.test.scala.org.apache.flink.yarn.TestingYarnTaskManager.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingTaskManager.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerComponentsStartupShutdownTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.TaskMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.TaskManagerJobGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.TaskManagerGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.OperatorGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.MetricGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.JobManagerJobGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.JobManagerGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.AbstractMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.minicluster.LocalFlinkMiniCluster.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricRegistry.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskManagerMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.OperatorMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.JobMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.JobManagerMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.GenericMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.AbstractMetricGroup.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.metrics.jmx.JMXReporterTest.java</file>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManager.scala</file>
    </fixedFiles>
  </bug>
  <bug id="4391" opendate="2016-8-13 00:00:00" fixdate="2016-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide support for asynchronous operations over streams</summary>
      <description>Many Flink users need to do asynchronous processing driven by data from a DataStream. The classic example would be joining against an external database in order to enrich a stream with extra information.It would be nice to add general support for this type of operation in the Flink API. Ideally this could simply take the form of a new operator that manages async operations, keeps so many of them in flight, and then emits results to downstream operators as the async operations complete.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamOperatorChainingTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.async.AsyncCollectorBufferTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.async.RichAsyncFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.TimestampedCollector.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.async.RichAsyncFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.async.collector.AsyncCollector.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.async.buffer.WatermarkEntry.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.async.buffer.StreamRecordEntry.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.async.buffer.StreamElementEntry.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.async.buffer.LatencyMarkerEntry.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.async.buffer.AsyncCollectorBuffer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.async.buffer.AbstractBufferEntry.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.async.AsyncFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.ExceptionUtils.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.api.StreamingOperatorsITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.queue.WatermarkQueueEntry.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.queue.UnorderedStreamElementQueue.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.queue.StreamRecordQueueEntry.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.queue.StreamElementQueueEntry.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.queue.StreamElementQueue.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.queue.OrderedStreamElementQueue.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.queue.AsyncWatermarkResult.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.queue.AsyncResult.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.queue.AsyncCollectionResult.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.OperatorActions.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.Emitter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.AsyncDataStream.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.async.AsyncIOExample.java</file>
    </fixedFiles>
  </bug>
  <bug id="4392" opendate="2016-8-13 00:00:00" fixdate="2016-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make RPC Service Thread Safe</summary>
      <description>The RPC Service should not fail in the presence of concurrent requests to start endpoints</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaGateway.java</file>
    </fixedFiles>
  </bug>
  <bug id="4409" opendate="2016-8-17 00:00:00" fixdate="2016-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>class conflict between jsr305-1.3.9.jar and flink-shaded-hadoop2-1.1.1.jar</summary>
      <description>It seems all classes in jsr305-1.3.9.jar can be found in flink-shaded-hadoop2-1.1.1.jar,too.I can exclude these jars for a success assembly and run when I was using sbtlibraryDependencies ++= Seq( "com.typesafe.play" %% "play-json" % "2.3.8", "org.apache.flink" %% "flink-scala" % "1.1.1" exclude("com.google.code.findbugs", "jsr305"), "org.apache.flink" %% "flink-connector-kafka-0.8" % "1.1.1" exclude("com.google.code.findbugs", "jsr305"), "org.apache.flink" %% "flink-streaming-scala" % "1.1.1" exclude("com.google.code.findbugs", "jsr305"), "org.apache.flink" %% "flink-clients" % "1.1.1" exclude("com.google.code.findbugs", "jsr305"), "joda-time" % "joda-time" % "2.9.4", "org.scalikejdbc" %% "scalikejdbc" % "2.2.7", "mysql" % "mysql-connector-java" % "5.1.15", "io.spray" %% "spray-caching" % "1.3.3")But I think it might be better to remove jsr305 dependency from Flink.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.2,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4411" opendate="2016-8-17 00:00:00" fixdate="2016-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Chained dual input children are not properly propagated</summary>
      <description></description>
      <version>1.1.0</version>
      <fixedVersion>1.1.2,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.main2.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="4422" opendate="2016-8-18 00:00:00" fixdate="2016-1-18 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Convert all time interval measurements to System.nanoTime()</summary>
      <description>In contrast to System.currentTimeMillis(), System.nanoTime() is monotonous. To measure delays and time intervals, System.nanoTime() is hence reliable, while System.currentTimeMillis() is not.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ClientConnectionTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KillerWatchDog.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.ClosableBlockingQueue.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="4478" opendate="2016-8-24 00:00:00" fixdate="2016-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement heartbeat logic</summary>
      <description>With the Flip-6 refactoring, we'll have the need for a dedicated heartbeat component. The heartbeat component is used to check the liveliness of the distributed components among each other. Furthermore, heartbeats are used to regularly transmit status updates to another component. For example, the TaskManager informs the ResourceManager with each heartbeat about the current slot allocation.The heartbeat is initiated from one component. This component sends a heartbeat request to another component which answers with an heartbeat response. Thus, one can differentiate between a sending and a receiving side. Apart from the triggering of the heartbeat request, the logic of treating heartbeats, marking components dead and payload delivery are the same and should be reusable by different distributed components (JM, TM, RM).Different models for the heartbeat reporting are conceivable. First of all, the heartbeat request could be sent as an ask operation where the heartbeat response is returned as a future on the sending side. Alternatively, the sending side could request a heartbeat response by sending a tell message. The heartbeat response is then delivered by an RPC back to the heartbeat sender. The latter model has the advantage that a heartbeat response is not tightly coupled to a heartbeat request. Such a tight coupling could cause that heartbeat response are ignored after the future has timed out even though they might still contain valuable information (receiver is still alive).Furthermore, different strategies for the heartbeat triggering and marking heartbeat targets as dead are conceivable. For example, we could periodically (with a fixed period) trigger a heartbeat request and mark all targets as dead if we didn't receive a heartbeat response in a given time period. Furthermore, we could adapt the heartbeat interval and heartbeat timeouts with respect to the latency of previous heartbeat responses. This would reflect the current load and network conditions better.For the first version, I would propose to use a fixed period heartbeat with a maximum heartbeat timeout before a target is marked dead. Furthermore, I would propose to use tell messages (fire and forget) to request and report heartbeats because they are the more flexible model imho.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotProtocolTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="4485" opendate="2016-8-25 00:00:00" fixdate="2016-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Finished jobs in yarn session fill /tmp filesystem</summary>
      <description>On a Yarn cluster I start a yarn-session with a few containers and task slots.Then I fire a 'large' number of Flink batch jobs in sequence against this yarn session. It is the exact same job (java code) yet it gets different parameters.In this scenario it is exporting HBase tables to files in HDFS and the parameters are about which data from which tables and the name of the target directory.After running several dozen jobs the jobs submission started to fail and we investigated.We found that the cause was that on the Yarn node which was hosting the jobmanager the /tmp file system was full (4GB was 100% full).How ever the output of du -hcs /tmp showed only 200MB in use.We found that a very large file (we guess it is the jar of the job) was put in /tmp , used, deleted yet the file handle was not closed by the jobmanager.As soon as we killed the jobmanager the disk space was freed.The summary of the impact of this is that a yarn-session that receives enough jobs brings down the Yarn node for all users.See parts of the output we got from lsof below.COMMAND PID USER FD TYPE DEVICE SIZE NODE NAMEjava 15034 nbasjes 550r REG 253,17 66219695 245 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000003 (deleted)java 15034 nbasjes 551r REG 253,17 66219695 252 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000007 (deleted)java 15034 nbasjes 552r REG 253,17 66219695 267 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000012 (deleted)java 15034 nbasjes 553r REG 253,17 66219695 250 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000005 (deleted)java 15034 nbasjes 554r REG 253,17 66219695 288 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000018 (deleted)java 15034 nbasjes 555r REG 253,17 66219695 298 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000025 (deleted)java 15034 nbasjes 557r REG 253,17 66219695 254 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000008 (deleted)java 15034 nbasjes 558r REG 253,17 66219695 292 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000019 (deleted)java 15034 nbasjes 559r REG 253,17 66219695 275 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000013 (deleted)java 15034 nbasjes 560r REG 253,17 66219695 159 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000002 (deleted)java 15034 nbasjes 562r REG 253,17 66219695 238 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000001 (deleted)java 15034 nbasjes 568r REG 253,17 66219695 246 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000004 (deleted)java 15034 nbasjes 569r REG 253,17 66219695 255 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000009 (deleted)java 15034 nbasjes 571r REG 253,17 66219695 299 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000026 (deleted)java 15034 nbasjes 572r REG 253,17 66219695 293 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000020 (deleted)java 15034 nbasjes 574r REG 253,17 66219695 256 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000010 (deleted)java 15034 nbasjes 575r REG 253,17 66219695 302 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000029 (deleted)java 15034 nbasjes 576r REG 253,17 66219695 294 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000021 (deleted)java 15034 nbasjes 577r REG 253,17 66219695 262 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000011 (deleted)java 15034 nbasjes 578r REG 253,17 66219695 251 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000006 (deleted)java 15034 nbasjes 580r REG 253,17 66219695 295 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000022 (deleted)java 15034 nbasjes 581r REG 253,17 66219695 300 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000027 (deleted)java 15034 nbasjes 582r REG 253,17 66219695 188 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/cache/blob_e318d1698aa6e7dc91e5f4a9f8ba29781aebd8c4 (deleted)java 15034 nbasjes 585r REG 253,17 66219695 279 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000014 (deleted)java 15034 nbasjes 586r REG 253,17 66219695 296 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000023 (deleted)java 15034 nbasjes 588r REG 253,17 66219695 301 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000028 (deleted)java 15034 nbasjes 589r REG 253,17 66219695 297 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000024 (deleted)java 15034 nbasjes 598r REG 253,17 66219695 280 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000015 (deleted)java 15034 nbasjes 601r REG 253,17 66219695 289 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000016 (deleted)java 15034 nbasjes 604r REG 253,17 66219695 284 /tmp/blobStore-fbe9c4cf-1f85-48cb-aad9-180e8d4ec7ce/incoming/temp-00000017 (deleted)</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.3,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.web.WebFrontendITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.client.JobClient.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobConfigHandler.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.JobWithJars.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="4486" opendate="2016-8-25 00:00:00" fixdate="2016-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JobManager not fully running when yarn-session.sh finishes</summary>
      <description>I start a detached yarn-session.sh.If the Yarn cluster is very busy then the yarn-session.sh script completes BEFORE all the task slots have been allocated. As a consequence I sometimes have a jobmanager without any task slots. Over time these task slots are assigned by the Yarn cluster but these are not available for the first job that is submitted.As a consequence I have found that the first few tasks in my job fail with this error "Not enough free slots available to run the job.".I think the desirable behavior is that yarn-session waits until the jobmanager is fully functional and capable of actually running the jobs.org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Not enough free slots available to run the job. You can decrease the operator parallelism or increase the number of slots per TaskManager in the configuration. Task to schedule: &lt; Attempt #0 (CHAIN DataSource (Read prefix '4') -&gt; Map (Map prefix '4') (8/10)) @ (unassigned) - [SCHEDULED] &gt; with groupID &lt; cd6c37df290564e603da908a8783a9bf &gt; in sharing group &lt; SlotSharingGroup [c0b6eff6ce93967182cdb6dfeae9359b, 8b2c3b39f3a55adf9f123243ab03c9c1, 55fb94dd8a3e5f59a10dbbf5c4925db4, 433b2e4a05a5e685b48c517249755a89, 8c74690c35454064e4815ac3756cdca2, 4b4fbd24f3483030fd852b38ff2249c1, 5e36a56ea4dece18fe5ba04352d90dc8, cd6c37df290564e603da908a8783a9bf, 64eafa845087bee70735f7250df9994f, 706a5d6fe48ae57724a00a9fce5dae8a, 7bee4297e0e839e53a153dfcbcca8624, 21b58f7d408d237540ae7b4734f81a1d, b429b1ff338d9d73677f42717cfc0dbc, cc7491db641f557c6aa8c749ebc2de62, f61cbf0ae00331f67aaf60ace78b05aa, 606f02ea9e0f4ad57f0cc0232dd70842] &gt;. Resources available to scheduler: Number of instances=1, total number of slots=7, available slots=0 at org.apache.flink.runtime.jobmanager.scheduler.Scheduler.scheduleTask(Scheduler.java:256) at org.apache.flink.runtime.jobmanager.scheduler.Scheduler.scheduleImmediately(Scheduler.java:131) at org.apache.flink.runtime.executiongraph.Execution.scheduleForExecution(Execution.java:306) at org.apache.flink.runtime.executiongraph.ExecutionVertex.scheduleForExecution(ExecutionVertex.java:454) at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.scheduleAll(ExecutionJobVertex.java:326) at org.apache.flink.runtime.executiongraph.ExecutionGraph.scheduleForExecution(ExecutionGraph.java:734) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$org$apache$flink$runtime$jobmanager$JobManager$$submitJob$1.apply$mcV$sp(JobManager.scala:1332) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$org$apache$flink$runtime$jobmanager$JobManager$$submitJob$1.apply(JobManager.scala:1291) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$org$apache$flink$runtime$jobmanager$JobManager$$submitJob$1.apply(JobManager.scala:1291) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:401) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.pollAndExecAll(ForkJoinPool.java:1253) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1346) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.2,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterClient.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.StandaloneClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ClusterClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="4497" opendate="2016-8-25 00:00:00" fixdate="2016-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for Scala tuples and case classes to Cassandra sink</summary>
      <description>The new Cassandra sink only supports streams of Flink Java tuples and Java POJOs that have been annotated for use by Datastax Mapper. The sink should be extended to support Scala types and case classes.</description>
      <version>1.1.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.streaming.connectors.cassandra.CassandraTupleSink.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.streaming.connectors.cassandra.CassandraSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.streaming.connectors.cassandra.CassandraSink.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4499" opendate="2016-8-25 00:00:00" fixdate="2016-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce findbugs maven plugin</summary>
      <description>As suggested by Stephan in FLINK-4482, this issue is to add findbugs-maven-plugin into the build process so that we can detect lack of proper locking and other defects automatically.We can begin with small set of rules.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4510" opendate="2016-8-26 00:00:00" fixdate="2016-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Always create CheckpointCoordinator</summary>
      <description>The checkpoint coordinator is only created if a checkpointing interval is configured. This means that no savepoints can be triggered if there is no checkpointing interval specified.Instead we should always create it and allow an interval of 0 for disabled periodic checkpoints.</description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="4514" opendate="2016-8-26 00:00:00" fixdate="2016-8-26 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>ExpiredIteratorException in Kinesis Consumer on long catch-ups to head of stream</summary>
      <description>Original mailing thread for the reported issue:http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Kinesis-connector-Iterator-expired-exception-td8711.htmlNormally, the exception is thrown when the consumer uses the same shard iterator after 5 minutes since it was retrieved. I've still yet to clarify &amp; reproduce the root cause of the ExpiredIteratorException, because from the code this seems to be impossible. I'm leaning towards suspecting this is a Kinesis-side issue (from the description in the ML, the behaviour also seems indeterminate).Either way, the exception can be fairly easily handled so that the consumer doesn't just fail. When caught, we request a new shard iterator from Kinesis with the last sequence number.</description>
      <version>1.1.0,1.1.1</version>
      <fixedVersion>1.1.3,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.FakeKinesisBehavioursFactory.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtil.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.ConsumerConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="4538" opendate="2016-8-31 00:00:00" fixdate="2016-10-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement slot allocation protocol with JobMaster</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.TestingSerialRpcService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.TestingRpcService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.SlotManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerHATest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.SlotStatus.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.SlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.SlotAssignment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.RegistrationResponse.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.JobMasterRegistration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.ResourceSlot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.ResourceProfile.java</file>
    </fixedFiles>
  </bug>
  <bug id="4555" opendate="2016-9-1 00:00:00" fixdate="2016-9-1 01:00:00" resolution="Not A Problem">
    <buginformation>
      <summary>Explicitly kill TaskManager on YARN when ApplicationMaster is shutting down</summary>
      <description>It seems that Flink is not explicitly destroying the TaskManager JVM when the ApplicationMaster is shutting down (when the YARN application is stopping).Since this was once in Flink (in 1.0.x) we should add a test case to ensure this feature stays in the code.</description>
      <version>1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.FlinkResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="456" opendate="2014-6-9 00:00:00" fixdate="2014-2-9 01:00:00" resolution="Done">
    <buginformation>
      <summary>Integrate runtime metrics / statistics</summary>
      <description>The engine should collect job execution statistics (e.g., via accumulators) such as: total number of input / output records per operator histogram of input/output ratio of UDF calls histogram of number of input records per reduce / cogroup UDF call histogram of number of output records per UDF call histogram of time spend in UDF calls number of local and remote bytes read (not via accumulators) ...These stats should be made available to the user after execution (via webfrontend). The purpose of this feature is to ease performance debugging of parallel jobs (e.g., to detect data skew).It should be possible to deactivate (or activate) the gathering of these statistics.---------------- Imported from GitHub ----------------Url: https://github.com/stratosphere/stratosphere/issues/456Created by: fhueskeLabels: enhancement, runtime, user satisfaction, Created at: Tue Feb 04 20:32:49 CET 2014State: open</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.YarnJobManager.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingUtils.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingJobManager.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingCluster.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.UnregisteredTaskMetricsGroup.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.MetricRegistryTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.groups.TaskGroupTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.groups.OperatorGroupTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.groups.JobGroupTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.MetricRegistry.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.groups.TaskMetricGroup.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.groups.TaskManagerMetricGroup.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.groups.scope.ScopeFormats.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.groups.scope.ScopeFormat.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.groups.JobMetricGroup.java</file>
    </fixedFiles>
  </bug>
  <bug id="4560" opendate="2016-9-2 00:00:00" fixdate="2016-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>enforcer java version as 1.7</summary>
      <description>1. maven-enforcer-plugin add java version enforce2. maven-enforcer-plugin version upgrade to 1.4.1explicit require java version</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4562" opendate="2016-9-2 00:00:00" fixdate="2016-4-2 01:00:00" resolution="Done">
    <buginformation>
      <summary>table examples make an divided module in flink-examples</summary>
      <description>example code should't packaged in table module.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.examples.scala.WordCountTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.examples.scala.WordCountSQL.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.examples.scala.TPCHQuery3Table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.examples.scala.StreamTableExample.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.examples.scala.StreamSQLExample.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.table.examples.java.WordCountTable.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.table.examples.java.WordCountSQL.java</file>
      <file type="M">flink-examples.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4563" opendate="2016-9-2 00:00:00" fixdate="2016-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[metrics] scope caching not adjusted for multiple reporters</summary>
      <description>Every metric group contains a scope string, representing what entities (job/task/etc.) a given metric belongs to, which is calculated on demand. Before this string is cached a CharacterFilter is applied to it, which is provided by the callee, usually a reporter. This was done since different reporters have different requirements in regards to valid characters. The filtered string is cached so that we don't have to refilter the string every time.This all works fine with a single reporter; with multiple however it is completely broken as only the first filter is ever applied.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.AbstractMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.AbstractMetricGroup.java</file>
    </fixedFiles>
  </bug>
  <bug id="4577" opendate="2016-9-4 00:00:00" fixdate="2016-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-enable transparent reshard handling in Kinesis Consumer</summary>
      <description>In FLINK-4341, we disabled transparent reshard handling in the Kinesis consumer as a short-term workaround before FLINK-4576 comes around.This ticket tracks the progress of re-enabling it again, by implementing a LowWatermarkListener interface as described in FLINK-4576.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.java</file>
      <file type="M">docs.dev.connectors.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug id="4718" opendate="2016-9-30 00:00:00" fixdate="2016-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Confusing label in Parallel Streams Diagram</summary>
      <description>The Event &amp;#91;id|timestamp&amp;#93; label in the Parallel Streams Diagram is confusing. The 'id' is in fact the key of the stream and not the id of the event record. Hence we have B35 and B33.The 'id' label should be changed to key or key_id to better represent its nature.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.fig.parallel.streams.watermarks.svg</file>
    </fixedFiles>
  </bug>
  <bug id="4827" opendate="2016-10-14 00:00:00" fixdate="2016-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The scala example of SQL on Streaming Tables with wrong variable name in flink document</summary>
      <description>val env = StreamExecutionEnvironment.getExecutionEnvironmentval tEnv = TableEnvironment.getTableEnvironment(env)// read a DataStream from an external sourceval ds: DataStream&amp;#91;(Long, String, Integer)&amp;#93; = env.addSource(...)// register the DataStream under the name "Orders"tableEnv.registerDataStream("Orders", ds, 'user, 'product, 'amount)// run a SQL query on the Table and retrieve the result as a new Tableval result = tableEnv.sql( "SELECT product, amount FROM Orders WHERE product LIKE '%Rubber%'")There is no variable named tableEnv had defined ,Only tEnv defined here</description>
      <version>1.1.0,1.1.2</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="5109" opendate="2016-11-21 00:00:00" fixdate="2016-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalid Content-Encoding Header in REST API responses</summary>
      <description>On REST API calls the Flink runtime responds with the header Content-Encoding, containing the value "utf-8". According to the HTTP/1.1 standard this header is invalid. ( https://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.5 ) Possible acceptable values are: gzip, compress, deflate. Or it should be omitted.The invalid header may cause malfunction in projects building against Flink.The invalid header may be present in earlier versions aswell.Proposed solution: Remove lines from the project, where CONTENT_ENCODING header is set to "utf-8". (I could do this in a PR.)Possible solution but may need further knowledge and skills than mine: Introduce content-encoding. Doing so may need some configuration beacuse then Flink would have to encode the responses properly (even paying attention to the request's Accept-Encoding headers).</description>
      <version>1.1.0,1.1.1,1.1.2,1.1.3,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.RuntimeMonitorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.PipelineErrorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.HttpRequestHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.HandlerRedirectUtils.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.ConstantTextHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="5128" opendate="2016-11-22 00:00:00" fixdate="2016-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Get Kafka partitions in FlinkKafkaProducer only if a partitioner is set</summary>
      <description>The fetched partitions list is only used when calling open(...) for a user supplied custom partitioner in FlinkKafkaProducer.Therefore, we can actually only fetch the partition list if the user used a partitioner (right now we always do the partition fetching).</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="5247" opendate="2016-12-3 00:00:00" fixdate="2016-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix incorrect check in allowedLateness() method. Make it a no-op for non-event time windows.</summary>
      <description>Related to FLINK-3714 and FLINK-4239</description>
      <version>1.1.0,1.1.1,1.1.2,1.1.3</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.WindowedStream.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.AllWindowedStream.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.WindowedStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.AllWindowedStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="5972" opendate="2017-3-6 00:00:00" fixdate="2017-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t allow shrinking merging windows</summary>
      <description>A misbehaving MergingWindowAssigner can cause a merge that results in a window that is smaller than the span of all the merged windows. This, in itself is not problematic. It becomes problematic when the end timestamp of a window that was not late before merging is now earlier than the watermark (the timestamp is smaller than the watermark).There are two choices: immediately process the window drop the windowprocessing the window will lead to late data downstream.The current behaviour is to silently drop the window but that logic has a bug: we only remove the dropped window from the MergingWindowSet but we don't properly clean up state and timers that the window still (possibly) has. We should fix this bug in the process of resolving this issue.We should either just fix the bug and still silently drop windows or add a check and throw an exception when the end timestamp falls below the watermark.</description>
      <version>1.1.0,1.2.0,1.3.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorContractTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.EvictingWindowOperator.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
