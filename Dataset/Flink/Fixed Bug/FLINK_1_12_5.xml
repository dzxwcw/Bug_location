<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="16348" opendate="2020-2-28 00:00:00" fixdate="2020-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add commas to large numeric accumulators</summary>
      <description>Make large numeric accumulator values easier to read.Ex 273232 -&gt; 273,232</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.accumulators.job-overview-drawer-accumulators.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="22722" opendate="2021-5-20 00:00:00" fixdate="2021-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Documentation for Kafka New Source</summary>
      <description>Documentation describing the usage of Kafka FLIP-27 new source is required in Flink documentations.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.datastream.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="23223" opendate="2021-7-2 00:00:00" fixdate="2021-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When flushAlways is enabled the subpartition may lose notification of data availability</summary>
      <description>When the flushAways is enabled (namely set buffer timeout to 0), there might be cases like: The subpartition emit an event which blocks the channel The subpartition produce more records. However, this records would not be notified since isBlocked = true. When the downstream tasks resume the subpartition later, the subpartition would only mark isBlocked to false. For local input channels although it tries to add the channel if isAvailable = true, but this check would not pass since flushRequest = false. One case for this issue is https://issues.apache.org/jira/browse/FLINK-22085 which uses LocalInputChannel.</description>
      <version>1.11.3,1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionWithReadViewTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannelTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="23460" opendate="2021-7-21 00:00:00" fixdate="2021-7-21 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add a global flag for enabling/disabling final checkpoints</summary>
      <description>We should have a feature toggle for the final checkpoint story.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TestSubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MockSubtaskCheckpointCoordinatorBuilder.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.FailoverStrategyCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorMasterHooksTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.CheckpointCoordinatorConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskFinalCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskChainedSourcesCheckpointingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.StreamTaskNetworkInputTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.UnalignedCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.UnalignedCheckpointsCancellationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.TestBarrierHandlerFactory.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGateTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierTrackerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlternatingCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlignedCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlignedCheckpointsMassiveRandomTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.InputProcessorUtil.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierTracker.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ExecutionOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="23462" opendate="2021-7-21 00:00:00" fixdate="2021-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate the abfs documentation to chinese</summary>
      <description>Translate the documentation changes that were made in this PR to chinese https://github.com/apache/flink/pull/16559/</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.deployment.filesystems.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.filesystems.azure.md</file>
    </fixedFiles>
  </bug>
  <bug id="23611" opendate="2021-8-4 00:00:00" fixdate="2021-9-4 01:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>YARNSessionCapacitySchedulerITCase.testVCoresAreSetCorrectlyAndJobManagerHostnameAreShownInWebInterfaceAndDynamicPropertiesAndYarnApplicationNameAndTaskManagerSlots hangs on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21439&amp;view=logs&amp;j=245e1f2e-ba5b-5570-d689-25ae21e5302f&amp;t=e7f339b2-a7c3-57d9-00af-3712d4b15354&amp;l=28959</description>
      <version>1.14.0,1.12.5</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug id="23692" opendate="2021-8-9 00:00:00" fixdate="2021-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clarify text on "Cancel Job" confirmation buttons</summary>
      <description>When prompting a user to confirm a job cancellation currently we present the following popup (see screenshots). These text descriptions are the default in the html layer, however it happens to be misleading when the action to be confirmed is cancel.Multiple users reported that this behavior is confusing and as it is only a display issue its scope is very limited. </description>
      <version>1.13.1,1.12.5</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.status.job-status.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="23818" opendate="2021-8-16 00:00:00" fixdate="2021-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about tgz files support for python archives</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">docs.layouts.shortcodes.generated.python.configuration.html</file>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.docs.dev.python.dependency.management.md</file>
      <file type="M">docs.content.docs.deployment.cli.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.python.dependency.management.md</file>
      <file type="M">docs.content.zh.docs.deployment.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="23845" opendate="2021-8-18 00:00:00" fixdate="2021-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clarify metric deletion guarantees for Prometheus PushGateway reporter on shutdown</summary>
      <description>see https://issues.apache.org/jira/browse/FLINK-20691 .  whatever the problem has always existed, we should avoid other guys met it</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.main.java.org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporterOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.prometheus.push.gateway.reporter.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="23871" opendate="2021-8-19 00:00:00" fixdate="2021-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dispatcher should handle finishing job exception when recover</summary>
      <description>The exception during run recovery job will trigger fatal error which is introduced in https://issues.apache.org/jira/browse/FLINK-9097.  If a job have reached a finished status. But crash at clean up phase or any other post phase. When recover job, it may recover a job in RunningJobsRegistry.JobSchedulingStatus.DONE status, this may lead to the dispatcher fatal again. I think we should deal with the  RunningJobsRegistry.JobSchedulingStatus.DONE with special exception like JobFinishingException, which represents the job/master crashed in job finishing phase. And only do the clean up work for this exception</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.JobMasterBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcessTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.OnCompletionActions.java</file>
    </fixedFiles>
  </bug>
  <bug id="23906" opendate="2021-8-21 00:00:00" fixdate="2021-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase akka.ask.timeout for tests using the MiniCluster</summary>
      <description>We have seen over the last couple of weeks/months an increased number of test failures because of TimeoutException that were triggered because the akka.ask.timeout was exceeded. The reason for this was that on our CI infrastructure it can happen that there are pauses of more than 10s (not sure about the exact reason) or our infrastructure simply being slow. In order to harden all tests relying on the MiniCluster I propose to increase the akka.ask.timeout to 5 minutes if nothing else has been configured.</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="23907" opendate="2021-8-21 00:00:00" fixdate="2021-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Type Migration: introducing primitive functional interfaces</summary>
      <description>Hey!We are a collaborative group of researchers from JetBrains Research and Oregon State University, and we are testing our data-driven plugin, which is based on the IntelliJ's Type Migration framework and adjusts it using custom structural-replace templates that express the adaptations required to perform the type changes.I want to apply several type changes using it and open the PR, thus introducing primitive functional interfaces in order to prevent unnecessary boxing (like BooleanSupplier instead Supplier&lt;Boolean&gt;, OptionalInt instead of Optional&lt;Integer&gt;, Predicate&lt;T&gt; instead of Function&lt;T, Boolean&gt;, etc.), since it can affect the performance of the code (Effective Java, Items 44, 61).The patch itself is already prepared (because it is done automatically using the plugin), so I guess I will need to open this ticket, receive your approval, and then open the PR?It would help us a lot to evaluate the usefulness of our approach!Thank you in advance!</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliInputView.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.memory.SharedResourcesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDeciderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.memory.SharedResources.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.memory.MemoryManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.DataSetMetaInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.ChannelStatePersister.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDecider.java</file>
    </fixedFiles>
  </bug>
  <bug id="23950" opendate="2021-8-24 00:00:00" fixdate="2021-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert FLINK-23738 (i.e. unhide config, API, docs)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironmentComplexConfigurationTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogStateBackendLoadingTest.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.FsStateChangelogOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CheckpointingOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.common.state.backends.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.checkpointing.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="24021" opendate="2021-8-27 00:00:00" fixdate="2021-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Potential job unrecoverable due to Network failure</summary>
      <description>Now we use zk to do leader election and retrieval for HA. And we register a fatalError handler in leaderElectionService and leaderRetrievalService to let jobManager or taskManager process exit at the time of some unexpected error.But we don't do this at the time of curatorFrameworkClient#start in ZookeeperUtils. This may lead to some unexpected error like :  ZookeeperUtils start curator client, but failed by network loss, this will not throw exception now, because we do not register an error handler. The network recover when master begin do leader election, so this will success The leaderRetrieval begin to work by get_data, but this will not be executed, because the curator client start failed in phase 1. So I think we should register a error handler in phase1 , so that we can fail fast.  </description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperTestEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ZooKeeperUtilsTreeCacheTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ZooKeeperUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunnerConfigurationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.util.DocumentingDispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderRetrievalTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderRetrievalConnectionHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.ZooKeeperJobGraphStoreWatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperRegistryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZKCheckpointIDCounterMultiServersTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesClusterDescriptor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="24036" opendate="2021-8-28 00:00:00" fixdate="2021-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SSL cannot be installed on CI</summary>
      <description># install libssl1.0.0 for netty tcnativewget http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.6_amd64.debsudo apt install ./libssl1.0.0_1.0.2n-1ubuntu5.6_amd64.deb--2021-08-27 20:48:49-- http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.6_amd64.debResolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 91.189.91.38, 2001:67c:1562::15, ...Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.HTTP request sent, awaiting response... 404 Not Found2021-08-27 20:48:49 ERROR 404: Not Found.</description>
      <version>1.14.0,1.12.5,1.13.2,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="24051" opendate="2021-8-30 00:00:00" fixdate="2021-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make consumer.group-id optional for KafkaSource</summary>
      <description>For most of the users it is not necessary to generate a group-id and the source itself can provide a meaningful group-id during startup.</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.KafkaSourceBuilder.java</file>
      <file type="M">docs.content.docs.connectors.datastream.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="24199" opendate="2021-9-7 00:00:00" fixdate="2021-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose StreamExecutionEnvironment#configure in Python API</summary>
      <description>There are certain parameters that can be configured only through the underlying configuration of StreamExecutionEnvironment e.g. (execution.checkpointing.checkpoints-after-tasks-finish.enabled).We should be able to set those in the Python API.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="24232" opendate="2021-9-9 00:00:00" fixdate="2021-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Archiving of suspended jobs prevents breaks subsequent archive attempts</summary>
      <description>To archive a job we write a file that uses the job ID as the name. Since suspended jobs are handled like other terminal jobs they are also being archived.When that job then later resumes any attempt to archive the job on termination will fail because an archive already exists.The simplest option is to add a suffix if an archive already exists, like "_1".</description>
      <version>1.14.0,1.13.1,1.12.5</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.AbstractDispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="24305" opendate="2021-9-16 00:00:00" fixdate="2021-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BatchPandasUDAFITTests.test_over_window_aggregate_function fails on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24170&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=c67e71ed-6451-5d26-8920-5a8cf9651901&amp;l=23011Sep 15 20:40:43 cls = &lt;class 'pyflink.table.tests.test_pandas_udaf.BatchPandasUDAFITTests'&gt;Sep 15 20:40:43 actual = JavaObject id=o8666Sep 15 20:40:43 expected = ['+I[1, 4.3333335, 13, 5.5, 3.0, 3.0, 4.3333335, 8.0, 5.0, 5.0]', '+I[1, 4.3333335, 5, 4.3333335, 3.0, 3.0, 2.5, 4.333....0, 4.0, 2.0]', '+I[2, 2.0, 9, 2.0, 4.0, 4.0, 2.0, 2.0, 4.0, 4.0]', '+I[3, 2.0, 3, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0]']Sep 15 20:40:43 Sep 15 20:40:43 @classmethodSep 15 20:40:43 def assert_equals(cls, actual, expected):Sep 15 20:40:43 if isinstance(actual, JavaObject):Sep 15 20:40:43 actual_py_list = cls.to_py_list(actual)Sep 15 20:40:43 else:Sep 15 20:40:43 actual_py_list = actualSep 15 20:40:43 actual_py_list.sort()Sep 15 20:40:43 expected.sort()Sep 15 20:40:43 assert len(actual_py_list) == len(expected)Sep 15 20:40:43 &gt; assert all(x == y for x, y in zip(actual_py_list, expected))Sep 15 20:40:43 E AssertionError: assert FalseSep 15 20:40:43 E + where False = all(&lt;generator object PyFlinkTestCase.assert_equals.&lt;locals&gt;.&lt;genexpr&gt; at 0x7f792d98b900&gt;)</description>
      <version>1.14.0,1.12.5,1.13.2,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  <bug id="24342" opendate="2021-9-21 00:00:00" fixdate="2021-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filesystem sink does not escape right bracket in partition name</summary>
      <description>How to reproduce the problemIn the following code snippet filesystem sink creates a partition named "{date}" and writes value "1" to file.create table sink ( val int, part string) partitioned by (part) with ( 'connector' = 'filesystem', 'path' = '/tmp/sink', 'format' = 'csv');insert into sink values (1, '{date}');Expected behaviorEscaped "{" and "}" in partition name$ ls /tmp/sink/part=%7Bdate%7DActual behaviorEscaped only "{" in partition name$ ls /tmp/sink/part=%7Bdate}</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.PartitionPathUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.PartitionPathUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="24376" opendate="2021-9-26 00:00:00" fixdate="2021-1-26 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Operator name in OperatorCoordinator should not use chained name</summary>
      <description>Currently the operator name passed to CoordinatedOperatorFactory#getCoordinatorProvider is a chained operator name (e.g. Source -&gt; Map) instead of the name of coordinating operator, which might be misleading. </description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.7,1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="24431" opendate="2021-9-30 00:00:00" fixdate="2021-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Kinesis][EFO] EAGER registration strategy does not work when job fails over</summary>
      <description>BackgroundThe EFO Kinesis connector will register and de-register stream consumers based on the configured registration strategy. When EAGER is used, the client (usually job manager) will register the consumer and then the task managers will de-register the consumer when job stops/fails. If the job is configured to restart on fail, then the consumer will not exist and the job will continuously fail over.SolutionThe proposal is to not deregister the stream consumer when EAGER is used. The documentation should be updated to reflect this.</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.13.3,1.12.8,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.StreamConsumerRegistrarUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.StreamConsumerRegistrarUtil.java</file>
      <file type="M">docs.content.docs.connectors.table.kinesis.md</file>
      <file type="M">docs.content.docs.connectors.datastream.kinesis.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.kinesis.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug id="24516" opendate="2021-10-12 00:00:00" fixdate="2021-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modernize Maven Archetype</summary>
      <description>The maven archetypes used by many to start their first Flink application do not reflect the project's current state. Issues: They still bundle the DataSet API and recommend it for batch processing The JavaDoc recommends deprecated APIs </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.project-configuration.md</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.StreamingJob.scala</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.BatchJob.scala</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.java.StreamingJob.java</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.java.BatchJob.java</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.quickstarts.sh</file>
    </fixedFiles>
  </bug>
  <bug id="24971" opendate="2021-11-21 00:00:00" fixdate="2021-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming File Sink s3 end-to-end test stalled on azure</summary>
      <description>Nov 21 00:04:36 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:04:41 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:04:46 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:04:51 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:04:56 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:05:01 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:05:06 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:05:11 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:05:16 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:05:21 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:05:26 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:05:31 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:05:36 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:05:41 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:05:46 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:05:52 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:05:57 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:06:02 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:06:07 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:06:12 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:06:17 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:06:22 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:06:27 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:06:32 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:06:37 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:06:42 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:06:47 Still waiting for restarts. Expected: 1 Current: 0Nov 21 00:06:51 Test (pid: 414853) did not finish after 900 seconds.Nov 21 00:06:51 Printing Flink logs and killing it: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26784&amp;view=logs&amp;j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&amp;t=3425d8ba-5f03-540a-c64b-51b8481bf7d6&amp;l=12438</description>
      <version>1.12.5</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.ssl.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="25022" opendate="2021-11-23 00:00:00" fixdate="2021-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClassLoader leak with ThreadLocals on the JM when submitting a job through the REST API</summary>
      <description>If a job is submitted using the REST API's /jars/:jarid/run endpoint, user code has to be executed on the JobManager and it is doing this in a couple of (pooled) dispatcher threads like Flink-DispatcherRestEndpoint-thread-*.If the user code is using thread locals (and not cleaning them up), they may remain in the thread with references to the ChildFirstClassloader of the job and thus leaking that.We saw this for the jsoniter scala library at the JM which creates ThreadLocal instances but doesn't remove them, but it can actually happen with any user code or (worse) library used in user code. There are a few workarounds a user can use, e.g. putting the library in Flink's lib/ folder or submitting via the Flink CLI, but these may actually not be possible to use, depending on the circumstances. A proper fix should happen in Flink by guarding against any of these things in the dispatcher threads. We could, for example, spawn a separate thread for executing the user's main() method and once the job is submitted exit that thread and destroy all thread locals along with it.</description>
      <version>1.14.0,1.12.5,1.13.3</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebSubmissionExtension.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarMessageParameters.java</file>
    </fixedFiles>
  </bug>
  <bug id="25053" opendate="2021-11-25 00:00:00" fixdate="2021-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document how to use the usrlib to load code in the user code class loader</summary>
      <description>With FLINK-13993 we introduced the usrlib directory that can be used to load code in the user code class loader. This functionality has not been properly documented so that it is very hard to use. I would suggest to change this so that our users can benefit from this cool feature.</description>
      <version>1.14.0,1.12.5,1.13.3,1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.debugging.debugging.classloading.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.yarn.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.content.zh.docs.ops.debugging.debugging.classloading.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.yarn.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug id="25085" opendate="2021-11-29 00:00:00" fixdate="2021-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a scheduled thread pool in Endpoint and close it when the endpoint is stopped</summary>
      <description>Add a dedicated thread pool in Endpoint to schedule tasks that have a long delay such as PhysicalSlotRequestBulkCheckerImpl, heatbeat checker and some other timeout checker in JM/TM/RM. Job should shut down the thread pool and all the pending tasks will be removed when it terminates.</description>
      <version>1.14.0,1.12.5,1.13.3</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.FencedRpcEndpointTest.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.concurrent.ThrowingScheduledFuture.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.RpcEndpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.ManuallyTriggeredComponentMainThreadExecutor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.ComponentMainThreadExecutorServiceAdapter.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.RpcEndpoint.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.FencedRpcEndpoint.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.concurrent.ComponentMainThreadExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="25123" opendate="2021-12-1 00:00:00" fixdate="2021-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve expression description in SQL operator</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.ValuesTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TwoStageAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TemporalTableFunctionJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TableAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CorrelateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.ColumnFunctionsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowDeduplicateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.ValuesTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.UnionTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RelTimeIndicatorConverterTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.PartitionableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.PartitionableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MatchRecognizeTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.LegacyTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.IntervalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.FilterableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.TwoStageAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.RemoveRedundantLocalSortAggRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.RemoveRedundantLocalHashAggRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.PushLocalAggIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RemoveUnreachableCoalesceArgumentsRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RemoveSingleAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RankNumberColumnRemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushFilterInCalcIntoTableSourceRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkLogicalRankRuleForRangeEndTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkLogicalRankRuleForConstantRangeTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkCalcMergeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkAggregateRemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkAggregateExpandDistinctAggregatesRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.ExpressionReductionRulesTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.DecomposeGroupingSetsRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.CorrelateSortToRankRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.AggregateReduceGroupingRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testDistinctSplitEnabled.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ValuesJsonPlanTest.jsonplan.testValues.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testPartitionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testFilterPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testPartitioning.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testRowTimeBoundedPartitionedRowsOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupAggregateJsonPlanTest.jsonplan.tesPythonAggCallsWithGroupBy.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctWithNonDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTableWithProjectionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTable.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testCrossJoinOverrideParameters.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testComplexCalc.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.common.ViewsExpandingTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.common.PartialInsertTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.stringexpr.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.stringexpr.CorrelateStringExpressionTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.CorrelateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.FlinkRelNode.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalCorrelateBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.common.CommonPhysicalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.common.CommonPhysicalLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalCorrelateBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalIntervalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalMatch.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalWatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.connector.file.table.FileSystemTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateReduceGroupingTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.HashAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.SortAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DeadlockBreakupTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.ShuffledHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SingleRowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SortMergeJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SortMergeSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.LegacyTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.MultipleInputCreationTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.PartitionableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.PartitionableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveShuffleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.UnionTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.ValuesTest.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25160" opendate="2021-12-3 00:00:00" fixdate="2021-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make doc clear: tolerable-failed-checkpoints counts consecutive failures</summary>
      <description>According to the code, tolerable-failed-checkpoints counts the consecutive failures. We should make this clear in the doc config </description>
      <version>1.14.0,1.12.5,1.13.3</version>
      <fixedVersion>1.13.6,1.14.4,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.checkpointing.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="25161" opendate="2021-12-3 00:00:00" fixdate="2021-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update dependency for japicmp-maven-plugin</summary>
      <description>compiliation with jdk 17 fails like belowupdate of jaxb-impl to 2.3.1 helpsjava.security.PrivilegedActionException: java.lang.NoSuchMethodException: sun.misc.Unsafe.defineClass(java.lang.String,[B,int,int,java.lang.ClassLoader,java.security.ProtectionDomain) at java.base/java.security.AccessController.doPrivileged(AccessController.java:573) at com.sun.xml.bind.v2.runtime.reflect.opt.Injector.&lt;clinit&gt;(Injector.java:197) at com.sun.xml.bind.v2.runtime.reflect.opt.AccessorInjector.prepare(AccessorInjector.java:81) at com.sun.xml.bind.v2.runtime.reflect.opt.OptimizedAccessorFactory.get(OptimizedAccessorFactory.java:125) at com.sun.xml.bind.v2.runtime.reflect.Accessor$GetterSetterReflection.optimize(Accessor.java:402) at com.sun.xml.bind.v2.runtime.reflect.TransducedAccessor$CompositeTransducedAccessorImpl.&lt;init&gt;(TransducedAccessor.java:235) at com.sun.xml.bind.v2.runtime.reflect.TransducedAccessor.get(TransducedAccessor.java:175) at com.sun.xml.bind.v2.runtime.property.AttributeProperty.&lt;init&gt;(AttributeProperty.java:91) at com.sun.xml.bind.v2.runtime.property.PropertyFactory.create(PropertyFactory.java:108) at com.sun.xml.bind.v2.runtime.ClassBeanInfoImpl.&lt;init&gt;(ClassBeanInfoImpl.java:181) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.getOrCreate(JAXBContextImpl.java:514) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.&lt;init&gt;(JAXBContextImpl.java:331) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.&lt;init&gt;(JAXBContextImpl.java:139) at com.sun.xml.bind.v2.runtime.JAXBContextImpl$JAXBContextBuilder.build(JAXBContextImpl.java:1156) at com.sun.xml.bind.v2.ContextFactory.createContext(ContextFactory.java:165) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:568) at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:297) at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:286) at javax.xml.bind.ContextFinder.find(ContextFinder.java:409) at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:721) at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:662) at japicmp.output.xml.XmlOutputGenerator.createXmlDocumentAndSchema(XmlOutputGenerator.java:119) at japicmp.output.xml.XmlOutputGenerator.generate(XmlOutputGenerator.java:70) at japicmp.maven.JApiCmpMojo.generateXmlOutput(JApiCmpMojo.java:866) at japicmp.maven.JApiCmpMojo.executeWithParameters(JApiCmpMojo.java:149) at japicmp.maven.JApiCmpMojo.execute(JApiCmpMojo.java:125)</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25162" opendate="2021-12-3 00:00:00" fixdate="2021-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink : Connectors : Hive fails with VectorizedRowBatch not found</summary>
      <description>While compiling with jdk17[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:compile (default-compile) on project flink-connector-hive_2.12: Compilation failure[ERROR] flink/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveInputFormat.java:[168,17] cannot access org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch[ERROR] class file for org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch not found[ERROR] [ERROR] -&gt; [Help 1][ERROR]</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-orc-nohive.src.main.java.org.apache.flink.orc.nohive.OrcNoHiveSplitReaderUtil.java</file>
      <file type="M">flink-formats.flink-orc-nohive.src.main.java.org.apache.flink.orc.nohive.OrcNoHiveColumnarRowInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="25163" opendate="2021-12-4 00:00:00" fixdate="2021-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more options for rocksdb state backend to make configuration more flexible</summary>
      <description>Now flink has less options than the configurations what Rocksdb can set. We can see many function in the org.rocksdb.DBOptions that can influence its behavior(e.g rocksdb background threads).It make us do less when we want to do some thing to tuning Rocksdb. In my opinion, there are at least there options: maxBackgroundFlushes, it can define the background flush threads. default 1. maxBackgroundCompactions, it can define the background compaction threads. default 1. maxBackgroundJobs, it can define the background threads. default 2.setIncreaseParallelism (the most we can do for the rocksdb backend background threads) seems like can do little. It can't change the flush threads. I think it's necessary to make rocksdb configuration flexible.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBResource.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.PredefinedOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.DefaultConfigurableOptionsFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="25227" opendate="2021-12-9 00:00:00" fixdate="2021-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Comparing the equality of the same (boxed) numeric values returns false</summary>
      <description>Add the following test case to TableEnvironmentITCase to reproduce this bug.@Testdef myTest(): Unit = { val data = Seq( Row.of( java.lang.Integer.valueOf(1000), java.lang.Integer.valueOf(2000), java.lang.Integer.valueOf(1000), java.lang.Integer.valueOf(2000)) ) tEnv.executeSql( s""" |create table T ( | a int, | b int, | c int, | d int |) with ( | 'connector' = 'values', | 'bounded' = 'true', | 'data-id' = '${TestValuesTableFactory.registerData(data)}' |) |""".stripMargin) tEnv.executeSql("select greatest(a, b) = greatest(c, d) from T").print()}The result is false, which is obviously incorrect.This is caused by the generated java code:public class StreamExecCalc$8 extends org.apache.flink.table.runtime.operators.TableStreamOperator implements org.apache.flink.streaming.api.operators.OneInputStreamOperator { private final Object[] references; org.apache.flink.table.data.BoxedWrapperRowData out = new org.apache.flink.table.data.BoxedWrapperRowData(1); private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null); public StreamExecCalc$8( Object[] references, org.apache.flink.streaming.runtime.tasks.StreamTask task, org.apache.flink.streaming.api.graph.StreamConfig config, org.apache.flink.streaming.api.operators.Output output, org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception { this.references = references; this.setup(task, config, output); if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) { ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this) .setProcessingTimeService(processingTimeService); } } @Override public void open() throws Exception { super.open(); } @Override public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception { org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) element.getValue(); int field$0; boolean isNull$0; int field$1; boolean isNull$1; int field$3; boolean isNull$3; int field$4; boolean isNull$4; boolean isNull$6; boolean result$7; isNull$3 = in1.isNullAt(2); field$3 = -1; if (!isNull$3) { field$3 = in1.getInt(2); } isNull$0 = in1.isNullAt(0); field$0 = -1; if (!isNull$0) { field$0 = in1.getInt(0); } isNull$1 = in1.isNullAt(1); field$1 = -1; if (!isNull$1) { field$1 = in1.getInt(1); } isNull$4 = in1.isNullAt(3); field$4 = -1; if (!isNull$4) { field$4 = in1.getInt(3); } out.setRowKind(in1.getRowKind()); java.lang.Integer result$2 = field$0; boolean nullTerm$2 = false; if (!nullTerm$2) { java.lang.Integer cur$2 = field$0; if (isNull$0) { nullTerm$2 = true; } else { int compareResult = result$2.compareTo(cur$2); if ((true &amp;&amp; compareResult &lt; 0) || (compareResult &gt; 0 &amp;&amp; !true)) { result$2 = cur$2; } } } if (!nullTerm$2) { java.lang.Integer cur$2 = field$1; if (isNull$1) { nullTerm$2 = true; } else { int compareResult = result$2.compareTo(cur$2); if ((true &amp;&amp; compareResult &lt; 0) || (compareResult &gt; 0 &amp;&amp; !true)) { result$2 = cur$2; } } } if (nullTerm$2) { result$2 = null; } java.lang.Integer result$5 = field$3; boolean nullTerm$5 = false; if (!nullTerm$5) { java.lang.Integer cur$5 = field$3; if (isNull$3) { nullTerm$5 = true; } else { int compareResult = result$5.compareTo(cur$5); if ((true &amp;&amp; compareResult &lt; 0) || (compareResult &gt; 0 &amp;&amp; !true)) { result$5 = cur$5; } } } if (!nullTerm$5) { java.lang.Integer cur$5 = field$4; if (isNull$4) { nullTerm$5 = true; } else { int compareResult = result$5.compareTo(cur$5); if ((true &amp;&amp; compareResult &lt; 0) || (compareResult &gt; 0 &amp;&amp; !true)) { result$5 = cur$5; } } } if (nullTerm$5) { result$5 = null; } isNull$6 = nullTerm$2 || nullTerm$5; result$7 = false; if (!isNull$6) { result$7 = result$2 == result$5; } if (isNull$6) { out.setNullAt(0); } else { out.setBoolean(0, result$7); } output.collect(outElement.replace(out)); } @Override public void close() throws Exception { super.close(); }}You can see that line 137 compares two boxed Integer types with == instead of .equals, which causes this problem.In older Flink versions where the return types of cast functions are also boxed types, casting strings to numeric values are also affected by this bug.Currently for a quick fix we can rewrite the generated code. But for a long term solution we shouldn't use boxed types as internal data structures.</description>
      <version>1.14.0,1.12.5,1.13.3</version>
      <fixedVersion>1.14.5,1.15.0,1.16.0,1.13.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.GreatestLeastFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="25228" opendate="2021-12-9 00:00:00" fixdate="2021-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce flink-table-test-utils</summary>
      <description>Introduce a package to ship test utilities for formats, connectors and end users.This package should provide: Assertions for data types, logical types and internal data structures. Test cases for formats and connnectorsThe end goal is to remove the test-jar planner dependency in formats and connectors and replace it with this package, so formats and connectors can then just depend on table-planner-loader.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">flink-table.README.md</file>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.TableAssertions.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.StringDataAssert.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.RowDataAssert.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.LogicalTypeConditions.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.LogicalTypeAssert.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.DataTypeConditions.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.test.DataTypeAssert.java</file>
    </fixedFiles>
  </bug>
  <bug id="25329" opendate="2021-12-15 00:00:00" fixdate="2021-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improvement of execution graph store in flink session cluster for jobs</summary>
      <description>Flink session cluster uses files to store info of jobs after they reach termination with `FileExecutionGraphInfoStore`, each job will generate one file. When the cluster executes many small jobs concurrently, there will be many disk related operations, which will1&gt; Increase the CPU usage of `Dispatcher`2&gt; Decrease the performance of the jobs in the cluster.We hope to improve the disk operations in `FileExecutionGraphInfoStore` to increase the performance of session cluster, or support memory store.</description>
      <version>1.14.0,1.12.5,1.13.3</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.ExecutionGraphInfoStoreTestUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.SessionClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.MemoryExecutionGraphInfoStore.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.job.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.jobmanager.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="25331" opendate="2021-12-15 00:00:00" fixdate="2021-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow testcontainer tests to run on Java 17</summary>
      <description>Tests using testcontainers for Flink are currently locked to Java 8.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.container.FlinkImageBuilder.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
