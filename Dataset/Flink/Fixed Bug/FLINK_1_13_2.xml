<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="21885" opendate="2021-3-19 00:00:00" fixdate="2021-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minor Typo Fix in JDBC Connector Documentation</summary>
      <description>There currently exists a minor typo within the JDBC Datastream documentation:A JDBC batch is executed as soon as one of the following condition is true: the configured batch interval time is elapsed the maximum batch size is reached a Flink checkpoint has startedSince it's plural, condition should be changed to conditions. </description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.datastream.jdbc.md</file>
    </fixedFiles>
  </bug>
  <bug id="21887" opendate="2021-3-20 00:00:00" fixdate="2021-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add show views test in CliClientITCase</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.catalog.database.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutorBuilder.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="21888" opendate="2021-3-20 00:00:00" fixdate="2021-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Maintain our own ASTNode class</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.SelectClauseASTParser.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.IdentifiersASTParser.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserStorageFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserBaseSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveASTParseUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveASTParser.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveASTParseDriver.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveASTLexer.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveASTHintParser.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.FromClauseASTParser.g</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserContext.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserAuthorizationParseUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserASTBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.desc.HiveParserCreateViewDesc.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.desc.CreateTableASDesc.java</file>
    </fixedFiles>
  </bug>
  <bug id="22002" opendate="2021-3-29 00:00:00" fixdate="2021-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>taskmanager.slot.timeout should fall back to akka.ask.timeout</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15634&amp;view=logs&amp;j=955770d3-1fed-5a0a-3db6-0c7554c910cb&amp;t=14447d61-56b4-5000-80c1-daa459247f6a&amp;l=6424org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase2021-03-29T00:27:25.3406344Z [ERROR] testSingleAggOnTable_HashAgg_WithLocalAgg(org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase) Time elapsed: 21.908 s &lt;&lt;&lt; ERROR!2021-03-29T00:27:25.3407190Z java.lang.RuntimeException: Failed to fetch next result2021-03-29T00:27:25.3407792Z at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)2021-03-29T00:27:25.3408502Z at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)2021-03-29T00:27:25.3409188Z at org.apache.flink.table.planner.sinks.SelectTableSinkBase$RowIteratorWrapper.hasNext(SelectTableSinkBase.java:117)2021-03-29T00:27:25.3416724Z at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:350)2021-03-29T00:27:25.3417510Z at java.util.Iterator.forEachRemaining(Iterator.java:115)2021-03-29T00:27:25.3418416Z at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:108)2021-03-29T00:27:25.3419031Z at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:298)2021-03-29T00:27:25.3419657Z at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:138)2021-03-29T00:27:25.3420638Z at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:104)2021-03-29T00:27:25.3421384Z at org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase.testSingleAggOnTable(AggregateReduceGroupingITCase.scala:182)2021-03-29T00:27:25.3422284Z at org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase.testSingleAggOnTable_HashAgg_WithLocalAgg(AggregateReduceGroupingITCase.scala:135)2021-03-29T00:27:25.3422975Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2021-03-29T00:27:25.3423504Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2021-03-29T00:27:25.3424298Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2021-03-29T00:27:25.3425229Z at java.lang.reflect.Method.invoke(Method.java:498)2021-03-29T00:27:25.3426107Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)2021-03-29T00:27:25.3426756Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2021-03-29T00:27:25.3427743Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)2021-03-29T00:27:25.3428520Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2021-03-29T00:27:25.3429128Z at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2021-03-29T00:27:25.3429715Z at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2021-03-29T00:27:25.3433435Z at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)2021-03-29T00:27:25.3433977Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2021-03-29T00:27:25.3434476Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)2021-03-29T00:27:25.3435607Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)2021-03-29T00:27:25.3436460Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)2021-03-29T00:27:25.3437054Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)2021-03-29T00:27:25.3437673Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)2021-03-29T00:27:25.3438765Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)2021-03-29T00:27:25.3439362Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)2021-03-29T00:27:25.3440504Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)2021-03-29T00:27:25.3441100Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2021-03-29T00:27:25.3441673Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2021-03-29T00:27:25.3442205Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2021-03-29T00:27:25.3442710Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2021-03-29T00:27:25.3443420Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)2021-03-29T00:27:25.3444095Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)2021-03-29T00:27:25.3444749Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2021-03-29T00:27:25.3445380Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)2021-03-29T00:27:25.3446224Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2021-03-29T00:27:25.3447054Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2021-03-29T00:27:25.3447698Z at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2021-03-29T00:27:25.3448295Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)2021-03-29T00:27:25.3448851Z Caused by: java.io.IOException: Failed to fetch job execution result2021-03-29T00:27:25.3449667Z at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:169)2021-03-29T00:27:25.3450406Z at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:118)2021-03-29T00:27:25.3451138Z at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)2021-03-29T00:27:25.3451674Z ... 42 more2021-03-29T00:27:25.3452201Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.2021-03-29T00:27:25.3452872Z at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)2021-03-29T00:27:25.3453864Z at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)2021-03-29T00:27:25.3454600Z at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:167)2021-03-29T00:27:25.3455163Z ... 44 more2021-03-29T00:27:25.3455633Z Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.2021-03-29T00:27:25.3456722Z at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)2021-03-29T00:27:25.3457420Z at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$2(MiniClusterJobClient.java:117)2021-03-29T00:27:25.3458088Z at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)2021-03-29T00:27:25.3458672Z at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)2021-03-29T00:27:25.3459380Z at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)2021-03-29T00:27:25.3460223Z at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:114)2021-03-29T00:27:25.3460987Z at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:166)2021-03-29T00:27:25.3461530Z ... 44 more2021-03-29T00:27:25.3462007Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy2021-03-29T00:27:25.3462740Z at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:118)2021-03-29T00:27:25.3463566Z at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:80)2021-03-29T00:27:25.3464342Z at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:233)2021-03-29T00:27:25.3465041Z at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:224)2021-03-29T00:27:25.3465873Z at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:215)2021-03-29T00:27:25.3466611Z at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:669)2021-03-29T00:27:25.3467405Z at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:56)2021-03-29T00:27:25.3468253Z at org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1869)2021-03-29T00:27:25.3469061Z at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1437)2021-03-29T00:27:25.3469687Z at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1377)2021-03-29T00:27:25.3470309Z at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1205)2021-03-29T00:27:25.3471109Z at org.apache.flink.runtime.executiongraph.Execution.lambda$deploy$11(Execution.java:856)2021-03-29T00:27:25.3471720Z at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)2021-03-29T00:27:25.3472333Z at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)2021-03-29T00:27:25.3472927Z at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)2021-03-29T00:27:25.3473530Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440)2021-03-29T00:27:25.3474147Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208)2021-03-29T00:27:25.3474801Z at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)2021-03-29T00:27:25.3475437Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)2021-03-29T00:27:25.3476005Z at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)2021-03-29T00:27:25.3476522Z at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)2021-03-29T00:27:25.3477047Z at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)2021-03-29T00:27:25.3477587Z at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)2021-03-29T00:27:25.3478127Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)2021-03-29T00:27:25.3478663Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)2021-03-29T00:27:25.3479199Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)2021-03-29T00:27:25.3479964Z at akka.actor.Actor$class.aroundReceive(Actor.scala:517)2021-03-29T00:27:25.3481778Z at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)2021-03-29T00:27:25.3482443Z at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)2021-03-29T00:27:25.3483152Z at akka.actor.ActorCell.invoke(ActorCell.scala:561)2021-03-29T00:27:25.3483668Z at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)2021-03-29T00:27:25.3484339Z at akka.dispatch.Mailbox.run(Mailbox.scala:225)2021-03-29T00:27:25.3484999Z at akka.dispatch.Mailbox.exec(Mailbox.scala:235)2021-03-29T00:27:25.3485922Z at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)2021-03-29T00:27:25.3486533Z at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)2021-03-29T00:27:25.3487139Z at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)2021-03-29T00:27:25.3487750Z at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)2021-03-29T00:27:25.3488903Z Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Invocation of public abstract java.util.concurrent.CompletableFuture org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.submitTask(org.apache.flink.runtime.deployment.TaskDeploymentDescriptor,org.apache.flink.runtime.jobmaster.JobMasterId,org.apache.flink.api.common.time.Time) timed out.2021-03-29T00:27:25.3490086Z at java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:326)2021-03-29T00:27:25.3490729Z at java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:338)2021-03-29T00:27:25.3491369Z at java.util.concurrent.CompletableFuture.uniRelay(CompletableFuture.java:925)2021-03-29T00:27:25.3492002Z at java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:913)2021-03-29T00:27:25.3492646Z at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)2021-03-29T00:27:25.3493299Z at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)2021-03-29T00:27:25.3494129Z at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:234)2021-03-29T00:27:25.3494833Z at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)2021-03-29T00:27:25.3495506Z at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)2021-03-29T00:27:25.3496159Z at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)2021-03-29T00:27:25.3496816Z at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)2021-03-29T00:27:25.3497477Z at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1044)2021-03-29T00:27:25.3498061Z at akka.dispatch.OnComplete.internal(Future.scala:263)2021-03-29T00:27:25.3498569Z at akka.dispatch.OnComplete.internal(Future.scala:261)2021-03-29T00:27:25.3499094Z at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)2021-03-29T00:27:25.3499642Z at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)2021-03-29T00:27:25.3500344Z at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)2021-03-29T00:27:25.3501155Z at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)2021-03-29T00:27:25.3502325Z at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)2021-03-29T00:27:25.3503122Z at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)2021-03-29T00:27:25.3503739Z at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:644)2021-03-29T00:27:25.3504306Z at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)2021-03-29T00:27:25.3504894Z at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)2021-03-29T00:27:25.3505528Z at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)2021-03-29T00:27:25.3506142Z at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)2021-03-29T00:27:25.3506803Z at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)2021-03-29T00:27:25.3507875Z at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)2021-03-29T00:27:25.3509196Z at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)2021-03-29T00:27:25.3510052Z at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)2021-03-29T00:27:25.3510597Z at java.lang.Thread.run(Thread.java:748)2021-03-29T00:27:25.3511727Z Caused by: java.util.concurrent.TimeoutException: Invocation of public abstract java.util.concurrent.CompletableFuture org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.submitTask(org.apache.flink.runtime.deployment.TaskDeploymentDescriptor,org.apache.flink.runtime.jobmaster.JobMasterId,org.apache.flink.api.common.time.Time) timed out.2021-03-29T00:27:25.3512850Z at org.apache.flink.runtime.jobmaster.RpcTaskManagerGateway.submitTask(RpcTaskManagerGateway.java:68)2021-03-29T00:27:25.3513557Z at org.apache.flink.runtime.executiongraph.Execution.lambda$deploy$10(Execution.java:832)2021-03-29T00:27:25.3514225Z at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)2021-03-29T00:27:25.3514854Z at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)2021-03-29T00:27:25.3515424Z at java.util.concurrent.FutureTask.run(FutureTask.java:266)2021-03-29T00:27:25.3516090Z at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)2021-03-29T00:27:25.3516881Z at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)2021-03-29T00:27:25.3517585Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2021-03-29T00:27:25.3518218Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2021-03-29T00:27:25.3518794Z ... 1 more2021-03-29T00:27:25.3547493Z Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/taskmanager_797#796750252]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.2021-03-29T00:27:25.3548645Z at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)2021-03-29T00:27:25.3549245Z at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)2021-03-29T00:27:25.3550046Z at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)2021-03-29T00:27:25.3550605Z at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)2021-03-29T00:27:25.3551179Z at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)2021-03-29T00:27:25.3551798Z at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)2021-03-29T00:27:25.3552404Z at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)2021-03-29T00:27:25.3553050Z at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)2021-03-29T00:27:25.3554300Z at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)2021-03-29T00:27:25.3555232Z at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)2021-03-29T00:27:25.3555928Z at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)2021-03-29T00:27:25.3556425Z ... 1 more</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.task.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.taskmanager.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="22643" opendate="2021-5-12 00:00:00" fixdate="2021-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Too many TCP connections among TaskManagers for large scale jobs</summary>
      <description>For the large scale jobs, there will be too many TCP connections among TaskManagers. Let's take an example.For a streaming job with 20 JobVertices, each JobVertex has 500 parallelism. We divide the vertices into 5 slot sharing groups. Each TaskManager has 5 slots. Thus there will be 400 taskmanagers in this job. Let's assume that job runs on a cluster with 20 machines.If all the job edges are all-to-all edges, there will be 19 * 20 * 399 * 2 = 303,240 TCP connections for each machine. If we run several jobs on this cluster, the TCP connections may exceed the maximum limit of linux, which is 1,048,576. This will stop the TaskManagers from creating new TCP connections and cause task failovers.As we run our production jobs on a K8S cluster, the job always failover due to exceptions related to network, such as Sending the partition request to 'null' failed, and etc.We think that we can decrease the number of connections by letting tasks reuse the same connection. We implemented a POC that makes all tasks on the same TaskManager reuse one TCP connection. For the example job we mentioned above, the number of connections will decrease from 303,240 to 15960. With the POC, the frequency of meeting exceptions related to network in our production jobs drops significantly.The POC is illustrated in: https://github.com/wsry/flink/commit/bf1c09e80450f40d018a1d1d4fe3dfd2de777fdc </description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyConnectionManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NettyShuffleEnvironmentBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.NettyShuffleEnvironmentConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyConnectionManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NettyShuffleServiceFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.NettyShuffleEnvironmentOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.netty.shuffle.environment.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.taskmanager.network.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="22821" opendate="2021-6-1 00:00:00" fixdate="2021-9-1 01:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>FlinkKafkaProducerMigrationTest fails with "Address already in use"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18469&amp;view=logs&amp;j=c5f0071e-1851-543e-9a45-9ac140befc32&amp;t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&amp;l=6832Jun 01 01:27:33 java.net.BindException: Address already in useJun 01 01:27:33 at sun.nio.ch.Net.bind0(Native Method)Jun 01 01:27:33 at sun.nio.ch.Net.bind(Net.java:461)Jun 01 01:27:33 at sun.nio.ch.Net.bind(Net.java:453)Jun 01 01:27:33 at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:222)Jun 01 01:27:33 at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:85)Jun 01 01:27:33 at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:78)Jun 01 01:27:33 at org.apache.zookeeper.server.NIOServerCnxnFactory.configure(NIOServerCnxnFactory.java:90)Jun 01 01:27:33 at org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:120)Jun 01 01:27:33 at org.apache.curator.test.TestingZooKeeperMain.runFromConfig(TestingZooKeeperMain.java:93)Jun 01 01:27:33 at org.apache.curator.test.TestingZooKeeperServer$1.run(TestingZooKeeperServer.java:148)Jun 01 01:27:33 at java.lang.Thread.run(Thread.java:748)</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.IPv6HostnamesITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorSubmissionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyTestUtil.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClientTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyConnectionManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.ClientTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonEnvUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.NetUtils.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ClientTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="22822" opendate="2021-6-1 00:00:00" fixdate="2021-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop usages of legacy planner in JDBC module</summary>
      <description>Remove references to flink-table-planner in the JDBC module.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22847" opendate="2021-6-2 00:00:00" fixdate="2021-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SET should print options quoted</summary>
      <description>In FLINK-22770 we exposed SET/RESET in the parser and introduced quoting the options when using SET, but kept the unquoted support in SQL client for now as well.To facilitate the quoted syntax for a future deprecation of the unquoted one, SET; should print the current options using quotes.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.config.YamlConfigUtilsTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.YamlConfigUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.misc.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql-client-help-command.out</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
    </fixedFiles>
  </bug>
  <bug id="22871" opendate="2021-6-4 00:00:00" fixdate="2021-8-4 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support to execute PyFlink jobs in YARN application mode</summary>
      <description>for now pyflink is not support hadoop yarn application mode, cause of yarn nodemanager may not have suitable python versionafter test for use venv(python virtual environment) that uploaded by 'python.files' properties, then change 'env.pythonExec' path, it also worksso,is there any possiable to support this in a suitable way  </description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptionsParser.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.python.PythonEnvUtilsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonDependencyUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonEnvUtils.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptionsUtils.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">docs.layouts.shortcodes.generated.python.configuration.html</file>
      <file type="M">docs.content.docs.deployment.cli.md</file>
      <file type="M">docs.content.zh.docs.deployment.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="2305" opendate="2015-7-1 00:00:00" fixdate="2015-7-1 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add documenation about Storm compatibility layer</summary>
      <description>Storm compatibility layer is currently no documented at the project web site.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.util.StormBoltFileSink.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.util.SimpleOutputFormatter.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.excamation.ExclamationTopology.java</file>
      <file type="M">docs..includes.navbar.html</file>
    </fixedFiles>
  </bug>
  <bug id="23073" opendate="2021-6-21 00:00:00" fixdate="2021-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix space handling in Row CSV timestamp parser</summary>
      <description>FLINK-21947 Added support for TIMESTAMP_LTZ in the CSV format by replacing java.sql.Timestamp.valueOf with java.time.LocalDateTime.parse. Timestamp.valueOf internally calls `trim()` on the string before parsing while LocalDateTime.parse does not. This caused a breaking change where the CSV format can no longer parse timestamps of CSV's with spaces after the delimiter. We should manually re-add the call to trim to revert the behavior.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvRowDeSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvToRowDataConverters.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowDeserializationSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="23223" opendate="2021-7-2 00:00:00" fixdate="2021-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When flushAlways is enabled the subpartition may lose notification of data availability</summary>
      <description>When the flushAways is enabled (namely set buffer timeout to 0), there might be cases like: The subpartition emit an event which blocks the channel The subpartition produce more records. However, this records would not be notified since isBlocked = true. When the downstream tasks resume the subpartition later, the subpartition would only mark isBlocked to false. For local input channels although it tries to add the channel if isAvailable = true, but this check would not pass since flushRequest = false. One case for this issue is https://issues.apache.org/jira/browse/FLINK-22085 which uses LocalInputChannel.</description>
      <version>1.11.3,1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionWithReadViewTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannelTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="23460" opendate="2021-7-21 00:00:00" fixdate="2021-7-21 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add a global flag for enabling/disabling final checkpoints</summary>
      <description>We should have a feature toggle for the final checkpoint story.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TestSubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MockSubtaskCheckpointCoordinatorBuilder.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.FailoverStrategyCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorMasterHooksTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.CheckpointCoordinatorConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskFinalCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskChainedSourcesCheckpointingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.StreamTaskNetworkInputTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.UnalignedCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.UnalignedCheckpointsCancellationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.TestBarrierHandlerFactory.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGateTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierTrackerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlternatingCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlignedCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlignedCheckpointsMassiveRandomTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.InputProcessorUtil.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierTracker.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ExecutionOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="23462" opendate="2021-7-21 00:00:00" fixdate="2021-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate the abfs documentation to chinese</summary>
      <description>Translate the documentation changes that were made in this PR to chinese https://github.com/apache/flink/pull/16559/</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.deployment.filesystems.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.filesystems.azure.md</file>
    </fixedFiles>
  </bug>
  <bug id="23649" opendate="2021-8-5 00:00:00" fixdate="2021-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add RocksDB packages to parent-first classloading patterns.</summary>
      <description>RocksDB classes are currently loaded child-first.Because of that, it can happen that the RocksDB library is attempted to be loaded multiple times (by different classloaders).That is prevented by JNI and results in an error as reported in this mail for examplehttps://lists.apache.org/x/thread.html/rbc3ca24efe13b25e802af9739a6877276503363ffbdc5914ffdad7be@%3Cuser.flink.apache.org%3EWe should prevent accidental repeated loading of RocksDB, because we rely on the fact that only one DB is created per task.</description>
      <version>1.13.2</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.class.loading.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.core.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="23686" opendate="2021-8-9 00:00:00" fixdate="2021-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaSource metric "commitsSucceeded" should count per-commit instead of per-partition</summary>
      <description>Currently if a successful offset commit includes multiple topic partition (let's say 4), the counter will increase by 4 instead of 1</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.reader.KafkaSourceReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.metrics.KafkaSourceReaderMetricsTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.reader.KafkaSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.metrics.KafkaSourceReaderMetrics.java</file>
    </fixedFiles>
  </bug>
  <bug id="23789" opendate="2021-8-16 00:00:00" fixdate="2021-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary setTotalOrderForSeek for Rocks iterator</summary>
      <description>From FLINK-17800, we have to explicitly add setTotalOrderForSeek to ensure data correctness if user choose to set OptimizeForPointLookup.However, this is unnecessary since we upgraded the RocksDB version (please refer to rocksDB PR) which removes the prefix extractor and hash-based indexing.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateMisuseOptionTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBResource.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBIncrementalCheckpointUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="23818" opendate="2021-8-16 00:00:00" fixdate="2021-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about tgz files support for python archives</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">docs.layouts.shortcodes.generated.python.configuration.html</file>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.docs.dev.python.dependency.management.md</file>
      <file type="M">docs.content.docs.deployment.cli.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.python.dependency.management.md</file>
      <file type="M">docs.content.zh.docs.deployment.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="23842" opendate="2021-8-17 00:00:00" fixdate="2021-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add log messages for reader registrations and split requests.</summary>
      <description>Currently, there are is nothing logged when source enumerators get reader registration events, or when they receive split requests.While some specific source implementations log this in their implementation, for the general case, this information is missing, even though it is super valuable when debugging and understanding the work assignment behavior.</description>
      <version>None</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="23843" opendate="2021-8-17 00:00:00" fixdate="2021-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exceptions during "SplitEnumeratorContext.runInCoordinatorThread()" should cause Global Failure instead of Process Kill</summary>
      <description>Currently, when a the method "SplitEnumeratorContext.runInCoordinatorThread()" throws an exception, the effect is a process kill of the JobManager process.The chain how the process kill happens is: An exception bubbling up in the executor, killing the executor thread The executor starts a replacement thread, which is forbidden by the thread factory (as a safety net) and causes a process kill.We should prevent such exceptions from bubbling up in the coordinator executor.</description>
      <version>1.13.2,1.14.4,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0,1.13.7</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorContextTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.MockOperatorCoordinatorContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="23845" opendate="2021-8-18 00:00:00" fixdate="2021-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clarify metric deletion guarantees for Prometheus PushGateway reporter on shutdown</summary>
      <description>see https://issues.apache.org/jira/browse/FLINK-20691 .  whatever the problem has always existed, we should avoid other guys met it</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.main.java.org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporterOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.prometheus.push.gateway.reporter.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="23868" opendate="2021-8-19 00:00:00" fixdate="2021-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JobExecutionResult printed even if suppressSysout is on</summary>
      <description>Environments prints job execution results to stdout by default and provides a flag `suppressSysout` to disable the behavior. This flag is useful when submitting jobs through REST API or other programmatic approaches. However, JobExecutionResult is still printed when this flag is on, which looks like a bug to me.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.StreamContextEnvironment.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ContextEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="2387" opendate="2015-7-21 00:00:00" fixdate="2015-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add test for live accumulators in Streaming</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.accumulators.AccumulatorLiveITCase.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.functions.source.FromElementsFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="23871" opendate="2021-8-19 00:00:00" fixdate="2021-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dispatcher should handle finishing job exception when recover</summary>
      <description>The exception during run recovery job will trigger fatal error which is introduced in https://issues.apache.org/jira/browse/FLINK-9097.  If a job have reached a finished status. But crash at clean up phase or any other post phase. When recover job, it may recover a job in RunningJobsRegistry.JobSchedulingStatus.DONE status, this may lead to the dispatcher fatal again. I think we should deal with the  RunningJobsRegistry.JobSchedulingStatus.DONE with special exception like JobFinishingException, which represents the job/master crashed in job finishing phase. And only do the clean up work for this exception</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.JobMasterBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcessTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.OnCompletionActions.java</file>
    </fixedFiles>
  </bug>
  <bug id="23906" opendate="2021-8-21 00:00:00" fixdate="2021-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase akka.ask.timeout for tests using the MiniCluster</summary>
      <description>We have seen over the last couple of weeks/months an increased number of test failures because of TimeoutException that were triggered because the akka.ask.timeout was exceeded. The reason for this was that on our CI infrastructure it can happen that there are pauses of more than 10s (not sure about the exact reason) or our infrastructure simply being slow. In order to harden all tests relying on the MiniCluster I propose to increase the akka.ask.timeout to 5 minutes if nothing else has been configured.</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="23907" opendate="2021-8-21 00:00:00" fixdate="2021-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Type Migration: introducing primitive functional interfaces</summary>
      <description>Hey!We are a collaborative group of researchers from JetBrains Research and Oregon State University, and we are testing our data-driven plugin, which is based on the IntelliJ's Type Migration framework and adjusts it using custom structural-replace templates that express the adaptations required to perform the type changes.I want to apply several type changes using it and open the PR, thus introducing primitive functional interfaces in order to prevent unnecessary boxing (like BooleanSupplier instead Supplier&lt;Boolean&gt;, OptionalInt instead of Optional&lt;Integer&gt;, Predicate&lt;T&gt; instead of Function&lt;T, Boolean&gt;, etc.), since it can affect the performance of the code (Effective Java, Items 44, 61).The patch itself is already prepared (because it is done automatically using the plugin), so I guess I will need to open this ticket, receive your approval, and then open the PR?It would help us a lot to evaluate the usefulness of our approach!Thank you in advance!</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliInputView.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.memory.SharedResourcesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDeciderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.memory.SharedResources.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.memory.MemoryManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.DataSetMetaInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.ChannelStatePersister.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDecider.java</file>
    </fixedFiles>
  </bug>
  <bug id="23909" opendate="2021-8-22 00:00:00" fixdate="2021-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove redundant variables and improve some style format in coding.</summary>
      <description></description>
      <version>1.13.2</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-scala-shell.src.main.java.org.apache.flink.api.java.JarHelper.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.xa.JdbcExactlyOnceSinkE2eTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcDataTypeTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalogTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="23912" opendate="2021-8-23 00:00:00" fixdate="2021-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary "Clearing resource requirements of job"</summary>
      <description>The ResourceManager will print the log each time it receives an empty resource declaration. For deduplication, we need to: At JM side, skip decreasing empty resources. At SlotManager side, does not log if it is already empty.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.ResourceTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="23929" opendate="2021-8-24 00:00:00" fixdate="2021-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Chaining optimization doesn&amp;#39;t handle properly for transformations with multiple outputs</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.chain.PythonOperatorChainingOptimizerTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.chain.PythonOperatorChainingOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="23950" opendate="2021-8-24 00:00:00" fixdate="2021-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert FLINK-23738 (i.e. unhide config, API, docs)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironmentComplexConfigurationTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogStateBackendLoadingTest.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.FsStateChangelogOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CheckpointingOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.common.state.backends.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.checkpointing.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="23962" opendate="2021-8-25 00:00:00" fixdate="2021-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UpdateKind trait is not propagated properly in changeLog inference for DAG optimizing</summary>
      <description>For sql jobs with multi-sinks, the plan is divided into relNode blocks, changeLog mode should be also inferred among blocks. Currently, updateKind trait is not propagated properly from parent block to child blocks for the following pattern.                              -&gt; block3 block0 -&gt; block1 -&gt; block4             -&gt; block2 In the above example, if block3 requires UB and block2, block4 do not require UB, block1 only contains Calc node.For Agg in block0, UB should be emitted, but the updateKind for block0 is inferred as ONLY_UPDATE_AFTER.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.scala</file>
    </fixedFiles>
  </bug>
  <bug id="23965" opendate="2021-8-25 00:00:00" fixdate="2021-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>E2E do not execute locally on MacOS</summary>
      <description>After FLINK-21346, the e2e tests are no longer executing locally on MacOS. The problem seems to be that the e2e configure a log directory that does not exist and this fails starting a Flink cluster.I suggest to change the directory to the old directory FLINK_DIR/log instead of FLINK_DIR/logs.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-single-test.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="23969" opendate="2021-8-25 00:00:00" fixdate="2021-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test Pulsar source end 2 end</summary>
      <description>Write a test application using Pulsar Source and execute it in distributed fashion. Check fault-tolerance by crashing and restarting a TM.Ideally, we test different subscription modes and sticky keys in particular.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.java-ci-tools.src.main.resources.modules-skipping-deployment.modulelist</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkContainerTestEnvironment.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntimeProvider.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntimeOperator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntime.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.PulsarMockProvider.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.container.PulsarContainerProvider.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestSuiteBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestEnvironment.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarPartitionDataWriter.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.cases.SingleTopicConsumingContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.cases.MultipleTopicConsumingContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.PulsarSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.topic.TopicRange.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23991" opendate="2021-8-26 00:00:00" fixdate="2021-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Specifying yarn.staging-dir fail when staging scheme is different from default fs scheme</summary>
      <description>When the yarn.staging-dir path scheme is different from the default fs scheme, the client will fail fast.</description>
      <version>1.13.2</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnClusterDescriptorTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="23999" opendate="2021-8-26 00:00:00" fixdate="2021-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support evaluating individual window table-valued function in planner</summary>
      <description>Currently, window table-valued function has to be used with other window operation, such as window aggregate, window topN and window join. In the ticket, we aim to support evaluating individual window table-valued function in planner.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.WindowTableFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowTableFunctionTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowRankTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowDeduplicateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowRank.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowDeduplicate.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.operator.StreamOperatorNameTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalWindowTableFunctionRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.ExpandWindowTableFunctionTransposeRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalWindowTableFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.utils.WindowTableFunctionUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.physical.stream.SimplifyWindowTableFunctionRules.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowTableFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecWindowTableFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecWindowTableFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="24010" opendate="2021-8-26 00:00:00" fixdate="2021-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HybridSource needs to forward checkpoint notifications</summary>
      <description>Since the reader currently swallows notifyCheckpointComplete, offset commit in contained Kafka consumer doesn't happen.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.hybrid.HybridSourceSplitEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.hybrid.HybridSourceReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.hybrid.HybridSourceSplitEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.hybrid.HybridSourceReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="24019" opendate="2021-8-27 00:00:00" fixdate="2021-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separately package Scala-reliant modules</summary>
      <description>Bundle all Scala-reliant modules (flink-scala, flink-streaming-scala, flink-scala-shell) into a separate jar, containing Scala-exclusive dependencies (e.g., Scala itself, Scala extension of Chill). This jar will be added to lib/ by default, but can be removed by users to get a Scala-free experience.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.licenses.LICENSE.scala</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24021" opendate="2021-8-27 00:00:00" fixdate="2021-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Potential job unrecoverable due to Network failure</summary>
      <description>Now we use zk to do leader election and retrieval for HA. And we register a fatalError handler in leaderElectionService and leaderRetrievalService to let jobManager or taskManager process exit at the time of some unexpected error.But we don't do this at the time of curatorFrameworkClient#start in ZookeeperUtils. This may lead to some unexpected error like :  ZookeeperUtils start curator client, but failed by network loss, this will not throw exception now, because we do not register an error handler. The network recover when master begin do leader election, so this will success The leaderRetrieval begin to work by get_data, but this will not be executed, because the curator client start failed in phase 1. So I think we should register a error handler in phase1 , so that we can fail fast.  </description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperTestEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ZooKeeperUtilsTreeCacheTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ZooKeeperUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunnerConfigurationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.util.DocumentingDispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderRetrievalTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderRetrievalConnectionHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.ZooKeeperJobGraphStoreWatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperRegistryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZKCheckpointIDCounterMultiServersTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesClusterDescriptor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="24029" opendate="2021-8-27 00:00:00" fixdate="2021-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix "dept_id" typo in SQL "Getting Started" page</summary>
      <description>Current sql in doc of branch release-1.13 is still dep_id and we should fix it to dept_id</description>
      <version>1.13.2</version>
      <fixedVersion>1.14.0,1.13.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.gettingStarted.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.gettingStarted.md</file>
    </fixedFiles>
  </bug>
  <bug id="24031" opendate="2021-8-27 00:00:00" fixdate="2021-1-27 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>I am trying to deploy Flink in kubernetes but when I launch the taskManager in other container I get a Exception</summary>
      <description> I explain here -&gt; https://github.com/apache/flink/pull/17020I have a problem when I try to run Flink in k8s with the follow manifestsI have the following exception JobManager :2021-08-27 09:16:57,917 ERROR akka.remote.EndpointWriter [] - dropping message &amp;#91;class akka.actor.ActorSelectionMessage&amp;#93; for non-local recipient [Actor&amp;#91;akka.tcp://flink@jobmanager-hs:6123/&amp;#93;] arriving at &amp;#91;akka.tcp://flink@jobmanager-hs:6123&amp;#93; inbound addresses are &amp;#91;akka.tcp://flink@cluster:6123&amp;#93; 2021-08-27 09:17:01,255 DEBUG org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Trigger heartbeat request. 2021-08-27 09:17:01,284 DEBUG org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Trigger heartbeat request. 2021-08-27 09:17:10,008 DEBUG akka.remote.transport.netty.NettyTransport [] - Remote connection to &amp;#91;/172.17.0.1:34827&amp;#93; was disconnected because of &amp;#91;id: 0x13ae1d03, /172.17.0.1:34827 :&gt; /172.17.0.23:6123&amp;#93; DISCONNECTED 2021-08-27 09:17:10,008 DEBUG akka.remote.transport.ProtocolStateActor [] - Association between local &amp;#91;tcp://flink@cluster:6123&amp;#93; and remote &amp;#91;tcp://flink@172.17.0.1:34827&amp;#93; was disassociated because the ProtocolStateActor failed: Unknown 2021-08-27 09:17:10,009 WARN akka.remote.ReliableDeliverySupervisor [] - Association with remote system &amp;#91;akka.tcp://flink@172.17.0.24:6122&amp;#93; has failed, address is now gated for &amp;#91;50&amp;#93; ms. Reason: &amp;#91;Disassociated&amp;#93;TaskManager:INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Could not resolve ResourceManager address akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_. INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Could not resolve ResourceManager address akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_.Best regards,Julio</description>
      <version>1.13.0,1.13.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.idea.vcs.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24033" opendate="2021-8-27 00:00:00" fixdate="2021-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Propagate unique keys for fromChangelogStream</summary>
      <description>Similar to FLINK-23915, we are not propagating unique keys for fromChangelogStream because it is not written into statistics.</description>
      <version>1.13.2</version>
      <fixedVersion>1.14.0,1.13.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.DataStreamJavaITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.stats.FlinkStatistic.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSourceUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.catalog.DatabaseCalciteSchema.java</file>
      <file type="M">docs.content.docs.dev.table.data.stream.api.md</file>
      <file type="M">docs.content.zh.docs.dev.table.data.stream.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="24034" opendate="2021-8-27 00:00:00" fixdate="2021-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade commons-compress to 1.21</summary>
      <description>CVE-2021-35517</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-streaming-java.pom.xml</file>
      <file type="M">flink-formats.flink-sql-avro.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="24036" opendate="2021-8-28 00:00:00" fixdate="2021-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SSL cannot be installed on CI</summary>
      <description># install libssl1.0.0 for netty tcnativewget http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.6_amd64.debsudo apt install ./libssl1.0.0_1.0.2n-1ubuntu5.6_amd64.deb--2021-08-27 20:48:49-- http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.6_amd64.debResolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 91.189.91.38, 2001:67c:1562::15, ...Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.HTTP request sent, awaiting response... 404 Not Found2021-08-27 20:48:49 ERROR 404: Not Found.</description>
      <version>1.14.0,1.12.5,1.13.2,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="24051" opendate="2021-8-30 00:00:00" fixdate="2021-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make consumer.group-id optional for KafkaSource</summary>
      <description>For most of the users it is not necessary to generate a group-id and the source itself can provide a meaningful group-id during startup.</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.KafkaSourceBuilder.java</file>
      <file type="M">docs.content.docs.connectors.datastream.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="24120" opendate="2021-9-2 00:00:00" fixdate="2021-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document MALLOC_ARENA_MAX as workaround for glibc memory leak</summary>
      <description>My task will do a savepoint every hour, so every hour will do a savepoint. From the memory monitoring, it can be seen that the memory of each hour will soar up, although the memory will drop a little later, but from every hour From the point of view of the memory peak on the whole point, the memory continues to rise little by little, and eventually rises to the limited memory, which will lead to being killed by k8S  </description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.content.docs.deployment.memory.mem.trouble.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.trouble.md</file>
    </fixedFiles>
  </bug>
  <bug id="24148" opendate="2021-9-3 00:00:00" fixdate="2021-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add bloom filter policy option in RocksDBConfiguredOptions</summary>
      <description>Bloom filter can efficiently enhance the read on RocksDB, especially for the reading among L0 files. (more details see https://github.com/facebook/rocksdb/wiki/RocksDB-Bloom-Filter)</description>
      <version>1.13.2,1.14.1</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBConfigurableOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.DefaultConfigurableOptionsFactory.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.configurable.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="24155" opendate="2021-9-3 00:00:00" fixdate="2021-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate documentation for how to configure the CheckpointFailureManager</summary>
      <description>Documentation added in FLINK-23916 should be translated to it's Chinese counterpart. Note that this applies to three separate commits:merged to master as cd01d4c0279merged to release-1.14 as 2e769746bf2merged to release-1.13 as e1a71219454</description>
      <version>1.14.0,1.13.2,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.checkpointing.md</file>
    </fixedFiles>
  </bug>
  <bug id="24228" opendate="2021-9-9 00:00:00" fixdate="2021-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[FLIP-171] Firehose implementation of Async Sink</summary>
      <description>MotivationUser stories:As a Flink user, I’d like to use Kinesis Firehose as sink for my data pipeline.Scope: Implement an asynchronous sink for Kinesis Firehose by inheriting the AsyncSinkBase class. The implementation can for now reside in its own module in flink-connectors. The module and package name can be anything reasonable e.g. flink-connector-aws-kinesis for the module name and org.apache.flink.connector.aws.kinesis for the package name. The implementation must use the Kinesis Java Client. The implementation must allow users to configure the Kinesis Client, with reasonable default settings. Implement an asynchornous sink writer for Firehose by extending the AsyncSinkWriter. The implementation must deal with failed requests and retry them using the requeueFailedRequestEntry method. If possible, the implementation should batch multiple requests (PutRecordsRequestEntry objects) to Firehose for increased throughput. The implemented Sink Writer will be used by the Sink class that will be created as part of this story. Unit/Integration testing. Use Kinesalite (in-memory Kinesis simulation). We already use this in KinesisTableApiITCase. Java / code-level docs. End to end testing: add tests that hits a real AWS instance. (How to best donate resources to the Flink project to allow this to happen?)ReferencesMore details to be found https://cwiki.apache.org/confluence/display/FLINK/FLIP-171%3A+Async+Sink</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkElementConverterTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.examples.SinkIntoFirehose.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkWriter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkElementConverter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-aws-base.src.test.java.org.apache.flink.connector.aws.testutils.AWSServicesTestUtils.java</file>
      <file type="M">docs.content.docs.connectors.datastream.firehose.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.firehose.md</file>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.AwsV2UtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AwsV2Util.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyV2Factory.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyV2.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.sink.writer.AsyncSinkWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.test.java.org.apache.flink.connector.kinesis.util.AWSKinesisDataStreamsUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.util.AWSKinesisDataStreamsUtil.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.sink.KinesisDataStreamsSinkWriter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.config.AWSKinesisDataStreamsConfigConstants.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2423" opendate="2015-7-28 00:00:00" fixdate="2015-8-28 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Properly test checkpoint notifications</summary>
      <description>Checkpoint notifications (via the CheckpointNotifier interface) are currently not properly tested. A test should be included to verify that checkpoint notifications are eventually called on successful checkpoints, and that they are only called once per checkpointID.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StreamCheckpointNotifierITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="24248" opendate="2021-9-10 00:00:00" fixdate="2021-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-clients dependency missing in Gradle Example</summary>
      <description>The Gradle example on the "Project Configuration" page misses ``` compile "org.apache.flink:flink-clients_${scalaBinaryVersion}:${flinkVersion}"```in order to be able to run the program locally.</description>
      <version>1.13.2</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
    </fixedFiles>
  </bug>
  <bug id="24274" opendate="2021-9-13 00:00:00" fixdate="2021-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong parameter order in documentation of State Processor API</summary>
      <description>Wrong order of parameters path and stateBackend in example code of State Processor Api # modifying-savepoints  </description>
      <version>None</version>
      <fixedVersion>1.14.5,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.libs.state.processor.api.md</file>
      <file type="M">docs.content.zh.docs.libs.state.processor.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="24275" opendate="2021-9-13 00:00:00" fixdate="2021-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow idempotent job cancellation</summary>
      <description>As a user of Flink, I want to be able to cancel a job from an external system in a fault-tolerant way without guessing if the job has already been cancelled. Currently, the cancel endpoint (PATCH /jobs/:jobid?mode=cancel) will return a 404 if the job is already cancelled. This makes it hard to detect if the job truly doesn't exist, or if it is already in the desired state.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobCancellationMessageParameters.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.util.TestRestHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.MultipartUploadResource.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.AbstractHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.FileUploadHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerRequestUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.resourcemanager.AbstractResourceManagerHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.AbstractMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.AbstractAggregatingMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobSubmitHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobPlanHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobIdsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobCancellationHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.AbstractCheckpointHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.AbstractSubtaskHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.AbstractSubtaskAttemptHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.AbstractJobVertexHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.HandlerRequest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.AbstractMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.util.HandlerRequestUtilsTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlers.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarUploadHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.AbstractHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.async.AbstractAsynchronousOperationHandlersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobConfigHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobSubmitHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingMetricsHandlerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.MetricsHandlerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.savepoints.StopWithSavepointHandlersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandlerTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientSavepointTriggerTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarListHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.AbstractRestHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.async.AbstractAsynchronousOperationHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.AbstractJobManagerFileHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.AbstractAccessExecutionGraphHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="24305" opendate="2021-9-16 00:00:00" fixdate="2021-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BatchPandasUDAFITTests.test_over_window_aggregate_function fails on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24170&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=c67e71ed-6451-5d26-8920-5a8cf9651901&amp;l=23011Sep 15 20:40:43 cls = &lt;class 'pyflink.table.tests.test_pandas_udaf.BatchPandasUDAFITTests'&gt;Sep 15 20:40:43 actual = JavaObject id=o8666Sep 15 20:40:43 expected = ['+I[1, 4.3333335, 13, 5.5, 3.0, 3.0, 4.3333335, 8.0, 5.0, 5.0]', '+I[1, 4.3333335, 5, 4.3333335, 3.0, 3.0, 2.5, 4.333....0, 4.0, 2.0]', '+I[2, 2.0, 9, 2.0, 4.0, 4.0, 2.0, 2.0, 4.0, 4.0]', '+I[3, 2.0, 3, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0]']Sep 15 20:40:43 Sep 15 20:40:43 @classmethodSep 15 20:40:43 def assert_equals(cls, actual, expected):Sep 15 20:40:43 if isinstance(actual, JavaObject):Sep 15 20:40:43 actual_py_list = cls.to_py_list(actual)Sep 15 20:40:43 else:Sep 15 20:40:43 actual_py_list = actualSep 15 20:40:43 actual_py_list.sort()Sep 15 20:40:43 expected.sort()Sep 15 20:40:43 assert len(actual_py_list) == len(expected)Sep 15 20:40:43 &gt; assert all(x == y for x, y in zip(actual_py_list, expected))Sep 15 20:40:43 E AssertionError: assert FalseSep 15 20:40:43 E + where False = all(&lt;generator object PyFlinkTestCase.assert_equals.&lt;locals&gt;.&lt;genexpr&gt; at 0x7f792d98b900&gt;)</description>
      <version>1.14.0,1.12.5,1.13.2,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  <bug id="24315" opendate="2021-9-17 00:00:00" fixdate="2021-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot rebuild watcher thread while the K8S API server is unavailable</summary>
      <description>In native k8s integration, Flink will try to rebuild the watcher thread if the API server is temporarily unavailable. However, if the jitter is longer than the web socket timeout, the rebuilding of the watcher will timeout and Flink cannot handle the pod event correctly.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.13.3,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriverTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriver.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="2432" opendate="2015-7-29 00:00:00" fixdate="2015-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Provide support for custom serialization</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.main.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Environment.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Constants.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.functions.ReduceFunction.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.functions.GroupReduceFunction.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.functions.Function.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.functions.CoGroupFunction.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.connection.Iterator.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.connection.Collector.py</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.Sender.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.Receiver.java</file>
    </fixedFiles>
  </bug>
  <bug id="24334" opendate="2021-9-18 00:00:00" fixdate="2021-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configuration kubernetes.flink.log.dir not working</summary>
      <description>After FLINK-21128, kubernetes.flink.log.dir could not take effect.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.13.6,1.14.4,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.AbstractKubernetesParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.kubernetes.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="24336" opendate="2021-9-18 00:00:00" fixdate="2021-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink TableEnvironment executes the SQL randomly MalformedURLException with the configuration for &amp;#39;pipeline.classpaths&amp;#39;</summary>
      <description>When I run flink client to submit a python based workflow, I got the MalformedURLException like this:https://gist.github.com/is/faabafc7f8750f3f3161fbb6517ed6ffAfter some debug work, I found the problem is related with TableEvneriontment.execute_sql. The root cause is TableEvenriontment._add_jars_to_j_env_config in pyflink/table/TableEnverionment.py.```if j_configuration.containsKey(config_key): for url in j_configuration.getString(config_key, "").split(";"): jar_urls_set.add(url)```In our case, pipeline.classpaths was set by empty list valuefrom FromProgramOption, so the upper code block willintroduce a empty string ("") into pipeline.classpaths, for example"a.jar;b.jar;;c.jar", and it will cause the according exception.Another problem, the order of string set in python is notdeterminate, so ";".join(jar_urls_set) does NOT keep theclasspaths order. The list is more suiteable in this case.</description>
      <version>1.14.0,1.13.2,1.14.1</version>
      <fixedVersion>1.13.3,1.12.8,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="24366" opendate="2021-9-23 00:00:00" fixdate="2021-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unnecessary/misleading error message about failing restores when tasks are already canceled.</summary>
      <description>The following line is logged in all cases where the restore operation fails. The check whether the task is canceled comes only after that line.The fix would be to move the log line to after the check.Exception while restoring my-stateful-task from alternative (1/1), will retry while more alternatives are available.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.BackendRestorerProcedure.java</file>
    </fixedFiles>
  </bug>
  <bug id="24376" opendate="2021-9-26 00:00:00" fixdate="2021-1-26 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Operator name in OperatorCoordinator should not use chained name</summary>
      <description>Currently the operator name passed to CoordinatedOperatorFactory#getCoordinatorProvider is a chained operator name (e.g. Source -&gt; Map) instead of the name of coordinating operator, which might be misleading. </description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.14.7,1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="24380" opendate="2021-9-27 00:00:00" fixdate="2021-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink should handle the state transition of the pod from Pending to Failed</summary>
      <description>In K8s, there is five phases in pod's lifecycle: Pending, Running, Secceeded, Failed and Unknown. Currently, Flink does not handle the state transition of the pod from Pending to Failed. If a pod failed from Pending by `OutOfCPU` or `OutOfMem`, it will never be released and Flink keep waiting for it.To fix this issue, Flink should terminate the pod in Failed phase proactively.</description>
      <version>1.14.0,1.13.2</version>
      <fixedVersion>1.13.3,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesPod.java</file>
    </fixedFiles>
  </bug>
  <bug id="24387" opendate="2021-9-27 00:00:00" fixdate="2021-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support a JSON_STRING</summary>
      <description>We should consider adding a JSON_STRING function which can simply convert some (supported) type directly into a JSON representation, e.g. the equivalent of doingJSON_QUERY(JSON_ARRAY(x), '$.[0]')</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.SqlJsonUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.JsonGenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.JsonObjectCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.JsonArrayCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.JsonFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.strategies.SpecificInputTypeStrategies.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-api-scala.src.main.scala.org.apache.flink.table.api.ImplicitExpressionConversions.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.Expressions.java</file>
      <file type="M">flink-python.pyflink.table.expressions.py</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="24389" opendate="2021-9-28 00:00:00" fixdate="2021-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>getDescription() in `CatalogTableImpl` should check null</summary>
      <description>```@Overridepublic Optional&lt;String&gt; getDescription() {    return Optional.of(getComment());}```If the table comment is not set, then `getDescription` will throw NullPointerException https://github.com/apache/flink/blame/5b9e7882207357120717966d8bf7efd53c53ede5/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/catalog/CatalogTableImpl.java#L69 </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.CatalogTableImpTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.AbstractCatalogView.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.AbstractCatalogTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="24431" opendate="2021-9-30 00:00:00" fixdate="2021-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Kinesis][EFO] EAGER registration strategy does not work when job fails over</summary>
      <description>BackgroundThe EFO Kinesis connector will register and de-register stream consumers based on the configured registration strategy. When EAGER is used, the client (usually job manager) will register the consumer and then the task managers will de-register the consumer when job stops/fails. If the job is configured to restart on fail, then the consumer will not exist and the job will continuously fail over.SolutionThe proposal is to not deregister the stream consumer when EAGER is used. The documentation should be updated to reflect this.</description>
      <version>1.14.0,1.12.5,1.13.2</version>
      <fixedVersion>1.13.3,1.12.8,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.StreamConsumerRegistrarUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.StreamConsumerRegistrarUtil.java</file>
      <file type="M">docs.content.docs.connectors.table.kinesis.md</file>
      <file type="M">docs.content.docs.connectors.datastream.kinesis.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.kinesis.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug id="24456" opendate="2021-10-6 00:00:00" fixdate="2021-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support bounded offset in the Kafka table connector</summary>
      <description>The setBounded API in the DataStream connector of Kafka is particularly useful when writing tests. Unfortunately the table connector of Kafka lacks the same API.It would be good to have this API added.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.KafkaSourceTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaConnectorOptionsUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.KafkaSource.java</file>
      <file type="M">docs.content.docs.connectors.table.kafka.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="24516" opendate="2021-10-12 00:00:00" fixdate="2021-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modernize Maven Archetype</summary>
      <description>The maven archetypes used by many to start their first Flink application do not reflect the project's current state. Issues: They still bundle the DataSet API and recommend it for batch processing The JavaDoc recommends deprecated APIs </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.project-configuration.md</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.StreamingJob.scala</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.BatchJob.scala</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.java.StreamingJob.java</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.java.BatchJob.java</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.quickstarts.sh</file>
    </fixedFiles>
  </bug>
  <bug id="25268" opendate="2021-12-13 00:00:00" fixdate="2021-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support task manager node-label in Yarn deployment</summary>
      <description>Now Flink only support application level node label, it's necessary to introduce task manager level node-label on Yarn deployment.Why we need it?Sometimes we will implement Flink to support deep learning payload using GPU, so if having this feature, job manager and task managers could use different nodes.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerDriverTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManagerDriver.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.yarn.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="25379" opendate="2021-12-20 00:00:00" fixdate="2021-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support limit push down in DATAGEN connector.</summary>
      <description>I used datagen to generate data, and I found that the source is never ending.SET sql-client.execution.result-mode=TABLEAU;CREATE TABLE datagen (a STRING) WITH ('connector'='datagen');SELECT * FROM datagen LIMIT 10;I think it's advisable for us to push limit down in datagen source.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.table.DataGeneratorConnectorITCase.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.connector.datagen.table.DataGenTableSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="2558" opendate="2015-8-21 00:00:00" fixdate="2015-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Streaming Connector for Elasticsearch</summary>
      <description>We should add a sink that can write to Elasticsearch. A source does not seem necessary because Elasticsearch would mostly be used for accessing results, for example using a dashboard.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.pom.xml</file>
      <file type="M">docs.apis.streaming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="26482" opendate="2022-3-4 00:00:00" fixdate="2022-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support WindowedStream.reduce in Python DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.window.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">docs.content.docs.dev.datastream.operators.windows.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.operators.windows.md</file>
    </fixedFiles>
  </bug>
  <bug id="26506" opendate="2022-3-7 00:00:00" fixdate="2022-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support StreamExecutionEnvironment.registerCachedFile in Python DataStream API</summary>
      <description>This API is missed in Python DataStream API.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="27352" opendate="2022-4-22 00:00:00" fixdate="2022-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-json</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.ogg.OggJsonSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.ogg.OggJsonFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.ogg.OggJsonFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.maxwell.MaxwellJsonSerDerTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.maxwell.MaxwellJsonFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonRowSchemaConverterTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonRowDataSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonNodeDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonBatchFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.debezium.DebeziumJsonSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.debezium.DebeziumJsonFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.debezium.DebeziumJsonFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.canal.CanalJsonSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.canal.CanalJsonFormatFactoryTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="27951" opendate="2022-6-8 00:00:00" fixdate="2022-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate the "Debugging Classloading" page into Chinese</summary>
      <description>The page "Debugging Classloading" needs to be translated into Chinese.I'm willing to work on this issue.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.ops.debugging.debugging.classloading.md</file>
    </fixedFiles>
  </bug>
  <bug id="28568" opendate="2022-7-15 00:00:00" fixdate="2022-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implements a new lookup join operator (sync mode only) with state to eliminate the non determinism</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.LookupJoinHarnessTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.join.lookup.LookupJoinWithCalcRunner.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AsyncLookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.LookupJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.utils.LookupJoinUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLookupJoin.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
