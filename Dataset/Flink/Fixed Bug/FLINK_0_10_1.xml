<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="3081" opendate="2015-11-26 00:00:00" fixdate="2015-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka Periodic Offset Committer does not properly terminate on canceling</summary>
      <description>The committer is only stopped at the end of the run method. Any termination of the run method via an exception keeps the periodic committer thread running.</description>
      <version>0.10.1</version>
      <fixedVersion>0.10.2,1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3092" opendate="2015-12-1 00:00:00" fixdate="2015-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Scala API quick start word count example</summary>
      <description>The word count example in the scala api quick start uses both print and execute which results in following exception.Exception in thread "main" java.lang.RuntimeException: No new data sinks have been defined since the last execution. The last execution refers to the latest call to 'execute()', 'count()', 'collect()', or 'print()'.```</description>
      <version>0.10.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.quickstart.scala.api.quickstart.md</file>
    </fixedFiles>
  </bug>
  <bug id="30921" opendate="2023-2-6 00:00:00" fixdate="2023-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Too many CI failed due to "Could not connect to azure.archive.ubuntu.com"</summary>
      <description> https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45762&amp;view=logs&amp;j=bea52777-eaf8-5663-8482-18fbc3630e81&amp;t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45766&amp;view=logs&amp;j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&amp;t=160c9ae5-96fd-516e-1c91-deb81f59292a</description>
      <version>None</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="3115" opendate="2015-12-4 00:00:00" fixdate="2015-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Elasticsearch connector to 2.X</summary>
      <description>The Elasticsearch connector is not up to date anymore. In version 2.X the API changed. The code needs to be adapted. Probably it makes sense to have a new class ElasticsearchSink2.</description>
      <version>0.10.0,0.10.1,1.0.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.pom.xml</file>
      <file type="M">docs.apis.streaming.connectors.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="3120" opendate="2015-12-4 00:00:00" fixdate="2015-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set number of event loop thread and number of buffer pool arenas to same number</summary>
      <description>If a user changes the default number of event loop threads, the number of buffer pool arenas is still the default (2 * cores). Both numbers should be the same to prevent buffer allocation contention during runtime.</description>
      <version>0.10.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NetworkEnvironmentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyTestUtil.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyServer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyConnectionManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyConfig.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="3125" opendate="2015-12-6 00:00:00" fixdate="2015-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web dashboard does not start when log files are not found</summary>
      <description></description>
      <version>0.10.1</version>
      <fixedVersion>0.10.2,1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.web.WebFrontendITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorUtils.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="3131" opendate="2015-12-7 00:00:00" fixdate="2015-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose checkpoint metrics</summary>
      <description>Metrics about checkpoints are only accessible via the job manager logs and only show information about the completed checkpoints.The checkpointing metrics should be exposed in the web frontend, including: number duration state size</description>
      <version>0.10.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskAsyncCheckpointTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTaskStateList.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.messages.CheckpointMessagesTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.SerializedStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.MemoryHeapKvStateSnapshot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.ByteStreamStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.LocalStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KvStateSnapshot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsHeapKvStateSnapshot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FileStreamStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FileSerializableStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.AbstractFileState.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.main.java.org.apache.flink.contrib.streaming.state.LazyDbKvState.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.main.java.org.apache.flink.contrib.streaming.state.DbStateHandle.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.main.java.org.apache.flink.contrib.streaming.state.DbStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateHandle.java</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.plan.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobs.jobs.svc.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobs.jobs.ctrl.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.index.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.jade</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.RuntimeEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.checkpoint.AcknowledgeCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateForTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="3136" opendate="2015-12-7 00:00:00" fixdate="2015-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala Closure Cleaner uses wrong ASM import</summary>
      <description>The closure cleaner uses Kryo's ReflectASM's ASM. That is currently in version 4.0, which is incompatible with later Scala versions.Using ASM directly gives version 5.0.Flink also shades that ASM version correctly away in the end.</description>
      <version>0.10.1</version>
      <fixedVersion>0.10.2,1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ClosureCleaner.scala</file>
      <file type="M">flink-libraries.flink-gelly-scala.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3157" opendate="2015-12-9 00:00:00" fixdate="2015-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web frontend json files contain author attribution</summary>
      <description>http://mail-archives.apache.org/mod_mbox/jakarta-jmeter-dev/200402.mbox/%3C4039F65E.7020406@atg.com%3Eauthor tags are officially discouraged. these create difficulties inestablishing the proper ownership and the protection of ourcommitters. there are other social issues dealing with collaborativedevelopment, but the Board is concerned about the legal ramificationsaround the use of author tagsFiles: flink-runtime-web/web-dashboard/*.json</description>
      <version>0.10.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.bower.json</file>
    </fixedFiles>
  </bug>
  <bug id="3158" opendate="2015-12-10 00:00:00" fixdate="2015-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shading does not remove google guava from flink-dist fat jar</summary>
      <description>It seems that guava somehow slipped our checks and made it into the flink-dist fat jar again.</description>
      <version>0.10.1,1.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="31598" opendate="2023-3-23 00:00:00" fixdate="2023-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup usage of deprecated TableEnvironment#registerTable</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableToDataStreamITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.LegacyTableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.UnnestITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalTableFunctionJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalSortITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SplitAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SortLimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SortITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.RankITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.OverAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.MatchRecognizeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.LimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.Limit0RemoveITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.IntervalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.DeduplicateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AsyncLookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateRemoveITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.harness.OverAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.table.validation.LegacyTableSinkValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.MatchRecognizeTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.LegacySinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.LegacySinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.DagOptimizationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableImpl.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.src.test.java.org.apache.flink.connector.hbase1.HBaseConnectorITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SemiAntiJoinStreamITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="31690" opendate="2023-4-3 00:00:00" fixdate="2023-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The current key is not set for KeyedCoProcessOperator</summary>
      <description>See https://apache-flink.slack.com/archives/C03G7LJTS2G/p1680294701254239 for more details.</description>
      <version>None</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.datastream.process.operations.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="3172" opendate="2015-12-14 00:00:00" fixdate="2015-1-14 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Specify jobmanager port in HA mode</summary>
      <description>In HA mode, the job manager port is picked up randomly. In firewalled setups this can be problematic. We should add a way to use HA mode without random ports (like the web frontend currently does).</description>
      <version>0.10.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMasterBase.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.setup.jobmanager.high.availability.md</file>
    </fixedFiles>
  </bug>
  <bug id="31790" opendate="2023-4-13 00:00:00" fixdate="2023-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filesystem batch sink should also respect to the PartitionCommitPolicy</summary>
      <description>Currently, the PartitionCommitPolicy only take effect in the streaming file sink and hive file sink. The filesystem sink in batch mode should also respect to the commit policy</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.FileSystemITCaseBase.scala</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="3185" opendate="2015-12-18 00:00:00" fixdate="2015-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Silent failure during job graph recovery</summary>
      <description>If there is a failure during job graph recovery from ZooKeeper and the configured state backend, there is no way of telling what happened, because the operation happens in a future and does not log any exceptions.The user sees "Recovering all jobs" and after that no relevant log messages follow.</description>
      <version>0.10.1</version>
      <fixedVersion>0.10.2,1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.TestJvmProcess.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerCheckpointRecoveryITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="31894" opendate="2023-4-23 00:00:00" fixdate="2023-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ExceptionHistory and REST API failure label integration</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobExceptionsInfoWithHistoryNoRootTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.JobExceptionsMessageParameters.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobExceptionsInfoWithHistory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="31895" opendate="2023-4-23 00:00:00" fixdate="2023-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end integration tests for failure labels</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32131" opendate="2023-5-19 00:00:00" fixdate="2023-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support specifying hadoop config dir for Python HiveCatalog</summary>
      <description>Hadoop config directory could be specified for HiveCatalog in Java, however, this is still not supported in Python HiveCatalog. This JIRA is to align them.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.catalog.py</file>
    </fixedFiles>
  </bug>
  <bug id="32280" opendate="2023-6-7 00:00:00" fixdate="2023-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HashAgg support operator fusion codegen</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.fusion.spec.HashJoinFusionCodegenSpec.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.fusion.FusionCodegenUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ProjectionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.WindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.SortAggCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.AggWithoutKeysCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecHashAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.OperatorFusionCodegenITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.codegen.agg.AggTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.AggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.MultipleInputNodeCreationProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="3232" opendate="2016-1-13 00:00:00" fixdate="2016-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to eagerly deploy channels</summary>
      <description>Intermediate partitions are consumed via input channels. These channels are instantiated lazily when the partition producer emits the first record. This can lead to higher latencies for the first records passing the pipeline.In order to decrease this latency, we can deploy channels eagerly.I would like to add this as a flag to the intermediate results of the execution graph. On deployment, the task can then deploy the input channels as soon as the intermediate stream has been registered at the partition manager.</description>
      <version>0.10.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphDeploymentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.ResultPartitionDeploymentDescriptorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.PartialInputChannelDeploymentDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.InputChannelDeploymentDescriptor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobgraph.JobTaskVertexTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NetworkEnvironmentTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.IntermediateDataSet.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NetworkEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.IntermediateResult.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.ResultPartitionDeploymentDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="3236" opendate="2016-1-14 00:00:00" fixdate="2016-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink user code classloader should have Flink classloader as parent classloader</summary>
      <description>Right now, the user code classloader delegates to the system classloader as parent. That works in Flink standalone settings, but not when the Flink core classes themselves are not loaded with the system classloader (certain embedded setups)Giving the classloader that was used to load the Flink core classes (like for example org.apache.flink.runtime.taskmanager.Task.class.getClassLoader() solves that problem and does not break behavior in standalone mode.</description>
      <version>0.10.1</version>
      <fixedVersion>0.10.2,1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="3254" opendate="2016-1-18 00:00:00" fixdate="2016-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CombineFunction interface not respected</summary>
      <description>The DataSet API offers a CombineFunction interface, which differs from the GroupCombineFunction interface by being restricted to return a single value instead of returning arbitrary many values through a Collector.The JavaDocs of the GroupCombineFunction point to the CombineFunction interface, advertising it as more efficient.However, the CombineFunction interface is nor respected by Flink, i.e., a GroupReduceFunction that implements this interface is executed without leveraging the combine method.</description>
      <version>0.10.1,1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.GroupReduceOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="3255" opendate="2016-1-18 00:00:00" fixdate="2016-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Chaining behavior should not depend on parallelism</summary>
      <description>Currently, operators are chained more aggressively when the parallelism is one. That makes debugging tougher as it changes threading behavior.The benefits are also limited: Real installations where that type of efficiency would be needed would not run in parallelism 1, or would not use a partitioning/broadcast step there (if explicitly required to run parallelism 1).In the future, when we want to allow parallelism to be adjusted dynamically, this will be even more tricky.</description>
      <version>0.10.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="32610" opendate="2023-7-17 00:00:00" fixdate="2023-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JSON format supports projection push down</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.formats.json.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.formats.json.md</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonRowDataSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDataSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDataDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonFormatOptions.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonToRowDataConverters.java</file>
    </fixedFiles>
  </bug>
  <bug id="32613" opendate="2023-7-17 00:00:00" fixdate="2023-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable check for single rowtime attribute for sinks</summary>
      <description>A common error that users face is the following:The query contains more than one rowtime attribute column [%s] for writing into table '%s'.Please select the column that should be used as the event-time timestamp for the table sink by casting all other columns to regular TIMESTAMP or TIMESTAMP_LTZ.However, not every sink requires the rowtime set on the stream record. For example, the Kafka sink does not use it because it exposes metadata columns. The user can define which column is the rowtime column by projection.Either we introduce a config option for disabling this check. Or we come up with an interface that a connector can implement. I would vote for the config option as an easy solution to get rid of this error message.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.ExecutionConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="3267" opendate="2016-1-20 00:00:00" fixdate="2016-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable reference tracking in Kryo fallback serializer</summary>
      <description>Kryo runs extra logic to track and resolve repeated references to the same object (similar as JavaSerialization)We should disable reference tracking reference tracking is costly it is virtually always unnecessary in the datatypes used in Flink most importantly, it is inconsistent with Flink's own serialization (which does not do reference tracking) It may have problems if elements are read in a different order than they are written.</description>
      <version>0.10.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.kryo.KryoGenericTypeSerializerTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3268" opendate="2016-1-20 00:00:00" fixdate="2016-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unstable test JobManagerSubmittedJobGraphsRecoveryITCase</summary>
      <description>Logs for the failed test: https://s3.amazonaws.com/archive.travis-ci.org/jobs/103625073/log.txt</description>
      <version>0.10.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperTestEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="3271" opendate="2016-1-21 00:00:00" fixdate="2016-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Using webhdfs in a flink topology throws classnotfound exception</summary>
      <description>I was just trying to run a storm topology on flink using flink-storm. I got this exception - Caused by: java.lang.NoClassDefFoundError: org/mortbay/util/ajax/JSON at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.jsonParse(WebHdfsFileSystem.java:325) at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathResponseRunner.getResponse(WebHdfsFileSystem.java:727) at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:610) at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:458) at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:487) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:483) at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.listStatus(WebHdfsFileSystem.java:1277)My topology list some files on hdfs using webhdfs API. org.mortbay.util.ajax.JSON was included in the application uber jar. I noticed that flink loads the application jar in a child classloader. This is what most likely happened - 1. WebHdfsFileSystem class was loaded through parent class loader since it is included in flink-dist.jar.2. WebHdfsFileSystem has reference to the org.mortbay.util.ajax.JSON but since it is loaded through parent class loader, WebHdfsFileSystem can't read a class in child class loader. Ideally all the referenced classes should be available in the distribution jar so that these sort of issues may not occur.</description>
      <version>0.10.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2.pom.xml</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop1.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3275" opendate="2016-1-22 00:00:00" fixdate="2016-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Add support for Dataset.setParallelism()</summary>
      <description>Missing feature that was reported here: http://stackoverflow.com/questions/34933833/set-degree-of-parallelism-for-a-single-operation-in-python</description>
      <version>0.10.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.OperationInfo.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Environment.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.DataSet.py</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonPlanBinder.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonOperationInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="32840" opendate="2023-8-11 00:00:00" fixdate="2023-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] The executiongraph package of flink-runtime module</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.VertexSlotSharingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.TestingJobStatusProvider.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.SpeculativeExecutionVertexTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.RegionPartitionGroupReleaseStrategyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.PointwisePatternTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.IntermediateResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.FinalizeOnMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.StronglyConnectedComponentsComputeUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.SchedulingPipelinedRegionComputeUtilTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.RestartBackoffTimeStrategyFactoryLoaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.RestartAllFailoverStrategyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.partitionrelease.PipelinedRegionExecutionViewTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.partitionrelease.PartitionGroupReleaseStrategyFactoryLoaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.partitionrelease.ConsumerRegionGroupExecutionViewMaintainerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.LogicalPipelinedRegionComputeUtilTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.FixedDelayRestartBackoffTimeStrategyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.FailureRateRestartBackoffTimeStrategyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.FailoverStrategyFactoryLoaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.ExponentialDelayRestartBackoffTimeStrategyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionVertexTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionVertexDeploymentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionVertexCancelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionHistoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphSuspendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphResultPartitionAvailabilityCheckerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphRestartTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphFinishTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphCoLocationRestartTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionAttemptIDTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ErrorInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.EdgeManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.EdgeManagerBuildUtilTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.DefaultVertexAttemptNumberStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.DefaultSubtaskAttemptNumberStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraphRescalingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraphDeploymentWithBlobServerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraphDeploymentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraphConstructionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.BlockingResultPartitionReleaseTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionGraphTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.AllVerticesIteratorTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="3286" opendate="2016-1-25 00:00:00" fixdate="2016-2-25 01:00:00" resolution="Done">
    <buginformation>
      <summary>Remove JDEB Debian Package code from flink-dist</summary>
      <description>There is currently code in the flink-dist project to create a debian package for Flink. This has been added by a contributor quite a while back, and never been maintained (probably also never used).I vote to remove that. It is out of date with paths and filenames and there seems no interest in maintaining it so far.</description>
      <version>0.10.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-dist.src.deb.control.postinst</file>
      <file type="M">flink-dist.src.deb.control.control</file>
      <file type="M">flink-dist.src.deb.bin.webclient</file>
      <file type="M">flink-dist.src.deb.bin.taskmanager</file>
      <file type="M">flink-dist.src.deb.bin.jobmanager</file>
    </fixedFiles>
  </bug>
  <bug id="3306" opendate="2016-1-29 00:00:00" fixdate="2016-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Auto type registration at Kryo is buggy</summary>
      <description>The auto type registration relies on a static hash set to deduplicate class types. This means in that repeated runs certain classes will not be registered.</description>
      <version>0.10.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.kryo.SerializersTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.kryo.KryoGenericTypeSerializerTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.kryo.Serializers.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
      <file type="M">flink-batch-connectors.flink-avro.src.test.java.org.apache.flink.api.io.avro.AvroRecordInputFormatTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="33061" opendate="2023-9-7 00:00:00" fixdate="2023-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate failure-enricher documentation to Chinese</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.deployment.advanced.failure.enrichers.md</file>
      <file type="M">docs.content.docs.deployment.advanced.failure.enrichers.md</file>
    </fixedFiles>
  </bug>
  <bug id="3308" opendate="2016-2-1 00:00:00" fixdate="2016-2-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Remove debug mode</summary>
      <description>The debug mode was broken in a previous commit and so cumbersome that i used it maybe twice since i added it. Hence it should be removed.On a separate note, error reporting should be improved to make narrowing down the error cause easier.</description>
      <version>0.10.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Environment.py</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.data.PythonStreamer.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonPlanBinder.java</file>
      <file type="M">docs.apis.batch.python.md</file>
    </fixedFiles>
  </bug>
  <bug id="3309" opendate="2016-2-1 00:00:00" fixdate="2016-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Resolve maven warnings</summary>
      <description>Compiling the flink-python module generates quite a lot of maven warnings about unchecked method calls etc., these should either be resolved or suppressed.</description>
      <version>0.10.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.util.SerializationUtils.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.plan.PythonPlanSender.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.data.PythonSender.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.data.PythonReceiver.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonPlanBinder.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.functions.util.SerializerMap.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.functions.util.NestedKeyDiscarder.java</file>
    </fixedFiles>
  </bug>
  <bug id="3357" opendate="2016-2-7 00:00:00" fixdate="2016-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop JobId.toShortString()</summary>
      <description>There are several places that use JobId.toShortString() instead of the full JobID, for unique directories, etc.I think we should drop that method. If the short String representation (8 bytes) is statistically "unique enough", then it is wrong in the first place that the unique IDs have 16 bytes If 16 bytes were chosen with a purpose, then using the short string representation in paths, etc, is clearly violating the guarantees the IDs try to give,For purely informational logging, it may be a nice util, bit it seems to encourage wrong use.</description>
      <version>0.10.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionID.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.InputGateDeploymentDescriptor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptAccumulatorsHandler.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.AbstractID.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.test.java.org.apache.flink.contrib.streaming.state.DbStateBackendTest.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.main.java.org.apache.flink.contrib.streaming.state.DbStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="3373" opendate="2016-2-9 00:00:00" fixdate="2016-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Using a newer library of Apache HttpClient than 4.2.6 will get class loading problems</summary>
      <description>When I trying to use Apache HTTP client 4.5.1 in my flink job it will crash with NoClassDefFound.This has to do that it load some classes from provided httpclient 4.2.5/6 in core flink.17:05:56,193 INFO org.apache.flink.runtime.taskmanager.Task - DuplicateFilter -&gt; InstallKeyLookup (11/16) switched to FAILED with exception.java.lang.NoSuchFieldError: INSTANCE at org.apache.http.conn.ssl.SSLConnectionSocketFactory.&lt;clinit&gt;(SSLConnectionSocketFactory.java:144) at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.getDefaultRegistry(PoolingHttpClientConnectionManager.java:109) at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.&lt;init&gt;(PoolingHttpClientConnectionManager.java:116) ...&lt;internal classes&gt; at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36) at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:89) at org.apache.flink.streaming.runtime.tasks.StreamTask.openAllOperators(StreamTask.java:305) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:227) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566) at java.lang.Thread.run(Thread.java:745)SSLConnectionSocketFactory and finds an earlier version of the AllowAllHostnameVerifier that does have the INSTANCE variable (instance variable was probably added in 4.3).jar tvf lib/flink-dist-1.0-SNAPSHOT.jar |grep AllowAllHostnameVerifier 791 Thu Dec 17 09:55:46 CET 2015 org/apache/http/conn/ssl/AllowAllHostnameVerifier.classSolutions would be: Fix the classloader so that my custom job does not conflict with internal flink-core classes... pretty hard Remove the dependency somehow.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3381" opendate="2016-2-9 00:00:00" fixdate="2016-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unstable Test: JobManagerSubmittedJobGraphsRecoveryITCase</summary>
      <description>https://s3.amazonaws.com/archive.travis-ci.org/jobs/108034634/log.txt</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperTestEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="3402" opendate="2016-2-15 00:00:00" fixdate="2016-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor Common Parts of Stream/Batch Documentation</summary>
      <description>I want to build on the work of uce in refactoring the streaming guide. With the release on the horizon I think it is important to have good structure in the documentation.I propose to move the following sections from the Streaming Doc to a new Section "Basic Concepts" (name up for discussion): Linking With Flink DataStream abstraction (remove, this can be covered by an extended "Lazy Evaluation") Lazy Evaluation Specifying Keys Passing Functions to Flink Data Types Debugging Program Packaging and Distributed Execution Parallel Execution Execution Plans</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.streaming.index.md</file>
      <file type="M">docs.apis.batch.index.md</file>
      <file type="M">docs.apis.batch.fig.plan.visualizer.png</file>
    </fixedFiles>
  </bug>
  <bug id="3403" opendate="2016-2-15 00:00:00" fixdate="2016-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create Section "Working with Time" in Streaming Guide</summary>
      <description>We should add a proper page for this. Where we explain the notions of time and how to setup programs.Also, we would explain how to work with watermarks and the different timestamp extractors.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.streaming.storm.compatibility.md</file>
      <file type="M">docs.apis.streaming.libs.index.md</file>
      <file type="M">docs.apis.streaming.libs.cep.md</file>
      <file type="M">docs.apis.streaming.index.md</file>
      <file type="M">docs.apis.streaming.fault.tolerance.md</file>
      <file type="M">docs.apis.streaming.connectors.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="3478" opendate="2016-2-23 00:00:00" fixdate="2016-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink serves arbitary files through the web interface</summary>
      <description>Flink serves arbitrary files through the web server of the 8081 port, e.g. ../../../../../../../../../../etc/passwd.The requested path needs to be validated before it is served.</description>
      <version>0.10.0,0.10.1,1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitorITCase.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.files.StaticFileServerHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="3564" opendate="2016-3-2 00:00:00" fixdate="2016-3-2 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement distinct() for Table API</summary>
      <description>This is only syntactic sugar for grouping of all fields.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">docs.apis.batch.libs.table.md</file>
    </fixedFiles>
  </bug>
  <bug id="3609" opendate="2016-3-11 00:00:00" fixdate="2016-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revisit selection of Calcite rules</summary>
      <description>We should revisit the selection of Calcite rules, e.g., remove rule to reorder joins or join inputs.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.UnionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.GroupedAggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.FlinkJoinUnionTransposeRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
</bugrepository>
