<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="10009" opendate="2018-7-31 00:00:00" fixdate="2018-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the casting problem for function TIMESTAMPADD in Table</summary>
      <description>There seems to be a bug in TIMESTAMPADD function. For example, TIMESTAMPADD(MINUTE, 1, DATE '2016-06-15') throws a ClassCastException ( java.lang.Integer cannot be cast to java.lang.Long). Actually, it tries to cast an integer date to a long timestamp in RexBuilder.java:1524 - return TimestampString.fromMillisSinceEpoch((Long) o).</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
    </fixedFiles>
  </bug>
  <bug id="10016" opendate="2018-8-1 00:00:00" fixdate="2018-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make YARN/Kerberos end-to-end test stricter</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.yarn.kerberos.docker.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.config.yarn-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10020" opendate="2018-8-1 00:00:00" fixdate="2018-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kinesis Consumer listShards should support more recoverable exceptions</summary>
      <description>Currently transient errors in listShards make the consumer fail and cause the entire job to reset. That is unnecessary for certain exceptions (like status 503 errors). It should be possible to control the exceptions that qualify for retry, similar to getRecords/isRecoverableSdkClientException.</description>
      <version>None</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.ConsumerConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="10066" opendate="2018-8-6 00:00:00" fixdate="2018-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Keep only archived version of previous executions</summary>
      <description>Currently, the execution vertex stores a limited amount of previous executions in a bounded list. This happens primarily for archiving purposes and to remember previous locations and allocation ids. We remember the whole execution to eventually convert it into an archived execution.This seems unnecessary and dangerous as we have observed that this strategy is prone to memory leaks in the job manager. With a very high vertex count or parallelism, remembering complete executions can become very memory intensive. Instead I suggest to eagerly transform the executions into the archived version before adding them to the list, i.e. only the archived version is ever still referenced after the execution becomes obsolete. This gives better control over which information about the execution should really be kept in memory.</description>
      <version>1.4.3,1.5.2,1.6.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.EvictingBoundedListTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.utils.ArchivedJobGenerationUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.utils.ArchivedExecutionBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.EvictingBoundedList.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecution.java</file>
    </fixedFiles>
  </bug>
  <bug id="10068" opendate="2018-8-6 00:00:00" fixdate="2018-8-6 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add documentation for async/RocksDB-based timers</summary>
      <description>Documentation how to activate RocksDB based timers, and update that snapshotting now works async, expect for heap-timers + rocks-incremental-snapshot).</description>
      <version>None</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.state.backends.md</file>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
      <file type="M">docs.dev.stream.operators.process.function.md</file>
    </fixedFiles>
  </bug>
  <bug id="10069" opendate="2018-8-6 00:00:00" fixdate="2018-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add docs for updates SSL model</summary>
      <description>Add docs about the "internal" versus "external" connectivity and new configuration options.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.security-ssl.md</file>
    </fixedFiles>
  </bug>
  <bug id="10072" opendate="2018-8-6 00:00:00" fixdate="2018-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Syntax and consistency issues in "The Broadcast State Pattern"</summary>
      <description>There are several issues in the documentation for "The Broadcast State Pattern": Indentation mixes up whitespace and tabs, causing the markdown layout to be crippled (especially related to indentation), Broken (nested) list layout, causing multi-item lists to be rendered as single-item lists, and inconsistent list layout.</description>
      <version>1.5.2</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.state.broadcast.state.md</file>
    </fixedFiles>
  </bug>
  <bug id="10073" opendate="2018-8-6 00:00:00" fixdate="2018-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow setting a restart strategy in SQL Client</summary>
      <description>Currently, it is not possible to set a restart strategy per job.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.PropertyStrings.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Execution.java</file>
      <file type="M">flink-libraries.flink-sql-client.conf.sql-client-defaults.yaml</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="10075" opendate="2018-8-6 00:00:00" fixdate="2018-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HTTP connections to a secured REST endpoint flood the log</summary>
      <description>When connecting with a browser (or other client tool) to a secured REST endpoint, the decoder throws many exceptions indicating that the received data is not an SSL record.This massively floods the log, drowning out everything else (see below).Proposed SolutionIf a NotSslRecordException is caught, Netty should send a response HTTP 301 with a new location of https://host:port/The response would need to bypass the SSL handler because it must come in plain text.Fallback SolutionIf the proper solution cannot work, we should reduce the log level for that particular exception to TRACE.Sample Log OutputLog message that is written per each request (there are many per web UI page)2018-08-06 19:07:57,734 WARN org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint - Unhandled exceptionorg.apache.flink.shaded.netty4.io.netty.handler.codec.DecoderException: org.apache.flink.shaded.netty4.io.netty.handler.ssl.NotSslRecordException: not an SSL/TLS record: 474554202f7061727469616c732f6f766572766965772e68746d6c20485454502f312e310d0a486f73743a206c6f63616c686f73743a383038310d0a436f6e6e656374696f6e3a206b6565702d616c6976650d0a4163636570743a20746578742f68746d6c0d0a557365722d4167656e743a204d6f7a696c6c612f352e3020285831313b204c696e7578207838365f363429204170706c655765624b69742f3533372e333620284b48544d4c2c206c696b65204765636b6f29204368726f6d652f34372e302e323532362e313131205361666172692f3533372e33360d0a526566657265723a20687474703a2f2f6c6f63616c686f73743a383038312f0d0a4163636570742d456e636f64696e673a20677a69702c206465666c6174652c20736463680d0a4163636570742d4c616e67756167653a20656e2d55532c656e3b713d302e382c64653b713d302e360d0a49662d4d6f6469666965642d53696e63653a204d6f6e2c2030362041756720323031382031353a34343a313720474d540d0a0d0a at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:459) at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1434) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:965) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.lang.Thread.run(Thread.java:745)</description>
      <version>1.5.2,1.6.0,1.7.0</version>
      <fixedVersion>1.5.5,1.6.2,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.RedirectingSslHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerRedirectUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="10076" opendate="2018-8-6 00:00:00" fixdate="2018-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Calcite dependency to 1.18</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.QueryDecorrelationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.OverWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.OverWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.stringexpr.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.schema.TimeIndicatorRelDataType.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.utils.AggSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql.validate.SqlValidatorImpl.java</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10101" opendate="2018-8-8 00:00:00" fixdate="2018-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mesos web ui url is missing.</summary>
      <description>Mesos web ui url is missing in new deploy mode.</description>
      <version>1.5.0,1.5.1,1.5.2</version>
      <fixedVersion>1.5.4,1.6.1,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosSessionClusterEntrypoint.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosJobClusterEntrypoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="10142" opendate="2018-8-14 00:00:00" fixdate="2018-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce synchronization overhead for credit notifications</summary>
      <description>When credit-based flow control was introduced, we also added some checks and optimisations for uncommon code paths that make common code paths unnecessarily more expensive, e.g. checking whether a channel was released before forwarding a credit notification to Netty. Such checks would have to be confirmed by the Netty thread anyway and thus only add additional load for something that happens only once (per channel).</description>
      <version>1.5.0,1.5.1,1.5.2,1.5.3,1.6.0,1.7.0</version>
      <fixedVersion>1.5.4,1.6.1,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="1018" opendate="2014-7-10 00:00:00" fixdate="2014-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Logistic Regression deadlocks</summary>
      <description>We are currently running our implementation of logistic regression with batch gradient descent on the cluster.Unfortunatelly for datasets &gt; 1GB it seems to deadlock inside of the iteration. This means the first iteration is never finished.The iteration does a map over all points, the map gets the iteration input as broadcast variable. The result of the map is reduced and the result of the reducer (1 tuple) is crossed with the iteration input.There should be no reason for the deadlock, since the data is still quite small compared to the cluster size (4 nodes a 32GB). Also the datasize stays constant throughout the algorithm.Here is the generated plan. I will also attach the full algorithm.{ "nodes": [ { "id": 2, "type": "source", "pact": "Data Source", "contents": "[([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.", "parallelism": "1", "subtasks_per_instance": "1", "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "(unknown)" } ], "costs": [ { "name": "Network", "value": "0.0 B" }, { "name": "Disk I/O", "value": "0.0 B" }, { "name": "CPU", "value": "0.0 " }, { "name": "Cumulative Network", "value": "0.0 B" }, { "name": "Cumulative Disk I/O", "value": "0.0 B" }, { "name": "Cumulative CPU", "value": "0.0 " } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] }, { "step_function": [ { "id": 8, "type": "source", "pact": "Data Source", "contents": "TextInputFormat (hdfs://cloud-7:45010/tmp/input/higgs.M.txt) - UTF-8", "parallelism": "64", "subtasks_per_instance": "16", "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "8.0.31 GB" }, { "name": "Est. Cardinality", "value": "109.90 M" } ], "costs": [ { "name": "Network", "value": "0.0 B" }, { "name": "Disk I/O", "value": "8.0.31 GB" }, { "name": "CPU", "value": "0.0 " }, { "name": "Cumulative Network", "value": "0.0 B" }, { "name": "Cumulative Disk I/O", "value": "8.0.31 GB" }, { "name": "Cumulative CPU", "value": "0.0 " } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] }, { "id": 7, "type": "pact", "pact": "Map", "contents": "de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$6", "parallelism": "64", "subtasks_per_instance": "16", "predecessors": [ {"id": 8, "ship_strategy": "Forward"} ], "driver_strategy": "Map", "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "109.90 M" } ], "costs": [ { "name": "Network", "value": "0.0 B" }, { "name": "Disk I/O", "value": "0.0 B" }, { "name": "CPU", "value": "0.0 " }, { "name": "Cumulative Network", "value": "0.0 B" }, { "name": "Cumulative Disk I/O", "value": "8.0.31 GB" }, { "name": "Cumulative CPU", "value": "0.0 " } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] }, { "id": 11, "type": "pact", "pact": "Map", "contents": "de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$1", "parallelism": "64", "subtasks_per_instance": "16", "predecessors": [ {"id": 7, "ship_strategy": "Forward"} ], "driver_strategy": "Map", "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "109.90 M" } ], "costs": [ { "name": "Network", "value": "0.0 B" }, { "name": "Disk I/O", "value": "0.0 B" }, { "name": "CPU", "value": "0.0 " }, { "name": "Cumulative Network", "value": "0.0 B" }, { "name": "Cumulative Disk I/O", "value": "4.0.15 GB" }, { "name": "Cumulative CPU", "value": "0.0 " } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] }, { "id": 10, "type": "pact", "pact": "Reduce", "contents": "de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$2", "parallelism": "64", "subtasks_per_instance": "16", "predecessors": [ {"id": 11, "ship_strategy": "Forward"} ], "driver_strategy": "Reduce All", "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "109.90 M" } ], "costs": [ { "name": "Network", "value": "0.0 B" }, { "name": "Disk I/O", "value": "0.0 B" }, { "name": "CPU", "value": "0.0 " }, { "name": "Cumulative Network", "value": "0.0 B" }, { "name": "Cumulative Disk I/O", "value": "4.0.15 GB" }, { "name": "Cumulative CPU", "value": "0.0 " } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] }, { "id": 9, "type": "pact", "pact": "Reduce", "contents": "de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$2", "parallelism": "1", "subtasks_per_instance": "1", "predecessors": [ {"id": 10, "ship_strategy": "Redistribute"} ], "driver_strategy": "Reduce All", "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "(unknown)" } ], "costs": [ { "name": "Network", "value": "(unknown)" }, { "name": "Disk I/O", "value": "0.0 B" }, { "name": "CPU", "value": "0.0 " }, { "name": "Cumulative Network", "value": "(unknown)" }, { "name": "Cumulative Disk I/O", "value": "4.0.15 GB" }, { "name": "Cumulative CPU", "value": "0.0 " } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] }, { "id": 12, "type": "pact", "pact": "Bulk Partial Solution", "contents": "Partial Solution", "parallelism": "64", "subtasks_per_instance": "16", "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "(unknown)" } ], "costs": [ { "name": "Network", "value": "0.0 B" }, { "name": "Disk I/O", "value": "0.0 B" }, { "name": "CPU", "value": "0.0 " }, { "name": "Cumulative Network", "value": "0.0 B" }, { "name": "Cumulative Disk I/O", "value": "0.0 B" }, { "name": "Cumulative CPU", "value": "0.0 " } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] }, { "id": 6, "type": "pact", "pact": "Map", "contents": "de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$3", "parallelism": "64", "subtasks_per_instance": "16", "predecessors": [ {"id": 7, "side": "first", "ship_strategy": "Forward", "temp_mode": "CACHED"}, {"id": 9, "side": "second", "ship_strategy": "Broadcast"}, {"id": 12, "side": "second", "ship_strategy": "Broadcast"} ], "driver_strategy": "Map", "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "109.90 M" } ], "costs": [ { "name": "Network", "value": "0.0 B" }, { "name": "Disk I/O", "value": "(unknown)" }, { "name": "CPU", "value": "(unknown)" }, { "name": "Cumulative Network", "value": "(unknown)" }, { "name": "Cumulative Disk I/O", "value": "(unknown)" }, { "name": "Cumulative CPU", "value": "(unknown)" } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] }, { "id": 5, "type": "pact", "pact": "Reduce", "contents": "de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$4", "parallelism": "64", "subtasks_per_instance": "16", "predecessors": [ {"id": 6, "ship_strategy": "Forward"} ], "driver_strategy": "Reduce All", "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "109.90 M" } ], "costs": [ { "name": "Network", "value": "0.0 B" }, { "name": "Disk I/O", "value": "0.0 B" }, { "name": "CPU", "value": "0.0 " }, { "name": "Cumulative Network", "value": "(unknown)" }, { "name": "Cumulative Disk I/O", "value": "(unknown)" }, { "name": "Cumulative CPU", "value": "(unknown)" } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] }, { "id": 4, "type": "pact", "pact": "Reduce", "contents": "de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$4", "parallelism": "1", "subtasks_per_instance": "1", "predecessors": [ {"id": 5, "ship_strategy": "Redistribute"} ], "driver_strategy": "Reduce All", "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "(unknown)" } ], "costs": [ { "name": "Network", "value": "(unknown)" }, { "name": "Disk I/O", "value": "0.0 B" }, { "name": "CPU", "value": "0.0 " }, { "name": "Cumulative Network", "value": "(unknown)" }, { "name": "Cumulative Disk I/O", "value": "(unknown)" }, { "name": "Cumulative CPU", "value": "(unknown)" } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] }, { "id": 3, "type": "pact", "pact": "Cross", "contents": "de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$5", "parallelism": "64", "subtasks_per_instance": "16", "predecessors": [ {"id": 4, "side": "first", "ship_strategy": "Broadcast"}, {"id": 12, "side": "second", "ship_strategy": "Forward", "temp_mode": "PIPELINE_BREAKER"} ], "driver_strategy": "Nested Loops (Blocked Outer: de.tu_berlin.impro3.stratosphere.classification.logreg.LogisticRegression$4)", "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "(unknown)" } ], "costs": [ { "name": "Network", "value": "(unknown)" }, { "name": "Disk I/O", "value": "(unknown)" }, { "name": "CPU", "value": "(unknown)" }, { "name": "Cumulative Network", "value": "(unknown)" }, { "name": "Cumulative Disk I/O", "value": "(unknown)" }, { "name": "Cumulative CPU", "value": "(unknown)" } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] } ], "partial_solution": 12, "next_partial_solution": 3, "id": 1, "type": "bulk_iteration", "pact": "Bulk Iteration", "contents": "Bulk Iteration", "parallelism": "64", "subtasks_per_instance": "16", "predecessors": [ {"id": 2, "ship_strategy": "Redistribute"} ], "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "(unknown)" } ], "costs": [ { "name": "Network", "value": "(unknown)" }, { "name": "Disk I/O", "value": "(unknown)" }, { "name": "CPU", "value": "(unknown)" }, { "name": "Cumulative Network", "value": "(unknown)" }, { "name": "Cumulative Disk I/O", "value": "(unknown)" }, { "name": "Cumulative CPU", "value": "(unknown)" } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] }, { "id": 0, "type": "sink", "pact": "Data Sink", "contents": "TextOutputFormat (hdfs://cloud-7:45010/tmp/output/logreg) - UTF-8", "parallelism": "64", "subtasks_per_instance": "16", "predecessors": [ {"id": 1, "ship_strategy": "Forward"} ], "global_properties": [ { "name": "Partitioning", "value": "RANDOM" }, { "name": "Partitioning Order", "value": "(none)" }, { "name": "Uniqueness", "value": "not unique" } ], "local_properties": [ { "name": "Order", "value": "(none)" }, { "name": "Grouping", "value": "not grouped" }, { "name": "Uniqueness", "value": "not unique" } ], "estimates": [ { "name": "Est. Output Size", "value": "(unknown)" }, { "name": "Est. Cardinality", "value": "(unknown)" } ], "costs": [ { "name": "Network", "value": "0.0 B" }, { "name": "Disk I/O", "value": "0.0 B" }, { "name": "CPU", "value": "0.0 " }, { "name": "Cumulative Network", "value": "(unknown)" }, { "name": "Cumulative Disk I/O", "value": "(unknown)" }, { "name": "Cumulative CPU", "value": "(unknown)" } ], "compiler_hints": [ { "name": "Output Size (bytes)", "value": "(none)" }, { "name": "Output Cardinality", "value": "(none)" }, { "name": "Avg. Output Record Size (bytes)", "value": "(none)" }, { "name": "Filter Factor", "value": "(none)" } ] } ]}</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.BranchingPlansCompilerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DriverStrategy.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.PipelineBreakerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="10274" opendate="2018-8-31 00:00:00" fixdate="2018-1-31 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>The stop-cluster.sh cannot stop cluster properly when there are multiple clusters running</summary>
      <description>When you are preparing to do a Flink framework version upgrading by using the strategy shadow copy , you have to run multiple clusters concurrently,  however when you are ready to stop the old version cluster after upgrading, you would find the stop-cluster.sh wouldn't work as you expected, the following is the steps to duplicate the issue: There is already a running Flink 1.5.x cluster instance; Installing another Flink 1.6.x cluster instance at the same cluster machines; Migrating the jobs from Flink 1.5.x  to Flink 1.6.x ; go to the bin dir of the Flink 1.5.x cluster instance and run stop-cluster.sh ;You would expect the old Flink 1.5.x cluster instance be stopped ,right? Unfortunately the stopped cluster is the new installed Flink 1.6.x cluster instance instead!</description>
      <version>1.5.1,1.5.2,1.5.3,1.6.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">docs..includes.generated.environment.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="10331" opendate="2018-9-13 00:00:00" fixdate="2018-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce number of flush requests to the network stack</summary>
      <description>With the re-design of the record writer interaction with the result(sub)partitions, flush requests can currently pile up in these scenarios: a previous flush request has not been completely handled yet and/or is still enqueued or the network stack is still polling from this subpartition and doesn't need a new notificationThese lead to increased notifications in low latency settings (low output flusher intervals) which can be avoided.</description>
      <version>1.5.0,1.5.1,1.5.2,1.5.3,1.6.0,1.7.0</version>
      <fixedVersion>1.5.5,1.6.2,1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="9289" opendate="2018-5-2 00:00:00" fixdate="2018-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parallelism of generated operators should have max parallism of input</summary>
      <description>The DataSet API aims to chain generated operators such as key extraction mappers to their predecessor. This is done by assigning the same parallelism as the input operator.If a generated operator has more than two inputs, the operator cannot be chained anymore and the operator is generated with default parallelism. This can lead to a NoResourceAvailableException: Not enough free slots available to run the job. as reported by a user on the mailing list: https://lists.apache.org/thread.html/60a8bffcce54717b6273bf3de0f43f1940fbb711590f4b90cd666c9a@%3Cuser.flink.apache.org%3EI suggest to set the parallelism of a generated operator to the max parallelism of all of its inputs to fix this problem.Until the problem is fixed, a workaround is to set the default parallelism at the ExecutionEnvironment:ExecutionEnvironment env = ...env.setParallelism(2);</description>
      <version>1.4.2,1.5.2,1.6.0</version>
      <fixedVersion>1.4.3,1.5.3,1.6.1,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonPlanBinder.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.linkanalysis.PageRank.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.UnionOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.KeyFunctions.java</file>
    </fixedFiles>
  </bug>
  <bug id="9969" opendate="2018-7-26 00:00:00" fixdate="2018-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unreasonable memory requirements to complete examples/batch/WordCount</summary>
      <description>setup on AWS EMR: 5 worker nodes (m4.4xlarge nodes)  1 master node (m4.large)following command fails with out of memory errors:export HADOOP_CLASSPATH=`hadoop classpath`./bin/flink run -m yarn-cluster -p 20 -yn 5 -ys 4 -ytm 16000 examples/batch/WordCount.jarOnly increasing memory over 17.2GB example completes. At the same time after disabling flip6 following command succeeds:export HADOOP_CLASSPATH=`hadoop classpath`./bin/flink run -m yarn-cluster -p 20 -yn 5 -ys 4 -ytm 1000 examples/batch/WordCount.jar</description>
      <version>1.5.0,1.5.1,1.5.2,1.6.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.UnilateralSortMerger.java</file>
    </fixedFiles>
  </bug>
  <bug id="9972" opendate="2018-7-26 00:00:00" fixdate="2018-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Debug memory logging not working</summary>
      <description>It seems like with introduction of Flip6, debug memory logging is not being initialised anymore and following config properties are ignored:  `taskmanager.debug.memory.log` `taskmanager.debug.memory.log-interval`after disabling flip6 it works just fine.</description>
      <version>1.5.0,1.5.1,1.5.2,1.6.0</version>
      <fixedVersion>1.5.3,1.6.1,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.MemoryLogger.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
