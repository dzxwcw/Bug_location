<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="13528" opendate="2019-8-1 00:00:00" fixdate="2019-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka E2E tests fail on Java 11</summary>
      <description>The kafka E2E tests fail on Java 11 with a timeout. Since kafka added support for Java 11 in 2.1.0 we may have to just disable them.[2019-08-15 07:21:47,491] WARN Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)java.net.ConnectException: Connection refused at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779) at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)Exception in thread "main" org.I0Itec.zkclient.exception.ZkTimeoutException: Unable to connect to zookeeper server 'localhost:2181' with timeout of 30000 ms at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:1233) at org.I0Itec.zkclient.ZkClient.&lt;init&gt;(ZkClient.java:157) at org.I0Itec.zkclient.ZkClient.&lt;init&gt;(ZkClient.java:131) at kafka.utils.ZkUtils$.createZkClientAndConnection(ZkUtils.scala:79) at kafka.utils.ZkUtils$.apply(ZkUtils.scala:61) at kafka.admin.TopicCommand$.main(TopicCommand.scala:53) at kafka.admin.TopicCommand.main(TopicCommand.scala)</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-pre-commit-tests.sh</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09SecuredRunITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaShortRetention08ITCase.java</file>
      <file type="M">tools.travis.splits.split.misc.sh</file>
    </fixedFiles>
  </bug>
  <bug id="15414" opendate="2019-12-27 00:00:00" fixdate="2019-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaITCase#prepare failed in travis</summary>
      <description>The travis for release-1.9 failed with the following error:org.apache.kafka.common.KafkaException: Socket server failed to bind to 0.0.0.0:44867: Address already in use. at org.apache.flink.streaming.connectors.kafka.KafkaITCase.prepare(KafkaITCase.java:58)Caused by: java.net.BindException: Address already in use at org.apache.flink.streaming.connectors.kafka.KafkaITCase.prepare(KafkaITCase.java:58)instance: https://api.travis-ci.org/v3/job/629636116/log.txt</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="16513" opendate="2020-3-9 00:00:00" fixdate="2020-4-9 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement API to persist channel state: checkpointing metadata</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.OperatorSnapshotUtil.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.RestoreStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.LocalStateForwardingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImplTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamOperatorStateHandlerTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskStateManagerImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.messages.CheckpointMessagesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.TaskStateSnapshotTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PrioritizedOperatorSubtaskStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.metadata.CheckpointTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointMetadataLoadingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateUtil.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ResultSubpartitionStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.InputChannelStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateObjectCollection.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorSubtaskState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.metadata.MetadataV3Serializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ResultSubpartitionInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.InputChannelInfo.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.splits.KeyGroupRangeInputSplit.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.OperatorSnapshotFuturesTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizerTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.OperatorSnapshotFutures.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StateHandleDummyUtil.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorRestoringTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.RoundRobinOperatorStateRepartitioner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PrioritizedOperatorSubtaskState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorStateRepartitioner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.OperatorStateInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="16516" opendate="2020-3-10 00:00:00" fixdate="2020-3-10 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Avoid codegen user-defined function for Python UDF</summary>
      <description>Currently we make use of codegen to generate PythonScalarFunction and PythonTableFunction, but it is unnecessary. We can directly create a static PythonScalarFunction and PythonTableFunction.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.util.python.PythonTableUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.PythonFunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.utils.python.PythonTableUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.PythonFunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.python.SimplePythonFunction.java</file>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.catalog.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="16768" opendate="2020-3-25 00:00:00" fixdate="2020-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HadoopS3RecoverableWriterITCase.testRecoverWithStateWithMultiPart hangs</summary>
      <description>Logs: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6584&amp;view=logs&amp;j=d44f43ce-542c-597d-bf94-b0718c71e5e8&amp;t=d26b3528-38b0-53d2-05f7-37557c2405e42020-03-24T15:52:18.9196862Z "main" #1 prio=5 os_prio=0 tid=0x00007fd36c00b800 nid=0xc21 runnable [0x00007fd3743ce000]2020-03-24T15:52:18.9197235Z java.lang.Thread.State: RUNNABLE2020-03-24T15:52:18.9197536Z at java.net.SocketInputStream.socketRead0(Native Method)2020-03-24T15:52:18.9197931Z at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)2020-03-24T15:52:18.9198340Z at java.net.SocketInputStream.read(SocketInputStream.java:171)2020-03-24T15:52:18.9198749Z at java.net.SocketInputStream.read(SocketInputStream.java:141)2020-03-24T15:52:18.9199171Z at sun.security.ssl.InputRecord.readFully(InputRecord.java:465)2020-03-24T15:52:18.9199840Z at sun.security.ssl.InputRecord.readV3Record(InputRecord.java:593)2020-03-24T15:52:18.9200265Z at sun.security.ssl.InputRecord.read(InputRecord.java:532)2020-03-24T15:52:18.9200663Z at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975)2020-03-24T15:52:18.9201213Z - locked &lt;0x00000000927583d8&gt; (a java.lang.Object)2020-03-24T15:52:18.9201589Z at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933)2020-03-24T15:52:18.9202026Z at sun.security.ssl.AppInputStream.read(AppInputStream.java:105)2020-03-24T15:52:18.9202583Z - locked &lt;0x0000000092758c00&gt; (a sun.security.ssl.AppInputStream)2020-03-24T15:52:18.9203029Z at org.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:137)2020-03-24T15:52:18.9203558Z at org.apache.http.impl.io.SessionInputBufferImpl.read(SessionInputBufferImpl.java:198)2020-03-24T15:52:18.9204121Z at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:176)2020-03-24T15:52:18.9204626Z at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:135)2020-03-24T15:52:18.9205121Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9205679Z at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)2020-03-24T15:52:18.9206164Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9206786Z at com.amazonaws.services.s3.internal.S3AbortableInputStream.read(S3AbortableInputStream.java:125)2020-03-24T15:52:18.9207361Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9207839Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9208327Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9208809Z at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)2020-03-24T15:52:18.9209273Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9210003Z at com.amazonaws.util.LengthCheckInputStream.read(LengthCheckInputStream.java:107)2020-03-24T15:52:18.9210658Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9211154Z at org.apache.hadoop.fs.s3a.S3AInputStream.lambda$read$3(S3AInputStream.java:445)2020-03-24T15:52:18.9211631Z at org.apache.hadoop.fs.s3a.S3AInputStream$$Lambda$42/1936375962.execute(Unknown Source)2020-03-24T15:52:18.9212044Z at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)2020-03-24T15:52:18.9212553Z at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:260)2020-03-24T15:52:18.9212972Z at org.apache.hadoop.fs.s3a.Invoker$$Lambda$23/1457226878.execute(Unknown Source)2020-03-24T15:52:18.9213408Z at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:317)2020-03-24T15:52:18.9213866Z at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:256)2020-03-24T15:52:18.9214273Z at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:231)2020-03-24T15:52:18.9214701Z at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:441)2020-03-24T15:52:18.9215443Z - locked &lt;0x00000000926e88b0&gt; (a org.apache.hadoop.fs.s3a.S3AInputStream)2020-03-24T15:52:18.9215852Z at java.io.DataInputStream.read(DataInputStream.java:149)2020-03-24T15:52:18.9216305Z at org.apache.flink.runtime.fs.hdfs.HadoopDataInputStream.read(HadoopDataInputStream.java:94)2020-03-24T15:52:18.9216781Z at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)2020-03-24T15:52:18.9217187Z at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)2020-03-24T15:52:18.9217571Z at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)2020-03-24T15:52:18.9218108Z - locked &lt;0x00000000926ea000&gt; (a java.io.InputStreamReader)2020-03-24T15:52:18.9218475Z at java.io.InputStreamReader.read(InputStreamReader.java:184)2020-03-24T15:52:18.9218876Z at java.io.BufferedReader.fill(BufferedReader.java:161)2020-03-24T15:52:18.9219261Z at java.io.BufferedReader.readLine(BufferedReader.java:324)2020-03-24T15:52:18.9219890Z - locked &lt;0x00000000926ea000&gt; (a java.io.InputStreamReader)2020-03-24T15:52:18.9220256Z at java.io.BufferedReader.readLine(BufferedReader.java:389)2020-03-24T15:52:18.9220914Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getContentsOfFile(HadoopS3RecoverableWriterITCase.java:423)2020-03-24T15:52:18.9221704Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:411)2020-03-24T15:52:18.9222457Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithMultiPartUploads(HadoopS3RecoverableWriterITCase.java:364)2020-03-24T15:52:18.9223222Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverWithStateWithMultiPart(HadoopS3RecoverableWriterITCase.java:330)2020-03-24T15:52:18.9223817Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-03-24T15:52:18.9224232Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-03-24T15:52:18.9224729Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-03-24T15:52:18.9225160Z at java.lang.reflect.Method.invoke(Method.java:498)2020-03-24T15:52:18.9225675Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)2020-03-24T15:52:18.9226171Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2020-03-24T15:52:18.9226682Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)2020-03-24T15:52:18.9227187Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2020-03-24T15:52:18.9227661Z at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2020-03-24T15:52:18.9228145Z at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2020-03-24T15:52:18.9228718Z at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)2020-03-24T15:52:18.9229112Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-03-24T15:52:18.9229582Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)2020-03-24T15:52:18.9230029Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)2020-03-24T15:52:18.9230525Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)2020-03-24T15:52:18.9230963Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)2020-03-24T15:52:18.9231546Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)2020-03-24T15:52:18.9231999Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)2020-03-24T15:52:18.9232432Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)2020-03-24T15:52:18.9232862Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)2020-03-24T15:52:18.9233307Z at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2020-03-24T15:52:18.9233833Z at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2020-03-24T15:52:18.9234284Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-03-24T15:52:18.9234700Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-03-24T15:52:18.9235076Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2020-03-24T15:52:18.9235599Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)2020-03-24T15:52:18.9236124Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)2020-03-24T15:52:18.9236648Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2020-03-24T15:52:18.9237167Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)2020-03-24T15:52:18.9237688Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2020-03-24T15:52:18.9238244Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2020-03-24T15:52:18.9238745Z at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2020-03-24T15:52:18.9239202Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)2020-03-24T15:52:18.9239549Z 2020-03-24T15:52:18.9239794Z "VM Thread" os_prio=0 tid=0x00007fd36c260800 nid=0xc58 runnable  </description>
      <version>1.10.0,1.11.0,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.watchdog.sh</file>
      <file type="M">tools.ci.test.controller.sh</file>
    </fixedFiles>
  </bug>
  <bug id="16795" opendate="2020-3-26 00:00:00" fixdate="2020-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End to end tests timeout on Azure</summary>
      <description>Example: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6650&amp;view=logs&amp;j=08866332-78f7-59e4-4f7e-49a56faa3179 or https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6637&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5##[error]The job running on agent Azure Pipelines 6 ran longer than the maximum time of 200 minutes. For more information, see https://go.microsoft.com/fwlink/?linkid=2077134and ##[error]The operation was canceled.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="16796" opendate="2020-3-26 00:00:00" fixdate="2020-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix The Bug of Python UDTF in SQL Query</summary>
      <description>When executes Python UDTF in sql query, it will cause some problem.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonCorrelate.scala</file>
      <file type="M">flink-python.pyflink.table.tests.test.udtf.py</file>
    </fixedFiles>
  </bug>
  <bug id="16945" opendate="2020-4-2 00:00:00" fixdate="2020-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Execute CheckpointFailureManager.FailJobCallback directly in main thread executor</summary>
      <description>Since we have put all non-IO operations of CheckpointCoordinator into main thread executor, the CheckpointFailureManager.FailJobCallback could be executed directly now. In this way execution graph would fail immediately when CheckpointFailureManager invokes the callback. We could avoid the inconsistent scenario of FLINK-13497.</description>
      <version>1.10.0,1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ExecutionGraphCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="16946" opendate="2020-4-2 00:00:00" fixdate="2020-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update user documentation for job manager memory model</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.state.backends.zh.md</file>
      <file type="M">docs.ops.state.state.backends.md</file>
      <file type="M">docs.ops.memory.mem.tuning.zh.md</file>
      <file type="M">docs.ops.memory.mem.tuning.md</file>
      <file type="M">docs.ops.memory.mem.trouble.zh.md</file>
      <file type="M">docs.ops.memory.mem.trouble.md</file>
      <file type="M">docs.ops.memory.mem.setup.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.md</file>
      <file type="M">docs.ops.memory.mem.migration.zh.md</file>
      <file type="M">docs.ops.memory.mem.migration.md</file>
      <file type="M">docs.ops.memory.mem.detail.zh.md</file>
      <file type="M">docs.ops.memory.mem.detail.md</file>
      <file type="M">docs.ops.config.zh.md</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="16947" opendate="2020-4-2 00:00:00" fixdate="2020-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ArtifactResolutionException: Could not transfer artifact. Entry [...] has not been leased from this pool</summary>
      <description>https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6982&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5Build of flink-metrics-availability-test failed with:[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (end-to-end-tests) on project flink-metrics-availability-test: Unable to generate classpath: org.apache.maven.artifact.resolver.ArtifactResolutionException: Could not transfer artifact org.apache.maven.surefire:surefire-grouper:jar:2.22.1 from/to google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/): Entry [id:13][route:{s}-&gt;https://maven-central-eu.storage-download.googleapis.com:443][state:null] has not been leased from this pool[ERROR] org.apache.maven.surefire:surefire-grouper:jar:2.22.1[ERROR] [ERROR] from the specified remote repositories:[ERROR] google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/, releases=true, snapshots=false),[ERROR] apache.snapshots (https://repository.apache.org/snapshots, releases=false, snapshots=true)[ERROR] Path to dependency:[ERROR] 1) dummy:dummy:jar:1.0[ERROR] 2) org.apache.maven.surefire:surefire-junit47:jar:2.22.1[ERROR] 3) org.apache.maven.surefire:common-junit48:jar:2.22.1[ERROR] 4) org.apache.maven.surefire:surefire-grouper:jar:2.22.1[ERROR] -&gt; [Help 1][ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException[ERROR] [ERROR] After correcting the problems, you can resume the build with the command[ERROR] mvn &lt;goals&gt; -rf :flink-metrics-availability-test</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug id="1695" opendate="2015-3-13 00:00:00" fixdate="2015-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create machine learning library</summary>
      <description>Create the infrastructure for Flink's machine learning library. This includes the creation of the module structure and the implementation of basic types such as vectors and matrices.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1716" opendate="2015-3-18 00:00:00" fixdate="2015-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CoCoA algorithm to flink-ml</summary>
      <description>Add the communication efficient distributed dual coordinate ascent algorithm to the flink machine learning library. See CoCoA for the implementation details.I propose to first implement it with hinge loss and l2-norm. This way, it will allow us to train SVMs in parallel.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.recommendation.ALSITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.SparseVectorSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.SparseMatrixSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.regression.MultipleLinearRegression.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.recommendation.ALS.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Vector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.SparseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.SparseMatrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Matrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseMatrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.Transformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.Learner.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.LabeledVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.FlinkTools.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.ChainedTransformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.ChainedLearner.scala</file>
    </fixedFiles>
  </bug>
  <bug id="17322" opendate="2020-4-22 00:00:00" fixdate="2020-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable latency tracker would corrupt the broadcast state</summary>
      <description>This bug is reported from user mail list: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Latency-tracking-together-with-broadcast-state-can-cause-job-failure-td34013.htmlExecute BroadcastStateIT#broadcastStateWorksWithLatencyTracking would easily reproduce this problem.From current information, the broadcast element would be corrupt once we enable env.getConfig().setLatencyTrackingInterval(2000). The exception stack trace would be: (based on current master branch)Caused by: java.io.IOException: Corrupt stream, found tag: 84 at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:217) ~[classes/:?] at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:46) ~[classes/:?] at org.apache.flink.runtime.plugable.NonReusingDeserializationDelegate.read(NonReusingDeserializationDelegate.java:55) ~[classes/:?] at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.getNextRecord(SpillingAdaptiveSpanningRecordDeserializer.java:157) ~[classes/:?] at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:123) ~[classes/:?] at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processInput(StreamTwoInputProcessor.java:181) ~[classes/:?] at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:332) ~[classes/:?] at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxStep(MailboxProcessor.java:206) ~[classes/:?] at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:196) ~[classes/:?] at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:505) ~[classes/:?] at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:485) ~[classes/:?] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:720) ~[classes/:?] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:544) ~[classes/:?] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_144]</description>
      <version>1.9.3,1.10.1,1.11.0,1.12.0</version>
      <fixedVersion>1.10.2,1.11.0,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.BroadcastRecordWriterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.BufferConsumer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.BroadcastRecordWriter.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.BufferBuilderAndConsumerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.RecordWriterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.RecordWriterDelegateTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.BufferBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.RecordWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="17404" opendate="2020-4-27 00:00:00" fixdate="2020-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Running HA per-job cluster (rocks, non-incremental) gets stuck killing a non-existing pid in Hadoop 3 build profile</summary>
      <description>CI log: https://api.travis-ci.org/v3/job/678609505/log.txtWaiting for text Completed checkpoint [1-9]* for job 00000000000000000000000000000000 to appear 2 of times in logs...grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directorygrep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directoryStarting standalonejob daemon on host travis-job-e606668f-b674-49c0-8590-e3508e22b99d.grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directorygrep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directoryKilled TM @ 18864kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]Killed TM @ No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#build-times-out-because-no-output-was-receivedThe build has been terminated</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17406" opendate="2020-4-27 00:00:00" fixdate="2020-6-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add documentation about dynamic table options</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
      <file type="M">docs.dev.table.sql.index.zh.md</file>
      <file type="M">docs.dev.table.sql.index.md</file>
      <file type="M">docs.dev.table.config.zh.md</file>
      <file type="M">docs.dev.table.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="17408" opendate="2020-4-27 00:00:00" fixdate="2020-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce GPUDriver</summary>
      <description>Introduce GPUDriver for GPU resource.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17424" opendate="2020-4-28 00:00:00" fixdate="2020-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client end-to-end test (Old planner) Elasticsearch (v7.5.1) failed due to download error</summary>
      <description>`SQL Client end-to-end test (Old planner) Elasticsearch (v7.5.1)` failed in release-1.10 crone job with below error:Preparing Elasticsearch(version=7)...Downloading Elasticsearch from https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.5.1-linux-x86_64.tar.gz ... % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 4 276M 4 13.3M 0 0 28.8M 0 0:00:09 --:--:-- 0:00:09 28.8M 42 276M 42 117M 0 0 80.7M 0 0:00:03 0:00:01 0:00:02 80.7M 70 276M 70 196M 0 0 79.9M 0 0:00:03 0:00:02 0:00:01 79.9M 89 276M 89 248M 0 0 82.3M 0 0:00:03 0:00:03 --:--:-- 82.4Mcurl: (56) GnuTLS recv error (-54): Error in the pull function. % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0curl: (7) Failed to connect to localhost port 9200: Connection refused[FAIL] Test script contains errors.https://api.travis-ci.org/v3/job/680222168/log.txt</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.elasticsearch-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="17425" opendate="2020-4-28 00:00:00" fixdate="2020-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support SupportsFilterPushDown in planner</summary>
      <description>Support the SupportsFilterPushDown interface for ScanTableSource.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.testTableSourceSinks.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.LegacyTableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.LegacyTableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.LegacyTableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.LegacyTableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="17426" opendate="2020-4-28 00:00:00" fixdate="2020-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support SupportsLimitPushDown in planner</summary>
      <description>Support the SupportsLimitPushDown interface for ScanTableSource.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TestLimitableTableSource.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.LimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.LimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.LimitTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.LimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="17427" opendate="2020-4-28 00:00:00" fixdate="2020-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support SupportsPartitionPushDown in planner</summary>
      <description>Support the SupportsPartitionPushDown interface for ScanTableSource.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoLegacyTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoLegacyTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushLimitIntoTableSourceScanRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="17428" opendate="2020-4-28 00:00:00" fixdate="2020-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support SupportsProjectionPushDown in planner</summary>
      <description>Support the SupportsProjectionPushDown interface for ScanTableSource.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.testTableSourceSinks.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamingTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.ScanTableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.ScanTableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoLegacyTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="17431" opendate="2020-4-28 00:00:00" fixdate="2020-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement table DDLs for Hive dialect part 1</summary>
      <description>Will cover CREATE, DROP, DESCRIBE, SHOW table in this ticket.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.type.ExtendedSqlRowTypeNameSpec.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.type.ExtendedSqlCollectionTypeNameSpec.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.dql.SqlRichDescribeTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.constraint.SqlTableConstraint.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlCreateHiveDatabase.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlAlterHiveDatabaseOwner.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlAlterHiveDatabase.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.HiveDDLUtils.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="17646" opendate="2020-5-13 00:00:00" fixdate="2020-5-13 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Reduce the python package size of PyFlink</summary>
      <description>Currently the python package size of PyFlink has increased to about 320MB, which exceeds the size limit of pypi.org (300MB). We need to remove unnecessary jars to reduce the package size.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  <bug id="17810" opendate="2020-5-19 00:00:00" fixdate="2020-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for K8s application mode</summary>
      <description>Add document for how to start/stop K8s application cluster.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.0,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug id="17814" opendate="2020-5-19 00:00:00" fixdate="2020-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate native kubernetes document to Chinese</summary>
      <description>https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/native_kubernetes.html Translate the native kubernetes document to Chinese.English updated in 7723774a0402e10bc914b1fa6128e3c80678dafe</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="17816" opendate="2020-5-19 00:00:00" fixdate="2020-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change Latency Marker to work with "scheduleAtFixedDelay" instead of "scheduleAtFixedRate"</summary>
      <description>Latency Markers and other periodic timers are scheduled with scheduleAtFixedRate. That means every X time the callable is called. If it blocks (backpressure) is can be called immediately again.I would suggest to switch this to scheduleAtFixedDelay to avoid calling for a lot of latency marker injections when there is no way to actually execute the injection call.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="17931" opendate="2020-5-25 00:00:00" fixdate="2020-6-25 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Document fromValues clause</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="17936" opendate="2020-5-26 00:00:00" fixdate="2020-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement type inference for AS</summary>
      <description>Type information gets lost due to the legacy planner expressions. The user might experience unexpected exceptions.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.fieldExpression.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
    </fixedFiles>
  </bug>
  <bug id="17937" opendate="2020-5-26 00:00:00" fixdate="2020-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change some hive connector tests to IT cases</summary>
      <description>Hive connector tests that use FlinkStandaloneHiveRunner should be IT cases. Besides, changing them to IT cases can avoid lib conflicts because we'll be using the shaded jars, e.g. guava.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveLookupJoinTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="17958" opendate="2020-5-27 00:00:00" fixdate="2020-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kubernetes session constantly allocates taskmanagers after cancel a job</summary>
      <description>When i am testing the kubernetes-session.sh, i find that the KubernetesResourceManager will constantly allocate taskmanager after cancel a job. I think it may be caused by a bug of the following code. When the dividend is 0 and divisor is bigger than 1, the return value will be 1. However, we expect it to be 0./** * Divide and rounding up to integer. * E.g., divideRoundUp(3, 2) returns 2. * @param dividend value to be divided by the divisor * @param divisor value by which the dividend is to be divided * @return the quotient rounding up to integer */public static int divideRoundUp(int dividend, int divisor) { return (dividend - 1) / divisor + 1;} How to reproduce this issue? Start a Kubernetes session Submit a Flink job to the existing session Cancel the job and wait for the TaskManager released via idle timeout More and more TaskManagers will be allocated</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.WorkerSpecContainerResourceAdapter.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.MathUtilTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.MathUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="18051" opendate="2020-6-1 00:00:00" fixdate="2020-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fail Maven setup on AZP if download fails</summary>
      <description>Setup maven task is green even though the install was not a success: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2481&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=7f98ac96-cfb0-5c1a-969b-c2a0e48a2291</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.maven-utils.sh</file>
    </fixedFiles>
  </bug>
  <bug id="18075" opendate="2020-6-2 00:00:00" fixdate="2020-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka connector does not call open method of (de)serialization schema</summary>
      <description>The Kafka consumer and producer do not call the open methods of plain (De)SerializationSchema interfaces. Only the Keyed and Kafka specific interfaces. The updated SQL implementations such as AvroRowDataSeriailzationSchema use these methods and so SQL queries using avro and kafka will fail in a null pointer exception. cc aljoscha</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.table.Kafka011DynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka011TableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.DataGenerators.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaShortRetentionTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="18076" opendate="2020-6-2 00:00:00" fixdate="2020-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sql client uses wrong class loader when parsing queries</summary>
      <description>Sql-client when parsing queries does not use the user class loader from ExecutionContext. This makes it impossible to query any sources if the dependencies are added with -j flag.In order to reproduce it try querying e.g. KafkaDynamicSource withCREATE TABLE MyUserTable ( f0 BIGINT) WITH ( 'connector' = 'kafka', 'topic' = 'topic_name', -- required: topic name from which the table is read -- required: specify the Kafka server connection string 'properties.bootstrap.servers' = 'localhost:9092', -- required for Kafka source, optional for Kafka sink, specify consumer group 'properties.group.id' = 'testGroup', -- optional: valid modes are "earliest-offset", "latest-offset", "group-offsets", "specific-offsets" or "timestamp"'scan.startup.mode' = 'earliest-offset', 'format' = 'avro');SELECT * FROM MyUserTable;It give exception:Exception in thread "main" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue. at org.apache.flink.table.client.SqlClient.main(SqlClient.java:213)Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: Invalidate SQL statement. at org.apache.flink.table.client.cli.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:95) at org.apache.flink.table.client.cli.SqlCommandParser.parse(SqlCommandParser.java:79) at org.apache.flink.table.client.cli.CliClient.parseCommand(CliClient.java:256) at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:212) at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:142) at org.apache.flink.table.client.SqlClient.start(SqlClient.java:114) at org.apache.flink.table.client.SqlClient.main(SqlClient.java:201)Caused by: org.apache.flink.table.api.ValidationException: Unable to create a source for reading table 'default_catalog.default_database.MyUserTable'.Table options are:'connector'='kafka''format'='avro''properties.bootstrap.servers'='localhost:9092''properties.group.id'='testGroup''scan.startup.mode'='earliest-offset''topic'='topic_name' at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:125) at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.buildTableScan(CatalogSourceTable.scala:135) at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.scala:78) at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3492) at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2415) at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2102) at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051) at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661) at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642) at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345) at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151) at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:773) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:745) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:238) at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:66) at org.apache.flink.table.client.cli.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:90) ... 6 moreCaused by: org.apache.flink.table.api.ValidationException: Cannot discover a connector using option ''connector'='kafka''. at org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:329) at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:118) ... 23 moreCaused by: org.apache.flink.table.api.ValidationException: Could not find any factory for identifier 'kafka' that implements 'org.apache.flink.table.factories.DynamicTableSourceFactory' in the classpath.Available factory identifiers are:datagen at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:240) at org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:326) ... 24 moreShutting down the session...done.Because the factories are present only in the user classloader.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="18081" opendate="2020-6-3 00:00:00" fixdate="2020-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix broken links in "Kerberos Authentication Setup and Configuration" doc</summary>
      <description>The config.html#kerberos-based-security is not valid now.</description>
      <version>1.10.1,1.11.0,1.12.0</version>
      <fixedVersion>1.10.3,1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.security-kerberos.zh.md</file>
      <file type="M">docs.ops.security-kerberos.md</file>
    </fixedFiles>
  </bug>
  <bug id="18082" opendate="2020-6-3 00:00:00" fixdate="2020-1-3 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>UnsignedTypeConversionITCase stalls in ch.vorburger.mariadb4j.DB.stop</summary>
      <description>CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2582&amp;view=logs&amp;j=d44f43ce-542c-597d-bf94-b0718c71e5e8&amp;t=03dca39c-73e8-5aaf-601d-328ae5c35f202020-06-02T19:01:31.8486456Z ==============================================================================2020-06-02T19:01:31.8487052Z Printing stack trace of Java process 86532020-06-02T19:01:31.8487424Z ==============================================================================2020-06-02T19:01:31.8541169Z Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError2020-06-02T19:01:32.1665740Z 2020-06-02 19:01:322020-06-02T19:01:32.1666470Z Full thread dump OpenJDK 64-Bit Server VM (25.242-b08 mixed mode):2020-06-02T19:01:32.1666735Z 2020-06-02T19:01:32.1667614Z "Attach Listener" #537 daemon prio=9 os_prio=0 tid=0x00007f61f8001000 nid=0x3b9f waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1668130Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1668311Z 2020-06-02T19:01:32.1668958Z "flink-akka.actor.default-dispatcher-193" #535 prio=5 os_prio=0 tid=0x00007f6034001000 nid=0x3af7 waiting on condition [0x00007f61a25b8000]2020-06-02T19:01:32.1669418Z java.lang.Thread.State: TIMED_WAITING (parking)2020-06-02T19:01:32.1669730Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1670301Z - parking to wait for &lt;0x0000000080c51528&gt; (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)2020-06-02T19:01:32.1670791Z at akka.dispatch.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:2135)2020-06-02T19:01:32.1671329Z at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2067)2020-06-02T19:01:32.1671763Z at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)2020-06-02T19:01:32.1672211Z at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)2020-06-02T19:01:32.1672491Z 2020-06-02T19:01:32.1673104Z "flink-akka.actor.default-dispatcher-191" #533 prio=5 os_prio=0 tid=0x00007f619801e000 nid=0x3ae1 waiting on condition [0x00007f60770f1000]2020-06-02T19:01:32.1673564Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1673839Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1674422Z - parking to wait for &lt;0x0000000080c51528&gt; (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)2020-06-02T19:01:32.1674865Z at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)2020-06-02T19:01:32.1675305Z at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)2020-06-02T19:01:32.1675751Z at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)2020-06-02T19:01:32.1676046Z 2020-06-02T19:01:32.1676669Z "jobmanager-future-thread-2" #466 daemon prio=5 os_prio=0 tid=0x00007f6124001000 nid=0x3795 waiting on condition [0x00007f61a23b6000]2020-06-02T19:01:32.1677316Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1677617Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1678220Z - parking to wait for &lt;0x00000000816a4c90&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1678702Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1679209Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1679822Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)2020-06-02T19:01:32.1680422Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1680962Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1681424Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1682062Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1682445Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1682656Z 2020-06-02T19:01:32.1683271Z "Flink-DispatcherRestEndpoint-thread-4" #349 daemon prio=5 os_prio=0 tid=0x00007f618c00a000 nid=0x29a4 waiting on condition [0x00007f61a029f000]2020-06-02T19:01:32.1683750Z java.lang.Thread.State: TIMED_WAITING (parking)2020-06-02T19:01:32.1684057Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1684648Z - parking to wait for &lt;0x0000000081731ff8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1685145Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-06-02T19:01:32.1685673Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)2020-06-02T19:01:32.1686400Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)2020-06-02T19:01:32.1687200Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1687724Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1688211Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1688679Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1689076Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1689266Z 2020-06-02T19:01:32.1689923Z "FlinkCompletableFutureDelayScheduler-thread-1" #123 daemon prio=5 os_prio=0 tid=0x00007f60e801d000 nid=0x277b waiting on condition [0x00007f61104dd000]2020-06-02T19:01:32.1690403Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1690698Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1691291Z - parking to wait for &lt;0x00000000879019c0&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1691779Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1692314Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1692905Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)2020-06-02T19:01:32.1693506Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1694040Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1694500Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1694988Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1695372Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1695583Z 2020-06-02T19:01:32.1696176Z "Flink-DispatcherRestEndpoint-thread-3" #84 daemon prio=5 os_prio=0 tid=0x00007f614c003800 nid=0x26ca waiting on condition [0x00007f6113bfa000]2020-06-02T19:01:32.1696685Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1697117Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1697737Z - parking to wait for &lt;0x0000000081731ff8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1698205Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1698749Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1699339Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)2020-06-02T19:01:32.1699942Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1700582Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1701042Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1701528Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1701926Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1702114Z 2020-06-02T19:01:32.1702714Z "mysql-cj-abandoned-connection-cleanup" #83 daemon prio=5 os_prio=0 tid=0x00007f61625c9000 nid=0x26c7 in Object.wait() [0x00007f6113dfc000]2020-06-02T19:01:32.1703201Z java.lang.Thread.State: TIMED_WAITING (on object monitor)2020-06-02T19:01:32.1703530Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1703857Z at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)2020-06-02T19:01:32.1704416Z - locked &lt;0x00000000814a4b00&gt; (a java.lang.ref.ReferenceQueue$Lock)2020-06-02T19:01:32.1704939Z at com.mysql.cj.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:85)2020-06-02T19:01:32.1705464Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2020-06-02T19:01:32.1705950Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1706329Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1706566Z 2020-06-02T19:01:32.1707083Z "Exec Stream Pumper" #76 daemon prio=5 os_prio=0 tid=0x00007f6118001000 nid=0x269c runnable [0x00007f6113ffe000]2020-06-02T19:01:32.1707485Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1707773Z at java.io.FileInputStream.readBytes(Native Method)2020-06-02T19:01:32.1708117Z at java.io.FileInputStream.read(FileInputStream.java:255)2020-06-02T19:01:32.1708522Z at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)2020-06-02T19:01:32.1708944Z at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)2020-06-02T19:01:32.1709357Z at java.io.BufferedInputStream.read(BufferedInputStream.java:345)2020-06-02T19:01:32.1709977Z - locked &lt;0x0000000081555688&gt; (a java.lang.UNIXProcess$ProcessPipeInputStream)2020-06-02T19:01:32.1710386Z at java.io.FilterInputStream.read(FilterInputStream.java:107)2020-06-02T19:01:32.1710779Z at org.apache.commons.exec.StreamPumper.run(StreamPumper.java:107)2020-06-02T19:01:32.1711147Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1711643Z 2020-06-02T19:01:32.1711987Z "Exec Default Executor" #75 prio=5 os_prio=0 tid=0x00007f63009ee800 nid=0x269a in Object.wait() [0x00007f61a019e000]2020-06-02T19:01:32.1712409Z java.lang.Thread.State: WAITING (on object monitor)2020-06-02T19:01:32.1712729Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1713189Z - waiting on &lt;0x00000000816a4ab0&gt; (a java.lang.UNIXProcess)2020-06-02T19:01:32.1713521Z at java.lang.Object.wait(Object.java:502)2020-06-02T19:01:32.1713849Z at java.lang.UNIXProcess.waitFor(UNIXProcess.java:395)2020-06-02T19:01:32.1714370Z - locked &lt;0x00000000816a4ab0&gt; (a java.lang.UNIXProcess)2020-06-02T19:01:32.1714778Z at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:364)2020-06-02T19:01:32.1715244Z at org.apache.commons.exec.DefaultExecutor.access$200(DefaultExecutor.java:48)2020-06-02T19:01:32.1715710Z at org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:200)2020-06-02T19:01:32.1716078Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1716292Z 2020-06-02T19:01:32.1716642Z "process reaper" #71 daemon prio=10 os_prio=0 tid=0x00007f61a805f000 nid=0x2654 runnable [0x00007f61a3441000]2020-06-02T19:01:32.1717218Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1717519Z at java.lang.UNIXProcess.waitForProcessExit(Native Method)2020-06-02T19:01:32.1717906Z at java.lang.UNIXProcess.lambda$initStreams$3(UNIXProcess.java:289)2020-06-02T19:01:32.1718285Z at java.lang.UNIXProcess$$Lambda$7/861659238.run(Unknown Source)2020-06-02T19:01:32.1718721Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2020-06-02T19:01:32.1719318Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1719700Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1719908Z 2020-06-02T19:01:32.1720537Z "Flink-DispatcherRestEndpoint-thread-2" #66 daemon prio=5 os_prio=0 tid=0x00007f614c002800 nid=0x2506 waiting on condition [0x00007f61a2bbc000]2020-06-02T19:01:32.1721013Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1721288Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1721899Z - parking to wait for &lt;0x0000000081731ff8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1722366Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1722891Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1723582Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)2020-06-02T19:01:32.1724166Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1724705Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1725166Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1725653Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1726050Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1726238Z 2020-06-02T19:01:32.1727051Z "mini-cluster-io-thread-10" #64 daemon prio=5 os_prio=0 tid=0x00007f613c00b000 nid=0x236e waiting on condition [0x00007f61a06a1000]2020-06-02T19:01:32.1727500Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1727795Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1728406Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1728888Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1729397Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1729941Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1730413Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1730874Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1731360Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1731740Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1731947Z 2020-06-02T19:01:32.1732513Z "mini-cluster-io-thread-9" #63 daemon prio=5 os_prio=0 tid=0x00007f613c006800 nid=0x236d waiting on condition [0x00007f61a07a2000]2020-06-02T19:01:32.1732963Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1733237Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1733844Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1734329Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1734836Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1735379Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1735833Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1736314Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1737055Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1737598Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1737787Z 2020-06-02T19:01:32.1738405Z "jobmanager-future-thread-1" #62 daemon prio=5 os_prio=0 tid=0x00007f613c01c000 nid=0x236c waiting on condition [0x00007f61a08a3000]2020-06-02T19:01:32.1738863Z java.lang.Thread.State: TIMED_WAITING (parking)2020-06-02T19:01:32.1739148Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1739755Z - parking to wait for &lt;0x00000000816a4c90&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1740227Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-06-02T19:01:32.1740771Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)2020-06-02T19:01:32.1741390Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)2020-06-02T19:01:32.1742053Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1742594Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1743076Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1743544Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1743942Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1744129Z 2020-06-02T19:01:32.1744722Z "mini-cluster-io-thread-8" #59 daemon prio=5 os_prio=0 tid=0x00007f6190005000 nid=0x2369 waiting on condition [0x00007f61a0ba6000]2020-06-02T19:01:32.1745147Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1745440Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1746027Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1746553Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1747227Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1747753Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1748230Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1748691Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1749186Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1749585Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1749771Z 2020-06-02T19:01:32.1750347Z "mini-cluster-io-thread-7" #58 daemon prio=5 os_prio=0 tid=0x00007f6190003000 nid=0x2368 waiting on condition [0x00007f61a0ca7000]2020-06-02T19:01:32.1750793Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1751092Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1751686Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1752170Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1752679Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1753226Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1753703Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1754167Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1754652Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1755031Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1755240Z 2020-06-02T19:01:32.1755806Z "mini-cluster-io-thread-6" #57 daemon prio=5 os_prio=0 tid=0x00007f6190001800 nid=0x2367 waiting on condition [0x00007f61a0da8000]2020-06-02T19:01:32.1756340Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1756674Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1757414Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1757914Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1758437Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1758963Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1759434Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1759895Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1760468Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1760864Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1761052Z 2020-06-02T19:01:32.1761621Z "mini-cluster-io-thread-5" #56 daemon prio=5 os_prio=0 tid=0x00007f6150008000 nid=0x2366 waiting on condition [0x00007f61a0ea9000]2020-06-02T19:01:32.1762063Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1762352Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1762938Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1763417Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1763926Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1764464Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1764944Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1765409Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1765886Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1766284Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1766511Z 2020-06-02T19:01:32.1767253Z "mini-cluster-io-thread-4" #55 daemon prio=5 os_prio=0 tid=0x00007f6300311000 nid=0x2365 waiting on condition [0x00007f61a0faa000]2020-06-02T19:01:32.1767692Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1767965Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1768575Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1769051Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1769558Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1770111Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1770563Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1771042Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1771526Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1771906Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1772094Z 2020-06-02T19:01:32.1772676Z "mini-cluster-io-thread-3" #54 daemon prio=5 os_prio=0 tid=0x00007f6198013800 nid=0x2364 waiting on condition [0x00007f61a10ab000]2020-06-02T19:01:32.1773113Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1773526Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1774138Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1774697Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1775231Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1775770Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1776225Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1776747Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1777357Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1777752Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1777943Z 2020-06-02T19:01:32.1778554Z "mini-cluster-io-thread-2" #53 daemon prio=5 os_prio=0 tid=0x00007f630030c800 nid=0x2363 waiting on condition [0x00007f61a11ac000]2020-06-02T19:01:32.1779065Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1779372Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1779968Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1780447Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1780980Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1781503Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1781977Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1782451Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1782920Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1783316Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1783510Z 2020-06-02T19:01:32.1784119Z "Flink-DispatcherRestEndpoint-thread-1" #52 daemon prio=5 os_prio=0 tid=0x00007f6161fad000 nid=0x2362 waiting on condition [0x00007f61a12ad000]2020-06-02T19:01:32.1784565Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1784856Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1785445Z - parking to wait for &lt;0x0000000081731ff8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1785925Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1786475Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1787228Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)2020-06-02T19:01:32.1787826Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1788354Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1788830Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1789300Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1789691Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1789880Z 2020-06-02T19:01:32.1790488Z "mini-cluster-io-thread-1" #51 daemon prio=5 os_prio=0 tid=0x00007f6161faa800 nid=0x2361 waiting on condition [0x00007f61a13ae000]2020-06-02T19:01:32.1790911Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1791202Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1791786Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1792265Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1792794Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1793408Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1793880Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1794359Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1794829Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1795224Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1795411Z 2020-06-02T19:01:32.1796007Z "flink-rest-server-netty-boss-thread-1" #50 daemon prio=5 os_prio=0 tid=0x00007f6161fa5000 nid=0x2360 runnable [0x00007f61a14af000]2020-06-02T19:01:32.1796466Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1796778Z at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)2020-06-02T19:01:32.1797363Z at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)2020-06-02T19:01:32.1797790Z at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)2020-06-02T19:01:32.1798206Z at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)2020-06-02T19:01:32.1798866Z - locked &lt;0x0000000081f004e8&gt; (a org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySet)2020-06-02T19:01:32.1799476Z - locked &lt;0x0000000081f00500&gt; (a java.util.Collections$UnmodifiableSet)2020-06-02T19:01:32.1799991Z - locked &lt;0x0000000081f004a0&gt; (a sun.nio.ch.EPollSelectorImpl)2020-06-02T19:01:32.1800359Z at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)2020-06-02T19:01:32.1800875Z at org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)2020-06-02T19:01:32.1801498Z at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:806)2020-06-02T19:01:32.1802036Z at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:454)2020-06-02T19:01:32.1802624Z at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)2020-06-02T19:01:32.1803260Z at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)2020-06-02T19:01:32.1803690Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1803895Z 2020-06-02T19:01:32.1804241Z "IOManager reader thread #1" #45 daemon prio=5 os_prio=0 tid=0x00007f63014fa000 nid=0x235d waiting on condition [0x00007f61a1db0000]2020-06-02T19:01:32.1804685Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1804981Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1805571Z - parking to wait for &lt;0x0000000081901368&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1806053Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1806604Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1807373Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1807892Z at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$ReaderThread.run(IOManagerAsync.java:354)2020-06-02T19:01:32.1808204Z 2020-06-02T19:01:32.1808550Z "IOManager writer thread #1" #44 daemon prio=5 os_prio=0 tid=0x00007f63014f9000 nid=0x235c waiting on condition [0x00007f61a1eb1000]2020-06-02T19:01:32.1808996Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1809285Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1809897Z - parking to wait for &lt;0x0000000081977a50&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1810382Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1810893Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1811530Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1812047Z at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$WriterThread.run(IOManagerAsync.java:460)2020-06-02T19:01:32.1812361Z 2020-06-02T19:01:32.1812888Z "Timer-2" #42 daemon prio=5 os_prio=0 tid=0x00007f63014c0000 nid=0x235b in Object.wait() [0x00007f61a1fb2000]2020-06-02T19:01:32.1813326Z java.lang.Thread.State: TIMED_WAITING (on object monitor)2020-06-02T19:01:32.1813641Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1814113Z - waiting on &lt;0x0000000081901c00&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1814450Z at java.util.TimerThread.mainLoop(Timer.java:552)2020-06-02T19:01:32.1814940Z - locked &lt;0x0000000081901c00&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1815270Z at java.util.TimerThread.run(Timer.java:505)2020-06-02T19:01:32.1815469Z 2020-06-02T19:01:32.1816092Z "Timer-1" #40 daemon prio=5 os_prio=0 tid=0x00007f63014be000 nid=0x235a in Object.wait() [0x00007f61a20b3000]2020-06-02T19:01:32.1816569Z java.lang.Thread.State: TIMED_WAITING (on object monitor)2020-06-02T19:01:32.1817028Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1817513Z - waiting on &lt;0x0000000081901570&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1817846Z at java.util.TimerThread.mainLoop(Timer.java:552)2020-06-02T19:01:32.1818339Z - locked &lt;0x0000000081901570&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1818650Z at java.util.TimerThread.run(Timer.java:505)2020-06-02T19:01:32.1818862Z 2020-06-02T19:01:32.1819196Z "BLOB Server listener at 39424" #36 daemon prio=5 os_prio=0 tid=0x00007f63014bb000 nid=0x2359 runnable [0x00007f61a21b4000]2020-06-02T19:01:32.1819610Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1819903Z at java.net.PlainSocketImpl.socketAccept(Native Method)2020-06-02T19:01:32.1820304Z at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)2020-06-02T19:01:32.1820726Z at java.net.ServerSocket.implAccept(ServerSocket.java:560)2020-06-02T19:01:32.1821107Z at java.net.ServerSocket.accept(ServerSocket.java:528)2020-06-02T19:01:32.1821511Z at org.apache.flink.runtime.blob.BlobServer.run(BlobServer.java:262)2020-06-02T19:01:32.1821756Z 2020-06-02T19:01:32.1822269Z "Timer-0" #37 daemon prio=5 os_prio=0 tid=0x00007f63014a5800 nid=0x2358 in Object.wait() [0x00007f61a22b5000]2020-06-02T19:01:32.1822709Z java.lang.Thread.State: TIMED_WAITING (on object monitor)2020-06-02T19:01:32.1823030Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1823478Z - waiting on &lt;0x0000000081901740&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1823816Z at java.util.TimerThread.mainLoop(Timer.java:552)2020-06-02T19:01:32.1824304Z - locked &lt;0x0000000081901740&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1824629Z at java.util.TimerThread.run(Timer.java:505)2020-06-02T19:01:32.1824823Z 2020-06-02T19:01:32.1825397Z "flink-metrics-scheduler-1" #32 prio=5 os_prio=0 tid=0x00007f6301468000 nid=0x2354 waiting on condition [0x00007f61a26b9000]2020-06-02T19:01:32.1825831Z java.lang.Thread.State: TIMED_WAITING (sleeping)2020-06-02T19:01:32.1826134Z at java.lang.Thread.sleep(Native Method)2020-06-02T19:01:32.1826559Z at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)2020-06-02T19:01:32.1827253Z at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)2020-06-02T19:01:32.1827792Z at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)2020-06-02T19:01:32.1828191Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1828392Z 2020-06-02T19:01:32.1829014Z "flink-scheduler-1" #27 prio=5 os_prio=0 tid=0x00007f6300ec1800 nid=0x22f5 waiting on condition [0x00007f61a2fbe000]2020-06-02T19:01:32.1829625Z java.lang.Thread.State: TIMED_WAITING (sleeping)2020-06-02T19:01:32.1830040Z at java.lang.Thread.sleep(Native Method)2020-06-02T19:01:32.1830784Z at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)2020-06-02T19:01:32.1831508Z at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)2020-06-02T19:01:32.1832253Z at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)2020-06-02T19:01:32.1832842Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1833195Z 2020-06-02T19:01:32.1833740Z "process reaper" #24 daemon prio=10 os_prio=0 tid=0x00007f61a8048000 nid=0x2222 waiting on condition [0x00007f61a3a81000]2020-06-02T19:01:32.1834332Z java.lang.Thread.State: TIMED_WAITING (parking)2020-06-02T19:01:32.1834709Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1835502Z - parking to wait for &lt;0x0000000080ba0518&gt; (a java.util.concurrent.SynchronousQueue$TransferStack)2020-06-02T19:01:32.1836129Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-06-02T19:01:32.1837204Z at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)2020-06-02T19:01:32.1837935Z at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)2020-06-02T19:01:32.1838607Z at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)2020-06-02T19:01:32.1839184Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)2020-06-02T19:01:32.1839801Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1840433Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1840957Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1841215Z 2020-06-02T19:01:32.1842084Z "surefire-forkedjvm-ping-30s" #23 daemon prio=5 os_prio=0 tid=0x00007f63003c3000 nid=0x221f waiting on condition [0x00007f61b07c6000]2020-06-02T19:01:32.1842680Z java.lang.Thread.State: TIMED_WAITING (parking)2020-06-02T19:01:32.1843116Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1843930Z - parking to wait for &lt;0x0000000080b92410&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1844585Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-06-02T19:01:32.1845348Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)2020-06-02T19:01:32.1846221Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)2020-06-02T19:01:32.1847278Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1848018Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1848653Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1849341Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1849850Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1850130Z 2020-06-02T19:01:32.1850983Z "surefire-forkedjvm-command-thread" #22 daemon prio=5 os_prio=0 tid=0x00007f63003ac000 nid=0x221c runnable [0x00007f61b0ad1000]2020-06-02T19:01:32.1851572Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1851989Z at java.io.FileInputStream.readBytes(Native Method)2020-06-02T19:01:32.1852454Z at java.io.FileInputStream.read(FileInputStream.java:255)2020-06-02T19:01:32.1852980Z at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)2020-06-02T19:01:32.1853504Z at java.io.BufferedInputStream.read(BufferedInputStream.java:265)2020-06-02T19:01:32.1854228Z - locked &lt;0x0000000080c53790&gt; (a java.io.BufferedInputStream)2020-06-02T19:01:32.1854701Z at java.io.DataInputStream.readInt(DataInputStream.java:387)2020-06-02T19:01:32.1855323Z at org.apache.maven.surefire.booter.MasterProcessCommand.decode(MasterProcessCommand.java:115)2020-06-02T19:01:32.1856252Z at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:391)2020-06-02T19:01:32.1857036Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1857304Z 2020-06-02T19:01:32.1857755Z "Service Thread" #21 daemon prio=9 os_prio=0 tid=0x00007f63002d5000 nid=0x221a runnable [0x0000000000000000]2020-06-02T19:01:32.1858280Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1858532Z 2020-06-02T19:01:32.1858991Z "C1 CompilerThread14" #20 daemon prio=9 os_prio=0 tid=0x00007f63002c8000 nid=0x2219 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1859548Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1859786Z 2020-06-02T19:01:32.1860247Z "C1 CompilerThread13" #19 daemon prio=9 os_prio=0 tid=0x00007f63002c6000 nid=0x2218 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1860802Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1861042Z 2020-06-02T19:01:32.1861653Z "C1 CompilerThread12" #18 daemon prio=9 os_prio=0 tid=0x00007f63002c4000 nid=0x2217 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1862227Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1862465Z 2020-06-02T19:01:32.1862931Z "C1 CompilerThread11" #17 daemon prio=9 os_prio=0 tid=0x00007f63002c2000 nid=0x2216 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1863458Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1863711Z 2020-06-02T19:01:32.1864153Z "C1 CompilerThread10" #16 daemon prio=9 os_prio=0 tid=0x00007f63002c0000 nid=0x2215 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1864696Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1864932Z 2020-06-02T19:01:32.1865389Z "C2 CompilerThread9" #15 daemon prio=9 os_prio=0 tid=0x00007f63002bd000 nid=0x2214 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1865921Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1866175Z 2020-06-02T19:01:32.1866632Z "C2 CompilerThread8" #14 daemon prio=9 os_prio=0 tid=0x00007f63002bb800 nid=0x2213 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1867418Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1867660Z 2020-06-02T19:01:32.1868123Z "C2 CompilerThread7" #13 daemon prio=9 os_prio=0 tid=0x00007f63002b9000 nid=0x2212 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1868652Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1868908Z 2020-06-02T19:01:32.1869440Z "C2 CompilerThread6" #12 daemon prio=9 os_prio=0 tid=0x00007f63002b7800 nid=0x2211 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1869969Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1870199Z 2020-06-02T19:01:32.1870646Z "C2 CompilerThread5" #11 daemon prio=9 os_prio=0 tid=0x00007f63002b5000 nid=0x2210 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1871166Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1871396Z 2020-06-02T19:01:32.1871860Z "C2 CompilerThread4" #10 daemon prio=9 os_prio=0 tid=0x00007f63002b3000 nid=0x220f waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1872387Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1872635Z 2020-06-02T19:01:32.1873060Z "C2 CompilerThread3" #9 daemon prio=9 os_prio=0 tid=0x00007f63002a9000 nid=0x220e waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1873590Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1873815Z 2020-06-02T19:01:32.1874267Z "C2 CompilerThread2" #8 daemon prio=9 os_prio=0 tid=0x00007f63002a6800 nid=0x220d waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1874805Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1875035Z 2020-06-02T19:01:32.1875466Z "C2 CompilerThread1" #7 daemon prio=9 os_prio=0 tid=0x00007f63002a4800 nid=0x220c waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1875992Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1876218Z 2020-06-02T19:01:32.1876676Z "C2 CompilerThread0" #6 daemon prio=9 os_prio=0 tid=0x00007f63002a2800 nid=0x220b waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1877580Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1877815Z 2020-06-02T19:01:32.1878222Z "Signal Dispatcher" #5 daemon prio=9 os_prio=0 tid=0x00007f63002a0800 nid=0x220a runnable [0x0000000000000000]2020-06-02T19:01:32.1878748Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1878977Z 2020-06-02T19:01:32.1879472Z "Surrogate Locker Thread (Concurrent GC)" #4 daemon prio=9 os_prio=0 tid=0x00007f630029f000 nid=0x2209 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1880026Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1880278Z 2020-06-02T19:01:32.1880699Z "Finalizer" #3 daemon prio=8 os_prio=0 tid=0x00007f630026e800 nid=0x2208 in Object.wait() [0x00007f620958e000]2020-06-02T19:01:32.1881124Z java.lang.Thread.State: WAITING (on object monitor)2020-06-02T19:01:32.1881408Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1882065Z - waiting on &lt;0x0000000080c53a60&gt; (a java.lang.ref.ReferenceQueue$Lock)2020-06-02T19:01:32.1882609Z at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)2020-06-02T19:01:32.1883144Z - locked &lt;0x0000000080c53a60&gt; (a java.lang.ref.ReferenceQueue$Lock)2020-06-02T19:01:32.1883522Z at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)2020-06-02T19:01:32.1883906Z at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)2020-06-02T19:01:32.1884151Z 2020-06-02T19:01:32.1884675Z "Reference Handler" #2 daemon prio=10 os_prio=0 tid=0x00007f630026a000 nid=0x2207 in Object.wait() [0x00007f620968f000]2020-06-02T19:01:32.1885086Z java.lang.Thread.State: WAITING (on object monitor)2020-06-02T19:01:32.1885390Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1885872Z - waiting on &lt;0x0000000080c53a50&gt; (a java.lang.ref.Reference$Lock)2020-06-02T19:01:32.1886186Z at java.lang.Object.wait(Object.java:502)2020-06-02T19:01:32.1886587Z at java.lang.ref.Reference.tryHandlePending(Reference.java:191)2020-06-02T19:01:32.1887443Z - locked &lt;0x0000000080c53a50&gt; (a java.lang.ref.Reference$Lock)2020-06-02T19:01:32.1887838Z at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)2020-06-02T19:01:32.1888077Z 2020-06-02T19:01:32.1888380Z "main" #1 prio=5 os_prio=0 tid=0x00007f630000b800 nid=0x21cf waiting on condition [0x00007f6306d43000]2020-06-02T19:01:32.1888757Z java.lang.Thread.State: TIMED_WAITING (sleeping)2020-06-02T19:01:32.1889050Z at java.lang.Thread.sleep(Native Method)2020-06-02T19:01:32.1889437Z at org.apache.commons.exec.DefaultExecuteResultHandler.waitFor(DefaultExecuteResultHandler.java:121)2020-06-02T19:01:32.1889917Z at ch.vorburger.exec.ManagedProcess.destroy(ManagedProcess.java:344)2020-06-02T19:01:32.1890289Z at ch.vorburger.mariadb4j.DB.stop(DB.java:327)2020-06-02T19:01:32.1890764Z - locked &lt;0x00000000816a5170&gt; (a ch.vorburger.mariadb4j.DB)2020-06-02T19:01:32.1891151Z at ch.vorburger.mariadb4j.junit.MariaDB4jRule.after(MariaDB4jRule.java:64)2020-06-02T19:01:32.1891571Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:50)2020-06-02T19:01:32.1892001Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-06-02T19:01:32.1892425Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-06-02T19:01:32.1892802Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-06-02T19:01:32.1893180Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2020-06-02T19:01:32.1893594Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)2020-06-02T19:01:32.1894085Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)2020-06-02T19:01:32.1894575Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2020-06-02T19:01:32.1895059Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)2020-06-02T19:01:32.1895567Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2020-06-02T19:01:32.1896211Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2020-06-02T19:01:32.1896744Z at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2020-06-02T19:01:32.1897423Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)</description>
      <version>1.12.0,1.15.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18087" opendate="2020-6-3 00:00:00" fixdate="2020-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Uploading user artifact for Yarn job cluster could not work</summary>
      <description>In FLINK-17632, we add the support remote user jar. However, uploading user artifact for Yarn job cluster is broken exceptionally. In the following code, we should only upload local files. Now it has the contrary behavior.// only upload local filesif (Utils.isRemotePath(entry.getValue().filePath)) { Path localPath = new Path(entry.getValue().filePath); Tuple2&lt;Path, Long&gt; remoteFileInfo = fileUploader.uploadLocalFileToRemote(localPath, entry.getKey()); jobGraph.setUserArtifactRemotePath(entry.getKey(), remoteFileInfo.f0.toString());} Another problem is the related tests testPerJobModeWithDistributedCache does not fail because we do not fetch the artifact from remote filesystem(i.e. HDFS). We directly get it from local file. It also needs to be enhanced.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.0,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="18117" opendate="2020-6-4 00:00:00" fixdate="2020-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Kerberized YARN per-job on Docker test" fails with "Could not start hadoop cluster."</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2683&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f52020-06-04T06:03:53.2844296Z Creating slave1 ... [32mdone [0m2020-06-04T06:03:53.4981251Z [1BWaiting for hadoop cluster to come up. We have been trying for 0 seconds, retrying ...2020-06-04T06:03:58.5980181Z Waiting for hadoop cluster to come up. We have been trying for 5 seconds, retrying ...2020-06-04T06:04:03.6997087Z Waiting for hadoop cluster to come up. We have been trying for 10 seconds, retrying ...2020-06-04T06:04:08.7910791Z Waiting for hadoop cluster to come up. We have been trying for 15 seconds, retrying ...2020-06-04T06:04:13.8921621Z Waiting for hadoop cluster to come up. We have been trying for 20 seconds, retrying ...2020-06-04T06:04:18.9648844Z Waiting for hadoop cluster to come up. We have been trying for 25 seconds, retrying ...2020-06-04T06:04:24.0381851Z Waiting for hadoop cluster to come up. We have been trying for 31 seconds, retrying ...2020-06-04T06:04:29.1220264Z Waiting for hadoop cluster to come up. We have been trying for 36 seconds, retrying ...2020-06-04T06:04:34.1882187Z Waiting for hadoop cluster to come up. We have been trying for 41 seconds, retrying ...2020-06-04T06:04:39.2784948Z Waiting for hadoop cluster to come up. We have been trying for 46 seconds, retrying ...2020-06-04T06:04:44.3843337Z Waiting for hadoop cluster to come up. We have been trying for 51 seconds, retrying ...2020-06-04T06:04:49.4703561Z Waiting for hadoop cluster to come up. We have been trying for 56 seconds, retrying ...2020-06-04T06:04:54.5463207Z Waiting for hadoop cluster to come up. We have been trying for 61 seconds, retrying ...2020-06-04T06:04:59.6650405Z Waiting for hadoop cluster to come up. We have been trying for 66 seconds, retrying ...2020-06-04T06:05:04.7500168Z Waiting for hadoop cluster to come up. We have been trying for 71 seconds, retrying ...2020-06-04T06:05:09.8177904Z Waiting for hadoop cluster to come up. We have been trying for 76 seconds, retrying ...2020-06-04T06:05:14.9751297Z Waiting for hadoop cluster to come up. We have been trying for 81 seconds, retrying ...2020-06-04T06:05:20.0336417Z Waiting for hadoop cluster to come up. We have been trying for 87 seconds, retrying ...2020-06-04T06:05:25.1627704Z Waiting for hadoop cluster to come up. We have been trying for 92 seconds, retrying ...2020-06-04T06:05:30.2583315Z Waiting for hadoop cluster to come up. We have been trying for 97 seconds, retrying ...2020-06-04T06:05:35.3283678Z Waiting for hadoop cluster to come up. We have been trying for 102 seconds, retrying ...2020-06-04T06:05:40.4184029Z Waiting for hadoop cluster to come up. We have been trying for 107 seconds, retrying ...2020-06-04T06:05:45.5388372Z Waiting for hadoop cluster to come up. We have been trying for 112 seconds, retrying ...2020-06-04T06:05:50.6155334Z Waiting for hadoop cluster to come up. We have been trying for 117 seconds, retrying ...2020-06-04T06:05:55.7225186Z Command: start_hadoop_cluster failed. Retrying...2020-06-04T06:05:55.7237999Z Starting Hadoop cluster2020-06-04T06:05:56.5188293Z kdc is up-to-date2020-06-04T06:05:56.5292716Z master is up-to-date2020-06-04T06:05:56.5301735Z slave2 is up-to-date2020-06-04T06:05:56.5306179Z slave1 is up-to-date2020-06-04T06:05:56.6800566Z Waiting for hadoop cluster to come up. We have been trying for 0 seconds, retrying ...2020-06-04T06:06:01.7668291Z Waiting for hadoop cluster to come up. We have been trying for 5 seconds, retrying ...2020-06-04T06:06:06.8620265Z Waiting for hadoop cluster to come up. We have been trying for 10 seconds, retrying ...2020-06-04T06:06:11.9753596Z Waiting for hadoop cluster to come up. We have been trying for 15 seconds, retrying ...2020-06-04T06:06:17.0402846Z Waiting for hadoop cluster to come up. We have been trying for 21 seconds, retrying ...2020-06-04T06:06:22.1650005Z Waiting for hadoop cluster to come up. We have been trying for 26 seconds, retrying ...2020-06-04T06:06:27.2500179Z Waiting for hadoop cluster to come up. We have been trying for 31 seconds, retrying ...2020-06-04T06:06:32.3133809Z Waiting for hadoop cluster to come up. We have been trying for 36 seconds, retrying ...2020-06-04T06:06:37.4432923Z Waiting for hadoop cluster to come up. We have been trying for 41 seconds, retrying ...2020-06-04T06:06:42.5658250Z Waiting for hadoop cluster to come up. We have been trying for 46 seconds, retrying ...2020-06-04T06:06:47.6682536Z Waiting for hadoop cluster to come up. We have been trying for 51 seconds, retrying ...2020-06-04T06:06:52.7810371Z Waiting for hadoop cluster to come up. We have been trying for 56 seconds, retrying ...2020-06-04T06:06:57.8860269Z Waiting for hadoop cluster to come up. We have been trying for 61 seconds, retrying ...2020-06-04T06:07:03.0337979Z Waiting for hadoop cluster to come up. We have been trying for 67 seconds, retrying ...2020-06-04T06:07:08.1080310Z Waiting for hadoop cluster to come up. We have been trying for 72 seconds, retrying ...2020-06-04T06:07:13.2297578Z Waiting for hadoop cluster to come up. We have been trying for 77 seconds, retrying ...2020-06-04T06:07:18.3779034Z Waiting for hadoop cluster to come up. We have been trying for 82 seconds, retrying ...2020-06-04T06:07:23.4789495Z Waiting for hadoop cluster to come up. We have been trying for 87 seconds, retrying ...2020-06-04T06:07:28.6063062Z Waiting for hadoop cluster to come up. We have been trying for 92 seconds, retrying ...2020-06-04T06:07:33.8220409Z Waiting for hadoop cluster to come up. We have been trying for 97 seconds, retrying ...2020-06-04T06:07:38.9439231Z Waiting for hadoop cluster to come up. We have been trying for 102 seconds, retrying ...2020-06-04T06:07:44.0193849Z Waiting for hadoop cluster to come up. We have been trying for 108 seconds, retrying ...2020-06-04T06:07:49.1241642Z Waiting for hadoop cluster to come up. We have been trying for 113 seconds, retrying ...2020-06-04T06:07:54.2425087Z Waiting for hadoop cluster to come up. We have been trying for 118 seconds, retrying ...2020-06-04T06:07:59.3835321Z Command: start_hadoop_cluster failed. Retrying...2020-06-04T06:07:59.3847275Z Starting Hadoop cluster2020-06-04T06:08:00.1959109Z kdc is up-to-date2020-06-04T06:08:00.1968717Z master is up-to-date2020-06-04T06:08:00.1982811Z slave1 is up-to-date2020-06-04T06:08:00.1988143Z slave2 is up-to-date2020-06-04T06:08:00.4014781Z Waiting for hadoop cluster to come up. We have been trying for 0 seconds, retrying ...2020-06-04T06:08:05.5168483Z Waiting for hadoop cluster to come up. We have been trying for 5 seconds, retrying ...2020-06-04T06:08:10.6759355Z Waiting for hadoop cluster to come up. We have been trying for 10 seconds, retrying ...2020-06-04T06:08:15.8307550Z Waiting for hadoop cluster to come up. We have been trying for 15 seconds, retrying ...2020-06-04T06:08:21.0143341Z Waiting for hadoop cluster to come up. We have been trying for 21 seconds, retrying ...2020-06-04T06:08:26.0932297Z Waiting for hadoop cluster to come up. We have been trying for 26 seconds, retrying ...2020-06-04T06:08:31.2526775Z Waiting for hadoop cluster to come up. We have been trying for 31 seconds, retrying ...2020-06-04T06:08:36.4356124Z Waiting for hadoop cluster to come up. We have been trying for 36 seconds, retrying ...2020-06-04T06:08:41.5607530Z Waiting for hadoop cluster to come up. We have been trying for 41 seconds, retrying ...2020-06-04T06:08:46.6407963Z Waiting for hadoop cluster to come up. We have been trying for 46 seconds, retrying ...2020-06-04T06:08:51.8464789Z Waiting for hadoop cluster to come up. We have been trying for 51 seconds, retrying ...2020-06-04T06:08:56.9735817Z Waiting for hadoop cluster to come up. We have been trying for 56 seconds, retrying ...2020-06-04T06:09:02.1023842Z Waiting for hadoop cluster to come up. We have been trying for 62 seconds, retrying ...2020-06-04T06:09:07.2390427Z Waiting for hadoop cluster to come up. We have been trying for 67 seconds, retrying ...2020-06-04T06:09:12.4433329Z Waiting for hadoop cluster to come up. We have been trying for 72 seconds, retrying ...2020-06-04T06:09:17.5390800Z Waiting for hadoop cluster to come up. We have been trying for 77 seconds, retrying ...2020-06-04T06:09:22.7020537Z Waiting for hadoop cluster to come up. We have been trying for 82 seconds, retrying ...2020-06-04T06:09:27.8754909Z Waiting for hadoop cluster to come up. We have been trying for 87 seconds, retrying ...2020-06-04T06:09:33.0447274Z Waiting for hadoop cluster to come up. We have been trying for 93 seconds, retrying ...2020-06-04T06:09:38.1804596Z Waiting for hadoop cluster to come up. We have been trying for 98 seconds, retrying ...2020-06-04T06:09:43.3636590Z Waiting for hadoop cluster to come up. We have been trying for 103 seconds, retrying ...2020-06-04T06:09:48.4975410Z Waiting for hadoop cluster to come up. We have been trying for 108 seconds, retrying ...2020-06-04T06:09:53.6117328Z Waiting for hadoop cluster to come up. We have been trying for 113 seconds, retrying ...2020-06-04T06:09:58.7785946Z Waiting for hadoop cluster to come up. We have been trying for 118 seconds, retrying ...2020-06-04T06:10:03.9748663Z Command: start_hadoop_cluster failed. Retrying...2020-06-04T06:10:03.9808244Z Command: start_hadoop_cluster failed 3 times.2020-06-04T06:10:03.9823071Z ERROR: Could not start hadoop cluster. Aborting...Frequent, suspicious logs2020-06-04T06:10:04.5032658Z 20/06/04 06:05:42 WARN ipc.Client: Failed to connect to server: master.docker-hadoop-cluster-network/172.19.0.3:9000: try once and fail.2020-06-04T06:10:04.5033211Z java.net.ConnectException: Connection refused...2020-06-04T06:10:04.6867876Z 20/06/04 06:04:11 ERROR namenode.NameNode: Failed to start namenode.2020-06-04T06:10:04.6868640Z java.net.BindException: Port in use: 0.0.0.0:504702020-06-04T06:10:04.6869062Z at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:998)2020-06-04T06:10:04.6869702Z at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:935)2020-06-04T06:10:04.6870199Z at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:171)2020-06-04T06:10:04.6870740Z at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:842)2020-06-04T06:10:04.6871235Z at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:693)2020-06-04T06:10:04.6871728Z at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:906)2020-06-04T06:10:04.6872202Z at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:885)2020-06-04T06:10:04.6872699Z at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1626)2020-06-04T06:10:04.6873701Z at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1694)2020-06-04T06:10:04.6874100Z Caused by: java.net.BindException: Address already in use2020-06-04T06:10:04.6901805Z at sun.nio.ch.Net.bind0(Native Method)2020-06-04T06:10:04.6902168Z at sun.nio.ch.Net.bind(Net.java:433)2020-06-04T06:10:04.6902478Z at sun.nio.ch.Net.bind(Net.java:425)2020-06-04T06:10:04.6902847Z at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)2020-06-04T06:10:04.6903296Z at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)2020-06-04T06:10:04.6903744Z at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)2020-06-04T06:10:04.6904395Z at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:993)2020-06-04T06:10:04.6904727Z ... 8 more2020-06-04T06:10:04.6905005Z 20/06/04 06:04:11 INFO util.ExitUtil: Exiting with status 12020-06-04T06:10:04.6905401Z 20/06/04 06:04:11 INFO namenode.NameNode: SHUTDOWN_MSG:</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.config.hdfs-site.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.Dockerfile</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.bootstrap.sh</file>
    </fixedFiles>
  </bug>
  <bug id="18125" opendate="2020-6-4 00:00:00" fixdate="2020-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip CI execution on documentation pull requests</summary>
      <description>In order to save some resources, we can skip the CI execution on documentation-only changes (whole changeset).</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">tools.azure-pipelines.free.disk.space.sh</file>
    </fixedFiles>
  </bug>
  <bug id="18137" opendate="2020-6-4 00:00:00" fixdate="2020-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JobMasterTriggerSavepointITCase.testStopJobAfterSavepoint fails with AskTimeoutException</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2747&amp;view=logs&amp;j=5c8e7682-d68f-54d1-16a2-a09310218a49&amp;t=45cc9205-bdb7-5b54-63cd-89fdc09833232020-06-04T16:17:20.4404189Z [ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 14.352 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase2020-06-04T16:17:20.4405548Z [ERROR] testStopJobAfterSavepoint(org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase) Time elapsed: 10.058 s &lt;&lt;&lt; ERROR!2020-06-04T16:17:20.4407342Z java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.triggerSavepoint(org.apache.flink.api.common.JobID,java.lang.String,boolean,org.apache.flink.api.common.time.Time) timed out.2020-06-04T16:17:20.4409562Z at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)2020-06-04T16:17:20.4410333Z at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)2020-06-04T16:17:20.4411259Z at org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase.cancelWithSavepoint(JobMasterTriggerSavepointITCase.java:264)2020-06-04T16:17:20.4412292Z at org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase.testStopJobAfterSavepoint(JobMasterTriggerSavepointITCase.java:127)2020-06-04T16:17:20.4413163Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-06-04T16:17:20.4413990Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-06-04T16:17:20.4414783Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-06-04T16:17:20.4415936Z at java.lang.reflect.Method.invoke(Method.java:498)2020-06-04T16:17:20.4416693Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)2020-06-04T16:17:20.4417632Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2020-06-04T16:17:20.4418637Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)2020-06-04T16:17:20.4419367Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2020-06-04T16:17:20.4420118Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-06-04T16:17:20.4420742Z at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)2020-06-04T16:17:20.4421909Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-06-04T16:17:20.4422493Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)2020-06-04T16:17:20.4423247Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)2020-06-04T16:17:20.4424263Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)2020-06-04T16:17:20.4424876Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)2020-06-04T16:17:20.4426346Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)2020-06-04T16:17:20.4427052Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)2020-06-04T16:17:20.4427772Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)2020-06-04T16:17:20.4428562Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)2020-06-04T16:17:20.4429158Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-06-04T16:17:20.4429861Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-06-04T16:17:20.4430448Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-06-04T16:17:20.4431060Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2020-06-04T16:17:20.4431678Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)2020-06-04T16:17:20.4432513Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)2020-06-04T16:17:20.4433396Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2020-06-04T16:17:20.4434298Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)2020-06-04T16:17:20.4440904Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2020-06-04T16:17:20.4443425Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2020-06-04T16:17:20.4444349Z at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2020-06-04T16:17:20.4445160Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)2020-06-04T16:17:20.4446389Z Caused by: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.triggerSavepoint(org.apache.flink.api.common.JobID,java.lang.String,boolean,org.apache.flink.api.common.time.Time) timed out.2020-06-04T16:17:20.4447610Z at com.sun.proxy.$Proxy31.triggerSavepoint(Unknown Source)2020-06-04T16:17:20.4448545Z at org.apache.flink.runtime.minicluster.MiniCluster.lambda$triggerSavepoint$8(MiniCluster.java:595)2020-06-04T16:17:20.4449259Z at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)2020-06-04T16:17:20.4449990Z at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)2020-06-04T16:17:20.4450789Z at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)2020-06-04T16:17:20.4451584Z at org.apache.flink.runtime.minicluster.MiniCluster.runDispatcherCommand(MiniCluster.java:621)2020-06-04T16:17:20.4452473Z at org.apache.flink.runtime.minicluster.MiniCluster.triggerSavepoint(MiniCluster.java:595)2020-06-04T16:17:20.4453572Z at org.apache.flink.client.program.MiniClusterClient.cancelWithSavepoint(MiniClusterClient.java:89)2020-06-04T16:17:20.4454746Z at org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase.cancelWithSavepoint(JobMasterTriggerSavepointITCase.java:262)2020-06-04T16:17:20.4455517Z ... 32 more2020-06-04T16:17:20.4457589Z Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/dispatcher_2#830345697]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.2020-06-04T16:17:20.4459164Z at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)2020-06-04T16:17:20.4460107Z at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)2020-06-04T16:17:20.4460819Z at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)2020-06-04T16:17:20.4461613Z at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)2020-06-04T16:17:20.4462444Z at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)2020-06-04T16:17:20.4463203Z at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)2020-06-04T16:17:20.4464089Z at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)2020-06-04T16:17:20.4464833Z at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)2020-06-04T16:17:20.4465800Z at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)2020-06-04T16:17:20.4466746Z at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)2020-06-04T16:17:20.4467579Z at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)2020-06-04T16:17:20.4468467Z at java.lang.Thread.run(Thread.java:748)</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTriggeringTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="18140" opendate="2020-6-5 00:00:00" fixdate="2020-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for ORC format</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.formats.parquet.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.parquet.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="18141" opendate="2020-6-5 00:00:00" fixdate="2020-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Parquet format</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.formats.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="18149" opendate="2020-6-5 00:00:00" fixdate="2020-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Taskmanager logs could not show up in native K8s deployment</summary>
      <description>In FLINK-17935, we use flinkConfig.get(DeploymentOptionsInternal.CONF_DIR) to replace CliFrontend.getConfigurationDirectoryFromEnv. It will cause problem in native K8s integration. The root cause we set the DeploymentOptionsInternal.CONF_DIR in flink-conf.yaml to a local path. However, it does not exist in JobManager pod.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesTestUtils.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.parameters.AbstractKubernetesParametersTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.FlinkConfMountDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.AbstractKubernetesParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.FlinkConfMountDecorator.java</file>
    </fixedFiles>
  </bug>
  <bug id="18173" opendate="2020-6-8 00:00:00" fixdate="2020-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bundle flink-csv and flink-json jars in lib</summary>
      <description>The biggest problem for distributions I see is the variety of problems caused by users' lack of format dependency. These three formats are very small and no third party dependence, and they are widely used by table users. Actually, we don't have any other built-in table formats now.. flink-csv-1.10.0.jar flink-json-1.10.0.jar We can just bundle them in "flink/lib/". It not solve all problems and it is independent of "fat" and "slim". But also improve usability.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.release-notes.flink-1.11.zh.md</file>
      <file type="M">docs.release-notes.flink-1.11.md</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">docs.dev.table.connectors.formats.avro.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.avro.md</file>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
    </fixedFiles>
  </bug>
  <bug id="18222" opendate="2020-6-9 00:00:00" fixdate="2020-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Avro Confluent Schema Registry nightly end-to-end test" unstable with "Kafka cluster did not start after 120 seconds"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3045&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f52020-06-09T15:16:48.1427795Z ==============================================================================2020-06-09T15:16:48.1428609Z Running 'Avro Confluent Schema Registry nightly end-to-end test'2020-06-09T15:16:48.1429204Z ==============================================================================2020-06-09T15:16:48.1438117Z TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-481432981702020-06-09T15:16:48.2985167Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT2020-06-09T15:16:48.3157575Z Downloading Kafka from https://archive.apache.org/dist/kafka/0.10.2.0/kafka_2.11-0.10.2.0.tgz2020-06-09T15:16:48.3214487Z % Total % Received % Xferd Average Speed Time Time Time Current2020-06-09T15:16:48.3215154Z Dload Upload Total Spent Left Speed2020-06-09T15:16:48.3215597Z 2020-06-09T15:16:48.3528820Z 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 02020-06-09T15:16:49.3421526Z 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 02020-06-09T15:16:50.3415678Z 8 35.8M 8 2960k 0 0 2896k 0 0:00:12 0:00:01 0:00:11 2896k2020-06-09T15:16:51.3406836Z 23 35.8M 23 8544k 0 0 4226k 0 0:00:08 0:00:02 0:00:06 4225k2020-06-09T15:16:51.6553485Z 70 35.8M 70 25.2M 0 0 8550k 0 0:00:04 0:00:03 0:00:01 8548k2020-06-09T15:16:51.6555606Z 100 35.8M 100 35.8M 0 0 10.7M 0 0:00:03 0:00:03 --:--:-- 10.7M2020-06-09T15:16:51.9818041Z Downloading confluent from http://packages.confluent.io/archive/3.2/confluent-oss-3.2.0-2.11.tar.gz2020-06-09T15:16:51.9880242Z % Total % Received % Xferd Average Speed Time Time Time Current2020-06-09T15:16:51.9880983Z Dload Upload Total Spent Left Speed2020-06-09T15:16:51.9914252Z 2020-06-09T15:16:52.3398614Z 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 02020-06-09T15:16:53.3399552Z 9 398M 9 39.5M 0 0 111M 0 0:00:03 --:--:-- 0:00:03 111M2020-06-09T15:16:53.9149276Z 47 398M 47 188M 0 0 139M 0 0:00:02 0:00:01 0:00:01 138M2020-06-09T15:16:53.9150980Z 100 398M 100 398M 0 0 206M 0 0:00:01 0:00:01 --:--:-- 206M2020-06-09T15:17:04.3565942Z Waiting for broker...2020-06-09T15:17:12.4215170Z Waiting for broker...2020-06-09T15:17:14.3012835Z Waiting for broker...2020-06-09T15:17:16.1965074Z Waiting for broker...2020-06-09T15:17:18.1102274Z Waiting for broker...2020-06-09T15:17:19.9929632Z Waiting for broker...2020-06-09T15:17:21.8607172Z Waiting for broker...2020-06-09T15:17:23.7802949Z Waiting for broker...2020-06-09T15:17:25.6695260Z Waiting for broker...2020-06-09T15:17:27.5536417Z Waiting for broker...2020-06-09T15:17:29.4327778Z Waiting for broker...2020-06-09T15:17:31.3203091Z Waiting for broker...2020-06-09T15:17:33.1987150Z Waiting for broker...2020-06-09T15:17:35.0694860Z Waiting for broker...2020-06-09T15:17:36.9595576Z Waiting for broker...2020-06-09T15:17:38.9243558Z Waiting for broker...2020-06-09T15:17:40.7984064Z Waiting for broker...2020-06-09T15:17:42.6676095Z Waiting for broker...2020-06-09T15:17:44.5628797Z Waiting for broker...2020-06-09T15:17:46.4374532Z Waiting for broker...2020-06-09T15:17:48.3086761Z Waiting for broker...2020-06-09T15:17:50.1574336Z Waiting for broker...2020-06-09T15:17:52.0432952Z Waiting for broker...2020-06-09T15:17:53.9406541Z Waiting for broker...2020-06-09T15:17:55.8162052Z Waiting for broker...2020-06-09T15:17:57.7090015Z Waiting for broker...2020-06-09T15:17:59.5747770Z Waiting for broker...2020-06-09T15:18:01.4601854Z Waiting for broker...2020-06-09T15:18:03.3332039Z Waiting for broker...2020-06-09T15:18:05.2210453Z Waiting for broker...2020-06-09T15:18:07.1133675Z Waiting for broker...2020-06-09T15:18:09.0132417Z Waiting for broker...2020-06-09T15:18:10.8769511Z Waiting for broker...2020-06-09T15:18:12.7601639Z Waiting for broker...2020-06-09T15:18:14.6389770Z Waiting for broker...2020-06-09T15:18:16.5210725Z Waiting for broker...2020-06-09T15:18:18.4088216Z Waiting for broker...2020-06-09T15:18:20.2732225Z Waiting for broker...2020-06-09T15:18:22.1558390Z Waiting for broker...2020-06-09T15:18:24.0400570Z Waiting for broker...2020-06-09T15:18:25.9134038Z Waiting for broker...2020-06-09T15:18:27.7922350Z Waiting for broker...2020-06-09T15:18:29.6748679Z Waiting for broker...2020-06-09T15:18:31.5340996Z Waiting for broker...2020-06-09T15:18:33.3998472Z Waiting for broker...2020-06-09T15:18:35.2718135Z Waiting for broker...2020-06-09T15:18:37.1426082Z Waiting for broker...2020-06-09T15:18:39.1282264Z Waiting for broker...2020-06-09T15:18:41.0029183Z Waiting for broker...2020-06-09T15:18:42.8700037Z Waiting for broker...2020-06-09T15:18:44.7531621Z Waiting for broker...2020-06-09T15:18:46.6465173Z Waiting for broker...2020-06-09T15:18:48.9504192Z Waiting for broker...2020-06-09T15:18:50.4165383Z Waiting for broker...2020-06-09T15:18:52.2931688Z Waiting for broker...2020-06-09T15:18:54.1669857Z Waiting for broker...2020-06-09T15:18:56.0238505Z Waiting for broker...2020-06-09T15:18:57.8931143Z Waiting for broker...2020-06-09T15:18:59.7607751Z Kafka cluster did not start after 120 seconds. Printing Kafka logs:There's a lot of log output I didn't analyze yet.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="18240" opendate="2020-6-10 00:00:00" fixdate="2020-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct modulus function usage in documentation or allow % operator</summary>
      <description>In the documentation: https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/sql/queries.html#scan-projection-and-filterThere is an example:SELECT * FROM Orders WHERE a % 2 = 0But % operator is not allowed in Flink:org.apache.calcite.sql.parser.SqlParseException: Percent remainder '%' is not allowed under the current SQL conformance levelEither we correct the documentation to use MOD function, or allow % operator. This is reported in user-zh ML: http://apache-flink.147419.n8.nabble.com/FLINK-SQL-td3822.html</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.validate.FlinkSqlConformance.java</file>
    </fixedFiles>
  </bug>
  <bug id="18246" opendate="2020-6-11 00:00:00" fixdate="2020-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink e2e fails with java version mismatch on JDK11 nightly build</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3213&amp;view=logs&amp;j=6caf31d6-847a-526e-9624-468e053467d6&amp;t=679407b1-ea2c-5965-2c8d-1467777fff88Preparing transaction: ...working... doneVerifying transaction: ...working... doneExecuting transaction: ...working... doneError: A JNI error has occurred, please check your installation and try againException in thread "main" java.lang.UnsupportedClassVersionError: org/apache/flink/client/cli/CliFrontend has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0 at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) at java.net.URLClassLoader.defineClass(URLClassLoader.java:467) at java.net.URLClassLoader.access$100(URLClassLoader.java:73) at java.net.URLClassLoader$1.run(URLClassLoader.java:368) at java.net.URLClassLoader$1.run(URLClassLoader.java:362) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:361) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:495)No taskexecutor daemon (pid: 123813) is running anymore on fv-az670.No standalonesession daemon to stop on host fv-az670.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
    </fixedFiles>
  </bug>
  <bug id="18248" opendate="2020-6-11 00:00:00" fixdate="2020-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update data type documentation for 1.11</summary>
      <description>Update the data type documentation for 1.11.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.types.md</file>
    </fixedFiles>
  </bug>
  <bug id="18291" opendate="2020-6-15 00:00:00" fixdate="2020-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming File Sink s3 end-to-end test stalls</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3444&amp;view=logs&amp;j=5c8e7682-d68f-54d1-16a2-a09310218a49&amp;t=f508e270-48d6-5f1e-3138-42a17e0714f02020-06-12T21:55:57.6277963Z Number of produced values 10870/600002020-06-12T21:57:10.5467073Z Number of produced values 22960/600002020-06-12T21:58:01.0025226Z Number of produced values 59650/600002020-06-12T21:58:52.5624619Z Number of produced values 60000/600002020-06-12T21:58:53.2407133Z Cancelling job 9412dcb358631ab461a3a1e851417b9e.2020-06-12T21:58:54.0819168Z Cancelled job 9412dcb358631ab461a3a1e851417b9e.2020-06-12T21:58:54.1097745Z Waiting for job (9412dcb358631ab461a3a1e851417b9e) to reach terminal state CANCELED ...2020-06-13T00:00:35.0502923Z ##[error]The operation was canceled.2020-06-13T00:00:35.0522780Z ##[section]Finishing: Run e2e tests</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.file.sink.sh</file>
    </fixedFiles>
  </bug>
  <bug id="18307" opendate="2020-6-15 00:00:00" fixdate="2020-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace "slave" file name with "workers"</summary>
      <description>See parent issue for a discussion of the rationale.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkDistribution.java</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.slaves</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.stop-cluster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-cluster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
      <file type="M">docs.ops.deployment.cluster.setup.zh.md</file>
      <file type="M">docs.ops.deployment.cluster.setup.md</file>
    </fixedFiles>
  </bug>
  <bug id="18323" opendate="2020-6-16 00:00:00" fixdate="2020-11-16 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement a Kafka Source based on new Source API</summary>
      <description>This is an umbrella ticket for a new Kafka Source implementation based on the new Source API.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="18324" opendate="2020-6-16 00:00:00" fixdate="2020-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate updated data type and function page into Chinese</summary>
      <description>The Chinese translations of the pages updated in FLINK-18248 and FLINK-18065 need an update.</description>
      <version>None</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.types.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="18343" opendate="2020-6-17 00:00:00" fixdate="2020-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable DEBUG logging for java e2e tests</summary>
      <description>Java e2e tests run with the default logging configuration, which only logs on INFO.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.main.java.org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResource.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.LocalStandaloneFlinkResource.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkDistribution.java</file>
    </fixedFiles>
  </bug>
  <bug id="18361" opendate="2020-6-18 00:00:00" fixdate="2020-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support username and password options for new Elasticsearch connector</summary>
      <description>We support username and password options for ES connector in FLINK-16788. As we introduce new ES connector in FLINK-17027, we should also add this support to the new ES connector.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.ElasticsearchOptions.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.ElasticsearchConfiguration.java</file>
      <file type="M">docs.dev.table.connectors.elasticsearch.zh.md</file>
      <file type="M">docs.dev.table.connectors.elasticsearch.md</file>
    </fixedFiles>
  </bug>
  <bug id="18368" opendate="2020-6-18 00:00:00" fixdate="2020-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HadoopRecoverableWriterOldHadoopWithNoTruncateSupportTest.createHDFS fails with "Running in secure mode, but config doesn&amp;#39;t have a keytab"</summary>
      <description>https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8184&amp;view=logs&amp;j=66592496-52df-56bb-d03e-37509e1d9d0f&amp;t=ae0269db-6796-5583-2e5f-d84757d711aa</description>
      <version>1.12.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-hadoop-fs.src.test.java.org.apache.flink.runtime.util.HadoopUtilsTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="18373" opendate="2020-6-19 00:00:00" fixdate="2020-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop performance unit tests in RocksDB state backend module</summary>
      <description>As discussed in FLINK-15318, current performance unit tests in RocksDB state backend module have been useless as https://github.com/apache/flink-benchmarks take the role to watch performance.Moreover, due to the unstable CI host environment, those unit tests might also break the whole CI phase sometimes.Thus, it's time to drop those performance unit tests.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.benchmark.RocksDBWriteBatchPerformanceTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.benchmark.RocksDBPerformanceTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.benchmark.RocksDBListStatePerformanceTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="18403" opendate="2020-6-22 00:00:00" fixdate="2020-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure that only exactly once checkpointing can be unaligned</summary>
      <description>Currently, it's possible to configure at least once and unaligned checkpoint enabled with undesired side-effect.We should sanitize the configuration; potentially with warnings.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.InputProcessorUtil.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="18445" opendate="2020-6-29 00:00:00" fixdate="2020-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Short circuit join condition for lookup join</summary>
      <description>Consider the following query:select *from probeleft joinbuild for system_time as of probe.tson probe.key=build.key and probe.col is not nullIn current implementation, we lookup each probe.key in build to decide whether a match is found. A possible optimization is to skip the lookup for rows whose col is null.</description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.LookupJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.FunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.LookupJoinHarnessTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.KeyedLookupJoinHarnessTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.AsyncLookupJoinHarnessTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.join.lookup.LookupJoinWithCalcRunner.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.join.lookup.KeyedLookupJoinWrapper.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.join.lookup.AsyncLookupJoinWithCalcRunner.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.join.lookup.AsyncLookupJoinRunner.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.NonDeterministicDagTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.NonDeterministicDagTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.LookupJoinJsonPlanITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.common.CommonPhysicalLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUpsertKeys.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.optimize.StreamNonDeterministicUpdatePlanVisitor.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLookupJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLookupJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecLookupJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="18461" opendate="2020-7-2 00:00:00" fixdate="2020-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Changelog source can&amp;#39;t be insert into upsert sink</summary>
      <description>CREATE TABLE t_pick_order ( order_no VARCHAR, status INT) WITH ( 'connector' = 'kafka', 'topic' = 'example', 'scan.startup.mode' = 'latest-offset', 'properties.bootstrap.servers' = '172.19.78.32:9092', 'format' = 'canal-json');CREATE TABLE order_status ( order_no VARCHAR, status INT, PRIMARY KEY (order_no) NOT ENFORCED) WITH ( 'connector' = 'jdbc', 'url' = 'jdbc:mysql://xxx:3306/flink_test', 'table-name' = 'order_status', 'username' = 'dev', 'password' = 'xxxx');INSERT INTO order_status SELECT order_no, status FROM t_pick_order ;The above queries throw the following exception:[ERROR] Could not execute SQL statement. Reason:org.apache.flink.table.api.TableException: Provided trait [BEFORE_AND_AFTER] can't satisfy required trait [ONLY_UPDATE_AFTER]. This is a bug in planner, please file an issue. Current node is TableSourceScan(table=[[default_catalog, default_database, t_pick_order]], fields=[order_no, status])It is a bug in planner that we didn't fallback to BEFORE_AND_AFTER trait when ONLY_UPDATE_AFTER can't be satisfied. This results in Changelog source can't be used to written into upsert sink.</description>
      <version>None</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcRowDataOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcRowDataOutputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="18485" opendate="2020-7-4 00:00:00" fixdate="2020-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kerberized YARN per-job on Docker test failed during unzip jce_policy-8.zip</summary>
      <description>Instance on 1.11 branch: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4230&amp;view=logs&amp;j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&amp;t=94459a52-42b6-5bfc-5d74-690b5d3c6de8+ curl -LOH Cookie: oraclelicense=accept-securebackup-cookie http://download.oracle.com/otn-pub/java/jce/8/jce_policy-8.zip % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0100 429 100 429 0 0 1616 0 --:--:-- --:--:-- --:--:-- 1616 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0100 7073 100 7073 0 0 20139 0 --:--:-- --:--:-- --:--:-- 20139+ unzip jce_policy-8.zipArchive: jce_policy-8.zip End-of-central-directory signature not found. Either this file is not a zipfile, or it constitutes one disk of a multi-part archive. In the latter case the central directory and zipfile comment will be found on the last disk(s) of this archive.unzip: cannot find zipfile directory in one of jce_policy-8.zip or jce_policy-8.zip.zip, and cannot find jce_policy-8.zip.ZIP, period.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.Dockerfile</file>
    </fixedFiles>
  </bug>
  <bug id="18486" opendate="2020-7-5 00:00:00" fixdate="2020-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for the &amp;#39;%&amp;#39; modulus function</summary>
      <description>This is a follow-up issue of FLINK-18240 to add documentation for the new system operator %.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.functions.systemFunctions.zh.md</file>
      <file type="M">docs.dev.table.functions.systemFunctions.md</file>
    </fixedFiles>
  </bug>
  <bug id="18532" opendate="2020-7-8 00:00:00" fixdate="2020-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Beta tag from MATCH_RECOGNIZE docs</summary>
      <description></description>
      <version>1.12.0</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.streaming.match.recognize.zh.md</file>
      <file type="M">docs.dev.table.streaming.match.recognize.md</file>
    </fixedFiles>
  </bug>
  <bug id="18600" opendate="2020-7-15 00:00:00" fixdate="2020-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kerberized YARN per-job on Docker test failed to download JDK 8u251</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4514&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529+ mkdir -p /usr/java/default+ curl -Ls https://download.oracle.com/otn-pub/java/jdk/8u251-b08/3d5a2bb8f8d4428bbe94aed7ec7ae784/jdk-8u251-linux-x64.tar.gz -H Cookie: oraclelicense=accept-securebackup-cookie+ tar --strip-components=1 -xz -C /usr/java/default/gzip: stdin: not in gzip formattar: Child returned status 1</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.Dockerfile</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="18619" opendate="2020-7-16 00:00:00" fixdate="2020-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update training to use WatermarkStrategy</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.learn-flink.streaming.analytics.zh.md</file>
      <file type="M">docs.learn-flink.streaming.analytics.md</file>
    </fixedFiles>
  </bug>
  <bug id="18699" opendate="2020-7-24 00:00:00" fixdate="2020-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow variables for column names in Scala Table API</summary>
      <description>User have reported that the Scala API lacks a way to reference columns via a name that is stored in a variable. String interpolation is inconvenient in this case:We should allow this also in Scala:tab.select($(keyVar), $(valueVar))</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-scala.src.test.scala.org.apache.flink.table.api.ExpressionsConsistencyCheckTest.scala</file>
      <file type="M">flink-table.flink-table-api-scala.src.main.scala.org.apache.flink.table.api.ImplicitExpressionConversions.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.WordCountTable.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.StreamSQLExample.scala</file>
    </fixedFiles>
  </bug>
  <bug id="18708" opendate="2020-7-24 00:00:00" fixdate="2020-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The links of the connector sql jar of Kafka 0.10 and 0.11 are extinct</summary>
      <description>The links of the connector sql jar of Kafka 0.10 and 0.11 are extinct. I will fix it as soon as possible.</description>
      <version>1.11.1,1.12.0</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.kafka.zh.md</file>
      <file type="M">docs.dev.table.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="18735" opendate="2020-7-27 00:00:00" fixdate="2020-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance DataGen source to support more types</summary>
      <description>DataGen connector is should support most types natively so it can be used in conjunction with LIKE clause. See https://lists.apache.org/thread.html/r4f9bc51f4da0b14b850f77b59d54f7a7b50d07749aabd2ddb130fc30%40%3Cdev.flink.apache.org%3E</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.DataGenVisitorBase.java</file>
      <file type="M">docs.dev.table.connectors.datagen.md</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.DataGenTableSourceFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.SequenceGeneratorVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.RandomGeneratorVisitor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.datagen.SequenceGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.datagen.RandomGenerator.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.DataGenTableSource.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.DataGenTableSourceFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.datagen.DataGeneratorSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="18752" opendate="2020-7-29 00:00:00" fixdate="2020-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn ship logic should support files</summary>
      <description>The --yarnship / -yt CLI parameter only supports shipping directories. In many cases it would be practical to support shipping single files as well.Is there any good reason why only directories are supported at the moment?</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.FlinkYarnSessionCliTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">docs..includes.generated.yarn.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="18755" opendate="2020-7-29 00:00:00" fixdate="2020-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RabbitMQ QoS Chinese Documentation</summary>
      <description>Please add documentation for the new QoS settings in the RabbitMQ connector. The added English documentation can be found in the PR here: https://github.com/apache/flink/pull/12729/files#diff-6b432359b51642a8fad3050c4b73f47cR134-R167  </description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.rabbitmq.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="18756" opendate="2020-7-30 00:00:00" fixdate="2020-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support IF NOT EXISTS for CREATE TABLE statement</summary>
      <description>Currently, the CREATE TABLE DDL statement doesn't support IF NOT EXISTS. I think this is a useful feature we missed to support, because all the other CREATE DDLs support it. https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/create.html#create-table</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="18760" opendate="2020-7-30 00:00:00" fixdate="2020-8-30 01:00:00" resolution="Done">
    <buginformation>
      <summary>Redundant task managers should be released when there&amp;#39;s no job running in session cluster</summary>
      <description>In FLINK-18625, we introduced redundant task managers, as backup resources for speeding up job recovery in cases of task manager lost.For a session cluster, when there's no job running, it would be better to not keep such redundant resources. Currently, Flink session cluster will not request redundant task managers until a job is submitted, but it also will not release redundant task managers after all jobs terminated.This ticket proposes to check and release task managers if there are redundant task managers only.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TaskManagerCheckInSlotManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="18764" opendate="2020-7-30 00:00:00" fixdate="2020-8-30 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support from_collection for Python DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.utils.python.PythonTableUtils.scala</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.api.common.python.PythonBridgeUtils.java</file>
      <file type="M">flink-python.pyflink.pyflink.gateway.server.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18765" opendate="2020-7-30 00:00:00" fixdate="2020-8-30 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support map() and flat_map() for Python DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamPythonStatelessFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.AbstractPythonStatelessFunctionFlatMap.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coders.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="18766" opendate="2020-7-30 00:00:00" fixdate="2020-8-30 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support add_sink() for Python DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.util.DataStreamTestCollectSink.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.util.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="18773" opendate="2020-7-30 00:00:00" fixdate="2020-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable parallel classloading</summary>
      <description>Currently, user and plugin classloader do not support parallel classloading. We should support that to accelerate job start.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.FlinkUserCodeClassLoader.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.ChildFirstClassLoader.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.plugin.PluginLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="18774" opendate="2020-7-30 00:00:00" fixdate="2020-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support debezium-avro format</summary>
      <description>Debezium+Avro+Confluent Schema Registry is a popular pattern in the industry. It would be great if we can support this. This depends on the implementation of avro-confluent format (FLINK-16048). The format name is up to discuss. I would propose to use debezium-avro-confluent to make it explicitly. As we may support Apicurio Registry in the future.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.debezium.DebeziumJsonFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.debezium.DebeziumJsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.RowDataToAvroConverters.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroRowDataSerializationSchema.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroFileFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.test.resources.debezium-avro-update.txt</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.test.resources.debezium-avro-insert.txt</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.test.resources.debezium-avro-delete.txt</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.test.java.org.apache.flink.formats.avro.registry.confluent.RegistryAvroRowDataSeDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.test.java.org.apache.flink.formats.avro.registry.confluent.RegistryAvroFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.test.java.org.apache.flink.formats.avro.registry.confluent.debezium.DebeziumAvroSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.test.java.org.apache.flink.formats.avro.registry.confluent.debezium.DebeziumAvroFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.java.org.apache.flink.formats.avro.registry.confluent.RegistryAvroFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.java.org.apache.flink.formats.avro.registry.confluent.debezium.DebeziumAvroSerializationSchema.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.java.org.apache.flink.formats.avro.registry.confluent.debezium.DebeziumAvroFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.java.org.apache.flink.formats.avro.registry.confluent.debezium.DebeziumAvroDeserializationSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="18776" opendate="2020-7-31 00:00:00" fixdate="2020-7-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"compile_cron_scala212" failed to compile</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5060&amp;view=logs&amp;j=ed6509f5-1153-558c-557a-5ee0afbcdf24&amp;t=241b1e5e-1a8e-5e6a-469a-a9b8cad87065[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-versions) @ flink-avro-confluent-registry ---[WARNING] Rule 0: org.apache.maven.plugins.enforcer.BannedDependencies failed with message:Found Banned Dependency: com.typesafe:ssl-config-core_2.11:jar:0.3.7Found Banned Dependency: com.typesafe.akka:akka-slf4j_2.11:jar:2.5.21Found Banned Dependency: com.typesafe.akka:akka-actor_2.11:jar:2.5.21Found Banned Dependency: org.scala-lang.modules:scala-java8-compat_2.11:jar:0.7.0Found Banned Dependency: com.typesafe.akka:akka-protobuf_2.11:jar:2.5.21Found Banned Dependency: org.apache.flink:flink-table-api-java-bridge_2.11:jar:1.12-SNAPSHOTFound Banned Dependency: org.apache.flink:flink-table-runtime-blink_2.11:jar:1.12-SNAPSHOTFound Banned Dependency: com.typesafe.akka:akka-stream_2.11:jar:2.5.21Found Banned Dependency: com.github.scopt:scopt_2.11:jar:3.5.0Found Banned Dependency: org.apache.flink:flink-runtime_2.11:jar:1.12-SNAPSHOTFound Banned Dependency: org.scala-lang.modules:scala-parser-combinators_2.11:jar:1.1.1Found Banned Dependency: com.twitter:chill_2.11:jar:0.7.6Found Banned Dependency: org.clapper:grizzled-slf4j_2.11:jar:1.3.2Found Banned Dependency: org.apache.flink:flink-streaming-java_2.11:jar:1.12-SNAPSHOTUse 'mvn dependency:tree' to locate the source of the banned dependencies.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro-confluent-registry.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18789" opendate="2020-7-31 00:00:00" fixdate="2020-3-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use TableEnvironment#executeSql to execute insert statement in sql client</summary>
      <description>Currently, sql client has a lot of logic to execute an insert job, which can be simplified through executeSql method.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.PrintUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.utils.TestTableResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.ProgramTargetDescriptor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18818" opendate="2020-8-5 00:00:00" fixdate="2020-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HadoopRenameCommitterHDFSTest.testCommitOneFile[Override: false] failed with "java.io.IOException: The stream is closed"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5177&amp;view=logs&amp;j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&amp;t=420bd9ec-164e-562e-8947-0dacde3cec912020-08-04T20:56:51.1835382Z [ERROR] testCommitOneFile[Override: false](org.apache.flink.formats.hadoop.bulk.committer.HadoopRenameCommitterHDFSTest) Time elapsed: 0.046 s &lt;&lt;&lt; ERROR!2020-08-04T20:56:51.1835950Z java.io.IOException: The stream is closed2020-08-04T20:56:51.1836413Z at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:118)2020-08-04T20:56:51.1836867Z at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)2020-08-04T20:56:51.1837313Z at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)2020-08-04T20:56:51.1837712Z at java.io.DataOutputStream.flush(DataOutputStream.java:123)2020-08-04T20:56:51.1838116Z at java.io.FilterOutputStream.close(FilterOutputStream.java:158)2020-08-04T20:56:51.1838527Z at org.apache.hadoop.hdfs.DataStreamer.closeStream(DataStreamer.java:987)2020-08-04T20:56:51.1838974Z at org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:839)2020-08-04T20:56:51.1839404Z at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:834)2020-08-04T20:56:51.1839775Z Suppressed: java.io.IOException: The stream is closed2020-08-04T20:56:51.1840184Z at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:118)2020-08-04T20:56:51.1840641Z at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)2020-08-04T20:56:51.1841087Z at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)2020-08-04T20:56:51.1841509Z at java.io.FilterOutputStream.close(FilterOutputStream.java:158)2020-08-04T20:56:51.1841910Z at java.io.FilterOutputStream.close(FilterOutputStream.java:159)2020-08-04T20:56:51.1842207Z ... 3 more</description>
      <version>1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-hadoop-bulk.src.test.java.org.apache.flink.formats.hadoop.bulk.AbstractFileCommitterTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="18831" opendate="2020-8-5 00:00:00" fixdate="2020-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the Python documentation about the operations in Table</summary>
      <description>Currently, there are a few documentation is out of date and should be updated. For example, Python UDTF has been already supported in Python Table API and we could use examples of Python UDTF instead of Java UDTF in the Python doc.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="18849" opendate="2020-8-7 00:00:00" fixdate="2020-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the code tabs of the Flink documents</summary>
      <description>Currently there are some minor problems on the code tabs of the Flink documents: There are some tab labels like `data-lang="Java/Scala"`, which can not be changed synchronously with the label `data-lang="Java"` and `data-lang="Scala"`. Case sensitive. If one code tab has a label `data-lang="java"` and another has the label `data-lang="Java"` in one page. They would not change synchronously. Duplicated content. Many contents in the "Java" tab are the same as the "Scala" tab.I would like to improve the situation by following way:      1. When parsing the label like `data-lang="Java/Scala"`, we can clone the tab content, let one has the label `data-lang="Java"`, another has the label `data-lang="Scala"`.      2. Then force the first character of the data-lang value to be upper case. i.e. if the label is `data-lang="java"`, it will be modified to `data-lang="Java"`.      3. Add a new attribute "data-hide-tabs" to the "tabcontents" to hide the tab headers, so that we can let the text above the codes changes synchronously.      4. Add a new url parameter "code_tab" to set the default code tab to display when entering the page.This way we can remove the duplicated content via merge them into one element with a `data-lang="Java/Scala"` label. And all the tab can be changed synchronously when they are clicked. </description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.page.js.flink.js</file>
      <file type="M">docs.dev.table.types.zh.md</file>
      <file type="M">docs.dev.table.types.md</file>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="18850" opendate="2020-8-7 00:00:00" fixdate="2020-10-7 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add late records dropped metric for row time over windows</summary>
      <description>Currently all the row time over windows in blink planner runtime discards late records silently, it would be good to have a metric about the late records dropping.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.over.RowTimeRangeBoundedPrecedingFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.RowTimeRowsBoundedPrecedingFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.RowTimeRangeBoundedPrecedingFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.AbstractRowTimeUnboundedPrecedingOver.java</file>
    </fixedFiles>
  </bug>
  <bug id="18851" opendate="2020-8-7 00:00:00" fixdate="2020-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add checkpoint type to checkpoint history entries in Web UI</summary>
      <description>It would be helpful to users to better understand checkpointing times, if the type of the checkpoint is displayed in the checkpoint history.Possible types are savepoint, aligned checkpoint, unaligned checkpoint.A possible place can be seen in the screenshot</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointingStatisticsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointStatistics.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.detail.job-checkpoints-detail.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.detail.job-checkpoints-detail.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-checkpoint.ts</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="18866" opendate="2020-8-10 00:00:00" fixdate="2020-8-10 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support filter() operation for Python DataStream API.</summary>
      <description>Support filter() interface for Python DataStream API.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="18902" opendate="2020-8-12 00:00:00" fixdate="2020-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot serve results of asynchronous REST operations in per-job mode</summary>
      <description>Due to the changes introduced with FLINK-18663 a Flink per-job cluster can no longer properly shut down. The problem is that we no longer serve asynchronous results (e.g. resulting from a cancel-with-savepoint operation) while the cluster waits to shut down.In order to solve this problem, Flink needs to serve REST request also while it waits for shutting itself down.</description>
      <version>1.10.2,1.11.2,1.12.0</version>
      <fixedVersion>1.10.2,1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.InFlightRequestTrackerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.InFlightRequestTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.AbstractHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="18934" opendate="2020-8-13 00:00:00" fixdate="2020-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Idle stream does not advance watermark in connected stream</summary>
      <description>Per Flink documents, when a stream is idle, it will allow watermarks of downstream operator to advance. However, when I connect an active data stream with an idle data stream, the output watermark of the CoProcessOperator does not increase.Here's a small test that reproduces the problem.https://github.com/kien-truong/flink-idleness-testing</description>
      <version>1.11.0,1.12.0,1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperatorTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamTaskBuilder.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamTask.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OperatorChainTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorWatermarksTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorLatencyMetricsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamSourceContextIdleDetectionTests.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.MockStreamStatusMaintainer.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperatorLifecycleTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTask.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.BoundedStreamTask.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.operators.StateBootstrapWrapperOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractInput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperatorV2.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.CountingOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.Input.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.Output.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.TimestampedCollector.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.TwoInputStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.AbstractDataOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.RecordWriterOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ChainingOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamIterationTail.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractStreamOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractStreamOperatorV2Test.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.CollectorOutput.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockOutput.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.TwoInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.input.FirstInputOfTwoInput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.input.OneInput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.input.SecondInputOfTwoInput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.output.BroadcastingOutput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.output.CopyingSecondInputOfTwoInputStreamOperatorOutput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.output.FirstInputOfTwoInputStreamOperatorOutput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.output.OneInputStreamOperatorOutput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.output.SecondInputOfTwoInputStreamOperatorOutput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.multipleinput.output.BlackHoleOutput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.over.NonBufferOverWindowOperatorTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.SortingBoundedInputITCase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.eventtime.WatermarkOutputMultiplexer.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonTimestampsAndWatermarksOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSourceContexts.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.TimestampsAndWatermarksOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.streamstatus.StreamStatusMaintainer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.streamstatus.StreamStatusProvider.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.CopyingBroadcastingOutputCollector.java</file>
    </fixedFiles>
  </bug>
  <bug id="18936" opendate="2020-8-13 00:00:00" fixdate="2020-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation about user-defined aggregate functions</summary>
      <description>The documentation needs an update because all functions support the new type inference now.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="18937" opendate="2020-8-13 00:00:00" fixdate="2020-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a "Environment Setup" section to the "Installation" document</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.installation.zh.md</file>
      <file type="M">docs.dev.python.installation.md</file>
    </fixedFiles>
  </bug>
  <bug id="18965" opendate="2020-8-15 00:00:00" fixdate="2020-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ExecutionContextTest.testCatalogs failed with "ClassNotFoundException: org.apache.hadoop.fs.BlockStoragePolicySpi"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5541&amp;view=logs&amp;j=a8bc9173-2af6-5ba8-775c-12063b4f1d54&amp;t=46a16c18-c679-5905-432b-9be5d8e27bc62020-08-14T21:10:09.3503802Z [ERROR] testCatalogs(org.apache.flink.table.client.gateway.local.ExecutionContextTest) Time elapsed: 0.148 s &lt;&lt;&lt; ERROR!2020-08-14T21:10:09.3505006Z org.apache.flink.table.client.gateway.SqlExecutionException: Could not create execution context.2020-08-14T21:10:09.3505856Z at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:870)2020-08-14T21:10:09.3506790Z at org.apache.flink.table.client.gateway.local.ExecutionContextTest.createExecutionContext(ExecutionContextTest.java:324)2020-08-14T21:10:09.3508011Z at org.apache.flink.table.client.gateway.local.ExecutionContextTest.createCatalogExecutionContext(ExecutionContextTest.java:360)2020-08-14T21:10:09.3509273Z at org.apache.flink.table.client.gateway.local.ExecutionContextTest.testCatalogs(ExecutionContextTest.java:133)2020-08-14T21:10:09.3510548Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-08-14T21:10:09.3511496Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-08-14T21:10:09.3512417Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-08-14T21:10:09.3513883Z at java.lang.reflect.Method.invoke(Method.java:498)2020-08-14T21:10:09.3514563Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)2020-08-14T21:10:09.3515604Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2020-08-14T21:10:09.3516643Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)2020-08-14T21:10:09.3517498Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2020-08-14T21:10:09.3518189Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)2020-08-14T21:10:09.3519625Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)2020-08-14T21:10:09.3520621Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)2020-08-14T21:10:09.3521328Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)2020-08-14T21:10:09.3521978Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)2020-08-14T21:10:09.3522787Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)2020-08-14T21:10:09.3523469Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)2020-08-14T21:10:09.3524045Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)2020-08-14T21:10:09.3524652Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2020-08-14T21:10:09.3525307Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)2020-08-14T21:10:09.3526086Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)2020-08-14T21:10:09.3526996Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2020-08-14T21:10:09.3527737Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)2020-08-14T21:10:09.3528564Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2020-08-14T21:10:09.3529381Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2020-08-14T21:10:09.3530153Z at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2020-08-14T21:10:09.3530883Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)2020-08-14T21:10:09.3531641Z Caused by: org.apache.flink.table.catalog.exceptions.CatalogException: Failed to create Hive Metastore client2020-08-14T21:10:09.3532669Z at org.apache.flink.table.catalog.hive.client.HiveShimV230.getHiveMetastoreClient(HiveShimV230.java:52)2020-08-14T21:10:09.3533609Z at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.createMetastoreClient(HiveMetastoreClientWrapper.java:240)2020-08-14T21:10:09.3534606Z at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.&lt;init&gt;(HiveMetastoreClientWrapper.java:71)2020-08-14T21:10:09.3535549Z at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientFactory.create(HiveMetastoreClientFactory.java:35)2020-08-14T21:10:09.3536359Z at org.apache.flink.table.catalog.hive.HiveCatalog.open(HiveCatalog.java:224)2020-08-14T21:10:09.3537715Z at org.apache.flink.table.client.gateway.local.DependencyTest$TestHiveCatalogFactory.createCatalog(DependencyTest.java:276)2020-08-14T21:10:09.3538628Z at org.apache.flink.table.client.gateway.local.ExecutionContext.createCatalog(ExecutionContext.java:378)2020-08-14T21:10:09.3539739Z at org.apache.flink.table.client.gateway.local.ExecutionContext.lambda$null$5(ExecutionContext.java:626)2020-08-14T21:10:09.3540476Z at java.util.HashMap.forEach(HashMap.java:1289)2020-08-14T21:10:09.3541368Z at org.apache.flink.table.client.gateway.local.ExecutionContext.lambda$initializeCatalogs$6(ExecutionContext.java:625)2020-08-14T21:10:09.3542770Z at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:264)2020-08-14T21:10:09.3544224Z at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeCatalogs(ExecutionContext.java:624)2020-08-14T21:10:09.3545180Z at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeTableEnvironment(ExecutionContext.java:523)2020-08-14T21:10:09.3546096Z at org.apache.flink.table.client.gateway.local.ExecutionContext.&lt;init&gt;(ExecutionContext.java:183)2020-08-14T21:10:09.3547025Z at org.apache.flink.table.client.gateway.local.ExecutionContext.&lt;init&gt;(ExecutionContext.java:136)2020-08-14T21:10:09.3547910Z at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:859)2020-08-14T21:10:09.3548486Z ... 28 more2020-08-14T21:10:09.3548911Z Caused by: java.lang.reflect.InvocationTargetException2020-08-14T21:10:09.3549464Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-08-14T21:10:09.3550087Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-08-14T21:10:09.3550860Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-08-14T21:10:09.3551523Z at java.lang.reflect.Method.invoke(Method.java:498)2020-08-14T21:10:09.3552430Z at org.apache.flink.table.catalog.hive.client.HiveShimV230.getHiveMetastoreClient(HiveShimV230.java:50)2020-08-14T21:10:09.3553356Z ... 43 more2020-08-14T21:10:09.3553943Z Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient2020-08-14T21:10:09.3554764Z at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1708)2020-08-14T21:10:09.3555577Z at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:83)2020-08-14T21:10:09.3556451Z at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)2020-08-14T21:10:09.3557374Z at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:89)2020-08-14T21:10:09.3557975Z ... 48 more2020-08-14T21:10:09.3558396Z Caused by: java.lang.reflect.InvocationTargetException2020-08-14T21:10:09.3558941Z at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)2020-08-14T21:10:09.3559664Z at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)2020-08-14T21:10:09.3560509Z at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)2020-08-14T21:10:09.3561281Z at java.lang.reflect.Constructor.newInstance(Constructor.java:423)2020-08-14T21:10:09.3561995Z at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1706)2020-08-14T21:10:09.3564984Z ... 51 more2020-08-14T21:10:09.3565646Z Caused by: MetaException(message:java.lang.NoClassDefFoundError: org/apache/hadoop/fs/BlockStoragePolicySpi)2020-08-14T21:10:09.3566467Z at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:83)2020-08-14T21:10:09.3567399Z at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)2020-08-14T21:10:09.3568474Z at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6891)2020-08-14T21:10:09.3569795Z at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:164)2020-08-14T21:10:09.3570366Z ... 56 more2020-08-14T21:10:09.3570953Z Caused by: java.lang.RuntimeException: java.lang.NoClassDefFoundError: org/apache/hadoop/fs/BlockStoragePolicySpi2020-08-14T21:10:09.3572567Z at org.apache.hadoop.hive.shims.ShimLoader.getHadoopShims(ShimLoader.java:91)2020-08-14T21:10:09.3573320Z at org.apache.hadoop.hive.metastore.ObjectStore.getDataSourceProps(ObjectStore.java:480)2020-08-14T21:10:09.3574237Z at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:279)2020-08-14T21:10:09.3574929Z at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)2020-08-14T21:10:09.3575654Z at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)2020-08-14T21:10:09.3576395Z at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:58)2020-08-14T21:10:09.3577207Z at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)2020-08-14T21:10:09.3578029Z at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)2020-08-14T21:10:09.3578863Z at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)2020-08-14T21:10:09.3579708Z at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)2020-08-14T21:10:09.3580543Z at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:655)2020-08-14T21:10:09.3581365Z at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)2020-08-14T21:10:09.3582048Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-08-14T21:10:09.3582783Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-08-14T21:10:09.3583560Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-08-14T21:10:09.3584236Z at java.lang.reflect.Method.invoke(Method.java:498)2020-08-14T21:10:09.3584931Z at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)2020-08-14T21:10:09.3585749Z at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)2020-08-14T21:10:09.3586543Z at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:79)2020-08-14T21:10:09.3587215Z ... 59 more2020-08-14T21:10:09.3587710Z Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/fs/BlockStoragePolicySpi2020-08-14T21:10:09.3588331Z at java.lang.ClassLoader.defineClass1(Native Method)2020-08-14T21:10:09.3588892Z at java.lang.ClassLoader.defineClass(ClassLoader.java:757)2020-08-14T21:10:09.3589529Z at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)2020-08-14T21:10:09.3590215Z at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)2020-08-14T21:10:09.3590838Z at java.net.URLClassLoader.access$100(URLClassLoader.java:74)2020-08-14T21:10:09.3591457Z at java.net.URLClassLoader$1.run(URLClassLoader.java:369)2020-08-14T21:10:09.3592015Z at java.net.URLClassLoader$1.run(URLClassLoader.java:363)2020-08-14T21:10:09.3592677Z at java.security.AccessController.doPrivileged(Native Method)2020-08-14T21:10:09.3593215Z at java.net.URLClassLoader.findClass(URLClassLoader.java:362)2020-08-14T21:10:09.3593778Z at java.lang.ClassLoader.loadClass(ClassLoader.java:419)2020-08-14T21:10:09.3594353Z at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)2020-08-14T21:10:09.3594945Z at java.lang.ClassLoader.loadClass(ClassLoader.java:352)2020-08-14T21:10:09.3595460Z at java.lang.Class.forName0(Native Method)2020-08-14T21:10:09.3595914Z at java.lang.Class.forName(Class.java:348)2020-08-14T21:10:09.3597121Z at org.apache.hadoop.hive.shims.Hadoop23Shims.&lt;init&gt;(Hadoop23Shims.java:113)2020-08-14T21:10:09.3597790Z at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)2020-08-14T21:10:09.3598487Z at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)2020-08-14T21:10:09.3599362Z at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)2020-08-14T21:10:09.3600126Z at java.lang.reflect.Constructor.newInstance(Constructor.java:423)2020-08-14T21:10:09.3600971Z at java.lang.Class.newInstance(Class.java:442)2020-08-14T21:10:09.3601577Z at org.apache.hadoop.hive.shims.ShimLoader.createShim(ShimLoader.java:130)2020-08-14T21:10:09.3602593Z at org.apache.hadoop.hive.shims.ShimLoader.loadShims(ShimLoader.java:124)2020-08-14T21:10:09.3603611Z at org.apache.hadoop.hive.shims.ShimLoader.getHadoopShims(ShimLoader.java:88)2020-08-14T21:10:09.3604096Z ... 77 more2020-08-14T21:10:09.3604616Z Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.fs.BlockStoragePolicySpi2020-08-14T21:10:09.3605271Z at java.net.URLClassLoader.findClass(URLClassLoader.java:382)2020-08-14T21:10:09.3605867Z at java.lang.ClassLoader.loadClass(ClassLoader.java:419)2020-08-14T21:10:09.3606454Z at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)2020-08-14T21:10:09.3607164Z at java.lang.ClassLoader.loadClass(ClassLoader.java:352)2020-08-14T21:10:09.3608030Z ... 100 more2020-08-14T21:10:09.3608257Z</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18966" opendate="2020-8-15 00:00:00" fixdate="2020-8-15 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support key_by() on ConnectedStreams for Python DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.datastream.runtime.operators.python.DataStreamTwoInputPythonStatelessFunctionOperator.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="18998" opendate="2020-8-19 00:00:00" fixdate="2020-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>No watermark is shown on Flink UI when ProcessingTime is used</summary>
      <description>As stated in the subject, no watermark is shown on Flink UI when ProcessingTime is used, see the attached screenshot. It is better to be more specific, like "Watermarks are only available if EventTime is used." </description>
      <version>1.11.1,1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.pipes.humanize-watermark.pipe.ts</file>
    </fixedFiles>
  </bug>
  <bug id="19013" opendate="2020-8-21 00:00:00" fixdate="2020-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log start/end of state restoration</summary>
      <description>State restoration can take a significant amount of time if the state is large enough, or in special cases like FLINK-19008.It would be useful for debugging if we'd log the start/end of RestoreOperation#restore.</description>
      <version>None</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.AbstractRocksDBRestoreOperation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapRestoreOperation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackendBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="19014" opendate="2020-8-21 00:00:00" fixdate="2020-1-21 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Investigate minimum e2e test duration</summary>
      <description>Currently, the minimum time for an e2e test is roughly 30 seconds.This even applies to tests that start a tiny cluster and only run a WordCount.It would be good to understand what exactly causes these tests to run for such a long time.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19041" opendate="2020-8-25 00:00:00" fixdate="2020-8-25 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add dependency management for ConnectedStream in Python DataStream API.</summary>
      <description>We failed to set merged configurations into DataStreamTwoInputPythonStatelessFunctionOperator when finally generating the StreamGraph.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="19045" opendate="2020-8-25 00:00:00" fixdate="2020-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove obsolete option &amp;#39;taskmanager.network.partition.force-release-on-consumption&amp;#39;</summary>
      <description>This option was a fallback/safeguard option introduced when the fine-grained failover for batch was added.With the fine-grained batch failover, the batch result partitions were no longer immediately cleaned up, but retained for recovery until the scheduler decided that they were no longer needed.The purpose of the flag was to restore the old "immediate cleanup" behavior should it be needed.The batch failover has proven stable and this fallback option can be removed now.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ReleaseOnConsumptionResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestQueueTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NettyShuffleEnvironmentBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.NettyShuffleEnvironmentConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NettyShuffleServiceFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.NettyShuffleEnvironmentOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="1907" opendate="2015-4-17 00:00:00" fixdate="2015-6-17 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Scala Interactive Shell</summary>
      <description>Build an interactive Shell for the Scala api.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils.src.main.scala.org.apache.flink.test.util.FlinkTestBase.scala</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.TestEnvironment.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.MultipleProgramsTestBase.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.AbstractTestBase.java</file>
      <file type="M">flink-staging.pom.xml</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.StreamingMultipleProgramsTestBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.RemoteEnvironment.java</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19070" opendate="2020-8-28 00:00:00" fixdate="2020-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive connector should throw a meaningful exception if user reads/writes ACID tables</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="19077" opendate="2020-8-28 00:00:00" fixdate="2020-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Import process time temporal join operator</summary>
      <description>import TemporalProcessTimeJoinOperator for Processing-Time temporal join.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalTimeJoinOperatorTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalRowTimeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalProcessTimeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalProcessTimeJoinOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.TemporalTableJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.LegacyTemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.TemporalTableJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.LegacyTemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.LegacyTemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.TemporalJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.LegacyTemporalJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecTemporalJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecLegacyTemporalJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableFunctionRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTemporalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTemporalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">docs.dev.table.hive.hive.streaming.zh.md</file>
      <file type="M">docs.dev.table.hive.hive.streaming.md</file>
    </fixedFiles>
  </bug>
  <bug id="19078" opendate="2020-8-28 00:00:00" fixdate="2020-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Import rowtime join temporal operator</summary>
      <description>import TemporalRowTimeJoinOperator for EventTime-Time temporal join.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.TemporalTimeJoinOperatorTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.TemporalRowTimeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.TemporalProcessTimeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalRowTimeJoinOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTemporalJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19079" opendate="2020-8-28 00:00:00" fixdate="2020-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support row time deduplicate operator</summary>
      <description>To convert a insert-only table to versioned table, the recommended way is use deduplicate query as following, the converted versioned_view owns primary key and event time and thus can be a versioned table.CREATE VIEW versioned_rates ASSELECT currency, rate, currency_timeFROM (      SELECT *,      ROW_NUMBER() OVER (PARTITION BY currency -- inferred primary key ORDER BY currency_time  -- the event time  DESC) AS rowNum FROM rates)WHERE rowNum = 1;But currently deduplicate operator only support on process time, this issue aims to support deduplicate on Event time. </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepLastRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepFirstRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepLastRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepFirstRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateFunctionTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepLastRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepFirstRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepLastRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepFirstRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateFunctionHelper.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.DeduplicateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.RankTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeysTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicityTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnUniquenessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecDeduplicateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDeduplicate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecChangelogNormalize.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeys.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19086" opendate="2020-8-28 00:00:00" fixdate="2020-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression 2020-08-27 in globalWindow benchmark</summary>
      <description>http://codespeed.dak8s.net:8000/timeline/?ben=globalWindow&amp;env=2http://codespeed.dak8s.net:8000/timeline/#/?exe=1,3&amp;ben=tumblingWindow&amp;env=2&amp;revs=200&amp;equid=off&amp;quarts=on&amp;extr=on The results started to decrease 2 days before decomissioning of an old jenkins node.The other tests, however, were stable. cc: pnowojski</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.runtime.PojoSerializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19121" opendate="2020-9-2 00:00:00" fixdate="2020-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid accessing HDFS frequently in HiveBulkWriterFactory</summary>
      <description>In HadoopPathBasedBulkWriter, getSize will invoke `FileSystem.exists` and `FileSystem.getFileStatus`, but it is invoked per record.There will be lots of visits to HDFS, may make HDFS pressure too high.</description>
      <version>1.11.1,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="19137" opendate="2020-9-4 00:00:00" fixdate="2020-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Apache Parquet to 1.11.1</summary>
      <description>Apache Parquet 1.11.1 fixed some important issues: https://issues.apache.org/jira/browse/PARQUET-1309 https://issues.apache.org/jira/browse/PARQUET-1510 https://issues.apache.org/jira/browse/PARQUET-1485Now Flink master branch relies parquet 1.10.0, and flink-sql-parquet artifact shaded parquet class files into flink-sql-parquet.jar. So this may lead to direct memory leak in PARQUET-1485 or parquet properties bug in PARQUET-1309 or repeat values with dictionary encoding error in PARQUET-1510. For example in PARQUET-1309:then in Flink:https://github.com/C08061/flink/blob/master/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ParquetInputFormat.java#L166  </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-parquet.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19142" opendate="2020-9-4 00:00:00" fixdate="2020-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Local recovery can be broken if slot hijacking happened during a full restart</summary>
      <description>The ticket originates from this PR discussion.The previous AllocationIDs are used by PreviousAllocationSlotSelectionStrategy to schedule subtasks into the slot where they were previously executed before a failover. If the previous slot (AllocationID) is not available, we do not want subtasks to take previous slots (AllocationIDs) of other subtasks.The MergingSharedSlotProfileRetriever gets all previous AllocationIDs of the bulk from SlotSharingExecutionSlotAllocator but only from the current bulk. The previous AllocationIDs of other bulks stay unknown. Therefore, the current bulk can potentially hijack the previous slots from the preceding bulks. On the other hand the previous AllocationIDs of other tasks should be taken if the other tasks are not going to run at the same time, e.g. not enough resources after failover or other bulks are done.Local recovery can be broken due to this. e.g. when multiple regions of a streaming job are restarted at the same time(due to global failover, or task failover with `full` failover strategy).</description>
      <version>1.12.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.TestExecutionSlotAllocatorFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.MergingSharedSlotProfileRetrieverTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocatorFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.MergingSharedSlotProfileRetrieverFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.ExecutionSlotAllocationContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerComponentsFactoryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultSchedulerComponents.java</file>
    </fixedFiles>
  </bug>
  <bug id="19161" opendate="2020-9-7 00:00:00" fixdate="2020-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port File Sources to FLIP-27 API</summary>
      <description>Porting the File sources to the FLIP-27 API means combining the FileInputFormat from the DataSet Batch API The Monitoring File Source from the DataStream API.The two currently share the same reader code already and partial enumeration code.StructureThe new File Source will have three components: File enumerators that discover the files. File split assigners that decide which reader gets what split File Reader Formats, which deal with the decoding.The main difference between the Bounded (Batch) version and the unbounded (Streaming) version is that the streaming version repeatedly invokes the file enumerator to search for new files.Checkpointing EnumeratorsThe enumerators need to checkpoint the not-yet-assigned splits, plus, if they are in continuous discovery mode (streaming) the paths / timestamps already processed.Checkpointing ReadersThe new File Source needs to ensure that every reader can be checkpointed.Some readers may be able to expose the position in the input file that corresponds to the latest emitted record, but many will not be able to do that due to storing compresses record batches using buffered decoders where exact position information is not accessibleWe therefore suggest to expose a mechanism that combines seekable file offsets and records to read and skip after that offset. In the extreme cases, files can work only with seekable positions or only with records-to-skip. Some sources, like Avro, can have periodic seek points (sync markers) and count records-to-skip after these markers.Efficient and Convenient ReadersTo balance efficiency (batch vectorized reading of ORC / Parquet for vectorized query processing) and convenience (plug in 3-rd party CSV decoder over stream) we offer three abstraction for record readers Bulk Formats that run over a file Path and return a iterable batch at a time (most efficient) File Record formats which read files record-by-record. The source framework hands over a pre-defined-size batch from Split Reader to Record Emitter. Stream Formats that decode an input stream and rely on the source framework to decide how to batch record handover (most convenient)</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperator.java</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.mocks.TestingReaderContext.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SingleThreadMultiplexSourceReaderBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="19162" opendate="2020-9-7 00:00:00" fixdate="2020-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow Split Reader based sources to reuse record batches</summary>
      <description>The Split Readers hand over a batch of records at a time from the I/O thread (fetching and decoding) to the main operator processing thread. These structures can memory intensive and expensive and performance greatly benefits from reusing them. This is especially true for high-performance format readers like ORC and Parquet.While previous sources (where I/O was in the main thread) could reuse objects in a trivial manner, the new Split Reader API (with multiple threads) needs an explicit recycle() hook to allow returning/reusing these objects.</description>
      <version>None</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.SourceReaderBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SplitsRecordIterator.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SourceReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.RecordsWithSplitIds.java</file>
    </fixedFiles>
  </bug>
  <bug id="19163" opendate="2020-9-8 00:00:00" fixdate="2020-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add building py38 wheel package of PyFlink in Azure CI</summary>
      <description>Add building py38 wheel package of PyFlink in Azure CI</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.build-wheels.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19174" opendate="2020-9-9 00:00:00" fixdate="2020-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>idleTimeMsPerSecond can report incorrect values (0 or values more than one second)</summary>
      <description>If task is blocked for more than 60 seconds (org.apache.flink.metrics.MeterView#DEFAULT_TIME_SPAN_IN_SECONDS), idleTimeMsPerSecond can be reported as zero, despite task being completely idle. Once the task is unblocked and the idleTimeMsPerSecond metric is updated, it can for the next 60 seconds exceed 1000ms/second.Average value over the longer periods of time will be correct and accurate.</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.TaskIOMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskIOMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.java</file>
      <file type="M">flink-metrics.flink-metrics-slf4j.src.test.java.org.apache.flink.metrics.slf4j.Slf4jReporterTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="19178" opendate="2020-9-10 00:00:00" fixdate="2020-9-10 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce the memory weights configuration option and interfaces</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.util.OperatorValidationUtils.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.ExecNode.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamNode.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.dag.TransformationTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.dag.Transformation.java</file>
    </fixedFiles>
  </bug>
  <bug id="19179" opendate="2020-9-10 00:00:00" fixdate="2020-9-10 01:00:00" resolution="Done">
    <buginformation>
      <summary>Implement the managed memory fraction calculation logic</summary>
      <description>This also means migrating the batch operator use cases.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.over.BufferDataOverWindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.String2SortMergeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.String2HashJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.RandomSortMergeInnerJoinTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.Int2HashJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.TableStreamOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.BatchAggTestBase.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.ArrowPythonAggregateFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19180" opendate="2020-9-10 00:00:00" fixdate="2020-10-10 01:00:00" resolution="Done">
    <buginformation>
      <summary>Make RocksDB respect the calculated managed memory fraction</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamConfigChainer.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImplTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamOperatorStateHandlerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StateInitializationContextImplTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperatorV2.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.memory.MemoryManager.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.KeyedStateInputFormat.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.BootstrapTransformation.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.dag.TransformationTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.ManagedMemoryUseCase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.over.BufferDataOverWindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.TableStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.config.memory.ManagedMemoryUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateBackendLoader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateBackend.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.TwoInputTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SinkTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.OneInputTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.KeyedMultipleInputTransformation.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.dag.Transformation.java</file>
    </fixedFiles>
  </bug>
  <bug id="19181" opendate="2020-9-10 00:00:00" fixdate="2020-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make python processes respect the calculated managed memory fraction</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonBase.scala</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonAggregateFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.AbstractStreamArrowPythonAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.AbstractBatchArrowPythonAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.PythonOptionsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.PythonConfigTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamTableStatelessPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamTableStatefulPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.AbstractPythonStatelessFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamDataStreamStatelessPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.StatelessTwoInputPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.StatelessOneInputPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonReduceOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonPartitionCustomOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonConfig.java</file>
      <file type="M">flink-python.pyflink.testing.test.case.utils.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">docs..includes.generated.python.configuration.html</file>
      <file type="M">docs.dev.python.user-guide.table.python.table.api.connectors.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.table.api.connectors.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="19182" opendate="2020-9-10 00:00:00" fixdate="2020-11-10 01:00:00" resolution="Done">
    <buginformation>
      <summary>Update document for intra-slot managed memory sharing</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.memory.mem.setup.tm.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.tm.md</file>
    </fixedFiles>
  </bug>
  <bug id="19184" opendate="2020-9-10 00:00:00" fixdate="2020-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Batch Physical Pandas Group Aggregate Rule and RelNode</summary>
      <description>Add Batch Physical Pandas Group Aggregate Rule and RelNode</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedAggFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.PythonUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecSortAggRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecHashAggRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19187" opendate="2020-9-10 00:00:00" fixdate="2020-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a new basic Table API example</summary>
      <description>Add a new basic Table API example that does not require a connector and can be used for simply play around with operators.It should show: How to get started without any prior knowledge. How to work with Table Environment. How to work with Table How to print/collect. How to perform simple ETL and some operations.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.WordCountTable.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.WordCountSQL.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.TPCHQuery3Table.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.StreamTableExample.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.StreamSQLExample.scala</file>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19188" opendate="2020-9-10 00:00:00" fixdate="2020-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a new streaming SQL examples</summary>
      <description>Add a new streaming SQL example. It should work entirely with SQL. It should show:How to get started with a connector (CSV or DataGen?) and event-time.How the DDL works.How to work with Table Environment and SQL.How a complex SQL query looks like. Windows, joins, maybe Top N.How to use pure streaming operators vs. changelog producing operators</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19189" opendate="2020-9-10 00:00:00" fixdate="2020-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable pipelined scheduling by default</summary>
      <description>This task is to enable pipelined region scheduling by default, via setting the default value of config option "jobmanager.scheduler.scheduling-strategy" to "region".Here are the required verifications before we can make this change:1. CI, including all UT/IT cases and E2E tests2. stability tests3. TPC-DS benchmark in 1T/10T data scale</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.misc.SuccessAfterNetworkBuffersFailureITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerComponentsFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.MiniClusterITCase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">flink-end-to-end-tests.flink-tpcds-test.src.main.java.org.apache.flink.table.tpcds.TpcdsTestProgram.java</file>
    </fixedFiles>
  </bug>
  <bug id="19201" opendate="2020-9-11 00:00:00" fixdate="2020-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink e2e tests is instable and failed with "Connection broken: OSError"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6452&amp;view=logs&amp;j=ff2e2ea5-07e3-5521-7b04-a4fc3ad765e9&amp;t=6945d9e3-ebef-5993-0c44-838d8ad079c02020-09-10T21:37:42.9988117Z install conda ... [SUCCESS]2020-09-10T21:37:43.0018449Z install miniconda... [SUCCESS]2020-09-10T21:37:43.0082244Z installing python environment...2020-09-10T21:37:43.0100408Z installing python3.5...2020-09-10T21:37:58.7214400Z install python3.5... [SUCCESS]2020-09-10T21:37:58.7253792Z installing python3.6...2020-09-10T21:38:06.5855143Z install python3.6... [SUCCESS]2020-09-10T21:38:06.5903358Z installing python3.7...2020-09-10T21:38:11.5444706Z 2020-09-10T21:38:11.5484852Z ('Connection broken: OSError("(104, \'ECONNRESET\')")', OSError("(104, 'ECONNRESET')"))2020-09-10T21:38:11.5513130Z 2020-09-10T21:38:11.8044086Z conda install 3.7 failed. You can retry to exec the script.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19247" opendate="2020-9-15 00:00:00" fixdate="2020-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Chinese documentation after removal of Kafka 0.10 and 0.11</summary>
      <description>In FLINK-19152 I removed Kafka 0.10 and 0.11. Most of the Chinese documentation was updated for this but some parts were too hard to fix for me as a non-chinese-speaker.Take a look at the PR for FLINK-19152 to see what changes still need to be applied to the Chinese documentation.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.project-configuration.zh.md</file>
      <file type="M">docs.dev.connectors.kafka.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="19289" opendate="2020-9-18 00:00:00" fixdate="2020-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove pods terminated during JM failover</summary>
      <description>For a senario,During JM is down (no JM is running), a TM down with error (for reasons from the node or TM inner), then an Error pod present there. After one JM recover, it will receive a ADDED event about this pod and do nothing.We should deal with this case in `onAdded` callback properly, I think.cc xintongsong.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriverTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="19304" opendate="2020-9-21 00:00:00" fixdate="2020-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add feature toggle for declarative resource management</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerHATest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.JobMasterBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRuntimeServicesConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRuntimeServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.DefaultSlotPoolFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="19305" opendate="2020-9-21 00:00:00" fixdate="2020-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setup Azure build for running core and flink-tests</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.test.controller.sh</file>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="19308" opendate="2020-9-21 00:00:00" fixdate="2020-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add SlotTracker</summary>
      <description>Move the slot bookkeeping into a separate data-structure.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TestingTaskManagerSlotInformation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.TaskManagerSlotInformation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.TaskManagerSlot.java</file>
    </fixedFiles>
  </bug>
  <bug id="19323" opendate="2020-9-21 00:00:00" fixdate="2020-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Small optimization of network layer record serialization</summary>
      <description>Currently, when serializing a record, the SpanningRecordSerializer will first skip 4 bytes for length and serialize the record. Then it gets the serialized record length and skip back to position 0 and write the length field. After that, it skip again to the tail of the serialized data. In fact, the last two skip can be avoid by writing length field to position 0 directly.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.RecordWriter.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.DataOutputSerializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19324" opendate="2020-9-21 00:00:00" fixdate="2020-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Map requested/allocated containers with priority on YARN</summary>
      <description>In the design doc of FLINK-14106, there was a discussion on how we map allocated containers with the requested ones on YARN. We rejected the design option that uses container priorities for mapping containers of different resources, because we do not want to priorities different container requests (which is the original purpose for this field). As a result, we have to interpret how the requested container request would be normalized by Yarn, and map the allocated/requested containers accordingly, which is complicated and fragile. See also FLINK-19151.Recently in our POC for fine grained resource management, we surprisingly discovered that Yarn actually doesn't work with container requests same priority and different resources. I do not find this described as an official protocol in any Yarn's documents. The issue has been raised in early Yarn versions (YARN-314) and has not been fixed util Hadoop 2.9 when allocationRequestId is introduced. In Hadoop 2.8, Yarn scheduler is still internally using priority as the key of a container request (see AppSchedulingInfo#updateResourceRequests ), thus requests same priority and different resources would overwrite each other.The new discovery suggests that, if we want to support containers with different resources on Hadoop 2.8 and earlier versions, we have to give them different priorities anyway. Thus, I would suggest to get rid of the container normalization simulation and go back to the previously rejected priority based design option.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerDriverTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.TestingRegisterApplicationMasterResponse.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.TaskExecutorProcessSpecContainerResourceAdapterTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManagerDriver.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.TaskExecutorProcessSpecContainerResourceAdapter.java</file>
    </fixedFiles>
  </bug>
  <bug id="19331" opendate="2020-9-21 00:00:00" fixdate="2020-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>State processor api has native resouce leak when working with RocksDB</summary>
      <description>State processor api uses AbstractStateBackend#getKeys and AbstractStateBackend#getKeysAndNamespaces to iterate over keys and namespaces in a savepoint. These methods return java.util.stream.Stream. The RocksDBKeyedStateBackend implemention of these methods use streams onClose callback to free native resources. However, spa eagerly turns this stream into an iterator. This causes the onClose method to be discarded leading to a native resource leak. This can lead to a segmentation fault when multiple spa jobs are submitted to the same session cluster.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.operator.WindowReaderOperator.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.operator.StateReaderOperator.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.operator.KeyedStateReaderOperator.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.MultiStateKeyIterator.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.KeyedStateInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="19339" opendate="2020-9-22 00:00:00" fixdate="2020-10-22 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support Avro&amp;#39;s unions with logical types</summary>
      <description>Avro 1.9.x introduced yet another mechanism for registering/looking up conversions for logical types. See https://issues.apache.org/jira/browse/AVRO-1891If a logical type is part of a union a static field MODEL$ of type SpecificData will be added to the generated Avro class with registered conversions for a logical type. We should use that SpecificData in AvroSerializer and Avro(De)SerializationSchema whenever available.</description>
      <version>None</version>
      <fixedVersion>1.10.3,1.11.3,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.src.test.resources.avro.user.avsc</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.utils.AvroTestUtils.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.typeutils.AvroSerializerSnapshot.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.typeutils.AvroFactory.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroDeserializationSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="19340" opendate="2020-9-22 00:00:00" fixdate="2020-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AggregateITCase.testListAggWithDistinct failed with "expected:&lt;List(1,A, 2,B, 3,C#A, 4,EF)&gt; but was:&lt;List(1,A, 2,B, 3,C#A, 4,EF#EF)&gt;"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6734&amp;view=logs&amp;j=e25d5e7e-2a9c-5589-4940-0b638d75a414&amp;t=a6e0f756-5bb9-5ea8-a468-5f60db442a292020-09-22T04:59:30.1229430Z [ERROR] testListAggWithDistinct[LocalGlobal=ON, MiniBatch=ON, StateBackend=HEAP](org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase) Time elapsed: 0.346 s &lt;&lt;&lt; FAILURE!2020-09-22T04:59:30.1230120Z java.lang.AssertionError: expected:&lt;List(1,A, 2,B, 3,C#A, 4,EF)&gt; but was:&lt;List(1,A, 2,B, 3,C#A, 4,EF#EF)&gt;2020-09-22T04:59:30.1232835Z at org.junit.Assert.fail(Assert.java:88)2020-09-22T04:59:30.1233314Z at org.junit.Assert.failNotEquals(Assert.java:834)2020-09-22T04:59:30.1233688Z at org.junit.Assert.assertEquals(Assert.java:118)2020-09-22T04:59:30.1234034Z at org.junit.Assert.assertEquals(Assert.java:144)2020-09-22T04:59:30.1234528Z at org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.testListAggWithDistinct(AggregateITCase.scala:667)</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.typeutils.RowDataSerializerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.RowDataSerializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19372" opendate="2020-9-23 00:00:00" fixdate="2020-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Pandas Batch Over Window Aggregation</summary>
      <description>We will add Batch Physical Pandas Over Window RelNode, Python Operation and PandasOverWindowFunctionCoder to support Pandas Batch Over Window Aggregation</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecOverAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonOverWindowAggregateFunctionOperator.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udaf.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coders.py</file>
    </fixedFiles>
  </bug>
  <bug id="19388" opendate="2020-9-24 00:00:00" fixdate="2020-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming bucketing end-to-end test failed with "Number of running task managers has not reached 4 within a timeout of 40 sec"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6876&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d3555292020-09-24T03:13:12.1370987Z Starting taskexecutor daemon on host fv-az661.2020-09-24T03:13:12.1773280Z Number of running task managers 2 is not yet 4.2020-09-24T03:13:16.4342638Z Number of running task managers 3 is not yet 4.2020-09-24T03:13:20.4570976Z Number of running task managers 0 is not yet 4.2020-09-24T03:13:24.4762428Z Number of running task managers 0 is not yet 4.2020-09-24T03:13:28.4955622Z Number of running task managers 0 is not yet 4.2020-09-24T03:13:32.5110079Z Number of running task managers 0 is not yet 4.2020-09-24T03:13:36.5272551Z Number of running task managers 0 is not yet 4.2020-09-24T03:13:40.5672343Z Number of running task managers 0 is not yet 4.2020-09-24T03:13:44.5857760Z Number of running task managers 0 is not yet 4.2020-09-24T03:13:48.6039181Z Number of running task managers 0 is not yet 4.2020-09-24T03:13:52.6056222Z Number of running task managers has not reached 4 within a timeout of 40 sec2020-09-24T03:13:52.9275629Z Stopping taskexecutor daemon (pid: 13694) on host fv-az661.2020-09-24T03:13:53.1753734Z No standalonesession daemon (pid: 10610) is running anymore on fv-az661.2020-09-24T03:13:53.5812242Z Skipping taskexecutor daemon (pid: 10912), because it is not running anymore on fv-az661.2020-09-24T03:13:53.5813449Z Stopping taskexecutor daemon (pid: 11330) on host fv-az661.2020-09-24T03:13:53.5818053Z Stopping taskexecutor daemon (pid: 11632) on host fv-az661.2020-09-24T03:13:53.5819341Z Skipping taskexecutor daemon (pid: 11965), because it is not running anymore on fv-az661.2020-09-24T03:13:53.5820870Z Skipping taskexecutor daemon (pid: 12906), because it is not running anymore on fv-az661.2020-09-24T03:13:53.5821698Z Stopping taskexecutor daemon (pid: 13392) on host fv-az661.2020-09-24T03:13:53.5839544Z [FAIL] Test script contains errors.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SharedSlotTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SharedSlot.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandlerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="19392" opendate="2020-9-24 00:00:00" fixdate="2020-10-24 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Detect the execution mode based on the sources in the job.</summary>
      <description>As part of FLIP-134, we introduce the option execution.runtime-mode which can take the values: BATCH, STREAMING, and AUTOMATIC. In case of the latter, the system will scan the sources and detect if the job is to be execute either using batch scheduling or streaming. If all the sources are bounded, the system will go with BATCH, if at least one is unbounded, then the system will go with STREAMING.This issue targets introducing the logic of detecting the runtime mode based on the sources without exposing it yet to the user. The latter will happen in a follow-up issue.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.executor.StreamExecutor.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SourceTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperatorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.delegation.BatchExecutorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.datastream.DataStreamSourceTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.LegacySourceTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="19412" opendate="2020-9-25 00:00:00" fixdate="2020-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-layer Python Operation Make it Possible to Provide only Python implementation</summary>
      <description>Now whenever we introduce a Python Operation, we need to provide the implementation of python version and cython version, but in many cases, we only need to provide the implementation of Python. We need to re-layer Python Operation make it possible to provide only Python implementation.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.sdk.worker.main.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pxd</file>
    </fixedFiles>
  </bug>
  <bug id="19414" opendate="2020-9-25 00:00:00" fixdate="2020-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce ParquetColumnarRowInputFormat</summary>
      <description>Introduce ParquetColumnarRowInputFormat for new FileSource API.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.util.RecyclableIterator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.RowPartitionComputer.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.utils.ParquetWriterUtil.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.ParquetDecimalVector.java</file>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.util.Utils.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.FileSourceParquetColumnarITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.FileSourceTextLinesITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.AbstractFileSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19417" opendate="2020-9-25 00:00:00" fixdate="2020-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the bug of the method from_data_stream in table_environement</summary>
      <description>The parameter fields should be str or expression *, not the current list &amp;#91;str&amp;#93;. And the table_env object passed to the Table object should be Python's TableEnvironment, not Java's TableEnvironment</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="19436" opendate="2020-9-28 00:00:00" fixdate="2020-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TPC-DS end-to-end test (Blink planner) failed during shutdown</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7009&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=2b7514ee-e706-5046-657b-3430666e7bd92020-09-27T22:37:53.2236467Z Stopping taskexecutor daemon (pid: 2992) on host fv-az655.2020-09-27T22:37:53.4450715Z Stopping standalonesession daemon (pid: 2699) on host fv-az655.2020-09-27T22:37:53.8014537Z Skipping taskexecutor daemon (pid: 11173), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8019740Z Skipping taskexecutor daemon (pid: 11561), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8022857Z Skipping taskexecutor daemon (pid: 11849), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8023616Z Skipping taskexecutor daemon (pid: 12180), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8024327Z Skipping taskexecutor daemon (pid: 12950), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8025027Z Skipping taskexecutor daemon (pid: 13472), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8025727Z Skipping taskexecutor daemon (pid: 16577), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8026417Z Skipping taskexecutor daemon (pid: 16959), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8027086Z Skipping taskexecutor daemon (pid: 17250), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8027770Z Skipping taskexecutor daemon (pid: 17601), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8028400Z Stopping taskexecutor daemon (pid: 18438) on host fv-az655.2020-09-27T22:37:53.8029314Z /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/bin/taskmanager.sh: line 99: 18438 Terminated "${FLINK_BIN_DIR}"/flink-daemon.sh $STARTSTOP $ENTRYPOINT "${ARGS[@]}"2020-09-27T22:37:53.8029895Z [FAIL] Test script contains errors.2020-09-27T22:37:53.8032092Z Checking for errors...2020-09-27T22:37:55.3713368Z No errors in log files.2020-09-27T22:37:55.3713935Z Checking for exceptions...2020-09-27T22:37:56.9046391Z No exceptions in log files.2020-09-27T22:37:56.9047333Z Checking for non-empty .out files...2020-09-27T22:37:56.9064402Z No non-empty .out files.2020-09-27T22:37:56.9064859Z 2020-09-27T22:37:56.9065588Z [FAIL] 'TPC-DS end-to-end test (Blink planner)' failed after 16 minutes and 54 seconds! Test exited with exit code 1</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.4,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.sql.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.batch.sql.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19441" opendate="2020-9-28 00:00:00" fixdate="2020-4-28 01:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>Performance regression on 24.09.2020</summary>
      <description>A couple of benchmarks are showing a small performance regression on 24.09.2020:http://codespeed.dak8s.net:8000/timeline/?ben=globalWindow&amp;env=2http://codespeed.dak8s.net:8000/timeline/?ben=tupleKeyBy&amp;env=2</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ConsumableNotifyingResultPartitionWriterDecorator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19445" opendate="2020-9-29 00:00:00" fixdate="2020-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Several tests for HBase connector 1.4 failed with "NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7042&amp;view=logs&amp;j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&amp;t=bfbc6239-57a0-5db0-63f3-41551b4f7d512020-09-28T21:28:29.4171075Z Running org.apache.flink.connector.hbase1.HBaseTablePlanTest2020-09-28T21:28:31.0367584Z Tests run: 5, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.62 sec &lt;&lt;&lt; FAILURE! - in org.apache.flink.connector.hbase1.HBaseTablePlanTest2020-09-28T21:28:31.0368925Z testProjectionPushDown(org.apache.flink.connector.hbase1.HBaseTablePlanTest) Time elapsed: 0.031 sec &lt;&lt;&lt; ERROR!2020-09-28T21:28:31.0369805Z org.apache.flink.table.api.ValidationException: 2020-09-28T21:28:31.0370409Z Unable to create a source for reading table 'default_catalog.default_database.hTable'.2020-09-28T21:28:31.0370707Z 2020-09-28T21:28:31.0370976Z Table options are:2020-09-28T21:28:31.0371204Z 2020-09-28T21:28:31.0371528Z 'connector'='hbase-1.4'2020-09-28T21:28:31.0371871Z 'table-name'='my_table'2020-09-28T21:28:31.0372255Z 'zookeeper.quorum'='localhost:2021'2020-09-28T21:28:31.0372812Z at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:125)2020-09-28T21:28:31.0373359Z at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.buildTableScan(CatalogSourceTable.scala:135)2020-09-28T21:28:31.0373905Z at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.scala:78)2020-09-28T21:28:31.0374390Z at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3492)2020-09-28T21:28:31.0375224Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2415)2020-09-28T21:28:31.0375867Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2102)2020-09-28T21:28:31.0376479Z at org.apache.flink.table.planner.calcite.FlinkPlannerImpl$$anon$1.convertFrom(FlinkPlannerImpl.scala:181)2020-09-28T21:28:31.0377077Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051)2020-09-28T21:28:31.0377593Z at org.apache.flink.table.planner.calcite.FlinkPlannerImpl$$anon$1.convertFrom(FlinkPlannerImpl.scala:181)2020-09-28T21:28:31.0378114Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661)2020-09-28T21:28:31.0378622Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)2020-09-28T21:28:31.0379132Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)2020-09-28T21:28:31.0379872Z at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)2020-09-28T21:28:31.0380477Z at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:196)2020-09-28T21:28:31.0381128Z at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:154)2020-09-28T21:28:31.0381666Z at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:823)2020-09-28T21:28:31.0382264Z at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:795)2020-09-28T21:28:31.0382968Z at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:250)2020-09-28T21:28:31.0383550Z at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)2020-09-28T21:28:31.0384172Z at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:640)2020-09-28T21:28:31.0384700Z at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:346)2020-09-28T21:28:31.0385201Z at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyPlan(TableTestBase.scala:271)2020-09-28T21:28:31.0385717Z at org.apache.flink.connector.hbase1.HBaseTablePlanTest.testProjectionPushDown(HBaseTablePlanTest.java:124)2020-09-28T21:28:31.0386166Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-09-28T21:28:31.0386575Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-09-28T21:28:31.0387257Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-09-28T21:28:31.0387822Z at java.lang.reflect.Method.invoke(Method.java:498)2020-09-28T21:28:31.0388229Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)2020-09-28T21:28:31.0388718Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2020-09-28T21:28:31.0389198Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)2020-09-28T21:28:31.0389745Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2020-09-28T21:28:31.0390262Z at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)2020-09-28T21:28:31.0390732Z at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)2020-09-28T21:28:31.0391179Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-09-28T21:28:31.0391582Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-09-28T21:28:31.0391964Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)2020-09-28T21:28:31.0392382Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)2020-09-28T21:28:31.0393053Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)2020-09-28T21:28:31.0393617Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)2020-09-28T21:28:31.0393997Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)2020-09-28T21:28:31.0394407Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)2020-09-28T21:28:31.0394817Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)2020-09-28T21:28:31.0395211Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)2020-09-28T21:28:31.0395608Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2020-09-28T21:28:31.0396041Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)2020-09-28T21:28:31.0396517Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)2020-09-28T21:28:31.0397026Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2020-09-28T21:28:31.0397512Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)2020-09-28T21:28:31.0398245Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)2020-09-28T21:28:31.0398778Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)2020-09-28T21:28:31.0399251Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)2020-09-28T21:28:31.0399838Z Caused by: java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V2020-09-28T21:28:31.0400340Z at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)2020-09-28T21:28:31.0400756Z at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)2020-09-28T21:28:31.0401304Z at org.apache.flink.connector.hbase1.HBase1DynamicTableFactory.createDynamicTableSource(HBase1DynamicTableFactory.java:113)2020-09-28T21:28:31.0401869Z at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:122)2020-09-28T21:28:31.0402307Z ... 50 more2020-09-28T21:28:31.0402624Z 2020-09-28T21:28:31.0402949Z Running org.apache.flink.connector.hbase1.HBaseDescriptorTest2020-09-28T21:28:31.0416116Z Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 sec - in org.apache.flink.connector.hbase1.HBaseDescriptorTest2020-09-28T21:28:31.4448287Z 2020-09-28T21:28:31.4448950Z Results :2020-09-28T21:28:31.4449082Z 2020-09-28T21:28:31.4449270Z Tests in error: 2020-09-28T21:28:31.4450556Z HBaseDynamicTableFactoryTest.testTableSourceFactory:104-&gt;createTableSource:332 Â» Validation2020-09-28T21:28:31.4451232Z HBaseTableFactoryTest.testTableSourceFactory:101 Â» NoSuchMethod com.google.com...2020-09-28T21:28:31.4451851Z HBaseTablePlanTest.testProjectionPushDown:124 Â» Validation Unable to create a ...</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19448" opendate="2020-9-29 00:00:00" fixdate="2020-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CoordinatedSourceITCase.testEnumeratorReaderCommunication hangs</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7042&amp;view=logs&amp;j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&amp;t=420bd9ec-164e-562e-8947-0dacde3cec912020-09-28T21:40:41.0736918Z [INFO] Running org.apache.flink.connector.base.source.reader.CoordinatedSourceITCase2020-09-28T21:57:23.4590733Z ==============================================================================2020-09-28T21:57:23.4591238Z Process produced no output for 900 seconds.2020-09-28T21:57:23.4591593Z ==============================================================================2020-09-28T21:57:23.4595995Z ==============================================================================2020-09-28T21:57:23.4596439Z The following Java processes are running (JPS)2020-09-28T21:57:23.4596789Z ==============================================================================2020-09-28T21:57:23.4638075Z Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError2020-09-28T21:57:23.6127853Z 21907 surefirebooter2023202237772619676.jar2020-09-28T21:57:23.6128185Z 534 Launcher2020-09-28T21:57:23.6128381Z 24630 Jps2020-09-28T21:57:23.6159852Z ==============================================================================2020-09-28T21:57:23.6160256Z Printing stack trace of Java process 219072020-09-28T21:57:23.6160806Z ==============================================================================2020-09-28T21:57:23.6203860Z Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError2020-09-28T21:57:23.9470219Z 2020-09-28 21:57:232020-09-28T21:57:23.9471512Z Full thread dump OpenJDK 64-Bit Server VM (25.242-b08 mixed mode):2020-09-28T21:57:23.9472274Z 2020-09-28T21:57:23.9472805Z "Attach Listener" #215 daemon prio=9 os_prio=0 tid=0x00007f13c8074800 nid=0x6052 waiting on condition [0x0000000000000000]2020-09-28T21:57:23.9473343Z java.lang.Thread.State: RUNNABLE2020-09-28T21:57:23.9473660Z 2020-09-28T21:57:23.9474554Z "flink-akka.actor.default-dispatcher-103" #214 prio=5 os_prio=0 tid=0x00007f13cc1b5000 nid=0x6018 waiting on condition [0x00007f13bb4f5000]2020-09-28T21:57:23.9475189Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9475815Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9476662Z - parking to wait for &lt;0x0000000087a80408&gt; (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)2020-09-28T21:57:23.9477295Z at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)2020-09-28T21:57:23.9477871Z at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)2020-09-28T21:57:23.9480210Z at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)2020-09-28T21:57:23.9480723Z 2020-09-28T21:57:23.9481669Z "flink-taskexecutor-io-thread-8" #125 daemon prio=5 os_prio=0 tid=0x00007f13e401d000 nid=0x571b waiting on condition [0x00007f13e84f8000]2020-09-28T21:57:23.9482321Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9482727Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9483562Z - parking to wait for &lt;0x0000000087a80860&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9484241Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9484899Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9485585Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:23.9486194Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9486818Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9487440Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9487970Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9488278Z 2020-09-28T21:57:23.9489120Z "flink-taskexecutor-io-thread-7" #124 daemon prio=5 os_prio=0 tid=0x00007f13e401c800 nid=0x571a waiting on condition [0x00007f13b8f46000]2020-09-28T21:57:23.9489760Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9490190Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9491003Z - parking to wait for &lt;0x0000000087a80860&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9491667Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9492821Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9493524Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:23.9494139Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9494754Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9496451Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9496992Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9497201Z 2020-09-28T21:57:23.9498016Z "flink-taskexecutor-io-thread-6" #123 daemon prio=5 os_prio=0 tid=0x00007f13e4019000 nid=0x5719 waiting on condition [0x00007f13e82f6000]2020-09-28T21:57:23.9498481Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9498788Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9500245Z - parking to wait for &lt;0x0000000087a80860&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9500756Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9501473Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9502044Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:23.9502519Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9503017Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9503595Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9503995Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9504209Z 2020-09-28T21:57:23.9504942Z "Flink-DispatcherRestEndpoint-thread-4" #113 daemon prio=5 os_prio=0 tid=0x00007f13f801c800 nid=0x5637 waiting on condition [0x00007f13e88fa000]2020-09-28T21:57:23.9505431Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9505743Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9506407Z - parking to wait for &lt;0x0000000087a80e70&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9506909Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9507460Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9508086Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)2020-09-28T21:57:23.9508870Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-09-28T21:57:23.9509439Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9509922Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9513273Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9513750Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9513951Z 2020-09-28T21:57:23.9514797Z "Flink-DispatcherRestEndpoint-thread-3" #112 daemon prio=5 os_prio=0 tid=0x00007f13dc09c800 nid=0x562c waiting on condition [0x00007f13babf0000]2020-09-28T21:57:23.9515275Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9515582Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9517202Z - parking to wait for &lt;0x0000000087a80e70&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9518111Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9518674Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9519312Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)2020-09-28T21:57:23.9519922Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-09-28T21:57:23.9520489Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9520990Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9521478Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9521891Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9522095Z 2020-09-28T21:57:23.9522967Z "Flink-DispatcherRestEndpoint-thread-2" #110 daemon prio=5 os_prio=0 tid=0x00007f13dc024000 nid=0x561e waiting on condition [0x00007f13e87f9000]2020-09-28T21:57:23.9523467Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9523759Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9524463Z - parking to wait for &lt;0x0000000087a80e70&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9525134Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9525671Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9526313Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)2020-09-28T21:57:23.9526940Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-09-28T21:57:23.9527561Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9528066Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9528570Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9528969Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9529182Z 2020-09-28T21:57:23.9529954Z "SourceCoordinator-Source: TestingSource -&gt; Sink: Unnamed" #107 prio=5 os_prio=0 tid=0x00007f13dc0a9000 nid=0x5611 waiting on condition [0x00007f13ba156000]2020-09-28T21:57:23.9530505Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9530819Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9531489Z - parking to wait for &lt;0x0000000087a81460&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9532000Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9532555Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9533109Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:23.9533600Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9534100Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9534591Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9535012Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9535207Z 2020-09-28T21:57:23.9535569Z "CloseableReaperThread" #105 daemon prio=5 os_prio=0 tid=0x00007f13d0024800 nid=0x560f in Object.wait() [0x00007f13ba257000]2020-09-28T21:57:23.9536032Z java.lang.Thread.State: WAITING (on object monitor)2020-09-28T21:57:23.9536342Z at java.lang.Object.wait(Native Method)2020-09-28T21:57:23.9536920Z - waiting on &lt;0x0000000087a816e8&gt; (a java.lang.ref.ReferenceQueue$Lock)2020-09-28T21:57:23.9537348Z at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)2020-09-28T21:57:23.9537954Z - locked &lt;0x0000000087a816e8&gt; (a java.lang.ref.ReferenceQueue$Lock)2020-09-28T21:57:23.9538370Z at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)2020-09-28T21:57:23.9539931Z at org.apache.flink.core.fs.SafetyNetCloseableRegistry$CloseableReaperThread.run(SafetyNetCloseableRegistry.java:209)2020-09-28T21:57:23.9540324Z 2020-09-28T21:57:23.9541134Z "Source: TestingSource -&gt; Sink: Unnamed (2/4)" #103 prio=5 os_prio=0 tid=0x00007f13d81db000 nid=0x560d waiting on condition [0x00007f13b8e45000]2020-09-28T21:57:23.9542119Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9545357Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9547428Z - parking to wait for &lt;0x0000000087a81890&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9547946Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9548949Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9550987Z at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:146)2020-09-28T21:57:23.9551574Z at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:298)2020-09-28T21:57:23.9552333Z at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:183)2020-09-28T21:57:23.9552915Z at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:594)2020-09-28T21:57:23.9556038Z at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:558)2020-09-28T21:57:23.9556541Z at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)2020-09-28T21:57:23.9556973Z at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)2020-09-28T21:57:23.9557477Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9557695Z 2020-09-28T21:57:23.9561877Z "AkkaRpcService-Supervisor-Termination-Future-Executor-thread-1" #97 daemon prio=5 os_prio=0 tid=0x00007f13e00dc000 nid=0x5607 waiting on condition [0x00007f13b944b000]2020-09-28T21:57:23.9562627Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9565093Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9570829Z - parking to wait for &lt;0x0000000087a81d20&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9575651Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9576427Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9594221Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:23.9598708Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9599269Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9599778Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9600181Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9600393Z 2020-09-28T21:57:23.9601273Z "flink-taskexecutor-io-thread-5" #96 daemon prio=5 os_prio=0 tid=0x00007f13d81da000 nid=0x5606 waiting on condition [0x00007f13b954c000]2020-09-28T21:57:23.9601763Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9602069Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9602803Z - parking to wait for &lt;0x0000000087a80860&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9603323Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9603860Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9604439Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:23.9604935Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9605438Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9605929Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9606352Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9606549Z 2020-09-28T21:57:23.9607253Z "flink-taskexecutor-io-thread-4" #95 daemon prio=5 os_prio=0 tid=0x00007f13d817e800 nid=0x5605 waiting on condition [0x00007f13ba459000]2020-09-28T21:57:23.9607730Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9608020Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9608696Z - parking to wait for &lt;0x0000000087a80860&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9609192Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9609749Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9610323Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:23.9610800Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9611507Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9612018Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9612415Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9612629Z 2020-09-28T21:57:23.9613327Z "flink-taskexecutor-io-thread-3" #94 daemon prio=5 os_prio=0 tid=0x00007f13d817d800 nid=0x5604 waiting on condition [0x00007f13b8d44000]2020-09-28T21:57:23.9613912Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9614219Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9614908Z - parking to wait for &lt;0x0000000087a80860&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9615418Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9615968Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9616525Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:23.9617016Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9617515Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9618002Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9618419Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9618620Z 2020-09-28T21:57:23.9619297Z "flink-taskexecutor-io-thread-2" #93 daemon prio=5 os_prio=0 tid=0x00007f13d817c800 nid=0x5603 waiting on condition [0x00007f13ba65b000]2020-09-28T21:57:23.9620126Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9620429Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9621171Z - parking to wait for &lt;0x0000000087a80860&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9621691Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9622227Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9622795Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:23.9623289Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9623772Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9624276Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9624689Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9624887Z 2020-09-28T21:57:23.9625567Z "flink-taskexecutor-io-thread-1" #92 daemon prio=5 os_prio=0 tid=0x00007f13d8179800 nid=0x5602 waiting on condition [0x00007f13ba75c000]2020-09-28T21:57:23.9626029Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9626340Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9627016Z - parking to wait for &lt;0x0000000087a80860&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9627506Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9628056Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9629291Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:23.9629807Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9630310Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9630817Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9631218Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9631432Z 2020-09-28T21:57:23.9632324Z "Flink-MetricRegistry-thread-1" #63 daemon prio=5 os_prio=0 tid=0x00007f13dc02a000 nid=0x55e3 waiting on condition [0x00007f13bacf1000]2020-09-28T21:57:23.9632818Z java.lang.Thread.State: TIMED_WAITING (parking)2020-09-28T21:57:23.9633136Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9633803Z - parking to wait for &lt;0x0000000087a828a8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9634320Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-09-28T21:57:23.9634992Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)2020-09-28T21:57:23.9635629Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)2020-09-28T21:57:23.9636253Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-09-28T21:57:23.9636809Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9637299Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9637806Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9638220Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9638416Z 2020-09-28T21:57:23.9639071Z "pool-2-thread-1" #60 prio=5 os_prio=0 tid=0x00007f13d8062000 nid=0x55e0 waiting on condition [0x00007f13bb3f4000]2020-09-28T21:57:23.9639501Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9639806Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9642766Z - parking to wait for &lt;0x0000000087a82ae8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9646981Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9653370Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9661523Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)2020-09-28T21:57:23.9677357Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-09-28T21:57:23.9679468Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9682652Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9683181Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9691283Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9691503Z 2020-09-28T21:57:23.9697420Z "jobmanager-future-thread-2" #57 daemon prio=5 os_prio=0 tid=0x00007f13d8057800 nid=0x55dd waiting on condition [0x00007f13bb2f3000]2020-09-28T21:57:23.9697929Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9698233Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9698936Z - parking to wait for &lt;0x0000000087a82d48&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9699453Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9699993Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9700634Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)2020-09-28T21:57:23.9706073Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-09-28T21:57:23.9706660Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9708027Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9708690Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9709275Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9709496Z 2020-09-28T21:57:23.9710354Z "jobmanager-future-thread-1" #52 daemon prio=5 os_prio=0 tid=0x00007f13fc006000 nid=0x55ce waiting on condition [0x00007f13bb1f2000]2020-09-28T21:57:23.9711981Z java.lang.Thread.State: TIMED_WAITING (parking)2020-09-28T21:57:23.9712340Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9713125Z - parking to wait for &lt;0x0000000087a82d48&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9713788Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-09-28T21:57:23.9714363Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)2020-09-28T21:57:23.9714998Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)2020-09-28T21:57:23.9715632Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-09-28T21:57:23.9716545Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9717049Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9717562Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9718752Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9718980Z 2020-09-28T21:57:23.9719773Z "mini-cluster-io-thread-8" #47 daemon prio=5 os_prio=0 tid=0x00007f13f0006800 nid=0x55c9 waiting on condition [0x00007f13bb7f6000]2020-09-28T21:57:23.9720529Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9720867Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9721624Z - parking to wait for &lt;0x0000000087a83170&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9722132Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9722688Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9723258Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:23.9723734Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9724235Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9724748Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9725149Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9725990Z 2020-09-28T21:57:23.9727058Z "mini-cluster-io-thread-7" #46 daemon prio=5 os_prio=0 tid=0x00007f13f0005000 nid=0x55c8 waiting on condition [0x00007f13bb8f7000]2020-09-28T21:57:23.9727545Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9727864Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9728536Z - parking to wait for &lt;0x0000000087a83170&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9729060Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9729614Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9730172Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:23.9730674Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9731176Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9731668Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9732083Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9732279Z 2020-09-28T21:57:23.9733140Z "mini-cluster-io-thread-6" #45 daemon prio=5 os_prio=0 tid=0x00007f13e0006000 nid=0x55c7 waiting on condition [0x00007f13bb9f8000]2020-09-28T21:57:23.9733609Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9733902Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9734576Z - parking to wait for &lt;0x0000000087a83170&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9735083Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9735712Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9749725Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:23.9750270Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9750760Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9751282Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9751701Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9751899Z 2020-09-28T21:57:23.9752776Z "mini-cluster-io-thread-5" #44 daemon prio=5 os_prio=0 tid=0x00007f13e4002800 nid=0x55c6 waiting on condition [0x00007f13bbaf9000]2020-09-28T21:57:23.9753233Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9753540Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9754248Z - parking to wait for &lt;0x0000000087a83170&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9754743Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9755298Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9755868Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:23.9756352Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9756854Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9757357Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9757757Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9757970Z 2020-09-28T21:57:23.9758625Z "mini-cluster-io-thread-4" #43 daemon prio=5 os_prio=0 tid=0x00007f13bd54c000 nid=0x55c5 waiting on condition [0x00007f13bbbfa000]2020-09-28T21:57:23.9759097Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9759406Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9760067Z - parking to wait for &lt;0x0000000087a83170&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9760573Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9761126Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9761689Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:23.9762182Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9762683Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9763174Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9763595Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9763791Z 2020-09-28T21:57:23.9764454Z "mini-cluster-io-thread-3" #42 daemon prio=5 os_prio=0 tid=0x00007f13d801c800 nid=0x55c4 waiting on condition [0x00007f13bbcfb000]2020-09-28T21:57:23.9764922Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9765214Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9765885Z - parking to wait for &lt;0x0000000087a83170&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9766588Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9767126Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9767696Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:23.9769412Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9770047Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9770555Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9770973Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9771170Z 2020-09-28T21:57:23.9771943Z "mini-cluster-io-thread-2" #41 daemon prio=5 os_prio=0 tid=0x00007f13bd547000 nid=0x55c3 waiting on condition [0x00007f13bbdfc000]2020-09-28T21:57:23.9772402Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9772708Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9773371Z - parking to wait for &lt;0x0000000087a83170&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9773881Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9774436Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9775011Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:23.9775487Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9775988Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9776493Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9776892Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9777107Z 2020-09-28T21:57:23.9777787Z "Flink-DispatcherRestEndpoint-thread-1" #39 daemon prio=5 os_prio=0 tid=0x00007f13bd529000 nid=0x55c1 waiting on condition [0x00007f13bbefd000]2020-09-28T21:57:23.9778283Z java.lang.Thread.State: TIMED_WAITING (parking)2020-09-28T21:57:23.9778601Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9779258Z - parking to wait for &lt;0x0000000087a80e70&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9779777Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-09-28T21:57:23.9780336Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)2020-09-28T21:57:23.9780990Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)2020-09-28T21:57:23.9781613Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-09-28T21:57:23.9782178Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9782664Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9783171Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9783581Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9783777Z 2020-09-28T21:57:23.9784443Z "mini-cluster-io-thread-1" #38 daemon prio=5 os_prio=0 tid=0x00007f1414c2c000 nid=0x55c0 waiting on condition [0x00007f13bbffe000]2020-09-28T21:57:23.9784895Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9785201Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9785857Z - parking to wait for &lt;0x0000000087a83170&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9786360Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9787010Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9787581Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:23.9788059Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9788713Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9789309Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9789709Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9789923Z 2020-09-28T21:57:23.9790639Z "flink-rest-server-netty-boss-thread-1" #37 daemon prio=5 os_prio=0 tid=0x00007f13bd523800 nid=0x55bf runnable [0x00007f13c48e6000]2020-09-28T21:57:23.9791090Z java.lang.Thread.State: RUNNABLE2020-09-28T21:57:23.9791576Z at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)2020-09-28T21:57:23.9792121Z at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)2020-09-28T21:57:23.9792575Z at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)2020-09-28T21:57:23.9793006Z at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)2020-09-28T21:57:23.9793799Z - locked &lt;0x0000000087a842d0&gt; (a org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySet)2020-09-28T21:57:23.9794485Z - locked &lt;0x0000000087a842c0&gt; (a java.util.Collections$UnmodifiableSet)2020-09-28T21:57:23.9795075Z - locked &lt;0x0000000087a84278&gt; (a sun.nio.ch.EPollSelectorImpl)2020-09-28T21:57:23.9795464Z at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)2020-09-28T21:57:23.9796024Z at org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)2020-09-28T21:57:23.9796658Z at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:806)2020-09-28T21:57:23.9797222Z at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:454)2020-09-28T21:57:23.9798242Z at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)2020-09-28T21:57:23.9798893Z at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)2020-09-28T21:57:23.9799364Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9799578Z 2020-09-28T21:57:23.9799949Z "IOManager reader thread #1" #31 daemon prio=5 os_prio=0 tid=0x00007f1415151800 nid=0x55bb waiting on condition [0x00007f13c47be000]2020-09-28T21:57:23.9800407Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9800699Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9801928Z - parking to wait for &lt;0x0000000087a84538&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9803039Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9803965Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9804873Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:23.9805730Z at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$ReaderThread.run(IOManagerAsync.java:354)2020-09-28T21:57:23.9806066Z 2020-09-28T21:57:23.9806445Z "IOManager writer thread #1" #30 daemon prio=5 os_prio=0 tid=0x00007f141514f000 nid=0x55ba waiting on condition [0x00007f13c4afa000]2020-09-28T21:57:23.9806912Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9807204Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9807993Z - parking to wait for &lt;0x0000000087a84740&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9808490Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9809192Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:23.9809764Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:23.9810289Z at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$WriterThread.run(IOManagerAsync.java:460)2020-09-28T21:57:23.9810634Z 2020-09-28T21:57:23.9811266Z "Timer-2" #28 daemon prio=5 os_prio=0 tid=0x00007f14150f9000 nid=0x55b9 in Object.wait() [0x00007f13c49e7000]2020-09-28T21:57:23.9811814Z java.lang.Thread.State: TIMED_WAITING (on object monitor)2020-09-28T21:57:23.9812154Z at java.lang.Object.wait(Native Method)2020-09-28T21:57:23.9812694Z - waiting on &lt;0x0000000087a84948&gt; (a java.util.TaskQueue)2020-09-28T21:57:23.9813061Z at java.util.TimerThread.mainLoop(Timer.java:552)2020-09-28T21:57:23.9813613Z - locked &lt;0x0000000087a84948&gt; (a java.util.TaskQueue)2020-09-28T21:57:23.9813947Z at java.util.TimerThread.run(Timer.java:505)2020-09-28T21:57:23.9814173Z 2020-09-28T21:57:23.9814750Z "Timer-1" #26 daemon prio=5 os_prio=0 tid=0x00007f14150f6000 nid=0x55b8 in Object.wait() [0x00007f13c4bfb000]2020-09-28T21:57:23.9815208Z java.lang.Thread.State: TIMED_WAITING (on object monitor)2020-09-28T21:57:23.9815545Z at java.lang.Object.wait(Native Method)2020-09-28T21:57:23.9816051Z - waiting on &lt;0x0000000087a84b28&gt; (a java.util.TaskQueue)2020-09-28T21:57:23.9816415Z at java.util.TimerThread.mainLoop(Timer.java:552)2020-09-28T21:57:23.9817018Z - locked &lt;0x0000000087a84b28&gt; (a java.util.TaskQueue)2020-09-28T21:57:23.9817360Z at java.util.TimerThread.run(Timer.java:505)2020-09-28T21:57:23.9817578Z 2020-09-28T21:57:23.9817929Z "BLOB Server listener at 46483" #22 daemon prio=5 os_prio=0 tid=0x00007f14150f3000 nid=0x55b7 runnable [0x00007f13c4cfc000]2020-09-28T21:57:23.9818357Z java.lang.Thread.State: RUNNABLE2020-09-28T21:57:23.9818684Z at java.net.PlainSocketImpl.socketAccept(Native Method)2020-09-28T21:57:23.9819087Z at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)2020-09-28T21:57:23.9819551Z at java.net.ServerSocket.implAccept(ServerSocket.java:560)2020-09-28T21:57:23.9819936Z at java.net.ServerSocket.accept(ServerSocket.java:528)2020-09-28T21:57:23.9820364Z at org.apache.flink.runtime.blob.BlobServer.run(BlobServer.java:262)2020-09-28T21:57:23.9820621Z 2020-09-28T21:57:23.9821232Z "Timer-0" #23 daemon prio=5 os_prio=0 tid=0x00007f14150a1800 nid=0x55b6 in Object.wait() [0x00007f13c4ffd000]2020-09-28T21:57:23.9821695Z java.lang.Thread.State: TIMED_WAITING (on object monitor)2020-09-28T21:57:23.9822017Z at java.lang.Object.wait(Native Method)2020-09-28T21:57:23.9822540Z - waiting on &lt;0x0000000087a85168&gt; (a java.util.TaskQueue)2020-09-28T21:57:23.9822906Z at java.util.TimerThread.mainLoop(Timer.java:552)2020-09-28T21:57:23.9823440Z - locked &lt;0x0000000087a85168&gt; (a java.util.TaskQueue)2020-09-28T21:57:23.9823789Z at java.util.TimerThread.run(Timer.java:505)2020-09-28T21:57:23.9823999Z 2020-09-28T21:57:23.9824635Z "flink-metrics-scheduler-1" #18 prio=5 os_prio=0 tid=0x00007f1415064800 nid=0x55b2 waiting on condition [0x00007f13e83f7000]2020-09-28T21:57:23.9825085Z java.lang.Thread.State: TIMED_WAITING (sleeping)2020-09-28T21:57:23.9825404Z at java.lang.Thread.sleep(Native Method)2020-09-28T21:57:23.9825820Z at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)2020-09-28T21:57:23.9826362Z at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)2020-09-28T21:57:23.9827122Z at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)2020-09-28T21:57:23.9827547Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9827758Z 2020-09-28T21:57:23.9829451Z "flink-scheduler-1" #14 prio=5 os_prio=0 tid=0x00007f1414c85000 nid=0x55ae waiting on condition [0x00007f13e89fb000]2020-09-28T21:57:23.9829947Z java.lang.Thread.State: TIMED_WAITING (sleeping)2020-09-28T21:57:23.9830402Z at java.lang.Thread.sleep(Native Method)2020-09-28T21:57:23.9830792Z at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)2020-09-28T21:57:23.9831345Z at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)2020-09-28T21:57:23.9831901Z at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)2020-09-28T21:57:23.9832320Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9832599Z 2020-09-28T21:57:23.9832943Z "process reaper" #11 daemon prio=10 os_prio=0 tid=0x00007f13c8030000 nid=0x55ab waiting on condition [0x00007f13e9439000]2020-09-28T21:57:23.9833394Z java.lang.Thread.State: TIMED_WAITING (parking)2020-09-28T21:57:23.9833709Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9834451Z - parking to wait for &lt;0x000000008006e918&gt; (a java.util.concurrent.SynchronousQueue$TransferStack)2020-09-28T21:57:23.9834943Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-09-28T21:57:23.9835465Z at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)2020-09-28T21:57:23.9836000Z at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)2020-09-28T21:57:23.9836502Z at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)2020-09-28T21:57:23.9836982Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)2020-09-28T21:57:23.9837474Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9837981Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9838391Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9838588Z 2020-09-28T21:57:23.9839272Z "surefire-forkedjvm-ping-30s" #10 daemon prio=5 os_prio=0 tid=0x00007f141434a800 nid=0x55a8 waiting on condition [0x00007f13e99c9000]2020-09-28T21:57:23.9839743Z java.lang.Thread.State: TIMED_WAITING (parking)2020-09-28T21:57:23.9840063Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9840727Z - parking to wait for &lt;0x000000008006eb78&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:23.9841241Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-09-28T21:57:23.9841815Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)2020-09-28T21:57:23.9842470Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)2020-09-28T21:57:23.9843078Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-09-28T21:57:23.9843632Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:23.9844245Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:23.9844753Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:23.9845164Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9845360Z 2020-09-28T21:57:23.9846040Z "surefire-forkedjvm-command-thread" #9 daemon prio=5 os_prio=0 tid=0x00007f1414333800 nid=0x55a7 runnable [0x00007f14041f8000]2020-09-28T21:57:23.9846483Z java.lang.Thread.State: RUNNABLE2020-09-28T21:57:23.9846787Z at java.io.FileInputStream.readBytes(Native Method)2020-09-28T21:57:23.9847158Z at java.io.FileInputStream.read(FileInputStream.java:255)2020-09-28T21:57:23.9847566Z at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)2020-09-28T21:57:23.9848006Z at java.io.BufferedInputStream.read(BufferedInputStream.java:265)2020-09-28T21:57:23.9848623Z - locked &lt;0x000000008006ee18&gt; (a java.io.BufferedInputStream)2020-09-28T21:57:23.9849002Z at java.io.DataInputStream.readInt(DataInputStream.java:387)2020-09-28T21:57:23.9849489Z at org.apache.maven.surefire.booter.MasterProcessCommand.decode(MasterProcessCommand.java:115)2020-09-28T21:57:23.9850148Z at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:391)2020-09-28T21:57:23.9850574Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:23.9850786Z 2020-09-28T21:57:23.9851108Z "Service Thread" #8 daemon prio=9 os_prio=0 tid=0x00007f1414202800 nid=0x55a5 runnable [0x0000000000000000]2020-09-28T21:57:23.9851509Z java.lang.Thread.State: RUNNABLE2020-09-28T21:57:23.9851751Z 2020-09-28T21:57:23.9852108Z "C1 CompilerThread1" #7 daemon prio=9 os_prio=0 tid=0x00007f14141ff800 nid=0x55a4 waiting on condition [0x0000000000000000]2020-09-28T21:57:23.9852538Z java.lang.Thread.State: RUNNABLE2020-09-28T21:57:23.9852718Z 2020-09-28T21:57:23.9853073Z "C2 CompilerThread0" #6 daemon prio=9 os_prio=0 tid=0x00007f14141fd000 nid=0x55a3 waiting on condition [0x0000000000000000]2020-09-28T21:57:23.9853482Z java.lang.Thread.State: RUNNABLE2020-09-28T21:57:23.9853684Z 2020-09-28T21:57:23.9854005Z "Signal Dispatcher" #5 daemon prio=9 os_prio=0 tid=0x00007f14141fb000 nid=0x55a2 runnable [0x0000000000000000]2020-09-28T21:57:23.9854406Z java.lang.Thread.State: RUNNABLE2020-09-28T21:57:23.9854585Z 2020-09-28T21:57:23.9854979Z "Surrogate Locker Thread (Concurrent GC)" #4 daemon prio=9 os_prio=0 tid=0x00007f14141f9800 nid=0x55a1 waiting on condition [0x0000000000000000]2020-09-28T21:57:23.9855438Z java.lang.Thread.State: RUNNABLE2020-09-28T21:57:23.9855621Z 2020-09-28T21:57:23.9855962Z "Finalizer" #3 daemon prio=8 os_prio=0 tid=0x00007f14141c9800 nid=0x55a0 in Object.wait() [0x00007f14182f8000]2020-09-28T21:57:23.9856393Z java.lang.Thread.State: WAITING (on object monitor)2020-09-28T21:57:23.9856718Z at java.lang.Object.wait(Native Method)2020-09-28T21:57:23.9857330Z - waiting on &lt;0x000000008006f798&gt; (a java.lang.ref.ReferenceQueue$Lock)2020-09-28T21:57:23.9857741Z at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)2020-09-28T21:57:23.9858369Z - locked &lt;0x000000008006f798&gt; (a java.lang.ref.ReferenceQueue$Lock)2020-09-28T21:57:23.9858783Z at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)2020-09-28T21:57:23.9859202Z at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)2020-09-28T21:57:23.9859468Z 2020-09-28T21:57:23.9859810Z "Reference Handler" #2 daemon prio=10 os_prio=0 tid=0x00007f14141c5000 nid=0x559f in Object.wait() [0x00007f14183f9000]2020-09-28T21:57:23.9860267Z java.lang.Thread.State: WAITING (on object monitor)2020-09-28T21:57:23.9860604Z at java.lang.Object.wait(Native Method)2020-09-28T21:57:23.9860900Z at java.lang.Object.wait(Object.java:502)2020-09-28T21:57:23.9861280Z at java.lang.ref.Reference.tryHandlePending(Reference.java:191)2020-09-28T21:57:23.9861895Z - locked &lt;0x000000008006f950&gt; (a java.lang.ref.Reference$Lock)2020-09-28T21:57:23.9862297Z at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)2020-09-28T21:57:23.9862564Z 2020-09-28T21:57:23.9862875Z "main" #1 prio=5 os_prio=0 tid=0x00007f141400b800 nid=0x5596 waiting on condition [0x00007f141de0b000]2020-09-28T21:57:23.9879457Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:23.9879924Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:23.9880710Z - parking to wait for &lt;0x0000000087a855c8&gt; (a java.util.concurrent.CompletableFuture$Signaller)2020-09-28T21:57:23.9881191Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:23.9881680Z at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)2020-09-28T21:57:23.9882170Z at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)2020-09-28T21:57:23.9882650Z at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)2020-09-28T21:57:23.9883135Z at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)2020-09-28T21:57:23.9883627Z at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:671)2020-09-28T21:57:23.9884338Z at org.apache.flink.streaming.util.TestStreamEnvironment.execute(TestStreamEnvironment.java:81)2020-09-28T21:57:23.9884947Z at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1706)2020-09-28T21:57:23.9885589Z at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1688)2020-09-28T21:57:23.9886253Z at org.apache.flink.connector.base.source.reader.CoordinatedSourceITCase.executeAndVerify(CoordinatedSourceITCase.java:84)2020-09-28T21:57:23.9887031Z at org.apache.flink.connector.base.source.reader.CoordinatedSourceITCase.testEnumeratorReaderCommunication(CoordinatedSourceITCase.java:52)2020-09-28T21:57:23.9887586Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-09-28T21:57:23.9888010Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-09-28T21:57:23.9888533Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-09-28T21:57:23.9888992Z at java.lang.reflect.Method.invoke(Method.java:498)2020-09-28T21:57:23.9889427Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)2020-09-28T21:57:23.9889947Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2020-09-28T21:57:23.9890464Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)2020-09-28T21:57:23.9890962Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2020-09-28T21:57:23.9891430Z at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)2020-09-28T21:57:23.9891821Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-09-28T21:57:23.9892230Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)2020-09-28T21:57:23.9892701Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)2020-09-28T21:57:23.9893193Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)2020-09-28T21:57:23.9893659Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)2020-09-28T21:57:23.9894085Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)2020-09-28T21:57:23.9894512Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)2020-09-28T21:57:23.9894951Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)2020-09-28T21:57:23.9895388Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)2020-09-28T21:57:23.9895827Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-09-28T21:57:23.9896286Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-09-28T21:57:23.9896709Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-09-28T21:57:23.9897093Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2020-09-28T21:57:23.9897553Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)2020-09-28T21:57:23.9898083Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)2020-09-28T21:57:23.9898609Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2020-09-28T21:57:23.9899132Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)2020-09-28T21:57:23.9899683Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2020-09-28T21:57:23.9900239Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2020-09-28T21:57:23.9900748Z at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2020-09-28T21:57:23.9901234Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)2020-09-28T21:57:23.9901507Z 2020-09-28T21:57:23.9901776Z "VM Thread" os_prio=0 tid=0x00007f14141bb800 nid=0x559e runnable 2020-09-28T21:57:23.9901994Z 2020-09-28T21:57:23.9902370Z "Gang worker#0 (Parallel GC Threads)" os_prio=0 tid=0x00007f1414020800 nid=0x5597 runnable 2020-09-28T21:57:23.9902632Z 2020-09-28T21:57:23.9902940Z "Gang worker#1 (Parallel GC Threads)" os_prio=0 tid=0x00007f1414022000 nid=0x5598 runnable 2020-09-28T21:57:23.9903193Z 2020-09-28T21:57:23.9903492Z "G1 Main Concurrent Mark GC Thread" os_prio=0 tid=0x00007f1414046000 nid=0x559c runnable 2020-09-28T21:57:23.9903745Z 2020-09-28T21:57:23.9904062Z "Gang worker#0 (G1 Parallel Marking Threads)" os_prio=0 tid=0x00007f1414047800 nid=0x559d runnable 2020-09-28T21:57:23.9904410Z 2020-09-28T21:57:23.9904710Z "G1 Concurrent Refinement Thread#0" os_prio=0 tid=0x00007f1414028000 nid=0x559b runnable 2020-09-28T21:57:23.9904964Z 2020-09-28T21:57:23.9905265Z "G1 Concurrent Refinement Thread#1" os_prio=0 tid=0x00007f1414026000 nid=0x559a runnable 2020-09-28T21:57:23.9905518Z 2020-09-28T21:57:23.9905816Z "G1 Concurrent Refinement Thread#2" os_prio=0 tid=0x00007f1414024800 nid=0x5599 runnable 2020-09-28T21:57:23.9906077Z 2020-09-28T21:57:23.9906381Z "VM Periodic Task Thread" os_prio=0 tid=0x00007f1414205800 nid=0x55a6 waiting on condition 2020-09-28T21:57:23.9906642Z 2020-09-28T21:57:23.9906849Z JNI global references: 14262020-09-28T21:57:23.9907016Z 2020-09-28T21:57:23.9907295Z ==============================================================================2020-09-28T21:57:23.9907632Z Printing stack trace of Java process 5342020-09-28T21:57:23.9907981Z ==============================================================================2020-09-28T21:57:23.9909557Z Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError2020-09-28T21:57:24.2975554Z 2020-09-28 21:57:242020-09-28T21:57:24.2976240Z Full thread dump OpenJDK 64-Bit Server VM (25.242-b08 mixed mode):2020-09-28T21:57:24.2976578Z 2020-09-28T21:57:24.2983040Z "Attach Listener" #321 daemon prio=9 os_prio=0 tid=0x00007f12fc005800 nid=0x6060 waiting on condition [0x0000000000000000]2020-09-28T21:57:24.2983527Z java.lang.Thread.State: RUNNABLE2020-09-28T21:57:24.2983816Z 2020-09-28T21:57:24.2984551Z "Thread-127" #319 daemon prio=5 os_prio=0 tid=0x00007f130c01d800 nid=0x5595 runnable [0x00007f12e7596000]2020-09-28T21:57:24.2984969Z java.lang.Thread.State: RUNNABLE2020-09-28T21:57:24.2985285Z at java.io.FileInputStream.readBytes(Native Method)2020-09-28T21:57:24.2985638Z at java.io.FileInputStream.read(FileInputStream.java:255)2020-09-28T21:57:24.2986064Z at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)2020-09-28T21:57:24.2986504Z at java.io.BufferedInputStream.read(BufferedInputStream.java:345)2020-09-28T21:57:24.2987176Z - locked &lt;0x00000000def7c180&gt; (a java.lang.UNIXProcess$ProcessPipeInputStream)2020-09-28T21:57:24.2987606Z at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)2020-09-28T21:57:24.2988006Z at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)2020-09-28T21:57:24.2988412Z at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)2020-09-28T21:57:24.2989286Z - locked &lt;0x00000000def81350&gt; (a java.io.InputStreamReader)2020-09-28T21:57:24.2989683Z at java.io.InputStreamReader.read(InputStreamReader.java:184)2020-09-28T21:57:24.2990090Z at java.io.BufferedReader.fill(BufferedReader.java:161)2020-09-28T21:57:24.2990494Z at java.io.BufferedReader.readLine(BufferedReader.java:324)2020-09-28T21:57:24.2991075Z - locked &lt;0x00000000def81350&gt; (a java.io.InputStreamReader)2020-09-28T21:57:24.2991463Z at java.io.BufferedReader.readLine(BufferedReader.java:389)2020-09-28T21:57:24.2991970Z at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamPumper.run(StreamPumper.java:76)2020-09-28T21:57:24.2992319Z 2020-09-28T21:57:24.2992910Z "Thread-126" #318 daemon prio=5 os_prio=0 tid=0x00007f130c01c800 nid=0x5594 runnable [0x00007f12e6f20000]2020-09-28T21:57:24.2993306Z java.lang.Thread.State: RUNNABLE2020-09-28T21:57:24.2993623Z at java.io.FileInputStream.readBytes(Native Method)2020-09-28T21:57:24.2993991Z at java.io.FileInputStream.read(FileInputStream.java:255)2020-09-28T21:57:24.2994639Z at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)2020-09-28T21:57:24.2995083Z at java.io.BufferedInputStream.read(BufferedInputStream.java:345)2020-09-28T21:57:24.2995782Z - locked &lt;0x00000000def7a0c0&gt; (a java.lang.UNIXProcess$ProcessPipeInputStream)2020-09-28T21:57:24.2996197Z at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)2020-09-28T21:57:24.2996610Z at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)2020-09-28T21:57:24.2997012Z at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)2020-09-28T21:57:24.2997729Z - locked &lt;0x00000000def7e748&gt; (a java.io.InputStreamReader)2020-09-28T21:57:24.2998126Z at java.io.InputStreamReader.read(InputStreamReader.java:184)2020-09-28T21:57:24.2998515Z at java.io.BufferedReader.fill(BufferedReader.java:161)2020-09-28T21:57:24.2998920Z at java.io.BufferedReader.readLine(BufferedReader.java:324)2020-09-28T21:57:24.2999510Z - locked &lt;0x00000000def7e748&gt; (a java.io.InputStreamReader)2020-09-28T21:57:24.2999891Z at java.io.BufferedReader.readLine(BufferedReader.java:389)2020-09-28T21:57:24.3000399Z at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamPumper.run(StreamPumper.java:76)2020-09-28T21:57:24.3000741Z 2020-09-28T21:57:24.3001366Z "Thread-125" #317 daemon prio=5 os_prio=0 tid=0x00007f130c01b800 nid=0x5592 waiting on condition [0x00007f12e7495000]2020-09-28T21:57:24.3001814Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:24.3002105Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:24.3002716Z - parking to wait for &lt;0x00000000dee62550&gt; (a java.util.concurrent.Semaphore$NonfairSync)2020-09-28T21:57:24.3003178Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:24.3003714Z at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)2020-09-28T21:57:24.3004379Z at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)2020-09-28T21:57:24.3005061Z at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)2020-09-28T21:57:24.3005589Z at java.util.concurrent.Semaphore.acquire(Semaphore.java:312)2020-09-28T21:57:24.3006158Z at org.apache.maven.plugin.surefire.booterclient.lazytestprovider.TestLessInputStream.awaitNextCommand(TestLessInputStream.java:165)2020-09-28T21:57:24.3006882Z at org.apache.maven.plugin.surefire.booterclient.lazytestprovider.TestLessInputStream.beforeNextCommand(TestLessInputStream.java:136)2020-09-28T21:57:24.3007574Z at org.apache.maven.plugin.surefire.booterclient.lazytestprovider.AbstractCommandStream.read(AbstractCommandStream.java:100)2020-09-28T21:57:24.3012904Z at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamFeeder.feed(StreamFeeder.java:123)2020-09-28T21:57:24.3014525Z at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamFeeder.run(StreamFeeder.java:60)2020-09-28T21:57:24.3015881Z 2020-09-28T21:57:24.3017173Z "ThreadedStreamConsumer" #315 daemon prio=5 os_prio=0 tid=0x00007f130c01a800 nid=0x5590 waiting on condition [0x00007f12e7798000]2020-09-28T21:57:24.3019315Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:24.3021382Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:24.3022314Z - parking to wait for &lt;0x00000000def45660&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:24.3022827Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:24.3023379Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:24.3024066Z at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)2020-09-28T21:57:24.3024632Z at org.apache.maven.plugin.surefire.booterclient.output.ThreadedStreamConsumer$Pumper.run(ThreadedStreamConsumer.java:83)2020-09-28T21:57:24.3025279Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:24.3025479Z 2020-09-28T21:57:24.3026200Z "surefire-fork-starter" #314 daemon prio=5 os_prio=0 tid=0x00007f12d17af800 nid=0x558f in Object.wait() [0x00007f12e789c000]2020-09-28T21:57:24.3026677Z java.lang.Thread.State: WAITING (on object monitor)2020-09-28T21:57:24.3026990Z at java.lang.Object.wait(Native Method)2020-09-28T21:57:24.3027525Z - waiting on &lt;0x00000000def77e48&gt; (a java.lang.UNIXProcess)2020-09-28T21:57:24.3027875Z at java.lang.Object.wait(Object.java:502)2020-09-28T21:57:24.3028358Z at java.lang.UNIXProcess.waitFor(UNIXProcess.java:395)2020-09-28T21:57:24.3029123Z - locked &lt;0x00000000def77e48&gt; (a java.lang.UNIXProcess)2020-09-28T21:57:24.3029639Z at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.CommandLineUtils$1.call(CommandLineUtils.java:260)2020-09-28T21:57:24.3030222Z at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:614)2020-09-28T21:57:24.3030775Z at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)2020-09-28T21:57:24.3031318Z at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:444)2020-09-28T21:57:24.3031840Z at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:420)2020-09-28T21:57:24.3032609Z at java.util.concurrent.FutureTask.run(FutureTask.java:266)2020-09-28T21:57:24.3033081Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2020-09-28T21:57:24.3033580Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:24.3033994Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:24.3034191Z 2020-09-28T21:57:24.3034882Z "ping-timer-10s" #312 daemon prio=5 os_prio=0 tid=0x00007f132e852000 nid=0x558e waiting on condition [0x00007f12e7697000]2020-09-28T21:57:24.3035344Z java.lang.Thread.State: TIMED_WAITING (parking)2020-09-28T21:57:24.3035645Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:24.3036329Z - parking to wait for &lt;0x00000000de6d84d8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:24.3036845Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-09-28T21:57:24.3037402Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)2020-09-28T21:57:24.3038051Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)2020-09-28T21:57:24.3038675Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-09-28T21:57:24.3039223Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:24.3039721Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:24.3040225Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:24.3040629Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:24.3040840Z 2020-09-28T21:57:24.3041480Z "timeout-check-timer" #311 daemon prio=5 os_prio=0 tid=0x00007f132e851000 nid=0x558d waiting on condition [0x00007f12e6c1d000]2020-09-28T21:57:24.3041950Z java.lang.Thread.State: TIMED_WAITING (parking)2020-09-28T21:57:24.3042265Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:24.3042922Z - parking to wait for &lt;0x00000000de6d8db8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:24.3043442Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-09-28T21:57:24.3044008Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)2020-09-28T21:57:24.3044639Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)2020-09-28T21:57:24.3045263Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-09-28T21:57:24.3045942Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:24.3046428Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:24.3046930Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:24.3047341Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:24.3047602Z 2020-09-28T21:57:24.3047942Z "process reaper" #289 daemon prio=10 os_prio=0 tid=0x00007f12e001a000 nid=0x54b7 runnable [0x00007f12e705a000]2020-09-28T21:57:24.3048333Z java.lang.Thread.State: RUNNABLE2020-09-28T21:57:24.3048666Z at java.lang.UNIXProcess.waitForProcessExit(Native Method)2020-09-28T21:57:24.3049066Z at java.lang.UNIXProcess.lambda$initStreams$3(UNIXProcess.java:289)2020-09-28T21:57:24.3049461Z at java.lang.UNIXProcess$$Lambda$7/59217951.run(Unknown Source)2020-09-28T21:57:24.3049901Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2020-09-28T21:57:24.3050408Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:24.3050806Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:24.3051017Z 2020-09-28T21:57:24.3051669Z "resolver-5" #16 daemon prio=5 os_prio=0 tid=0x00007f132df24800 nid=0x230 waiting on condition [0x00007f12e79b5000]2020-09-28T21:57:24.3054788Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:24.3055141Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:24.3055926Z - parking to wait for &lt;0x0000000094fd64f0&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:24.3056437Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:24.3056977Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:24.3057552Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:24.3058045Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:24.3058542Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:24.3059031Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:24.3059447Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:24.3059648Z 2020-09-28T21:57:24.3060278Z "resolver-4" #15 daemon prio=5 os_prio=0 tid=0x00007f132df24000 nid=0x22f waiting on condition [0x00007f12e7cfd000]2020-09-28T21:57:24.3060704Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:24.3061011Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:24.3061728Z - parking to wait for &lt;0x0000000094fd64f0&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:24.3062221Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:24.3062780Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:24.3063349Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:24.3063826Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:24.3064326Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:24.3064834Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:24.3065235Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:24.3065444Z 2020-09-28T21:57:24.3066063Z "resolver-3" #13 daemon prio=5 os_prio=0 tid=0x00007f132dc18000 nid=0x22d waiting on condition [0x00007f12e7dfe000]2020-09-28T21:57:24.3066506Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:24.3066811Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:24.3067648Z - parking to wait for &lt;0x0000000094fd64f0&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:24.3068156Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:24.3069049Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:24.3069624Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:24.3070243Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:24.3070744Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:24.3071237Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:24.3071652Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:24.3071850Z 2020-09-28T21:57:24.3072543Z "resolver-2" #12 daemon prio=5 os_prio=0 tid=0x00007f132dac7000 nid=0x22c waiting on condition [0x00007f12f04e5000]2020-09-28T21:57:24.3072997Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:24.3073289Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:24.3073967Z - parking to wait for &lt;0x0000000094fd64f0&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:24.3074477Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:24.3075016Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:24.3075591Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:24.3076083Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:24.3076568Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:24.3077069Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:24.3077483Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:24.3077681Z 2020-09-28T21:57:24.3078303Z "resolver-1" #11 daemon prio=5 os_prio=0 tid=0x00007f132dc04800 nid=0x22b waiting on condition [0x00007f12f05e6000]2020-09-28T21:57:24.3078733Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:24.3079039Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:24.3079710Z - parking to wait for &lt;0x0000000094fd64f0&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-09-28T21:57:24.3080208Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:24.3080757Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-09-28T21:57:24.3081323Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-09-28T21:57:24.3081799Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-09-28T21:57:24.3082300Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-09-28T21:57:24.3082801Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-28T21:57:24.3083200Z at java.lang.Thread.run(Thread.java:748)2020-09-28T21:57:24.3083410Z 2020-09-28T21:57:24.3083732Z "Service Thread" #7 daemon prio=9 os_prio=0 tid=0x00007f132c0bc000 nid=0x227 runnable [0x0000000000000000]2020-09-28T21:57:24.3084136Z java.lang.Thread.State: RUNNABLE2020-09-28T21:57:24.3084322Z 2020-09-28T21:57:24.3084678Z "C1 CompilerThread1" #6 daemon prio=9 os_prio=0 tid=0x00007f132c0b9000 nid=0x226 waiting on condition [0x0000000000000000]2020-09-28T21:57:24.3085104Z java.lang.Thread.State: RUNNABLE2020-09-28T21:57:24.3085287Z 2020-09-28T21:57:24.3085639Z "C2 CompilerThread0" #5 daemon prio=9 os_prio=0 tid=0x00007f132c0b6000 nid=0x225 waiting on condition [0x0000000000000000]2020-09-28T21:57:24.3086046Z java.lang.Thread.State: RUNNABLE2020-09-28T21:57:24.3086350Z 2020-09-28T21:57:24.3086673Z "Signal Dispatcher" #4 daemon prio=9 os_prio=0 tid=0x00007f132c0b4800 nid=0x224 runnable [0x0000000000000000]2020-09-28T21:57:24.3087079Z java.lang.Thread.State: RUNNABLE2020-09-28T21:57:24.3087258Z 2020-09-28T21:57:24.3087594Z "Finalizer" #3 daemon prio=8 os_prio=0 tid=0x00007f132c084800 nid=0x223 in Object.wait() [0x00007f131c5f4000]2020-09-28T21:57:24.3088033Z java.lang.Thread.State: WAITING (on object monitor)2020-09-28T21:57:24.3088347Z at java.lang.Object.wait(Native Method)2020-09-28T21:57:24.3089023Z - waiting on &lt;0x00000000951fb148&gt; (a java.lang.ref.ReferenceQueue$Lock)2020-09-28T21:57:24.3089446Z at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)2020-09-28T21:57:24.3090057Z - locked &lt;0x00000000951fb148&gt; (a java.lang.ref.ReferenceQueue$Lock)2020-09-28T21:57:24.3090471Z at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)2020-09-28T21:57:24.3090890Z at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)2020-09-28T21:57:24.3091164Z 2020-09-28T21:57:24.3092959Z "Reference Handler" #2 daemon prio=10 os_prio=0 tid=0x00007f132c080000 nid=0x222 in Object.wait() [0x00007f131c6f5000]2020-09-28T21:57:24.3094988Z java.lang.Thread.State: WAITING (on object monitor)2020-09-28T21:57:24.3095317Z at java.lang.Object.wait(Native Method)2020-09-28T21:57:24.3095981Z - waiting on &lt;0x00000000951fb188&gt; (a java.lang.ref.Reference$Lock)2020-09-28T21:57:24.3096349Z at java.lang.Object.wait(Object.java:502)2020-09-28T21:57:24.3096743Z at java.lang.ref.Reference.tryHandlePending(Reference.java:191)2020-09-28T21:57:24.3097345Z - locked &lt;0x00000000951fb188&gt; (a java.lang.ref.Reference$Lock)2020-09-28T21:57:24.3097756Z at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)2020-09-28T21:57:24.3098015Z 2020-09-28T21:57:24.3098337Z "main" #1 prio=5 os_prio=0 tid=0x00007f132c00c800 nid=0x21e waiting on condition [0x00007f13351b9000]2020-09-28T21:57:24.3098744Z java.lang.Thread.State: WAITING (parking)2020-09-28T21:57:24.3099042Z at sun.misc.Unsafe.park(Native Method)2020-09-28T21:57:24.3099626Z - parking to wait for &lt;0x00000000dea8eb00&gt; (a java.util.concurrent.FutureTask)2020-09-28T21:57:24.3100069Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-09-28T21:57:24.3100498Z at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:429)2020-09-28T21:57:24.3100920Z at java.util.concurrent.FutureTask.get(FutureTask.java:191)2020-09-28T21:57:24.3101412Z at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:476)2020-09-28T21:57:24.3101997Z at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:457)2020-09-28T21:57:24.3102558Z at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:298)2020-09-28T21:57:24.3103084Z at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)2020-09-28T21:57:24.3103631Z at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)2020-09-28T21:57:24.3104266Z at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)2020-09-28T21:57:24.3104884Z at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)2020-09-28T21:57:24.3105442Z at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)2020-09-28T21:57:24.3105989Z at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)2020-09-28T21:57:24.3106492Z at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)2020-09-28T21:57:24.3106984Z at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)2020-09-28T21:57:24.3107543Z at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)2020-09-28T21:57:24.3108148Z at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)2020-09-28T21:57:24.3109086Z at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)2020-09-28T21:57:24.3110696Z at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)2020-09-28T21:57:24.3111201Z at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)2020-09-28T21:57:24.3111622Z at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)2020-09-28T21:57:24.3112176Z at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)2020-09-28T21:57:24.3112582Z at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)2020-09-28T21:57:24.3112967Z at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)2020-09-28T21:57:24.3113351Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-09-28T21:57:24.3113783Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-09-28T21:57:24.3114293Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-09-28T21:57:24.3114747Z at java.lang.reflect.Method.invoke(Method.java:498)2020-09-28T21:57:24.3115196Z at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)2020-09-28T21:57:24.3115685Z at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)2020-09-28T21:57:24.3116188Z at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)2020-09-28T21:57:24.3116695Z at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)2020-09-28T21:57:24.3116964Z 2020-09-28T21:57:24.3117226Z "VM Thread" os_prio=0 tid=0x00007f132c076000 nid=0x221 runnable 2020-09-28T21:57:24.3117440Z 2020-09-28T21:57:24.3117732Z "GC task thread#0 (ParallelGC)" os_prio=0 tid=0x00007f132c022000 nid=0x21f runnable 2020-09-28T21:57:24.3117979Z 2020-09-28T21:57:24.3118270Z "GC task thread#1 (ParallelGC)" os_prio=0 tid=0x00007f132c023800 nid=0x220 runnable 2020-09-28T21:57:24.3118524Z 2020-09-28T21:57:24.3118824Z "VM Periodic Task Thread" os_prio=0 tid=0x00007f132c0be800 nid=0x228 waiting on condition 2020-09-28T21:57:24.3119083Z 2020-09-28T21:57:24.3119286Z JNI global references: 853</description>
      <version>1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.log4j.properties</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.SourceReaderBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.mocks.MockSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SourceReaderBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="19458" opendate="2020-9-29 00:00:00" fixdate="2020-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ZooKeeperLeaderElectionITCase.testJobExecutionOnClusterWithLeaderChange: ZooKeeper unexpectedly modified</summary>
      <description>https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8422&amp;view=logs&amp;j=70ad9b63-500e-5dc9-5a3c-b60356162d7e&amp;t=944c7023-8984-5aa2-b5f8-54922bd90d3a2020-09-29T13:34:18.1803081Z [ERROR] testJobExecutionOnClusterWithLeaderChange(org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase) Time elapsed: 23.524 s &lt;&lt;&lt; ERROR!2020-09-29T13:34:18.1803707Z java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit job.2020-09-29T13:34:18.1804343Z at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)2020-09-29T13:34:18.1804738Z at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)2020-09-29T13:34:18.1805274Z at org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.testJobExecutionOnClusterWithLeaderChange(ZooKeeperLeaderElectionITCase.java:117)2020-09-29T13:34:18.1805772Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-09-29T13:34:18.1806136Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-09-29T13:34:18.1806555Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-09-29T13:34:18.1806936Z at java.lang.reflect.Method.invoke(Method.java:498)2020-09-29T13:34:18.1807313Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)2020-09-29T13:34:18.1807731Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2020-09-29T13:34:18.1808341Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)2020-09-29T13:34:18.1808973Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2020-09-29T13:34:18.1809376Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-09-29T13:34:18.1809851Z at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)2020-09-29T13:34:18.1810201Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-09-29T13:34:18.1810632Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)2020-09-29T13:34:18.1811035Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)2020-09-29T13:34:18.1811700Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)2020-09-29T13:34:18.1812082Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)2020-09-29T13:34:18.1812447Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)2020-09-29T13:34:18.1812824Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)2020-09-29T13:34:18.1813190Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)2020-09-29T13:34:18.1813565Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)2020-09-29T13:34:18.1813964Z at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2020-09-29T13:34:18.1814364Z at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2020-09-29T13:34:18.1814752Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2020-09-29T13:34:18.1815298Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)2020-09-29T13:34:18.1816096Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)2020-09-29T13:34:18.1816552Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2020-09-29T13:34:18.1816984Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)2020-09-29T13:34:18.1817421Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2020-09-29T13:34:18.1817894Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2020-09-29T13:34:18.1818318Z at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2020-09-29T13:34:18.1818888Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)2020-09-29T13:34:18.1819294Z Suppressed: org.apache.flink.util.FlinkException: Could not close resource.2020-09-29T13:34:18.1819698Z at org.apache.flink.util.AutoCloseableAsync.close(AutoCloseableAsync.java:42)2020-09-29T13:34:18.1820260Z at org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.testJobExecutionOnClusterWithLeaderChange(ZooKeeperLeaderElectionITCase.java:136)2020-09-29T13:34:18.1820678Z ... 30 more2020-09-29T13:34:18.1821326Z Caused by: org.apache.flink.runtime.rpc.exceptions.FencingTokenException: Fencing token not set: Ignoring message LocalFencedMessage(null, LocalRpcInvocation(deregisterApplication(ApplicationStatus, String))) sent to akka://flink/user/rpc/resourcemanager_4 because the fencing token is null.2020-09-29T13:34:18.1822143Z at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:63)2020-09-29T13:34:18.1822621Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)2020-09-29T13:34:18.1823024Z at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)2020-09-29T13:34:18.1823397Z at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)2020-09-29T13:34:18.1823776Z at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)2020-09-29T13:34:18.1824306Z at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)2020-09-29T13:34:18.1824686Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)2020-09-29T13:34:18.1825066Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)2020-09-29T13:34:18.1825528Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)2020-09-29T13:34:18.1825883Z at akka.actor.Actor$class.aroundReceive(Actor.scala:517)2020-09-29T13:34:18.1826238Z at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)2020-09-29T13:34:18.1826579Z at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)2020-09-29T13:34:18.1826912Z at akka.actor.ActorCell.invoke(ActorCell.scala:561)2020-09-29T13:34:18.1827302Z at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)2020-09-29T13:34:18.1827609Z at akka.dispatch.Mailbox.run(Mailbox.scala:225)2020-09-29T13:34:18.1827912Z at akka.dispatch.Mailbox.exec(Mailbox.scala:235)2020-09-29T13:34:18.1828252Z at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)2020-09-29T13:34:18.1828629Z at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)2020-09-29T13:34:18.1829062Z at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)2020-09-29T13:34:18.1829468Z at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)2020-09-29T13:34:18.1829871Z Caused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit job.2020-09-29T13:34:18.1830321Z at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$internalSubmitJob$2(Dispatcher.java:348)2020-09-29T13:34:18.1830936Z at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)2020-09-29T13:34:18.1831356Z at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)2020-09-29T13:34:18.1831797Z at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)2020-09-29T13:34:18.1832231Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2020-09-29T13:34:18.1832645Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-09-29T13:34:18.1833180Z at java.lang.Thread.run(Thread.java:748)2020-09-29T13:34:18.1833761Z Caused by: java.util.ConcurrentModificationException: ZooKeeper unexpectedly modified2020-09-29T13:34:18.1834413Z at org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStore.addAndLock(ZooKeeperStateHandleStore.java:158)2020-09-29T13:34:18.1834954Z at org.apache.flink.runtime.jobmanager.ZooKeeperJobGraphStore.putJobGraph(ZooKeeperJobGraphStore.java:228)2020-09-29T13:34:18.1835452Z at org.apache.flink.runtime.dispatcher.Dispatcher.persistAndRunJob(Dispatcher.java:356)2020-09-29T13:34:18.1835943Z at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$waitForTerminatingJob$28(Dispatcher.java:827)2020-09-29T13:34:18.1836432Z at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedConsumer$3(FunctionUtils.java:94)2020-09-29T13:34:18.1837422Z at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670)2020-09-29T13:34:18.1837857Z at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)2020-09-29T13:34:18.1838282Z at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)2020-09-29T13:34:18.1838730Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)2020-09-29T13:34:18.1839184Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)2020-09-29T13:34:18.1839647Z at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)2020-09-29T13:34:18.1840123Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)2020-09-29T13:34:18.1840531Z at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)2020-09-29T13:34:18.1840895Z at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)2020-09-29T13:34:18.1841272Z at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)2020-09-29T13:34:18.1841659Z at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)2020-09-29T13:34:18.1842029Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)2020-09-29T13:34:18.1842490Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)2020-09-29T13:34:18.1842878Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)2020-09-29T13:34:18.1843223Z at akka.actor.Actor$class.aroundReceive(Actor.scala:517)2020-09-29T13:34:18.1843742Z at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)2020-09-29T13:34:18.1844151Z at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)2020-09-29T13:34:18.1844467Z at akka.actor.ActorCell.invoke(ActorCell.scala:561)2020-09-29T13:34:18.1844793Z at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)2020-09-29T13:34:18.1845114Z at akka.dispatch.Mailbox.run(Mailbox.scala:225)2020-09-29T13:34:18.1845399Z at akka.dispatch.Mailbox.exec(Mailbox.scala:235)2020-09-29T13:34:18.1845735Z at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)2020-09-29T13:34:18.1846125Z at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)2020-09-29T13:34:18.1846508Z at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)2020-09-29T13:34:18.1846905Z at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)2020-09-29T13:34:18.1847397Z Caused by: org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists2020-09-29T13:34:18.1847933Z at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:122)2020-09-29T13:34:18.1848418Z at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:1015)2020-09-29T13:34:18.1848889Z at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:919)2020-09-29T13:34:18.1849421Z at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorTransactionImpl.doOperation(CuratorTransactionImpl.java:197)2020-09-29T13:34:18.1850005Z at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorTransactionImpl.access$000(CuratorTransactionImpl.java:37)2020-09-29T13:34:18.1850598Z at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorTransactionImpl$2.call(CuratorTransactionImpl.java:130)2020-09-29T13:34:18.1851818Z at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorTransactionImpl$2.call(CuratorTransactionImpl.java:126)2020-09-29T13:34:18.1865959Z at org.apache.flink.shaded.curator4.org.apache.curator.connection.StandardConnectionHandlingPolicy.callWithRetry(StandardConnectionHandlingPolicy.java:64)2020-09-29T13:34:18.1866608Z at org.apache.flink.shaded.curator4.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:100)2020-09-29T13:34:18.1867149Z at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorTransactionImpl.commit(CuratorTransactionImpl.java:123)2020-09-29T13:34:18.1867700Z at org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStore.addAndLock(ZooKeeperStateHandleStore.java:152)2020-09-29T13:34:18.1868034Z ... 29 more2020-09-29T13:34:18.1868164Z</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.ZooKeeperTestUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="19459" opendate="2020-9-29 00:00:00" fixdate="2020-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-dist won&amp;#39;t build locally with newer (3.3+) maven versions</summary>
      <description>flink-dist will fail on non Maven 3.2.5 versions because of banned dependencies.These are the messages you'll see:[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (ban-unsafe-snakeyaml) @ flink-dist_2.11 ---[WARNING] Rule 0: org.apache.maven.plugins.enforcer.BannedDependencies failed with message:Found Banned Dependency: org.yaml:snakeyaml:jar:1.24Use 'mvn dependency:tree' to locate the source of the banned dependencies.[INFO] ------------------------------------------------------------------------[INFO] Reactor Summary for Flink : 1.12-SNAPSHOT:...[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.0.0-M1:enforce (ban-unsafe-snakeyaml) on project flink-dist_2.11: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -&gt; [Help 1]</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch7-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch6-test.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1946" opendate="2015-4-27 00:00:00" fixdate="2015-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make yarn tests logging less verbose</summary>
      <description>Currently, the yarn tests log on the INFO level making the test outputs confusing. Furthermore some status messages are written to stdout. I think these messages are not necessary to be shown to the user.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterClient.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="19462" opendate="2020-9-29 00:00:00" fixdate="2020-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkpoint statistics for unfinished task snapshots</summary>
      <description>If a checkpoint times out, there are currently no stats on the not-yet-finished tasks in the Web UI, so you have to crawl into (debug?) logs.It would be nice to have these incomplete stats in there instead so that you know quickly what was going on. I could think of these ways to accomplish this: the checkpoint coordinator could ask the TMs for it after failing the checkpoint or the TMs could send the stats when they notice that the checkpoint is abortedMaybe there are more options, but I think, this improvement in general would benefit debugging checkpoints.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.FailedCheckpointStats.java</file>
      <file type="M">docs.ops.monitoring.checkpoint.monitoring.md</file>
      <file type="M">docs.ops.metrics.zh.md</file>
      <file type="M">docs.ops.metrics.md</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointMetricsBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointMetrics.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.OperatorSnapshotFuturesTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.OperatorSnapshotFutures.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateUtilTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateUtil.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TaskCheckpointingBehaviourTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBAsyncSnapshotTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TestCheckpointResponder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.NoOpCheckpointResponder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestTaskStateManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.TestingSchedulerNG.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.checkpoints.TaskCheckpointStatisticsWithSubtaskDetailsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.TestingJobMasterGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.TaskStateStatsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.SubtaskStateStatsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointStatsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.CheckpointResponder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.rpc.RpcCheckpointResponder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskStateManagerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskStateManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerNG.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.SubtaskCheckpointStatistics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.TaskStateStats.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.SubtaskStateStats.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorGateway.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.subtask.job-checkpoints-subtask.component.html</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointTaskStateManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStatsTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpointStats.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointStatsTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointStatsHistory.java</file>
    </fixedFiles>
  </bug>
  <bug id="19465" opendate="2020-9-29 00:00:00" fixdate="2020-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CheckpointStorage interface</summary>
      <description>Add checkpoint storage interface and wire it through the runtime</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.utils.ExecutorUtils.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointConfigInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.utils.ArchivedExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCacheTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointConfigInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.JobCheckpointingSettings.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.AccessExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.Checkpoints.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CheckpointingOptions.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.StateBackendITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamTaskBuilder.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TestSpyWrapperStateBackend.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTerminationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImplTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.StateBackendTestContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.MockTtlStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestingCheckpointStorageAccessCoordinatorView.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendMigrationTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.CheckpointStorageLoaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SchedulerTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointSettingsSerializableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.AbstractFileStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ConfigurableCheckpointStorage.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.output.SnapshotUtilsTest.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.StubStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="19483" opendate="2020-10-1 00:00:00" fixdate="2020-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink Table end-to-end test failed with "FileExistsError: [Errno 17] File exists: &amp;#39;/home/vsts/work/1/s/flink-python/dev/.conda/pkgs&amp;#39;"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7130&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d3555292020-09-30T17:13:14.7489481Z Collecting package metadata (current_repodata.json): ...working... failed2020-09-30T17:13:14.7699351Z 2020-09-30T17:13:14.7699995Z # &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ERROR REPORT &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;2020-09-30T17:13:14.7700398Z 2020-09-30T17:13:14.7700782Z Traceback (most recent call last):2020-09-30T17:13:14.7702095Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/gateways/disk/update.py", line 107, in touch2020-09-30T17:13:14.7702736Z mkdir_p_sudo_safe(dirpath)2020-09-30T17:13:14.7703608Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/gateways/disk/__init__.py", line 84, in mkdir_p_sudo_safe2020-09-30T17:13:14.7704221Z os.mkdir(path)2020-09-30T17:13:14.7704992Z FileExistsError: [Errno 17] File exists: '/home/vsts/work/1/s/flink-python/dev/.conda/pkgs'2020-09-30T17:13:14.7705512Z 2020-09-30T17:13:14.7705956Z During handling of the above exception, another exception occurred:2020-09-30T17:13:14.7706402Z 2020-09-30T17:13:14.7706789Z Traceback (most recent call last):2020-09-30T17:13:14.7707615Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 185, in _load2020-09-30T17:13:14.7708341Z mtime = getmtime(self.cache_path_json)2020-09-30T17:13:14.7709527Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 153, in cache_path_json2020-09-30T17:13:14.7710340Z return self.cache_path_base + '.json'2020-09-30T17:13:14.7711227Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 144, in cache_path_base2020-09-30T17:13:14.7711832Z create_cache_dir(),2020-09-30T17:13:14.7712821Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 645, in create_cache_dir2020-09-30T17:13:14.7715308Z cache_dir = join(PackageCacheData.first_writable(context.pkgs_dirs).pkgs_dir, 'cache')2020-09-30T17:13:14.7715986Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/package_cache_data.py", line 162, in first_writable2020-09-30T17:13:14.7716407Z created = create_package_cache_directory(package_cache.pkgs_dir)2020-09-30T17:13:14.7717084Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/gateways/disk/create.py", line 435, in create_package_cache_directory2020-09-30T17:13:14.7717522Z touch(join(pkgs_dir, PACKAGE_CACHE_MAGIC_FILE), mkdir=True, sudo_safe=sudo_safe)2020-09-30T17:13:14.7718150Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/gateways/disk/update.py", line 125, in touch2020-09-30T17:13:14.7718694Z raise NotWritableError(path, e.errno, caused_by=e)2020-09-30T17:13:14.7719040Z conda.exceptions.NotWritableError: The current user does not have write permissions to a required path.2020-09-30T17:13:14.7719797Z path: /home/vsts/work/1/s/flink-python/dev/.conda/pkgs/urls.txt2020-09-30T17:13:14.7720054Z uid: 10012020-09-30T17:13:14.7720217Z gid: 1182020-09-30T17:13:14.7720375Z 2020-09-30T17:13:14.7720625Z If you feel that permissions on this path are set incorrectly, you can manually2020-09-30T17:13:14.7720898Z change them by executing2020-09-30T17:13:14.7721072Z 2020-09-30T17:13:14.7721513Z $ sudo chown 1001:118 /home/vsts/work/1/s/flink-python/dev/.conda/pkgs/urls.txt2020-09-30T17:13:14.7721778Z 2020-09-30T17:13:14.7722334Z In general, it's not advisable to use 'sudo conda'.2020-09-30T17:13:14.7722539Z 2020-09-30T17:13:14.7722680Z 2020-09-30T17:13:14.7722946Z During handling of the above exception, another exception occurred:2020-09-30T17:13:14.7723200Z 2020-09-30T17:13:14.7723407Z Traceback (most recent call last):2020-09-30T17:13:14.7724000Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/exceptions.py", line 1062, in __call__2020-09-30T17:13:14.7724516Z return func(*args, **kwargs)2020-09-30T17:13:14.7725075Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/cli/main.py", line 84, in _main2020-09-30T17:13:14.7725408Z exit_code = do_call(args, p)2020-09-30T17:13:14.7725983Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/cli/conda_argparse.py", line 82, in do_call2020-09-30T17:13:14.7726556Z exit_code = getattr(module, func_name)(args, parser)2020-09-30T17:13:14.7727157Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/cli/main_install.py", line 20, in execute2020-09-30T17:13:14.7727688Z install(args, parser, 'install')2020-09-30T17:13:14.7728274Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/cli/install.py", line 256, in install2020-09-30T17:13:14.7728663Z force_reinstall=context.force_reinstall or context.force,2020-09-30T17:13:14.7729601Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/solve.py", line 112, in solve_for_transaction2020-09-30T17:13:14.7729964Z force_remove, force_reinstall)2020-09-30T17:13:14.7730791Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/solve.py", line 150, in solve_for_diff2020-09-30T17:13:14.7731132Z force_remove)2020-09-30T17:13:14.7731878Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/solve.py", line 249, in solve_final_state2020-09-30T17:13:14.7732267Z ssc = self._collect_all_metadata(ssc)2020-09-30T17:13:14.7732844Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/common/io.py", line 88, in decorated2020-09-30T17:13:14.7733199Z return f(*args, **kwds)2020-09-30T17:13:14.7733780Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/solve.py", line 389, in _collect_all_metadata2020-09-30T17:13:14.7734177Z index, r = self._prepare(prepared_specs)2020-09-30T17:13:14.7734922Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/solve.py", line 974, in _prepare2020-09-30T17:13:14.7735307Z self.subdirs, prepared_specs, self._repodata_fn)2020-09-30T17:13:14.7736067Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/index.py", line 214, in get_reduced_index2020-09-30T17:13:14.7736435Z repodata_fn=repodata_fn)2020-09-30T17:13:14.7737001Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 91, in query_all2020-09-30T17:13:14.7737494Z result = tuple(concat(executor.map(subdir_query, channel_urls)))2020-09-30T17:13:14.7738148Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/concurrent/futures/_base.py", line 586, in result_iterator2020-09-30T17:13:14.7738507Z yield fs.pop().result()2020-09-30T17:13:14.7739046Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/concurrent/futures/_base.py", line 425, in result2020-09-30T17:13:14.7739578Z return self.__get_result()2020-09-30T17:13:14.7740341Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result2020-09-30T17:13:14.7740685Z raise self._exception2020-09-30T17:13:14.7741217Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/concurrent/futures/thread.py", line 57, in run2020-09-30T17:13:14.7741767Z result = self.fn(*self.args, **self.kwargs)2020-09-30T17:13:14.7742581Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 87, in &lt;lambda&gt;2020-09-30T17:13:14.7743099Z package_ref_or_match_spec))2020-09-30T17:13:14.7743680Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 96, in query2020-09-30T17:13:14.7744019Z self.load()2020-09-30T17:13:14.7744563Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 160, in load2020-09-30T17:13:14.7744932Z _internal_state = self._load()2020-09-30T17:13:14.7745837Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 188, in _load2020-09-30T17:13:14.7746162Z self.cache_path_json)2020-09-30T17:13:14.7746724Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 153, in cache_path_json2020-09-30T17:13:14.7747244Z return self.cache_path_base + '.json'2020-09-30T17:13:14.7748013Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 144, in cache_path_base2020-09-30T17:13:14.7748381Z create_cache_dir(),2020-09-30T17:13:14.7749159Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py", line 646, in create_cache_dir2020-09-30T17:13:14.7749537Z mkdir_p_sudo_safe(cache_dir)2020-09-30T17:13:14.7750896Z File "/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/gateways/disk/__init__.py", line 84, in mkdir_p_sudo_safe2020-09-30T17:13:14.7751274Z os.mkdir(path)2020-09-30T17:13:14.7751795Z FileExistsError: [Errno 17] File exists: '/home/vsts/work/1/s/flink-python/dev/.conda/pkgs/cache'</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19485" opendate="2020-10-1 00:00:00" fixdate="2020-10-1 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Decouple runtime operator implementation from DataStream operations</summary>
      <description>Although DataStream is going to be the unified API for Batch and Streaming applications, some operations, e.g. Sinks, may need to have different runtime implementations depending on if they are intended to run on bounded or unbounded data. This is not necessarily only for optimisations but also for the exposed semantics, i.e. correctness.So far, DataStream had a 1-to-1 mapping between an API call and an operator. In a sense, the DataStream API was an "explicit" API. With this addition, we will decouple the API calls from the actual runtime implementations and thus allow different operations to have more than one runtime implementations, depending (for now) on the execution.runtime-mode.</description>
      <version>1.12.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.UnionTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.TwoInputTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SourceTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SinkTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SideOutputTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.PartitionTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.OneInputTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.LegacySourceTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.FeedbackTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.CoFeedbackTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.AbstractMultipleInputTransformation.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.dag.TransformationTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.dag.Transformation.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.DataGenerators.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch7.Elasticsearch7UpsertTableSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6UpsertTableSinkFactoryTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="19492" opendate="2020-10-2 00:00:00" fixdate="2020-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Consolidate Source Events between Source API and Split Reader API</summary>
      <description>The Source API (flink-core) and the SplitReader API (flink-connector-base) have currently separate copies of events to request splits and to signal that no splits are available.We should consolidate those in flink-core.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.lib.util.SplitRequestEvent.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.lib.util.NoSplitAvailableEvent.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.lib.util.IteratorSourceReader.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.lib.util.IteratorSourceEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.FileSourceHeavyThroughputTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.impl.StaticFileSplitEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.impl.FileSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.impl.ContinuousFileSplitEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.mocks.MockSplitEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.reader.SourceReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.event.RequestSplitEvent.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.event.NoMoreSplitsEvent.java</file>
    </fixedFiles>
  </bug>
  <bug id="19496" opendate="2020-10-2 00:00:00" fixdate="2020-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DataGen source DECIMAL always returns null</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.DataGenTableSourceFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.RandomGeneratorVisitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="19516" opendate="2020-10-6 00:00:00" fixdate="2020-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PerJobMiniClusterFactoryTest.testJobClient()</summary>
      <description>Log:https://dev.azure.com/sewen0794/19b23adf-d190-4fb4-ae6e-2e92b08923a3/_apis/build/builds/151/logs/137Exception:[ERROR] testJobClient(org.apache.flink.client.program.PerJobMiniClusterFactoryTest) Time elapsed: 0.392 s &lt;&lt;&lt; FAILURE!java.lang.AssertionError: Expected: is &lt;false&gt; but: was &lt;true&gt; at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8) at org.apache.flink.client.program.PerJobMiniClusterFactoryTest.assertThatMiniClusterIsShutdown(PerJobMiniClusterFactoryTest.java:161) at org.apache.flink.client.program.PerJobMiniClusterFactoryTest.testJobClient(PerJobMiniClusterFactoryTest.java:93)</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterJobClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="19518" opendate="2020-10-7 00:00:00" fixdate="2020-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Duration of running job is shown as 0 in web UI</summary>
      <description>Most likely caused by FLINK-16866, the web UI is showing the "Duration" of a job as 0 in the overview. Once you open the detail page of a job, you see the correct duration.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.FileArchivedExecutionGraphStoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.webmonitor.JobDetails.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.MemoryArchivedExecutionGraphStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.FileArchivedExecutionGraphStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherJob.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.WebFrontendITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="19520" opendate="2020-10-7 00:00:00" fixdate="2020-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add reliable test randomization for checkpointing</summary>
      <description>With the larger refactoring of checkpoint alignment and the additional of more unaligned checkpoint settings, it becomes increasingly important to provide a large test coverage.Unfortunately, adding sufficient test cases in a test matrix appears to be unrealistic: many of the encountered issues were subtle, sometimes caused by race conditions or unusual test configurations and often only visible in e2e tests.Hence, we like to rely on all existing Flink tests to provide a sufficient coverage for checkpointing. However, as more and more options in unaligned checkpoint are going to be implemented in this and the upcoming release, running all Flink tests - especially e2e - in a test matrix is prohibitively expensive, even for nightly builds.Thus, we want to introduce test randomization for all tests that do not use a specific checkpointing mode. In a similar way, we switched from aligned checkpoints by default in tests to unaligned checkpoint during the last release cycle.To not burden the developers of other components too much, we set the following requirements: Randomization should be seeded in a way that both builds on Azure pipelines and local builds will result in the same settings to ease debugging and ensure reproducibility. Randomized options should be shown in the test log. Execution order of test cases will not influence the randomization. Randomization is hidden, no change on any test is needed. Randomization only happens during local/remote test execution. User deployments are not affected. Test developers are able to avoid randomization by explicitly providing checkpoint configs.</description>
      <version>1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.TestStreamEnvironment.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.TestLogger.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.EnvironmentInformation.java</file>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19547" opendate="2020-10-9 00:00:00" fixdate="2020-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Partial Record Cleanup after the Consumer Task Fails and Restart</summary>
      <description>Partial records happen if a record can not fit into one buffer, then the remaining part of the same record is put into the next buffer. Hence partial records only exist at the beginning of a buffer. Partial record clean-up is needed in the mode of approximate local recovery. If a record is spanning over multiple buffers, and the first (several) buffers have got lost due to the failure of the receiver task, the remaining data belonging to the same record in transition should be cleaned up.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.BufferConsumer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.CheckpointedResultSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.BufferBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.RecoveredChannelStateHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="19553" opendate="2020-10-9 00:00:00" fixdate="2020-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The format of checkpoint Completion Time and Failure Time should be changed from HH:mm:ss to yyyy-MM-dd HH:mm:ss</summary>
      <description>As shown in the picture below, The latest completed checkpoint ID is 11113, but the latest failed checkpoint ID is 5370. The two IDs are too far apart .The failure time in HH:mm:ss format is difficult to determine the specific failure date of the checkpoint. </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.subtask.job-checkpoints-subtask.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.detail.job-checkpoints-detail.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="19554" opendate="2020-10-9 00:00:00" fixdate="2020-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unified testing framework for connectors</summary>
      <description>As the community and eco-system of Flink growing up, more and more Flink-owned and third party connectors are developed and added into Flink community. In order to provide a standardized quality controlling for all connectors, it's necessary to develop a unified connector testing framework to simplify and standardize end-to-end test of connectors. </description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.CommonTestUtils.java</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.pom.xml</file>
      <file type="M">tools.ci.java-ci-tools.src.main.resources.modules-skipping-deployment.modulelist</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.reader.KafkaSourceReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.KafkaSourceTestEnv.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.KafkaSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.enumerator.subscriber.KafkaSubscriberTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.enumerator.KafkaEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-test-utils-parent.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkContainer.java</file>
    </fixedFiles>
  </bug>
  <bug id="1956" opendate="2015-4-29 00:00:00" fixdate="2015-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Runtime context not initialized in RichWindowMapFunction</summary>
      <description>Trying to access the runtime context in a rich window map function results in an exception. The following snippet demonstrates the bug: env.generateSequence(0, 1000) .window(Count.of(10)) .mapWindow(new RichWindowMapFunction&lt;Long, Tuple2&lt;Long, Long&gt;&gt;() { @Override public void mapWindow(Iterable&lt;Long&gt; input, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out) throws Exception { long self = getRuntimeContext().getIndexOfThisSubtask(); for (long value : input) { out.collect(new Tuple2&lt;&gt;(self, value)); } } }).flatten().print();</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowMapper.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowFolder.java</file>
    </fixedFiles>
  </bug>
  <bug id="19572" opendate="2020-10-12 00:00:00" fixdate="2020-10-12 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create the TwoInputTransformation translator</summary>
      <description></description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19573" opendate="2020-10-12 00:00:00" fixdate="2020-10-12 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create the MultipleInputTransformation translator</summary>
      <description></description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19574" opendate="2020-10-12 00:00:00" fixdate="2020-10-12 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create the SourceTransformation translator</summary>
      <description></description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19575" opendate="2020-10-12 00:00:00" fixdate="2020-10-12 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create the LegacySourceTransformation translator</summary>
      <description></description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19576" opendate="2020-10-12 00:00:00" fixdate="2020-10-12 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create the SinkTransformation translator</summary>
      <description></description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19577" opendate="2020-10-12 00:00:00" fixdate="2020-10-12 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create the UnionTransformation translator</summary>
      <description></description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19578" opendate="2020-10-12 00:00:00" fixdate="2020-10-12 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create the PartitionTransformation translator</summary>
      <description></description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19579" opendate="2020-10-12 00:00:00" fixdate="2020-10-12 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create the SideOutputTransformation translator</summary>
      <description></description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19583" opendate="2020-10-12 00:00:00" fixdate="2020-10-12 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Expose the execution.runtime-mode to the users</summary>
      <description>With FLIP-134 we aim to expose the config option execution.runtime-mode to the user, so that he/she can execute a pipeline in BATCH or STREAMING mode, as described in the FLIP.In addition, we will allow the users to set it to AUTOMATIC, in which case one of the BATCH or STREAMING mode is going to be picked based on the sources of the pipeline: BATCH if all sources are bounded, STREAMING otherwise.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorExecutionModeDetectionTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorBatchExecutionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.RuntimeExecutionMode.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ExecutionOptions.java</file>
      <file type="M">docs..includes.generated.execution.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="19586" opendate="2020-10-12 00:00:00" fixdate="2020-10-12 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement StreamCommitterOperator for new Sink API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.TestHarnessUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="19599" opendate="2020-10-13 00:00:00" fixdate="2020-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce BulkFormatFactory to integrate new FileSource to table</summary>
      <description>Introduce BulkFormatFactory: BulkFormat&lt;T&gt; createBulkFormat(context).Filesystem connector use this factory to create BulkFormat, and use new FileSource to read files.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.filesystem.FileSystemTableFactoryTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSource.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSink.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableFactory.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.AbstractFileSystemTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.utils.TestRowDataCsvInputFormat.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.utils.TestCsvFileSystemFormatFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetColumnarRowInputFormatTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.ParquetVectorizedInputFormat.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.ParquetFileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.ParquetColumnarRowInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="1960" opendate="2015-4-29 00:00:00" fixdate="2015-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add comments and docs for withForwardedFields and related operators</summary>
      <description>The withForwardedFields and related operators have no docs for the Scala API. It would be useful to have code comments and example usage in the docs.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.DataSet.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19600" opendate="2020-10-13 00:00:00" fixdate="2020-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce PartitionedFile and the corresponding writer/reader for sort-merge based blocking shuffle</summary>
      <description>PartitionedFile is the persistent file type of sort-merge based blocking shuffle. It stores data of all subpartitions in subpartition index order. PartitionedFile can be produced by PartitionedFileWriter and consumed by PartitionedFileReader. </description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionSortedBufferTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BufferReaderWriterUtil.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.IOUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="19602" opendate="2020-10-13 00:00:00" fixdate="2020-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce new config options to enable sort-merge based blocking shuffle</summary>
      <description>1. taskmanager.network.sort-merge-blocking-shuffle.buffers-per-partition: Number of network buffers required for each sort-merge blocking result partition.2. taskmanager.network.sort-merge-blocking-shuffle.min-parallelism: Minimum parallelism to enable the sort-merge based blocking shuffle.With these new config options, the default behavior of blocking shuffle stays unchanged.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.ShuffleCompressionITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NettyShuffleEnvironmentBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.NettyShuffleEnvironmentConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NettyShuffleServiceFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.NettyShuffleEnvironmentOptions.java</file>
      <file type="M">docs..includes.generated.netty.shuffle.environment.configuration.html</file>
      <file type="M">docs..includes.generated.all.taskmanager.network.section.html</file>
      <file type="M">docs.ops.memory.mem.tuning.zh.md</file>
      <file type="M">docs.ops.memory.mem.tuning.md</file>
    </fixedFiles>
  </bug>
  <bug id="19603" opendate="2020-10-13 00:00:00" fixdate="2020-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce shuffle data compression to sort-merge based blocking shuffle</summary>
      <description>Shuffle data compression can reduce the storage overhead and improve the disk/network IO performance.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.ShuffleCompressionITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="19605" opendate="2020-10-13 00:00:00" fixdate="2020-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement cumulative windowing for window aggregate operator</summary>
      <description>Support cumulative windows for existing window aggregate operator, i.e. WindowOperator.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.window.WindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.WindowOperatorBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="19606" opendate="2020-10-13 00:00:00" fixdate="2020-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement streaming window join operator</summary>
      <description>Implement streaming window join operator in blink runtime.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.window.WindowRankOperatorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TestData.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.AbstractStreamingJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19607" opendate="2020-10-13 00:00:00" fixdate="2020-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement streaming window TopN operator</summary>
      <description>Implement streaming window TopN operator in blink runtime.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.TopNBuffer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.AbstractTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.SlicingWindowAggOperatorBuilder.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.processors.SliceUnsharedWindowAggProcessor.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.processors.SliceSharedWindowAggProcessor.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.processors.AbstractWindowAggProcessor.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.combines.WindowCombineFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.combines.CombineRecordsFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.buffers.WindowBuffer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.buffers.RecordsWindowBuffer.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.WindowRankTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalWindowRank.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19609" opendate="2020-10-13 00:00:00" fixdate="2020-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support streaming window join in planner</summary>
      <description>Support window TVF based window join in planner. We will introduce new physical nodes for StreamExecWindowPropertyJoin and StreamExecWindowAssigner, and optimize them into StreamExecWindowJoin if possible.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalTemporalJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdWindowProperties.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.logical.TumblingWindowSpec.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.logical.HoppingWindowSpec.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.logical.CumulativeWindowSpec.java</file>
    </fixedFiles>
  </bug>
  <bug id="19616" opendate="2020-10-13 00:00:00" fixdate="2020-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink : Formats : Parquet compilation failure</summary>
      <description>https://dev.azure.com/pnowojski/Flink/_build/results?buildId=175&amp;view=logs&amp;j=66592496-52df-56bb-d03e-37509e1d9d0f&amp;t=ae0269db-6796-5583-2e5f-d84757d711aa[WARNING] [PROTOC] Unable to invoke protoc, will retry 1 time(s)org.codehaus.plexus.util.cli.CommandLineException: Error while executing process. at org.codehaus.plexus.util.cli.Commandline.execute(Commandline.java:680) at org.codehaus.plexus.util.cli.CommandLineUtils.executeCommandLineAsCallable(CommandLineUtils.java:136) at org.codehaus.plexus.util.cli.CommandLineUtils.executeCommandLine(CommandLineUtils.java:106) at org.codehaus.plexus.util.cli.CommandLineUtils.executeCommandLine(CommandLineUtils.java:89) at org.xolstice.maven.plugin.protobuf.Protoc.execute(Protoc.java:190) at org.xolstice.maven.plugin.protobuf.AbstractProtocMojo.execute(AbstractProtocMojo.java:529) at org.xolstice.maven.plugin.protobuf.AbstractProtocTestCompileMojo.execute(AbstractProtocTestCompileMojo.java:31) at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80) at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51) at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355) at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155) at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584) at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216) at org.apache.maven.cli.MavenCli.main(MavenCli.java:160) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289) at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229) at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415) at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)Caused by: java.io.IOException: Cannot run program "/__w/1/s/flink-formats/flink-parquet/target/protoc-plugins/protoc-3.5.1-linux-x86_64.exe": error=13, Permission denied at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048) at java.lang.Runtime.exec(Runtime.java:621) at org.codehaus.plexus.util.cli.Commandline.execute(Commandline.java:660) ... 27 moreCaused by: java.io.IOException: error=13, Permission denied at java.lang.UNIXProcess.forkAndExec(Native Method) at java.lang.UNIXProcess.&lt;init&gt;(UNIXProcess.java:247) at java.lang.ProcessImpl.start(ProcessImpl.java:134) at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.unpack.build.artifact.sh</file>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19635" opendate="2020-10-14 00:00:00" fixdate="2020-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBaseConnectorITCase.testTableSourceSinkWithDDL is unstable with a result mismatch</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7562&amp;view=logs&amp;j=d44f43ce-542c-597d-bf94-b0718c71e5e8&amp;t=03dca39c-73e8-5aaf-601d-328ae5c35f202020-10-14T04:35:36.9268975Z testTableSourceSinkWithDDL[planner = BLINK_PLANNER, legacy = false](org.apache.flink.connector.hbase2.HBaseConnectorITCase) Time elapsed: 3.131 sec &lt;&lt;&lt; FAILURE!2020-10-14T04:35:36.9276246Z java.lang.AssertionError: expected:&lt;[1,10,Hello-1,100,1.01,false,Welt-1,2019-08-18T19:00,2019-08-18,19:00,12345678.0001, 2,20,Hello-2,200,2.02,true,Welt-2,2019-08-18T19:01,2019-08-18,19:01,12345678.0002, 3,30,Hello-3,300,3.03,false,Welt-3,2019-08-18T19:02,2019-08-18,19:02,12345678.0003, 4,40,null,400,4.04,true,Welt-4,2019-08-18T19:03,2019-08-18,19:03,12345678.0004, 5,50,Hello-5,500,5.05,false,Welt-5,2019-08-19T19:10,2019-08-19,19:10,12345678.0005, 6,60,Hello-6,600,6.06,true,Welt-6,2019-08-19T19:20,2019-08-19,19:20,12345678.0006, 7,70,Hello-7,700,7.07,false,Welt-7,2019-08-19T19:30,2019-08-19,19:30,12345678.0007, 8,80,null,800,8.08,true,Welt-8,2019-08-19T19:40,2019-08-19,19:40,12345678.0008]&gt; but was:&lt;[1,10,Hello-1,100,1.01,false,Welt-1,2019-08-18T19:00,2019-08-18,19:00,12345678.0001, 2,20,Hello-2,200,2.02,true,Welt-2,2019-08-18T19:01,2019-08-18,19:01,12345678.0002, 3,30,Hello-3,300,3.03,false,Welt-3,2019-08-18T19:02,2019-08-18,19:02,12345678.0003]&gt;2020-10-14T04:35:36.9281340Z at org.junit.Assert.fail(Assert.java:88)2020-10-14T04:35:36.9282023Z at org.junit.Assert.failNotEquals(Assert.java:834)2020-10-14T04:35:36.9328385Z at org.junit.Assert.assertEquals(Assert.java:118)2020-10-14T04:35:36.9338939Z at org.junit.Assert.assertEquals(Assert.java:144)2020-10-14T04:35:36.9339880Z at org.apache.flink.connector.hbase2.HBaseConnectorITCase.testTableSourceSinkWithDDL(HBaseConnectorITCase.java:449)2020-10-14T04:35:36.9341003Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.src.test.java.org.apache.flink.connector.hbase2.util.HBaseTestingClusterAutoStarter.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.src.test.java.org.apache.flink.connector.hbase1.util.HBaseTestingClusterAutoStarter.java</file>
    </fixedFiles>
  </bug>
  <bug id="19636" opendate="2020-10-14 00:00:00" fixdate="2020-12-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add declarative SlotPool</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.JobScopedResourceTracker.java</file>
    </fixedFiles>
  </bug>
  <bug id="19646" opendate="2020-10-15 00:00:00" fixdate="2020-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StreamExecutionEnvironment support new Source interface based on FLIP-27</summary>
      <description>StreamExecutionEnvironment currently supports new Source interface based on FLIP-27 in Java, but doesn't support in Python. PyFlink StreamExecutionEnvironment should add methods related to new Source interface like from_source etc.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.py</file>
    </fixedFiles>
  </bug>
  <bug id="19654" opendate="2020-10-15 00:00:00" fixdate="2020-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the execution time of PyFlink end-to-end tests</summary>
      <description>Thanks for the sharing from rmetzger, currently the test duration for PyFlink end-to-end test is as following:test caseaverage execution-timemaximum execution-timePyFlink Table end-to-end test1340s1877sPyFlink DataStream end-to-end test387s575sKubernetes PyFlink application test606s694sWe need to investigate how to improve them to reduce the execution time.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-python-test.python.add.one.py</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.table.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.datastream.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19664" opendate="2020-10-15 00:00:00" fixdate="2020-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upload logs before end to end tests time out</summary>
      <description>Due to a bug in azure pipelines, we can not see the e2e output when a run times out.This ticket is to add some tooling for rescuing the logs before it's too late</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="1967" opendate="2015-5-4 00:00:00" fixdate="2015-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce (Event)time in Streaming</summary>
      <description>This requires introducing a timestamp in streaming record and a change in the sources to add timestamps to records. This will also introduce punctuations (or low watermarks) to allow windows to work correctly on unordered, timestamped input data. In the process of this, the windowing subsystem also needs to be adapted to use the punctuations. Furthermore, all operators need to be made aware of punctuations and correctly forward them. Then, a new operator must be introduced to to allow modification of timestamps.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.JumpingCountPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.SourceFunctionUtil.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.MockRecordWriterFactory.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.MockOutput.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.MockContext.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.MockCoContext.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestBase.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamMockEnvironment.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.partitioner.ShufflePartitionerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.partitioner.GlobalPartitionerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.partitioner.ForwardPartitionerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.partitioner.FieldsPartitionerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.partitioner.DistributePartitionerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.partitioner.BroadcastPartitionerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.io.CoRecordReaderTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.io.BarrierBufferTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.io.BarrierBufferIOTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.windowbuffer.TumblingPreReducerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.windowbuffer.TumblingGroupedPreReducerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingTimePreReducerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingTimeGroupedPreReducerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingCountPreReducerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingCountGroupedPreReducerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.windowbuffer.JumpingTimePreReducerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.windowbuffer.JumpingCountPreReducerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.windowbuffer.JumpingCountGroupedPreReducerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.windowbuffer.BasicWindowBufferTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.streamtask.MockRecordWriter.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.state.StatefulOperatorTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.windowing.WindowIntegrationTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.windowing.ParallelMergeTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.ProjectTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.MapTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.GroupedReduceTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.GroupedFoldTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.FlatMapTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.FilterTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.co.CoWindowTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.co.CoMapTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.co.CoGroupedReduceTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.co.CoFlatMapTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.CounterTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.functions.ListSourceContext.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.collector.StreamCollectorTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamIterationTail.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamIterationHead.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamingSuperstep.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamingRuntimeContext.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.OutputHandler.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.streamrecord.StreamRecordSerializer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.streamrecord.StreamRecord.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.partitioner.FieldsPartitioner.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.partitioner.CustomPartitionerWrapper.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.StreamRecordWriter.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.StreamingMutableRecordReader.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.StreamingAbstractRecordReader.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.InputGateFactory.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.IndexedReaderIterator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.IndexedMutableReader.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.CoRecordReader.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.CoReaderIterator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.BlockingQueueBroker.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.BarrierBuffer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.WindowBuffer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.TumblingPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.TumblingGroupedPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.JumpingTimePreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.JumpingTimeGroupedPreReducer.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-core.src.main.java.org.apache.flink.stormcompatibility.wrappers.StormBoltCollector.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-core.src.main.java.org.apache.flink.stormcompatibility.wrappers.StormBoltWrapper.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-core.src.test.java.org.apache.flink.stormcompatibility.wrappers.StormBoltWrapperTest.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-core.src.test.java.org.apache.flink.stormcompatibility.wrappers.TestContext.java</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.RecordWriter.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.pom.xml</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.collector.CollectorWrapper.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.collector.selector.BroadcastOutputSelectorWrapper.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.collector.selector.DirectedOutputSelectorWrapper.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.collector.selector.OutputSelectorWrapper.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.collector.StreamOutput.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.functions.source.SourceFunction.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.StreamNode.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.co.CoStreamFlatMap.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.co.CoStreamGroupedReduce.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.co.CoStreamMap.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.co.CoStreamReduce.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.co.CoStreamWindow.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.OneInputStreamOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.Output.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamCounter.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamFilter.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamFlatMap.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamFold.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamGroupedFold.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamGroupedReduce.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamMap.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamProject.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamReduce.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamSink.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.TwoInputStreamOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.GroupedActiveDiscretizer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.GroupedStreamDiscretizer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.GroupedWindowBuffer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.StreamDiscretizer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.StreamWindowBuffer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowFlattener.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowMerger.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowPartitioner.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.state.PartitionedStreamOperatorState.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.BasicWindowBuffer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.JumpingCountGroupedPreReducer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19682" opendate="2020-10-16 00:00:00" fixdate="2020-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Actively timeout checkpoint barriers on the inputs</summary>
      <description>After receiving the first checkpoint barrier announcement, we should some kind of register a processing time timeout to switch to unaligned checkpoint.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TestSubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskCancellationBarrierTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.MockInputGate.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.ValidatingCheckpointHandler.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.UnalignedControllerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.UnalignedControllerCancellationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.TestBarrierHandlerFactory.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.InputProcessorUtilTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierTrackerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlternatingControllerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlignedControllerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlignedControllerMassiveRandomTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.UnalignedController.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.InputProcessorUtil.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierBehaviourController.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.AlternatingController.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.AlignedController.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.TestStreamEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="19684" opendate="2020-10-16 00:00:00" fixdate="2020-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The Jdbc-connector&amp;#39;s &amp;#39;lookup.max-retries&amp;#39; option implementation is different from the meaning</summary>
      <description> The code of 'lookup.max-retries' option :for (int retry = 1; retry &lt;= maxRetryTimes; retry++) { statement.clearParameters(); .....} From the code, If this option is set to 0, the JDBC query will not be executed. From documents,  the max retry times if lookup database failed. &amp;#91;1&amp;#93;When set to 0, there is a query, but no retry. So,the code of 'lookup.max-retries' option should be:for (int retry = 0; retry &lt;= maxRetryTimes; retry++) { statement.clearParameters(); .....}   &amp;#91;1&amp;#93; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/jdbc.html#lookup-max-retries</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcLookupTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunction.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.JdbcExecutionOptions.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="19699" opendate="2020-10-19 00:00:00" fixdate="2020-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PrometheusReporterEndToEndITCase crashes with exit code 143</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7814&amp;view=logs&amp;j=68a897ab-3047-5660-245a-cce8f83859f6&amp;t=16ca2cca-2f63-5cce-12d2-d519b930a7292020-10-18T23:46:04.9667443Z [ERROR] The forked VM terminated without properly saying goodbye. VM crash or System.exit called?2020-10-18T23:46:04.9669237Z [ERROR] Command was /bin/sh -c cd /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target &amp;&amp; /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/surefire/surefirebooter6797466627443523305.jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/surefire 2020-10-18T23-44-09_467-jvmRun2 surefire930806459376622178tmp surefire_41970585275084524978tmp2020-10-18T23:46:04.9670440Z [ERROR] Error occurred in starting fork, check output in log2020-10-18T23:46:04.9671283Z [ERROR] Process Exit Code: 1432020-10-18T23:46:04.9671614Z [ERROR] Crashed tests:2020-10-18T23:46:04.9672025Z [ERROR] org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase2020-10-18T23:46:04.9672649Z [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?2020-10-18T23:46:04.9674834Z [ERROR] Command was /bin/sh -c cd /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target &amp;&amp; /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/surefire/surefirebooter6797466627443523305.jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/surefire 2020-10-18T23-44-09_467-jvmRun2 surefire930806459376622178tmp surefire_41970585275084524978tmp2020-10-18T23:46:04.9676153Z [ERROR] Error occurred in starting fork, check output in log2020-10-18T23:46:04.9676556Z [ERROR] Process Exit Code: 1432020-10-18T23:46:04.9676882Z [ERROR] Crashed tests:2020-10-18T23:46:04.9677288Z [ERROR] org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase2020-10-18T23:46:04.9677827Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:669)2020-10-18T23:46:04.9678408Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:282)2020-10-18T23:46:04.9678965Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:245)2020-10-18T23:46:04.9679575Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)2020-10-18T23:46:04.9680983Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)2020-10-18T23:46:04.9681749Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)2020-10-18T23:46:04.9682246Z [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)2020-10-18T23:46:04.9682728Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)2020-10-18T23:46:04.9683179Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)2020-10-18T23:46:04.9683609Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)2020-10-18T23:46:04.9684102Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)2020-10-18T23:46:04.9684639Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)2020-10-18T23:46:04.9685180Z [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)2020-10-18T23:46:04.9685711Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)2020-10-18T23:46:04.9686145Z [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)2020-10-18T23:46:04.9686516Z [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)2020-10-18T23:46:04.9689517Z [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)2020-10-18T23:46:04.9689917Z [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)2020-10-18T23:46:04.9690262Z [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)2020-10-18T23:46:04.9690606Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-10-18T23:46:04.9690994Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-10-18T23:46:04.9691435Z [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-10-18T23:46:04.9691856Z [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)2020-10-18T23:46:04.9692450Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)2020-10-18T23:46:04.9693419Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)2020-10-18T23:46:04.9693885Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)2020-10-18T23:46:04.9694334Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)2020-10-18T23:46:04.9694939Z [ERROR] -&gt; [Help 1]</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.maven-utils.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.TestLogger.java</file>
    </fixedFiles>
  </bug>
  <bug id="19707" opendate="2020-10-19 00:00:00" fixdate="2020-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor table streaming file sink</summary>
      <description>Refactor: Extract some classes Make writer more generic Provides StreamingSink util class</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.filesystem.stream.StreamingFileWriterTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.stream.StreamingFileWriter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.stream.StreamingFileCommitter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="19712" opendate="2020-10-19 00:00:00" fixdate="2020-10-19 01:00:00" resolution="Done">
    <buginformation>
      <summary>Pipelined region failover should skip restarting tasks in CREATED</summary>
      <description>When a task fails and it is pipelined region failover strategy, all tasks in the region of the failed task and in the downstream regions will be canceled for later re-scheduling. However, these tasks can be still in CREATED state so that there is no need to cancel these tasks. Skipping canceling these tasks can speed up the failover and reduce a lot of unnecessary CANCELING logs.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.BatchFineGrainedRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategyTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy.java</file>
    </fixedFiles>
  </bug>
  <bug id="19716" opendate="2020-10-19 00:00:00" fixdate="2020-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unable to use Assume Role with EFO record publisher</summary>
      <description>Reproduction Steps Setup an application to consume from a Kinesis Stream Use ASSUME_ROLE credential provider consumerConfig.setProperty(AWSConfigConstants.AWS_CREDENTIALS_PROVIDER, ASSUME_ROLE.name());consumerConfig.setProperty(AWSConfigConstants.AWS_ROLE_ARN, "&lt;role-arn&gt;");consumerConfig.setProperty(AWSConfigConstants.AWS_ROLE_SESSION_NAME, "test-efo"); Expected Result Consumer is able to authorise and consume from the streamActual Result The following error is thrown (full stack attached) Caused by: org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.exception.SdkClientException: Unable to load an HTTP implementation from any provider in the chain. You must declare a dependency on an appropriate HTTP implementation or pass in an SdkHttpClient explicitly to the client builder. DiagnosisThis issue occurs because Assume Role credential provider requires a Sync HTTP Client. The Apache HTTP Client is on the classpath but it is not detected due to the shading relocation. It is looking for: org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.SdkHttpServiceHowever the service manifest is defining: software.amazon.awssdk.http.SdkHttpServiceSolution Add a service manifest such that the shaded HTTP client is used Also needed to update the HTTP client/core version due to incompatibilities TestingTested using EFO and POLLING record consumer  </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19720" opendate="2020-10-20 00:00:00" fixdate="2020-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce new Providers and parallelism API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FactoryUtil.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.sink.DynamicTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="19726" opendate="2020-10-20 00:00:00" fixdate="2020-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement new providers for blink planner</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sink.SinkOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPhysicalSink.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19727" opendate="2020-10-20 00:00:00" fixdate="2020-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement ParallelismProvider for sink in blink planner</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPhysicalSink.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.sink.OutputFormatProvider.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.connector.sink.SinkFunctionProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="19732" opendate="2020-10-20 00:00:00" fixdate="2020-10-20 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Disable Checkpointing when in BATCH mode</summary>
      <description>When we are executing a pipeline in BATCH mode, given that scheduling happens using independent regions and not all tasks are present at the same time, we should disable checkpointing.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorExecutionModeDetectionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.CheckpointConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="19734" opendate="2020-10-20 00:00:00" fixdate="2020-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace &amp;#39;collection&amp;#39; connector by &amp;#39;values&amp;#39; connector for temporal join plan tests</summary>
      <description>Currently, both COLLECTION and VALUES connectors are `LookupTableSoure`, we can add a non lookup table source connector to cover  scan-only source.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.TemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="19739" opendate="2020-10-20 00:00:00" fixdate="2020-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CompileException when windowing in batch mode: A method named "replace" is not declared in any enclosing class nor any supertype</summary>
      <description>Example script:from pyflink.table import EnvironmentSettings, BatchTableEnvironmentfrom pyflink.table.window import Tumbleenv_settings = ( EnvironmentSettings.new_instance().in_batch_mode().use_blink_planner().build())table_env = BatchTableEnvironment.create(environment_settings=env_settings)table_env.execute_sql( """ CREATE TABLE table1 ( amount INT, ts TIMESTAMP(3), WATERMARK FOR ts AS ts - INTERVAL '5' SECOND ) WITH ( 'connector.type' = 'filesystem', 'format.type' = 'csv', 'connector.path' = '/home/alex/work/test-flink/data1.csv' )""")table1 = table_env.from_path("table1")table = ( table1 .window(Tumble.over("5.days").on("ts").alias("__window")) .group_by("__window") .select("amount.sum"))print(table.to_pandas())Output:WARNING: An illegal reflective access operation has occurredWARNING: Illegal reflective access by org.apache.flink.api.python.shaded.io.netty.util.internal.ReflectionUtil (file:/home/alex/work/flink/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT/opt/flink-python_2.11-1.12-SNAPSHOT.jar) to constructor java.nio.DirectByteBuffer(long,int)WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.python.shaded.io.netty.util.internal.ReflectionUtilWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operationsWARNING: All illegal access operations will be denied in a future release/* 1 *//* 2 */ public class LocalHashWinAggWithoutKeys$59 extends org.apache.flink.table.runtime.operators.TableStreamOperator/* 3 */ implements org.apache.flink.streaming.api.operators.OneInputStreamOperator, org.apache.flink.streaming.api.operators.BoundedOneInput {/* 4 *//* 5 */ private final Object[] references;/* 6 */ /* 7 */ private static final org.slf4j.Logger LOG$2 =/* 8 */ org.slf4j.LoggerFactory.getLogger("LocalHashWinAgg");/* 9 */ /* 10 */ private transient org.apache.flink.table.types.logical.LogicalType[] aggMapKeyTypes$5;/* 11 */ private transient org.apache.flink.table.types.logical.LogicalType[] aggBufferTypes$6;/* 12 */ private transient org.apache.flink.table.runtime.operators.aggregate.BytesHashMap aggregateMap$7;/* 13 */ org.apache.flink.table.data.binary.BinaryRowData emptyAggBuffer$9 = new org.apache.flink.table.data.binary.BinaryRowData(1);/* 14 */ org.apache.flink.table.data.writer.BinaryRowWriter emptyAggBufferWriterTerm$10 = new org.apache.flink.table.data.writer.BinaryRowWriter(emptyAggBuffer$9);/* 15 */ org.apache.flink.table.data.GenericRowData hashAggOutput = new org.apache.flink.table.data.GenericRowData(2);/* 16 */ private transient org.apache.flink.table.data.binary.BinaryRowData reuseAggMapKey$17 = new org.apache.flink.table.data.binary.BinaryRowData(1);/* 17 */ private transient org.apache.flink.table.data.binary.BinaryRowData reuseAggBuffer$18 = new org.apache.flink.table.data.binary.BinaryRowData(1);/* 18 */ private transient org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.Entry reuseAggMapEntry$19 = new org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.Entry(reuseAggMapKey$17, reuseAggBuffer$18);/* 19 */ org.apache.flink.table.data.binary.BinaryRowData aggMapKey$3 = new org.apache.flink.table.data.binary.BinaryRowData(1);/* 20 */ org.apache.flink.table.data.writer.BinaryRowWriter aggMapKeyWriter$4 = new org.apache.flink.table.data.writer.BinaryRowWriter(aggMapKey$3);/* 21 */ private boolean hasInput = false;/* 22 */ org.apache.flink.streaming.runtime.streamrecord.StreamRecord element = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord((Object)null);/* 23 */ private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);/* 24 *//* 25 */ public LocalHashWinAggWithoutKeys$59(/* 26 */ Object[] references,/* 27 */ org.apache.flink.streaming.runtime.tasks.StreamTask task,/* 28 */ org.apache.flink.streaming.api.graph.StreamConfig config,/* 29 */ org.apache.flink.streaming.api.operators.Output output,/* 30 */ org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception {/* 31 */ this.references = references;/* 32 */ aggMapKeyTypes$5 = (((org.apache.flink.table.types.logical.LogicalType[]) references[0]));/* 33 */ aggBufferTypes$6 = (((org.apache.flink.table.types.logical.LogicalType[]) references[1]));/* 34 */ this.setup(task, config, output);/* 35 */ if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) {/* 36 */ ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this)/* 37 */ .setProcessingTimeService(processingTimeService);/* 38 */ }/* 39 */ }/* 40 *//* 41 */ @Override/* 42 */ public void open() throws Exception {/* 43 */ super.open();/* 44 */ aggregateMap$7 = new org.apache.flink.table.runtime.operators.aggregate.BytesHashMap(this.getContainingTask(),this.getContainingTask().getEnvironment().getMemoryManager(),computeMemorySize(), aggMapKeyTypes$5, aggBufferTypes$6);/* 45 */ /* 46 */ /* 47 */ emptyAggBufferWriterTerm$10.reset();/* 48 */ /* 49 */ /* 50 */ if (true) {/* 51 */ emptyAggBufferWriterTerm$10.setNullAt(0);/* 52 */ } else {/* 53 */ emptyAggBufferWriterTerm$10.writeInt(0, ((int) -1));/* 54 */ }/* 55 */ /* 56 */ emptyAggBufferWriterTerm$10.complete();/* 57 */ /* 58 */ }/* 59 *//* 60 */ @Override/* 61 */ public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {/* 62 */ org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) element.getValue();/* 63 */ /* 64 */ org.apache.flink.table.data.binary.BinaryRowData currentAggBuffer$8;/* 65 */ int field$11;/* 66 */ boolean isNull$11;/* 67 */ int field$12;/* 68 */ boolean isNull$12;/* 69 */ boolean isNull$13;/* 70 */ int result$14;/* 71 */ org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.LookupInfo lookupInfo$20;/* 72 */ org.apache.flink.table.data.TimestampData field$21;/* 73 */ boolean isNull$21;/* 74 */ boolean isNull$22;/* 75 */ long result$23;/* 76 */ boolean isNull$24;/* 77 */ long result$25;/* 78 */ boolean isNull$26;/* 79 */ long result$27;/* 80 */ boolean isNull$28;/* 81 */ long result$29;/* 82 */ boolean isNull$30;/* 83 */ long result$31;/* 84 */ boolean isNull$32;/* 85 */ long result$33;/* 86 */ boolean isNull$34;/* 87 */ boolean result$35;/* 88 */ boolean isNull$36;/* 89 */ long result$37;/* 90 */ boolean isNull$38;/* 91 */ long result$39;/* 92 */ boolean isNull$40;/* 93 */ long result$41;/* 94 */ boolean isNull$42;/* 95 */ long result$43;/* 96 */ boolean isNull$44;/* 97 */ long result$45;/* 98 */ boolean isNull$46;/* 99 */ long result$47;/* 100 */ boolean isNull$48;/* 101 */ long result$49;/* 102 */ boolean isNull$50;/* 103 */ long result$51;/* 104 */ boolean isNull$52;/* 105 */ long result$53;/* 106 */ boolean isNull$55;/* 107 */ long result$56;/* 108 */ boolean isNull$57;/* 109 */ long result$58;/* 110 */ /* 111 */ /* 112 */ if (!in1.isNullAt(1)) {/* 113 */ hasInput = true;/* 114 */ // input field access for group key projection, window/pane assign/* 115 */ // and aggregate map update/* 116 */ isNull$11 = in1.isNullAt(0);/* 117 */ field$11 = -1;/* 118 */ if (!isNull$11) {/* 119 */ field$11 = in1.getInt(0);/* 120 */ }/* 121 */ isNull$21 = in1.isNullAt(1);/* 122 */ field$21 = null;/* 123 */ if (!isNull$21) {/* 124 */ field$21 = in1.getTimestamp(1, 3);/* 125 */ }/* 126 */ // assign timestamp(window or pane)/* 127 */ /* 128 */ /* 129 */ /* 130 */ /* 131 */ /* 132 */ isNull$22 = isNull$21;/* 133 */ result$23 = -1L;/* 134 */ if (!isNull$22) {/* 135 */ /* 136 */ result$23 = field$21.getMillisecond();/* 137 */ /* 138 */ }/* 139 */ /* 140 */ /* 141 */ isNull$24 = isNull$22 || false;/* 142 */ result$25 = -1L;/* 143 */ if (!isNull$24) {/* 144 */ /* 145 */ result$25 = (long) (result$23 * ((long) 1L));/* 146 */ /* 147 */ }/* 148 */ /* 149 */ isNull$26 = isNull$21;/* 150 */ result$27 = -1L;/* 151 */ if (!isNull$26) {/* 152 */ /* 153 */ result$27 = field$21.getMillisecond();/* 154 */ /* 155 */ }/* 156 */ /* 157 */ /* 158 */ isNull$28 = isNull$26 || false;/* 159 */ result$29 = -1L;/* 160 */ if (!isNull$28) {/* 161 */ /* 162 */ result$29 = (long) (result$27 * ((long) 1L));/* 163 */ /* 164 */ }/* 165 */ /* 166 */ /* 167 */ isNull$30 = isNull$28 || false;/* 168 */ result$31 = -1L;/* 169 */ if (!isNull$30) {/* 170 */ /* 171 */ result$31 = (long) (result$29 - ((long) 0L));/* 172 */ /* 173 */ }/* 174 */ /* 175 */ /* 176 */ isNull$32 = isNull$30 || false;/* 177 */ result$33 = -1L;/* 178 */ if (!isNull$32) {/* 179 */ /* 180 */ result$33 = (long) (result$31 % ((long) 432000000L));/* 181 */ /* 182 */ }/* 183 */ /* 184 */ /* 185 */ isNull$34 = isNull$32 || false;/* 186 */ result$35 = false;/* 187 */ if (!isNull$34) {/* 188 */ /* 189 */ result$35 = result$33 &lt; ((int) 0);/* 190 */ /* 191 */ }/* 192 */ /* 193 */ long result$54 = -1L;/* 194 */ boolean isNull$54;/* 195 */ if (result$35) {/* 196 */ /* 197 */ /* 198 */ /* 199 */ /* 200 */ /* 201 */ /* 202 */ isNull$36 = isNull$21;/* 203 */ result$37 = -1L;/* 204 */ if (!isNull$36) {/* 205 */ /* 206 */ result$37 = field$21.getMillisecond();/* 207 */ /* 208 */ }/* 209 */ /* 210 */ /* 211 */ isNull$38 = isNull$36 || false;/* 212 */ result$39 = -1L;/* 213 */ if (!isNull$38) {/* 214 */ /* 215 */ result$39 = (long) (result$37 * ((long) 1L));/* 216 */ /* 217 */ }/* 218 */ /* 219 */ /* 220 */ isNull$40 = isNull$38 || false;/* 221 */ result$41 = -1L;/* 222 */ if (!isNull$40) {/* 223 */ /* 224 */ result$41 = (long) (result$39 - ((long) 0L));/* 225 */ /* 226 */ }/* 227 */ /* 228 */ /* 229 */ isNull$42 = isNull$40 || false;/* 230 */ result$43 = -1L;/* 231 */ if (!isNull$42) {/* 232 */ /* 233 */ result$43 = (long) (result$41 % ((long) 432000000L));/* 234 */ /* 235 */ }/* 236 */ /* 237 */ /* 238 */ isNull$44 = isNull$42 || false;/* 239 */ result$45 = -1L;/* 240 */ if (!isNull$44) {/* 241 */ /* 242 */ result$45 = (long) (result$43 + ((long) 432000000L));/* 243 */ /* 244 */ }/* 245 */ /* 246 */ isNull$54 = isNull$44;/* 247 */ if (!isNull$54) {/* 248 */ result$54 = result$45;/* 249 */ }/* 250 */ }/* 251 */ else {/* 252 */ /* 253 */ /* 254 */ /* 255 */ /* 256 */ /* 257 */ isNull$46 = isNull$21;/* 258 */ result$47 = -1L;/* 259 */ if (!isNull$46) {/* 260 */ /* 261 */ result$47 = field$21.getMillisecond();/* 262 */ /* 263 */ }/* 264 */ /* 265 */ /* 266 */ isNull$48 = isNull$46 || false;/* 267 */ result$49 = -1L;/* 268 */ if (!isNull$48) {/* 269 */ /* 270 */ result$49 = (long) (result$47 * ((long) 1L));/* 271 */ /* 272 */ }/* 273 */ /* 274 */ /* 275 */ isNull$50 = isNull$48 || false;/* 276 */ result$51 = -1L;/* 277 */ if (!isNull$50) {/* 278 */ /* 279 */ result$51 = (long) (result$49 - ((long) 0L));/* 280 */ /* 281 */ }/* 282 */ /* 283 */ /* 284 */ isNull$52 = isNull$50 || false;/* 285 */ result$53 = -1L;/* 286 */ if (!isNull$52) {/* 287 */ /* 288 */ result$53 = (long) (result$51 % ((long) 432000000L));/* 289 */ /* 290 */ }/* 291 */ /* 292 */ isNull$54 = isNull$52;/* 293 */ if (!isNull$54) {/* 294 */ result$54 = result$53;/* 295 */ }/* 296 */ }/* 297 */ isNull$55 = isNull$24 || isNull$54;/* 298 */ result$56 = -1L;/* 299 */ if (!isNull$55) {/* 300 */ /* 301 */ result$56 = (long) (result$25 - result$54);/* 302 */ /* 303 */ }/* 304 */ /* 305 */ /* 306 */ isNull$57 = isNull$55 || false;/* 307 */ result$58 = -1L;/* 308 */ if (!isNull$57) {/* 309 */ /* 310 */ result$58 = (long) (result$56 - ((long) 0L));/* 311 */ /* 312 */ }/* 313 */ /* 314 */ // process each input/* 315 */ /* 316 */ // build aggregate map key/* 317 */ /* 318 */ /* 319 */ aggMapKeyWriter$4.reset();/* 320 */ /* 321 */ /* 322 */ if (false) {/* 323 */ aggMapKeyWriter$4.setNullAt(0);/* 324 */ } else {/* 325 */ aggMapKeyWriter$4.writeLong(0, result$58);/* 326 */ }/* 327 */ /* 328 */ aggMapKeyWriter$4.complete();/* 329 */ /* 330 */ // aggregate by each input with assigned timestamp/* 331 */ // look up output buffer using current key (grouping keys ..., assigned timestamp)/* 332 */ lookupInfo$20 = aggregateMap$7.lookup(aggMapKey$3);/* 333 */ currentAggBuffer$8 = lookupInfo$20.getValue();/* 334 */ if (!lookupInfo$20.isFound()) {/* 335 */ /* 336 */ // append empty agg buffer into aggregate map for current group key/* 337 */ try {/* 338 */ currentAggBuffer$8 =/* 339 */ aggregateMap$7.append(lookupInfo$20, emptyAggBuffer$9);/* 340 */ } catch (java.io.EOFException exp) {/* 341 */ /* 342 */ LOG$2.info("BytesHashMap out of memory with {} entries, output directly.", aggregateMap$7.getNumElements());/* 343 */ // hash map out of memory, output directly/* 344 */ /* 345 */ org.apache.flink.util.MutableObjectIterator&lt;org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.Entry&gt; iterator =/* 346 */ aggregateMap$7.getEntryIterator();/* 347 */ while (iterator.next(reuseAggMapEntry$19) != null) {/* 348 */ /* 349 */ /* 350 */ /* 351 */ hashAggOutput.replace(reuseAggMapKey$17, reuseAggBuffer$18);/* 352 */ /* 353 */ output.collect(outElement.replace(hashAggOutput));/* 354 */ }/* 355 */ /* 356 */ // retry append/* 357 */ /* 358 */ // reset aggregate map retry append/* 359 */ aggregateMap$7.reset();/* 360 */ lookupInfo$20 = aggregateMap$7.lookup(aggMapKey$3);/* 361 */ try {/* 362 */ currentAggBuffer$8 =/* 363 */ aggregateMap$7.append(lookupInfo$20, emptyAggBuffer$9);/* 364 */ } catch (java.io.EOFException e) {/* 365 */ throw new OutOfMemoryError("BytesHashMap Out of Memory.");/* 366 */ }/* 367 */ /* 368 */ /* 369 */ }/* 370 */ }/* 371 */ // aggregate buffer fields access/* 372 */ isNull$12 = currentAggBuffer$8.isNullAt(0);/* 373 */ field$12 = -1;/* 374 */ if (!isNull$12) {/* 375 */ field$12 = currentAggBuffer$8.getInt(0);/* 376 */ }/* 377 */ // do aggregate and update agg buffer/* 378 */ int result$16 = -1;/* 379 */ boolean isNull$16;/* 380 */ if (isNull$11) {/* 381 */ /* 382 */ isNull$16 = isNull$12;/* 383 */ if (!isNull$16) {/* 384 */ result$16 = field$12;/* 385 */ }/* 386 */ }/* 387 */ else {/* 388 */ int result$15 = -1;/* 389 */ boolean isNull$15;/* 390 */ if (isNull$12) {/* 391 */ /* 392 */ isNull$15 = isNull$11;/* 393 */ if (!isNull$15) {/* 394 */ result$15 = field$11;/* 395 */ }/* 396 */ }/* 397 */ else {/* 398 */ /* 399 */ /* 400 */ /* 401 */ isNull$13 = isNull$12 || isNull$11;/* 402 */ result$14 = -1;/* 403 */ if (!isNull$13) {/* 404 */ /* 405 */ result$14 = (int) (field$12 + field$11);/* 406 */ /* 407 */ }/* 408 */ /* 409 */ isNull$15 = isNull$13;/* 410 */ if (!isNull$15) {/* 411 */ result$15 = result$14;/* 412 */ }/* 413 */ }/* 414 */ isNull$16 = isNull$15;/* 415 */ if (!isNull$16) {/* 416 */ result$16 = result$15;/* 417 */ }/* 418 */ }/* 419 */ if (isNull$16) {/* 420 */ currentAggBuffer$8.setNullAt(0);/* 421 */ } else {/* 422 */ currentAggBuffer$8.setInt(0, result$16);/* 423 */ }/* 424 */ /* 425 */ }/* 426 */ }/* 427 *//* 428 */ /* 429 */ @Override/* 430 */ public void endInput() throws Exception {/* 431 */ org.apache.flink.table.data.binary.BinaryRowData currentAggBuffer$8;/* 432 */ int field$11;/* 433 */ boolean isNull$11;/* 434 */ int field$12;/* 435 */ boolean isNull$12;/* 436 */ boolean isNull$13;/* 437 */ int result$14;/* 438 */ org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.LookupInfo lookupInfo$20;/* 439 */ org.apache.flink.table.data.TimestampData field$21;/* 440 */ boolean isNull$21;/* 441 */ boolean isNull$22;/* 442 */ long result$23;/* 443 */ boolean isNull$24;/* 444 */ long result$25;/* 445 */ boolean isNull$26;/* 446 */ long result$27;/* 447 */ boolean isNull$28;/* 448 */ long result$29;/* 449 */ boolean isNull$30;/* 450 */ long result$31;/* 451 */ boolean isNull$32;/* 452 */ long result$33;/* 453 */ boolean isNull$34;/* 454 */ boolean result$35;/* 455 */ boolean isNull$36;/* 456 */ long result$37;/* 457 */ boolean isNull$38;/* 458 */ long result$39;/* 459 */ boolean isNull$40;/* 460 */ long result$41;/* 461 */ boolean isNull$42;/* 462 */ long result$43;/* 463 */ boolean isNull$44;/* 464 */ long result$45;/* 465 */ boolean isNull$46;/* 466 */ long result$47;/* 467 */ boolean isNull$48;/* 468 */ long result$49;/* 469 */ boolean isNull$50;/* 470 */ long result$51;/* 471 */ boolean isNull$52;/* 472 */ long result$53;/* 473 */ boolean isNull$55;/* 474 */ long result$56;/* 475 */ boolean isNull$57;/* 476 */ long result$58;/* 477 */ /* 478 */ org.apache.flink.util.MutableObjectIterator&lt;org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.Entry&gt; iterator =/* 479 */ aggregateMap$7.getEntryIterator();/* 480 */ while (iterator.next(reuseAggMapEntry$19) != null) {/* 481 */ /* 482 */ /* 483 */ /* 484 */ hashAggOutput.replace(reuseAggMapKey$17, reuseAggBuffer$18);/* 485 */ /* 486 */ output.collect(outElement.replace(hashAggOutput));/* 487 */ }/* 488 */ /* 489 */ }/* 490 */ /* 491 *//* 492 */ @Override/* 493 */ public void close() throws Exception {/* 494 */ super.close();/* 495 */ aggregateMap$7.free();/* 496 */ /* 497 */ }/* 498 *//* 499 */ /* 500 */ }/* 501 */ Traceback (most recent call last): File "/home/alex/.config/JetBrains/PyCharm2020.2/scratches/scratch_903.py", line 32, in &lt;module&gt; print(table.to_pandas()) File "/home/alex/work/flink/flink-python/pyflink/table/table.py", line 829, in to_pandas if batches.hasNext(): File "/home/alex/.cache/pypoetry/virtualenvs/relycomply-WA1zLZ2n-py3.8/lib/python3.8/site-packages/py4j/java_gateway.py", line 1285, in __call__ return_value = get_return_value( File "/home/alex/work/flink/flink-python/pyflink/util/exceptions.py", line 147, in deco return f(*a, **kw) File "/home/alex/.cache/pypoetry/virtualenvs/relycomply-WA1zLZ2n-py3.8/lib/python3.8/site-packages/py4j/protocol.py", line 326, in get_return_value raise Py4JJavaError(py4j.protocol.Py4JJavaError: An error occurred while calling o51.hasNext.: java.lang.RuntimeException: Failed to fetch next result at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106) at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:77) at org.apache.flink.table.planner.sinks.SelectTableSinkBase$RowIteratorWrapper.hasNext(SelectTableSinkBase.java:115) at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:355) at org.apache.flink.table.runtime.arrow.ArrowUtils$1.hasNext(ArrowUtils.java:644) at org.apache.flink.table.runtime.arrow.ArrowUtils$2.hasNext(ArrowUtils.java:666) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282) at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79) at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238) at java.base/java.lang.Thread.run(Thread.java:834)Caused by: java.io.IOException: Failed to fetch job execution result at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:175) at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:126) at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:103) ... 16 moreCaused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2022) at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:172) ... 18 moreCaused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147) at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$2(MiniClusterJobClient.java:119) at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:680) at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658) at java.base/java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:2094) at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:117) ... 19 moreCaused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116) at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78) at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:217) at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:210) at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:204) at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:526) at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:413) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.RuntimeException: Could not instantiate generated class 'LocalHashWinAggWithoutKeys$59' at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:67) at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40) at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:70) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperator(OperatorChain.java:613) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:583) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:526) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:574) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:526) at org.apache.flink.streaming.runtime.tasks.OperatorChain.&lt;init&gt;(OperatorChain.java:164) at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:485) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:533) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) at java.base/java.lang.Thread.run(Thread.java:834)Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:68) at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:78) at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:65) ... 13 moreCaused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739) at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:66) ... 15 moreCaused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:81) at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:66) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ... 18 moreCaused by: org.codehaus.commons.compiler.CompileException: Line 351, Column 33: A method named "replace" is not declared in any enclosing class nor any supertype, nor through a static import at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12124) at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:8997) at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5060) at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4421) at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4394) at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5062) at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4394) at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5575) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3781) at org.codehaus.janino.UnitCompiler.access$5900(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3760) at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3732) at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5062) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3732) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360) at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2871) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) at org.codehaus.janino.Java$Block.accept(Java.java:2776) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1842) at org.codehaus.janino.UnitCompiler.access$2200(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1498) at org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$WhileStatement.accept(Java.java:3052) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) at org.codehaus.janino.Java$Block.accept(Java.java:2776) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileTryCatch(UnitCompiler.java:3136) at org.codehaus.janino.UnitCompiler.compileTryCatchFinally(UnitCompiler.java:2966) at org.codehaus.janino.UnitCompiler.compileTryCatchFinallyWithResources(UnitCompiler.java:2770) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2742) at org.codehaus.janino.UnitCompiler.access$2300(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitTryStatement(UnitCompiler.java:1499) at org.codehaus.janino.UnitCompiler$6.visitTryStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$TryStatement.accept(Java.java:3238) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) at org.codehaus.janino.Java$Block.accept(Java.java:2776) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476) at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$IfStatement.accept(Java.java:2947) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) at org.codehaus.janino.Java$Block.accept(Java.java:2776) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476) at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$IfStatement.accept(Java.java:2947) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:78) ... 24 moreHowever it works fine in streaming mode:env_settings = ( EnvironmentSettings.new_instance().in_streaming_mode().use_blink_planner().build())table_env = StreamTableEnvironment.create(environment_settings=env_settings)How the table is created seems irrelevant - this raises the same error:from datetime import datetimefrom pyflink.table import DataTypes, BatchTableEnvironment, EnvironmentSettingsfrom pyflink.table.window import Tumbleenv_settings = ( EnvironmentSettings.new_instance().in_batch_mode().use_blink_planner().build())table_environment = BatchTableEnvironment.create(environment_settings=env_settings)transactions = table_environment.from_elements( [ (1, datetime(2000, 1, 1, 0, 0, 0)), (-2, datetime(2000, 1, 2, 0, 0, 0)), (3, datetime(2000, 1, 3, 0, 0, 0)), (-4, datetime(2000, 1, 4, 0, 0, 0)), ], DataTypes.ROW( [ DataTypes.FIELD("amount", DataTypes.BIGINT()), DataTypes.FIELD("ts", DataTypes.TIMESTAMP(3)), ] ),)table = ( transactions .window(Tumble.over("5.days").on("ts").alias("__window")) .group_by("__window") .select("amount.sum"))print(table.to_pandas())</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.WindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashWindowCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19758" opendate="2020-10-22 00:00:00" fixdate="2020-11-22 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement a new unified File Sink based on the new Sink API</summary>
      <description></description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.RollingPolicyTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.bucketassigners.DateTimeBucketAssignerTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.WriterProperties.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.RowWisePartWriter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.RowWiseBucketWriter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.RollingPolicy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.OnCheckpointRollingPolicy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.DefaultRollingPolicy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.CheckpointRollingPolicy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.PartFileInfo.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.OutputStreamBasedPartFileWriter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.OutputFileConfig.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.InProgressFileWriter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.BulkPartWriter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.BulkBucketWriter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketWriter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.bucketassigners.SimpleVersionedStringSerializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.bucketassigners.DateTimeBucketAssigner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.bucketassigners.BasePathBucketAssigner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketAssigner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.AbstractPartFileWriter.java</file>
      <file type="M">flink-streaming-java.pom.xml</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-files.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19779" opendate="2020-10-23 00:00:00" fixdate="2020-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the "record_" field name prefix for Confluent Avro format deserialization</summary>
      <description>Reported by Maciej Bryński :Problem is this is not compatible. I'm unable to read anything from Kafka using Confluent Registry. Example:I have data in Kafka with following value schema:{ "type": "record", "name": "myrecord", "fields": [ { "name": "f1", "type": "string" } ]}I'm creating table using this avro-confluent format:create table `test` ( `f1` STRING) WITH ( 'connector' = 'kafka', 'topic' = 'test', 'properties.bootstrap.servers' = 'localhost:9092', 'properties.group.id' = 'test1234', 'scan.startup.mode' = 'earliest-offset', 'format' = 'avro-confluent' 'avro-confluent.schema-registry.url' = 'http://localhost:8081');When trying to select data I'm getting error:Unable to find source-code formatter for language: noformat. Available languages are: actionscript, ada, applescript, bash, c, c#, c++, cpp, css, erlang, go, groovy, haskell, html, java, javascript, js, json, lua, none, nyan, objc, perl, php, python, r, rainbow, ruby, scala, sh, sql, swift, visualbasic, xml, yamlSELECT * FROM test;[ERROR] Could not execute SQL statement. Reason:org.apache.avro.AvroTypeException: Found myrecord, expecting record, missing required field record_f1</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.typeutils.AvroSchemaConverterTest.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.typeutils.AvroSchemaConverter.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.test.java.org.apache.flink.formats.avro.registry.confluent.RegistryAvroRowDataSeDeSchemaTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="19781" opendate="2020-10-23 00:00:00" fixdate="2020-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade commons_codec to 1.13 or newer</summary>
      <description>A user reported a dependency vulnerability which affects commons_codec &amp;#91;1&amp;#93;. We should try to upgrade this version to 1.13 or newer.&amp;#91;1&amp;#93; https://lists.apache.org/thread.html/r0dd7ff197b2e3bdd80a0326587ca3d0c22e10d1dba17c769d6da7d7a%40%3Cuser.flink.apache.org%3E</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="19782" opendate="2020-10-23 00:00:00" fixdate="2020-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade antlr to 4.7.1 or newer</summary>
      <description>A user reported dependency vulnerabilities which affect antlr &amp;#91;1&amp;#93;. We should upgrade this dependency to 4.7.1 or newer.&amp;#91;1&amp;#93; https://lists.apache.org/thread.html/r0dd7ff197b2e3bdd80a0326587ca3d0c22e10d1dba17c769d6da7d7a%40%3Cuser.flink.apache.org%3E</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19783" opendate="2020-10-23 00:00:00" fixdate="2020-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade mesos to 1.7 or newer</summary>
      <description>A user reported a dependency vulnerability which affects mesos &amp;#91;1&amp;#93;. We should upgrade mesos to 1.7.0 or newer.&amp;#91;1&amp;#93; https://lists.apache.org/thread.html/r0dd7ff197b2e3bdd80a0326587ca3d0c22e10d1dba17c769d6da7d7a%40%3Cuser.flink.apache.org%3E</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-mesos.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19785" opendate="2020-10-23 00:00:00" fixdate="2020-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade commons-io to 2.7 or newer</summary>
      <description>A user reported a dependency vulnerability which affects commons-io &amp;#91;1&amp;#93;. We should try to upgrade this dependency to 2.7 or newer.&amp;#91;1&amp;#93; https://lists.apache.org/thread.html/r0dd7ff197b2e3bdd80a0326587ca3d0c22e10d1dba17c769d6da7d7a%40%3Cuser.flink.apache.org%3E</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="19787" opendate="2020-10-23 00:00:00" fixdate="2020-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate Filesystem connector to new table source sink interface</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSource.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSink.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.PartitionableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.PartitionableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FileSystemFormatFactory.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.utils.SqlParserHelper.java</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetFsStreamingSinkITCase.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcFsStreamingSinkITCase.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroFilesystemStreamITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="19792" opendate="2020-10-23 00:00:00" fixdate="2020-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Interval join with equal time attributes is not recognized</summary>
      <description>A user reported that interval joins with equal time attribute predicate are not recognized, instead a regular inner join is used:For example:table1 = table_env.from_path("table1")table2 = table_env.from_path("table2")print(table1.join(table2).where("ts = ts2 &amp;&amp; id = id2").select("id, ts")The documentation clearly states that this should be supported:For example, the following predicates are valid interval join conditions:ltime === rtimeltime &gt;= rtime &amp;&amp; ltime &lt; rtime + 10.minutesSource: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/tableApi.html#joinsSee also the discussion here:https://stackoverflow.com/q/64445207/806430</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.IntervalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.IntervalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalJoinRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalIntervalJoinRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19814" opendate="2020-10-26 00:00:00" fixdate="2020-10-26 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Disable FeedbackTransformation for BATCH mode</summary>
      <description>For now we do not support iterations in BATCH mode.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorBatchExecutionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19815" opendate="2020-10-26 00:00:00" fixdate="2020-10-26 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Disable CoFeedbackTransformation for BATCH mode</summary>
      <description>Iterations are not supported in BATCH mode.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorBatchExecutionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19831" opendate="2020-10-27 00:00:00" fixdate="2020-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support grace period for not enough resources being available at cluster startup</summary>
      <description>Implement the hook for ignoring that resources are available, to avoid unnecessary notifications during the startup of the cluster.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="19835" opendate="2020-10-27 00:00:00" fixdate="2020-10-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Don&amp;#39;t emit intermediate watermarks from sources in BATCH execution mode</summary>
      <description>Currently, both sources and watermark/timestamp operators can emit watermarks that we don't really need. We only need a final watermark in BATCH execution mode.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorExecutionModeDetectionTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorBatchExecutionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSource.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.source.TimestampsAndWatermarks.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.source.StreamingTimestampsAndWatermarks.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.source.BatchTimestampsAndWatermarks.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.source.TestingSourceOperator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.source.SourceOperatorEventTimeTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.SourceOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.SourceTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SourceTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperatorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19836" opendate="2020-10-27 00:00:00" fixdate="2020-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add SimpleVersionedSerializerTypeSerializerProxy</summary>
      <description>This allows using a SimpleVersionedSerializer, which the Source and Sink API provide, in places where we need a TypeSerializer, such as when setting the serializer that is used for records in a DataStream/that are sent between operators.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.ClosureCleanerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="19837" opendate="2020-10-27 00:00:00" fixdate="2020-11-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Don&amp;#39;t emit intermediate watermarks from watermark operators in BATCH execution mode</summary>
      <description>Currently, both sources and watermark/timestamp operators can emit watermarks that we don't really need. We only need a final watermark in BATCH execution mode.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.TimestampsAndWatermarksOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.SourceTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.TimestampsAndWatermarksOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperatorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="1984" opendate="2015-5-7 00:00:00" fixdate="2015-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate Flink with Apache Mesos</summary>
      <description>There are some users asking for an integration of Flink into Mesos.There also is a pending pull request for adding Mesos support for Flink: https://github.com/apache/flink/pull/251Update (May '16): a new effort is now underway, building on the recent ResourceManager work.Update (Oct '16): the core functionality is in the master branch. New sub-tasks track remaining work for a first release.Design document: (google doc)</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-mesos.pom.xml</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.cli.FlinkMesosSessionCli.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.LaunchableMesosWorker.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosFlinkResourceManager.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerRunner.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.store.MesosWorkerStore.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.store.StandaloneMesosWorkerStore.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.store.ZooKeeperMesosWorkerStore.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.AcceptOffers.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.Connected.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.Disconnected.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.Error.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.OfferRescinded.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.Registered.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.ReRegistered.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.ResourceOffers.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.SlaveLost.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.StatusUpdate.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.SchedulerProxy.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.TaskSchedulerBuilder.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.Utils.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosArtifactServer.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosConfiguration.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.ZooKeeperUtils.java</file>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManager.scala</file>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.mesos.scheduler.LaunchCoordinator.scala</file>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.mesos.scheduler.ReconciliationCoordinator.scala</file>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.mesos.scheduler.TaskMonitor.scala</file>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.mesos.scheduler.Tasks.scala</file>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.runtime.clusterframework.ContaineredJobManager.scala</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosFlinkResourceManagerTest.java</file>
      <file type="M">flink-mesos.src.test.scala.org.apache.flink.mesos.scheduler.LaunchCoordinatorTest.scala</file>
      <file type="M">flink-mesos.src.test.scala.org.apache.flink.mesos.scheduler.ReconciliationCoordinatorTest.scala</file>
      <file type="M">flink-mesos.src.test.scala.org.apache.flink.mesos.scheduler.TaskMonitorTest.scala</file>
      <file type="M">flink-mesos.src.test.scala.org.apache.flink.mesos.Utils.scala</file>
      <file type="M">flink-mesos.src.test.scala.org.apache.flink.runtime.akka.FSMSpec.scala</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnApplicationMasterRunner.java</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.YarnJobManager.scala</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosConfigKeys.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.RegisteredMesosWorkerNode.scala</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.LaunchableTask.java</file>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.mesos.runtime.clusterframework.MesosJobManager.scala</file>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.mesos.scheduler.ConnectionMonitor.scala</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.ContaineredJobManager.scala</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19844" opendate="2020-10-28 00:00:00" fixdate="2020-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Python UDAF</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="19849" opendate="2020-10-28 00:00:00" fixdate="2020-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check NOTICE files for 1.12 release</summary>
      <description>This will be automated through FLINK-19810</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-ml-parent.flink-ml-uber.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.licenses.LICENSE.javax.activation</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.licenses.LICENSE.javax.activation</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-examples.flink-examples-build-helper.flink-examples-streaming-state-machine.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="19856" opendate="2020-10-28 00:00:00" fixdate="2020-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add EndOfChannelRecovery rescaling epoch</summary>
      <description>This event would allow to tear down "virtual channels"     This event would allow to tear down "virtual channels"     used to read channel state on recovery with unaligned checkpoints and     rescaling.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.InputProcessorUtil.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointedInputGate.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ChannelPersistenceITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.SequentialChannelStateReaderImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ConsumableNotifyingResultPartitionWriterDecorator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedApproximateSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.EndOfChannelStateEvent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.CheckpointedResultSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.CheckpointedResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.Buffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.SequentialChannelStateReaderImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.SequentialChannelStateReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.RecoveredChannelStateHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="19858" opendate="2020-10-28 00:00:00" fixdate="2020-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the new table factory for upsert-kafka connector</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.TestFormatFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.FactoryUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaChangelogTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaSerializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="19864" opendate="2020-10-29 00:00:00" fixdate="2020-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TwoInputStreamTaskTest.testWatermarkMetrics failed with "expected:&lt;1&gt; but was:&lt;-9223372036854775808&gt;"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8541&amp;view=logs&amp;j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&amp;t=7c61167f-30b3-5893-cc38-a9e3d057e3922020-10-28T22:40:44.2528420Z [ERROR] testWatermarkMetrics(org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest) Time elapsed: 1.528 s &lt;&lt;&lt; FAILURE! 2020-10-28T22:40:44.2529225Z java.lang.AssertionError: expected:&lt;1&gt; but was:&lt;-9223372036854775808&gt; 2020-10-28T22:40:44.2541228Z at org.junit.Assert.fail(Assert.java:88) 2020-10-28T22:40:44.2542157Z at org.junit.Assert.failNotEquals(Assert.java:834) 2020-10-28T22:40:44.2542954Z at org.junit.Assert.assertEquals(Assert.java:645) 2020-10-28T22:40:44.2543456Z at org.junit.Assert.assertEquals(Assert.java:631) 2020-10-28T22:40:44.2544002Z at org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.testWatermarkMetrics(TwoInputStreamTaskTest.java:540)</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
    </fixedFiles>
  </bug>
  <bug id="19894" opendate="2020-10-30 00:00:00" fixdate="2020-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use iloc for positional slicing instead of direct slicing in from_pandas</summary>
      <description>When you use floats are index of pandas, it produces a wrong results: &gt;&gt;&gt; import pandas as pd&gt;&gt;&gt; t_env.from_pandas(pd.DataFrame({'a': [1, 2, 3]}, index=[2., 3., 4.])).to_pandas() a0 11 2 This is because direct slicing uses the value as index when the index contains floats: &gt;&gt;&gt; pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.])[2:] a2.0 13.0 24.0 3&gt;&gt;&gt; pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.]).iloc[2:] a4.0 3&gt;&gt;&gt; pd.DataFrame({'a': [1,2,3]}, index=[2, 3, 4])[2:] a4 3 </description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.conversion.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="19896" opendate="2020-10-30 00:00:00" fixdate="2020-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve first-n-row fetching in the rank operator</summary>
      <description>Currently Deduplicate operator only supports first-row deduplication (ordered by proc-time). In scenario of first-n-rows deduplication, the planner has to resort to Rank operator. However, Rank operator is less efficient than Deduplicate due to larger state and more state access.This issue proposes to extend DeduplicateKeepFirstRowFunction to support first-n-rows deduplication. And the original first-row deduplication would be a special case of first-n-rows deduplication.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.RankJsonPlanITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecRank.java</file>
    </fixedFiles>
  </bug>
  <bug id="19897" opendate="2020-10-30 00:00:00" fixdate="2020-11-30 01:00:00" resolution="Done">
    <buginformation>
      <summary>Improve UI related to FLIP-102</summary>
      <description>This ticket collects issues that came up after merging FLIP-102 related changes into master. The following issues should be fixed. Add Tooltip to Heap metrics cell pointing out that the max metrics might differ from the configured maximum value. This tooltip could be made optional and only appears if heap max is different from the configured value. Here's a proposal for the tooltip text: The maximum heap displayed might differ from the configured values depending on the used GC algorithm for this process. Rename "Network Memory Segments" into "Netty Shuffle Buffers" Rename "Network Garbage Collection" into "Garbage Collection"</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.styles.rewrite.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="19899" opendate="2020-10-30 00:00:00" fixdate="2020-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Kinesis][EFO] Optimise error handling to use a separate exception delivery mechanism</summary>
      <description>BackgroundThere is a queue used to pass events between the network client and consumer application. When an error is thrown in the network thread, the queue is cleared to make space for the error event. This means that records will be thrown away to make space for errors (the records would be subsequently reloaded from the shard).ScopeAdd a new mechanism to pass exceptions between threads, meaning data does not need to be discarded. When an error is thrown, the error event will be processed by the consumer once all of the records have been processed.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.FakeKinesisFanOutBehavioursFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyV2Test.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.java</file>
    </fixedFiles>
  </bug>
  <bug id="19908" opendate="2020-10-31 00:00:00" fixdate="2020-11-31 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>FlinkLogicalTableSourceScan and CommonPhysicalTableSourceScan should respect source reuse config option</summary>
      <description>Currently we have the table.optimizer.reuse-sub-plan-enabled config option to configure the reuse of sources. However FlinkLogicalTableSourceScan and CommonPhysicalTableSourceScan do not respect this option and are always reused.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.reuse.SubplanReuser.scala</file>
    </fixedFiles>
  </bug>
  <bug id="1992" opendate="2015-5-8 00:00:00" fixdate="2015-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add convergence criterion to SGD optimizer</summary>
      <description>Currently, Flink's SGD optimizer runs for a fixed number of iterations. It would be good to support a dynamic convergence criterion, too.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.optimization.RegularizationITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.optimization.LossFunctionITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.optimization.GradientDescentITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.optimization.Solver.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.optimization.Regularization.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.optimization.LossFunction.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.optimization.GradientDescent.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19924" opendate="2020-11-2 00:00:00" fixdate="2020-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disallow unaligned checkpoints for iterative jobs</summary>
      <description>For rescaling of Unaligned Checkpoints added in 1.12, EndOfChannelRecovery and upstream alignment will be added.However, this alignment won't work for iterative jobs.There is a need to pass this info to subtasks (whether is iterative) validate on startup a flag to skip validation</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.CheckpointConfig.java</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">docs..includes.generated.execution.checkpointing.configuration.html</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="19931" opendate="2020-11-2 00:00:00" fixdate="2020-11-2 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Do not emit intermediate results for reduce operation BATCH execution mode</summary>
      <description>As described in the FLIP-134 we should emit only a single result at the end of each key in a BATCH execution mode</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.api.datastream.DataStreamBatchExecutionITCase.java</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.KeyedStream.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.AggregationFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.OneInputTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.KeyedStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="19933" opendate="2020-11-2 00:00:00" fixdate="2020-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Execute and collect with limit fails on bounded datastream jobs</summary>
      <description></description>
      <version>1.12.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.api.datastream.DataStreamCollectTestITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.ClientAndIterator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19938" opendate="2020-11-3 00:00:00" fixdate="2020-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement shuffle data read scheduling for sort-merge blocking shuffle</summary>
      <description>As described in https://cwiki.apache.org/confluence/display/FLINK/FLIP-148%3A+Introduce+Sort-Merge+Based+Blocking+Shuffle+to+Flink. shuffle IO scheduling is important for performance. We'd like to Introduce it to sort-merge shuffle first.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriteReadTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.BatchShuffleReadBufferPoolTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeSubpartitionReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFileReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NettyShuffleServiceFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.BatchShuffleReadBufferPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="19939" opendate="2020-11-3 00:00:00" fixdate="2020-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove redundant union from multiple input node</summary>
      <description>Consider the following SQL and the execution plan.WITH T1 AS (SELECT COUNT(*) AS cnt FROM x GROUP BY a), T2 AS (SELECT COUNT(*) AS cnt FROM y GROUP BY d), T3 AS (SELECT b AS cnt FROM x INNER JOIN y ON x.b = y.e)SELECT cnt FROM (SELECT cnt FROM T1) UNION ALL (SELECT cnt FROM T2) UNION ALL (SELECT cnt FROM T3)MultipleInputNode(readOrder=[1,0,0,0], members=[\nUnion(all=[true], union=[cnt])\n:- Union(all=[true], union=[cnt])\n: :- Calc(select=[CAST(cnt) AS cnt])\n: : +- HashAggregate(isMerge=[true], groupBy=[a], select=[a, Final_COUNT(count1$0) AS cnt])\n: : +- [#3] Exchange(distribution=[hash[a]])\n: +- Calc(select=[CAST(cnt) AS cnt])\n: +- HashAggregate(isMerge=[true], groupBy=[d], select=[d, Final_COUNT(count1$0) AS cnt])\n: +- [#4] Exchange(distribution=[hash[d]])\n+- Calc(select=[b AS cnt])\n +- HashJoin(joinType=[InnerJoin], where=[=(b, e)], select=[b, e], build=[right])\n :- [#1] Exchange(distribution=[hash[b]])\n +- [#2] Exchange(distribution=[hash[e]])\n]):- Exchange(distribution=[hash[b]]): +- Calc(select=[b]): +- LegacyTableSourceScan(table=[[default_catalog, default_database, x, source: [TestTableSource(a, b, c, nx)]]], fields=[a, b, c, nx]):- Exchange(distribution=[hash[e]]): +- Calc(select=[e]): +- LegacyTableSourceScan(table=[[default_catalog, default_database, y, source: [TestTableSource(d, e, f, ny)]]], fields=[d, e, f, ny]):- Exchange(distribution=[hash[a]]): +- LocalHashAggregate(groupBy=[a], select=[a, Partial_COUNT(*) AS count1$0]): +- Calc(select=[a]): +- LegacyTableSourceScan(table=[[default_catalog, default_database, x, source: [TestTableSource(a, b, c, nx)]]], fields=[a, b, c, nx])+- Exchange(distribution=[hash[d]]) +- LocalHashAggregate(groupBy=[d], select=[d, Partial_COUNT(*) AS count1$0]) +- Calc(select=[d]) +- LegacyTableSourceScan(table=[[default_catalog, default_database, y, source: [TestTableSource(d, e, f, ny)]]], fields=[d, e, f, ny])The two unions here in multiple input here is actually redundant, as the amount of data shuffled will not change even if they're moved out of the multiple input node. We should remove such redundant union from multiple input nodes.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.MultipleInputCreationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.agg.GroupingSetsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveShuffleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.MultipleInputCreationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.processors.MultipleInputNodeCreationProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="1994" opendate="2015-5-8 00:00:00" fixdate="2015-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add different gain calculation schemes to SGD</summary>
      <description>The current SGD implementation uses as gain for the weight updates the formula stepsize/sqrt(iterationNumber). It would be good to make the gain calculation configurable and to provide different strategies for that. For example: stepsize/(1 + iterationNumber) stepsize*(1 + regularization * stepsize * iterationNumber)^(-3/4)See also how to properly select the gains &amp;#91;1&amp;#93;.Resources:&amp;#91;1&amp;#93; http://arxiv.org/pdf/1107.2490.pdf</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-ml.src.test.scala.org.apache.flink.ml.optimization.GradientDescentITSuite.scala</file>
      <file type="M">flink-libraries.flink-ml.src.main.scala.org.apache.flink.ml.regression.MultipleLinearRegression.scala</file>
      <file type="M">flink-libraries.flink-ml.src.main.scala.org.apache.flink.ml.optimization.Solver.scala</file>
      <file type="M">flink-libraries.flink-ml.src.main.scala.org.apache.flink.ml.optimization.GradientDescent.scala</file>
      <file type="M">flink-batch-connectors.flink-avro.src.test.java.org.apache.flink.api.io.avro.AvroRecordInputFormatTest.java</file>
      <file type="M">docs.libs.ml.optimization.md</file>
      <file type="M">docs.libs.ml.multiple.linear.regression.md</file>
    </fixedFiles>
  </bug>
  <bug id="19940" opendate="2020-11-3 00:00:00" fixdate="2020-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Task names on web UI should change when an operator chain is chained with sources</summary>
      <description>Currently task names on web UI doesn't change even if the operator chain is chained with sources. We should change its name to show that the sources are chained.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19941" opendate="2020-11-3 00:00:00" fixdate="2020-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support sink parallelism configuration to Kafka connector</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.connector.sink.SinkFunctionProvider.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSink.java</file>
      <file type="M">docs.dev.table.connectors.kafka.zh.md</file>
      <file type="M">docs.dev.table.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="19942" opendate="2020-11-3 00:00:00" fixdate="2020-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support sink parallelism configuration to JDBC connector</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.sink.OutputFormatProvider.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.options.JdbcOptions.java</file>
      <file type="M">docs.dev.table.connectors.jdbc.zh.md</file>
      <file type="M">docs.dev.table.connectors.jdbc.md</file>
    </fixedFiles>
  </bug>
  <bug id="19944" opendate="2020-11-3 00:00:00" fixdate="2020-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support sink parallelism configuration to Hive connector</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveDynamicTableFactory.java</file>
      <file type="M">docs.content.docs.connectors.table.filesystem.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.filesystem.md</file>
    </fixedFiles>
  </bug>
  <bug id="19945" opendate="2020-11-3 00:00:00" fixdate="2020-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support sink parallelism configuration to FileSystem connector</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.filesystem.TestRowDataCsvInputFormat.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.stream.StreamingSink.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSink.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableFactory.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">docs.content.docs.connectors.table.filesystem.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.filesystem.md</file>
    </fixedFiles>
  </bug>
  <bug id="19946" opendate="2020-11-3 00:00:00" fixdate="2020-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support sink parallelism configuration to Hbase connector</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.main.java.org.apache.flink.connector.hbase.options.HBaseWriteOptions.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.src.test.java.org.apache.flink.connector.hbase2.HBaseDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.src.main.java.org.apache.flink.connector.hbase2.sink.HBaseDynamicTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.src.main.java.org.apache.flink.connector.hbase2.HBase2DynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.src.test.java.org.apache.flink.connector.hbase1.HBaseDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.src.main.java.org.apache.flink.connector.hbase1.sink.HBaseDynamicTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.src.main.java.org.apache.flink.connector.hbase1.HBase1DynamicTableFactory.java</file>
      <file type="M">docs.dev.table.connectors.hbase.zh.md</file>
      <file type="M">docs.dev.table.connectors.hbase.md</file>
    </fixedFiles>
  </bug>
  <bug id="19947" opendate="2020-11-3 00:00:00" fixdate="2020-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support sink parallelism configuration to Print connector</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.table.PrintConnectorITCase.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.PrintTableSinkFactory.java</file>
      <file type="M">docs.dev.table.connectors.print.zh.md</file>
      <file type="M">docs.dev.table.connectors.print.md</file>
    </fixedFiles>
  </bug>
  <bug id="19948" opendate="2020-11-3 00:00:00" fixdate="2020-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Calling NOW() function throws compile exception</summary>
      <description>The following test code in ScalarOperatorsTest will fail with a compile exceptiontestSqlApi("CAST(NOW() AS BIGINT)", "??")java.lang.RuntimeException: Could not instantiate generated class 'TestFunction$24' at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:57) at org.apache.flink.table.planner.expressions.utils.ExpressionTestBase.evaluateExprs(ExpressionTestBase.scala:143) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:68) at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:78) at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:52) ... 25 moreCaused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739) at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:66) ... 27 moreCaused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:81) at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:66) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ... 30 moreCaused by: org.codehaus.commons.compiler.CompileException: Line 64, Column 22: Assignment conversion not possible from type "long" to type "org.apache.flink.table.data.TimestampData" at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211) at org.codehaus.janino.UnitCompiler.assignmentConversion(UnitCompiler.java:11062) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3790) at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3754) at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3734) at org.codehaus.janino.Java$Assignment.accept(Java.java:4477) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360) at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:78) ... 36 moreThis should be a bug in the code generation for NOW() function.</description>
      <version>None</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlDateTimeUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.NonDeterministicTests.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.BuiltInMethods.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19964" opendate="2020-11-4 00:00:00" fixdate="2020-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Gelly ITCase stuck on Azure in HITSITCase.testPrintWithRMatGraph</summary>
      <description>The HITSITCase has gotten stuck on Azure. Chances are that something in the scheduling or network has broken it.https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8919&amp;view=logs&amp;j=c5f0071e-1851-543e-9a45-9ac140befc32&amp;t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.LocalBufferPoolTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.LocalBufferPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="19974" opendate="2020-11-4 00:00:00" fixdate="2020-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQLClientKafkaITCase.testKafka times out while creating topic caused by "PyFlink end-to-end test"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8967&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d3555292020-11-04T12:00:36.0501135Z Nov 04 12:00:36 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 57.959 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.tests.util.kafka.SQLClientKafkaITCase2020-11-04T12:00:36.0538557Z Nov 04 12:00:36 [ERROR] testKafka[0: kafka-version:2.4.1 kafka-sql-version:universal](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase) Time elapsed: 57.949 s &lt;&lt;&lt; ERROR!2020-11-04T12:00:36.0539781Z Nov 04 12:00:36 java.io.IOException: Process failed due to timeout.2020-11-04T12:00:36.0540659Z Nov 04 12:00:36 at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:130)2020-11-04T12:00:36.0548817Z Nov 04 12:00:36 at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:108)2020-11-04T12:00:36.0549582Z Nov 04 12:00:36 at org.apache.flink.tests.util.AutoClosableProcess.runBlocking(AutoClosableProcess.java:70)2020-11-04T12:00:36.0550231Z Nov 04 12:00:36 at org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResource.createTopic(LocalStandaloneKafkaResource.java:261)2020-11-04T12:00:36.0550909Z Nov 04 12:00:36 at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:136)</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19979" opendate="2020-11-4 00:00:00" fixdate="2020-1-4 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Sanity check after bash e2e tests for no leftover processes</summary>
      <description>As seen in FLINK-19974, if an e2e test is not cleaning up properly, other e2e tests might fail with difficult to diagnose issues.I propose to check that no leftover processes (including docker containers) are running after each bash e2e test.</description>
      <version>1.12.0</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.e2e.uploading.watchdog.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19982" opendate="2020-11-4 00:00:00" fixdate="2020-1-4 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>AggregateReduceGroupingITCase.testSingleAggOnTable_SortAgg fails with "RuntimeException: Job restarted"</summary>
      <description>https://dev.azure.com/georgeryan1322/Flink/_build/results?buildId=336&amp;view=logs&amp;j=a1590513-d0ea-59c3-3c7b-aad756c48f25&amp;t=5129dea2-618b-5c74-1b8f-9ec63a37a8a6[ERROR] Tests run: 16, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 59.688 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase[ERROR] testSingleAggOnTable_SortAgg(org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase) Time elapsed: 2.789 s &lt;&lt;&lt; ERROR!java.lang.RuntimeException: Job restarted at org.apache.flink.streaming.api.operators.collect.UncheckpointedCollectResultBuffer.sinkRestarted(UncheckpointedCollectResultBuffer.java:41) at org.apache.flink.streaming.api.operators.collect.AbstractCollectResultBuffer.dealWithResponse(AbstractCollectResultBuffer.java:87) at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:127) at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:103) at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:77) at org.apache.flink.table.planner.sinks.SelectTableSinkBase$RowIteratorWrapper.hasNext(SelectTableSinkBase.java:115) at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:355) at java.util.Iterator.forEachRemaining(Iterator.java:115) at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:114) at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:298) at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:138) at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:104) at org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase.testSingleAggOnTable(AggregateReduceGroupingITCase.scala:153) at org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase.testSingleAggOnTable_SortAgg(AggregateReduceGroupingITCase.scala:122) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)In the logs, I find occurrences of this:16:37:49,262 [ main] WARN org.apache.flink.streaming.api.operators.collect.CollectResultFetcher [] - An exception occurs when fetching query resultsjava.util.concurrent.ExecutionException: org.apache.flink.runtime.dispatcher.UnavailableDispatcherOperationException: Unable to get JobMasterGateway for initializing job. The requested operation is not available while the JobManager is initializing. at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_242] at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908) ~[?:1.8.0_242] at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sendRequest(CollectResultFetcher.java:163) ~[flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:134) [flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:103) [flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:77) [flink-streaming-java_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.table.planner.sinks.SelectTableSinkBase$RowIteratorWrapper.hasNext(SelectTableSinkBase.java:115) [classes/:?] at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:355) [flink-table-api-java-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at java.util.Iterator.forEachRemaining(Iterator.java:115) [?:1.8.0_242] at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:114) [flink-core-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:298) [test-classes/:?] at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:138) [test-classes/:?] at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:104) [test-classes/:?] at org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase.testSingleAggOnTable(AggregateReduceGroupingITCase.scala:153) [test-classes/:?] at org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase.testSingleAggOnTable_SortAgg(AggregateReduceGroupingITCase.scala:122) [test-classes/:?] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_242] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_242] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_242] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_242] at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) [junit-4.12.jar:4.12] at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.12.jar:4.12] at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) [junit-4.12.jar:4.12] at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.12.jar:4.12] at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.12.jar:4.12] at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.12.jar:4.12] at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) [junit-4.12.jar:4.12] at org.junit.rules.RunRules.evaluate(RunRules.java:20) [junit-4.12.jar:4.12] at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) [junit-4.12.jar:4.12] at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) [junit-4.12.jar:4.12] at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) [junit-4.12.jar:4.12] at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) [junit-4.12.jar:4.12] at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) [junit-4.12.jar:4.12] at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) [junit-4.12.jar:4.12] at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) [junit-4.12.jar:4.12] at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) [junit-4.12.jar:4.12] at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) [junit-4.12.jar:4.12] at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) [junit-4.12.jar:4.12] at org.junit.rules.RunRules.evaluate(RunRules.java:20) [junit-4.12.jar:4.12] at org.junit.runners.ParentRunner.run(ParentRunner.java:363) [junit-4.12.jar:4.12] at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) [surefire-junit4-2.22.1.jar:2.22.1] at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) [surefire-junit4-2.22.1.jar:2.22.1] at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) [surefire-junit4-2.22.1.jar:2.22.1] at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) [surefire-junit4-2.22.1.jar:2.22.1] at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) [surefire-booter-2.22.1.jar:2.22.1] at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) [surefire-booter-2.22.1.jar:2.22.1] at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) [surefire-booter-2.22.1.jar:2.22.1] at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418) [surefire-booter-2.22.1.jar:2.22.1]Caused by: org.apache.flink.runtime.dispatcher.UnavailableDispatcherOperationException: Unable to get JobMasterGateway for initializing job. The requested operation is not available while the JobManager is initializing. at org.apache.flink.runtime.dispatcher.Dispatcher.getJobMasterGateway(Dispatcher.java:793) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.runtime.dispatcher.Dispatcher.performOperationOnJobMasterGateway(Dispatcher.java:800) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.runtime.dispatcher.Dispatcher.deliverCoordinationRequestToCoordinator(Dispatcher.java:631) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at sun.reflect.GeneratedMethodAccessor65.invoke(Unknown Source) ~[?:?] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_242] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_242] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:286) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:201) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:154) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) ~[akka-actor_2.11-2.5.21.jar:2.5.21] at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) ~[akka-actor_2.11-2.5.21.jar:2.5.21] at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) ~[scala-library-2.11.12.jar:?] at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) ~[akka-actor_2.11-2.5.21.jar:2.5.21] at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) ~[scala-library-2.11.12.jar:?] at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[scala-library-2.11.12.jar:?] at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[scala-library-2.11.12.jar:?] at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[akka-actor_2.11-2.5.21.jar:2.5.21] at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) ~[akka-actor_2.11-2.5.21.jar:2.5.21] at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[akka-actor_2.11-2.5.21.jar:2.5.21] at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[akka-actor_2.11-2.5.21.jar:2.5.21] at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~[akka-actor_2.11-2.5.21.jar:2.5.21] at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~[akka-actor_2.11-2.5.21.jar:2.5.21] at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~[akka-actor_2.11-2.5.21.jar:2.5.21] at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[akka-actor_2.11-2.5.21.jar:2.5.21] at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ~[akka-actor_2.11-2.5.21.jar:2.5.21] at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ~[akka-actor_2.11-2.5.21.jar:2.5.21] at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ~[akka-actor_2.11-2.5.21.jar:2.5.21]These are just WARNings, not sure if it is related or not.</description>
      <version>1.12.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectSinkFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="19984" opendate="2020-11-5 00:00:00" fixdate="2020-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add TypeSerializerTestCoverageTest to check whether tests based on SerializerTestBase and TypeSerializerUpgradeTestBase</summary>
      <description>Currently, we have TypeInfoTestCoverageTest which checks that we have a test that extends TypeInformationTestBase for all type infos. But TypeSerializer doesn’t have the same thing that would verify that TypeSerializer has tests that extend SerializerTestBase and TypeSerializerUpgradeTestBase. Therefore we don’t know if test coverage of TypeSerializer is good.This would add TypeSerializerTestCoverageTest to check whether to have tests based on SerializerTestBase and TypeSerializerUpgradeTestBase because all serializers should have tests based on both of them.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.test.java.org.apache.flink.api.scala.typeutils.ScalaOptionSerializerUpgradeTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="19987" opendate="2020-11-5 00:00:00" fixdate="2020-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBaseDynamicTableFactoryTest.testTableSourceFactory failed with "NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9020&amp;view=logs&amp;j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&amp;t=bfbc6239-57a0-5db0-63f3-41551b4f7d512020-11-04T22:30:58.3273231Z testTableSourceFactory(org.apache.flink.connector.hbase1.HBaseDynamicTableFactoryTest) Time elapsed: 0.894 sec &lt;&lt;&lt; ERROR! 2020-11-04T22:30:58.3274285Z org.apache.flink.table.api.ValidationException: 2020-11-04T22:30:58.3274931Z Unable to create a source for reading table 'default.default.t1'. 2020-11-04T22:30:58.3275159Z 2020-11-04T22:30:58.3275364Z Table options are: 2020-11-04T22:30:58.3275523Z 2020-11-04T22:30:58.3275882Z 'connector'='hbase-1.4' 2020-11-04T22:30:58.3276283Z 'table-name'='testHBastTable' 2020-11-04T22:30:58.3276712Z 'zookeeper.quorum'='localhost:2181' 2020-11-04T22:30:58.3277158Z 'zookeeper.znode.parent'='/flink' 2020-11-04T22:30:58.3277553Z at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:125) 2020-11-04T22:30:58.3278358Z at org.apache.flink.connector.hbase1.HBaseDynamicTableFactoryTest.createTableSource(HBaseDynamicTableFactoryTest.java:332) 2020-11-04T22:30:58.3279046Z at org.apache.flink.connector.hbase1.HBaseDynamicTableFactoryTest.testTableSourceFactory(HBaseDynamicTableFactoryTest.java:104) 2020-11-04T22:30:58.3279623Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 2020-11-04T22:30:58.3280067Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 2020-11-04T22:30:58.3280703Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 2020-11-04T22:30:58.3281149Z at java.lang.reflect.Method.invoke(Method.java:498) 2020-11-04T22:30:58.3281615Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) 2020-11-04T22:30:58.3282135Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 2020-11-04T22:30:58.3282637Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) 2020-11-04T22:30:58.3283151Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 2020-11-04T22:30:58.3283749Z at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239) 2020-11-04T22:30:58.3284221Z at org.junit.rules.RunRules.evaluate(RunRules.java:20) 2020-11-04T22:30:58.3284624Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) 2020-11-04T22:30:58.3285091Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) 2020-11-04T22:30:58.3285590Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) 2020-11-04T22:30:58.3286053Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) 2020-11-04T22:30:58.3286487Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) 2020-11-04T22:30:58.3286911Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) 2020-11-04T22:30:58.3287348Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) 2020-11-04T22:30:58.3287779Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) 2020-11-04T22:30:58.3288262Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363) 2020-11-04T22:30:58.3288721Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367) 2020-11-04T22:30:58.3289254Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274) 2020-11-04T22:30:58.3289781Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) 2020-11-04T22:30:58.3290312Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161) 2020-11-04T22:30:58.3291103Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290) 2020-11-04T22:30:58.3291649Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242) 2020-11-04T22:30:58.3292158Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121) 2020-11-04T22:30:58.3292703Z Caused by: java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V 2020-11-04T22:30:58.3293230Z at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357) 2020-11-04T22:30:58.3293876Z at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338) 2020-11-04T22:30:58.3294441Z at org.apache.flink.connector.hbase1.HBase1DynamicTableFactory.createDynamicTableSource(HBase1DynamicTableFactory.java:113) 2020-11-04T22:30:58.3295032Z at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:122) 2020-11-04T22:30:58.3295401Z ... 28 more</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19989" opendate="2020-11-5 00:00:00" fixdate="2020-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add collect operation in Python DataStream API</summary>
      <description>DataStream.executeAndCollect() has already been supported in FLINK-19508. We should also support it in the Python DataStream API.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.api.common.python.PythonBridgeUtils.java</file>
      <file type="M">flink-python.pyflink.table.utils.py</file>
      <file type="M">flink-python.pyflink.table.table.result.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">flink-python.pyflink.common.typeinfo.py</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.TupleTypeInfoBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.RowTypeInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="19992" opendate="2020-11-5 00:00:00" fixdate="2020-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate new orc to Hive source</summary>
      <description>After introducing `OrcColumnarRowFileInputFormat`We need integrate it to Hive, including Hive 2+ and Hive 1.X</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter.java</file>
    </fixedFiles>
  </bug>
  <bug id="19996" opendate="2020-11-5 00:00:00" fixdate="2020-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add end-to-end IT case for Debezium + Kafka + temporal join</summary>
      <description>This is one of the most important use case when we propose FLIP-132. We should add an end-to-end test for this. We should use the source.ts_ms metadata as the rowtime attribute of the kafka debezium table. This is blocked by FLINK-19276.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="19997" opendate="2020-11-5 00:00:00" fixdate="2020-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement an e2e test for sql-client with Confluent Registry Avro format</summary>
      <description>We should add an e2e test that would verify the format as well as packaging of the format sql jar.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.util.FileUtils.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.SQLJobSubmission.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.LocalStandaloneFlinkResourceFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2000" opendate="2015-5-11 00:00:00" fixdate="2015-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add SQL-style aggregations for Table API</summary>
      <description>Right now, the syntax for aggregations is "a.count, a.min" and so on. We could in addition offer "COUNT(a), MIN(a)" and so on.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.GroupedAggreagationsITCase.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.parser.ExpressionParser.scala</file>
    </fixedFiles>
  </bug>
  <bug id="20021" opendate="2020-11-6 00:00:00" fixdate="2020-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup several "Dispatcher"/"Jobmanager" misuses in the docs</summary>
      <description>There are a few places in the ops/monitoring docs where we use "Dispatcher" instead of "JobManager", and "JobManager" instead of "JobMaster".</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.security-ssl.zh.md</file>
      <file type="M">docs.ops.security-ssl.md</file>
      <file type="M">docs.ops.deployment.mesos.zh.md</file>
      <file type="M">docs.ops.deployment.mesos.md</file>
      <file type="M">docs.ops.deployment.local.zh.md</file>
      <file type="M">docs.ops.deployment.local.md</file>
      <file type="M">docs.monitoring.rest.api.zh.md</file>
      <file type="M">docs.monitoring.rest.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="20022" opendate="2020-11-6 00:00:00" fixdate="2020-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move statebackend tradeoffs to from production readiness checklist to statebackend page</summary>
      <description>The production readiness checklist contains a section for choosing the right statebackend, providing pros/const for each statebackend.This should be moved to the actual statebackend page, which is rather light on the topic.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.state.backends.zh.md</file>
      <file type="M">docs.ops.state.state.backends.md</file>
      <file type="M">docs.ops.production.ready.zh.md</file>
      <file type="M">docs.ops.production.ready.md</file>
    </fixedFiles>
  </bug>
  <bug id="20028" opendate="2020-11-6 00:00:00" fixdate="2020-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FileCompactionITCase is unstable</summary>
      <description>The ParquetFileCompactionITCase hangs and times out.Log: https://dev.azure.com/sewen0794/Flink/_build/results?buildId=178&amp;view=logs&amp;j=66592496-52df-56bb-d03e-37509e1d9d0f&amp;t=ae0269db-6796-5583-2e5f-d84757d711aaException:org.junit.runners.model.TestTimedOutException: test timed out after 60 seconds at sun.misc.Unsafe.park(Native Method) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707) at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323) at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908) at org.apache.flink.table.api.internal.TableResultImpl.awaitInternal(TableResultImpl.java:119) at org.apache.flink.table.pi.internal.TableResultImpl.await(TableResultImpl.java:86) at org.apache.flink.table.planner.runtime.stream.sql.FileCompactionITCaseBase.testNonPartition(FileCompactionITCaseBase.java:91)</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.ParallelFiniteTestSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="20029" opendate="2020-11-6 00:00:00" fixdate="2020-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix computed column can&amp;#39;t be defined on the metadata column</summary>
      <description>Currenlty it's not allowed to define the computed column on metadata. It's very useful when users extract the metadata from record and use the information as watermark. </description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SourceWatermarkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.MergeTableLikeUtilTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.MergeTableLikeUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="2003" opendate="2015-5-12 00:00:00" fixdate="2015-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Building on some encrypted filesystems leads to "File name too long" error</summary>
      <description>The classnames generated from the build system can be too long.Creating too long filenames in some encrypted filesystems is not possible, including encfs which is what Ubuntu uses.This the same as this Spark issueThe workaround (taken from the linked issue) is to add in Maven under the compile options: + &lt;arg&gt;-Xmax-classfile-name&lt;/arg&gt;+ &lt;arg&gt;128&lt;/arg&gt;And in SBT add:+ scalacOptions in Compile ++= Seq("-Xmax-classfile-name", "128"),</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.building.md</file>
    </fixedFiles>
  </bug>
  <bug id="20031" opendate="2020-11-6 00:00:00" fixdate="2020-11-6 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Keep the UID of SinkWriter same as the SinkTransformation</summary>
      <description>In this case that we want to migrate the StreamingFileSink to the new sink api we might need to let user set the SinkWriter's uid same as the StreamingFileSink's. So that SinkWriter operator has the opportunity to reuse the old state. (This is just a option.) For this we need to let SinkWriter operator's uid is the same as the SinkTransformation. </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.SinkTransformationTranslatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.SinkTransformationTranslator.java</file>
    </fixedFiles>
  </bug>
  <bug id="20041" opendate="2020-11-7 00:00:00" fixdate="2020-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Watermark push down for kafka connector</summary>
      <description>Support watermark push down for kafka connector.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="20042" opendate="2020-11-7 00:00:00" fixdate="2020-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add end-to-end tests for Kinesis Table sources and sinks</summary>
      <description>Follow-up issue to add end-to-end tests for the recently added KinesisDynamicSource and KinesisDynamicSink. See the discussion in PR #13770 for details.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.KinesisPubsubClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="20045" opendate="2020-11-8 00:00:00" fixdate="2020-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ZooKeeperLeaderElectionTest.testZooKeeperLeaderElectionRetrieval failed with "TimeoutException: Contender was not elected as the leader within 200000ms"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9251&amp;view=logs&amp;j=0da23115-68bb-5dcd-192c-bd4c8adebde1&amp;t=05b74a19-4ee4-5036-c46f-ada307df6cf02020-11-07T10:34:07.5063203Z [ERROR] testZooKeeperLeaderElectionRetrieval(org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest) Time elapsed: 202.445 s &lt;&lt;&lt; ERROR!2020-11-07T10:34:07.5064331Z java.util.concurrent.TimeoutException: Contender was not elected as the leader within 200000ms2020-11-07T10:34:07.5064946Z at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:153)2020-11-07T10:34:07.5065762Z at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:139)2020-11-07T10:34:07.5066565Z at org.apache.flink.runtime.leaderelection.TestingLeaderBase.waitForLeader(TestingLeaderBase.java:48)2020-11-07T10:34:07.5067185Z at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.testZooKeeperLeaderElectionRetrieval(ZooKeeperLeaderElectionTest.java:144)</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingLeaderElectionEventHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="20046" opendate="2020-11-8 00:00:00" fixdate="2020-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StreamTableAggregateTests.test_map_view_iterate is instable</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9279&amp;view=logs&amp;j=821b528f-1eed-5598-a3b4-7f748b13f261&amp;t=4fad9527-b9a5-5015-1b70-8356e5c914902020-11-07T22:50:57.4180758Z _______________ StreamTableAggregateTests.test_map_view_iterate ________________2020-11-07T22:50:57.4181301Z 2020-11-07T22:50:57.4181965Z self = &lt;pyflink.table.tests.test_aggregate.StreamTableAggregateTests testMethod=test_map_view_iterate&gt;2020-11-07T22:50:57.4182348Z 2020-11-07T22:50:57.4182535Z def test_map_view_iterate(self):2020-11-07T22:50:57.4182812Z test_iterate = udaf(TestIterateAggregateFunction())2020-11-07T22:50:57.4183320Z self.t_env.get_config().set_idle_state_retention(datetime.timedelta(days=1))2020-11-07T22:50:57.4183763Z self.t_env.get_config().get_configuration().set_string(2020-11-07T22:50:57.4297555Z "python.fn-execution.bundle.size", "2")2020-11-07T22:50:57.4297922Z # trigger the cache eviction in a bundle.2020-11-07T22:50:57.4308028Z self.t_env.get_config().get_configuration().set_string(2020-11-07T22:50:57.4308653Z "python.state.cache-size", "2")2020-11-07T22:50:57.4308945Z self.t_env.get_config().get_configuration().set_string(2020-11-07T22:50:57.4309382Z "python.map-state.read-cache-size", "2")2020-11-07T22:50:57.4309676Z self.t_env.get_config().get_configuration().set_string(2020-11-07T22:50:57.4310428Z "python.map-state.write-cache-size", "2")2020-11-07T22:50:57.4310701Z self.t_env.get_config().get_configuration().set_string(2020-11-07T22:50:57.4311130Z "python.map-state.iterate-response-batch-size", "2")2020-11-07T22:50:57.4311361Z t = self.t_env.from_elements(2020-11-07T22:50:57.4311691Z [(1, 'Hi_', 'hi'),2020-11-07T22:50:57.4312004Z (1, 'Hi', 'hi'),2020-11-07T22:50:57.4312316Z (2, 'hello', 'hello'),2020-11-07T22:50:57.4312639Z (3, 'Hi_', 'hi'),2020-11-07T22:50:57.4312975Z (3, 'Hi', 'hi'),2020-11-07T22:50:57.4313285Z (4, 'hello', 'hello'),2020-11-07T22:50:57.4313609Z (5, 'Hi2_', 'hi'),2020-11-07T22:50:57.4313908Z (5, 'Hi2', 'hi'),2020-11-07T22:50:57.4314238Z (6, 'hello2', 'hello'),2020-11-07T22:50:57.4314558Z (7, 'Hi', 'hi'),2020-11-07T22:50:57.4315053Z (8, 'hello', 'hello'),2020-11-07T22:50:57.4315396Z (9, 'Hi2', 'hi'),2020-11-07T22:50:57.4315773Z (13, 'Hi3', 'hi')], ['a', 'b', 'c'])2020-11-07T22:50:57.4316023Z self.t_env.create_temporary_view("source", t)2020-11-07T22:50:57.4316299Z table_with_retract_message = self.t_env.sql_query(2020-11-07T22:50:57.4316615Z "select LAST_VALUE(b) as b, LAST_VALUE(c) as c from source group by a")2020-11-07T22:50:57.4316919Z result = table_with_retract_message.group_by(t.c) \2020-11-07T22:50:57.4317197Z .select(test_iterate(t.b).alias("a"), t.c) \2020-11-07T22:50:57.4317619Z .select(col("a").get(0).alias("a"),2020-11-07T22:50:57.4318111Z col("a").get(1).alias("b"),2020-11-07T22:50:57.4318357Z col("a").get(2).alias("c"),2020-11-07T22:50:57.4318586Z col("a").get(3).alias("d"),2020-11-07T22:50:57.4318814Z t.c.alias("e"))2020-11-07T22:50:57.4319023Z assert_frame_equal(2020-11-07T22:50:57.4319208Z &gt; result.to_pandas(),2020-11-07T22:50:57.4319408Z pd.DataFrame([2020-11-07T22:50:57.4319872Z ["hello,hello2", "1,3", 'hello:3,hello2:1', 2, "hello"],2020-11-07T22:50:57.4320398Z ["Hi,Hi2,Hi3", "1,2,3", "Hi:3,Hi2:2,Hi3:1", 3, "hi"]],2020-11-07T22:50:57.4321047Z columns=['a', 'b', 'c', 'd', 'e']))2020-11-07T22:50:57.4321198Z 2020-11-07T22:50:57.4321385Z pyflink/table/tests/test_aggregate.py:468: 2020-11-07T22:50:57.4321648Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 2020-11-07T22:50:57.4322040Z pyflink/table/table.py:807: in to_pandas2020-11-07T22:50:57.4322299Z .collectAsPandasDataFrame(self._j_table, max_arrow_batch_size)2020-11-07T22:50:57.4322794Z .tox/py35-cython/lib/python3.5/site-packages/py4j/java_gateway.py:1286: in __call__2020-11-07T22:50:57.4323103Z answer, self.gateway_client, self.target_id, self.name)2020-11-07T22:50:57.4323351Z pyflink/util/exceptions.py:147: in deco2020-11-07T22:50:57.4323537Z return f(*a, **kw)2020-11-07T22:50:57.4323783Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 2020-11-07T22:50:57.4323963Z 2020-11-07T22:50:57.4324225Z answer = 'xro8653'2020-11-07T22:50:57.4324496Z gateway_client = &lt;py4j.java_gateway.GatewayClient object at 0x7fe5c619db70&gt;2020-11-07T22:50:57.4324943Z target_id = 'z:org.apache.flink.table.runtime.arrow.ArrowUtils'2020-11-07T22:50:57.4325312Z name = 'collectAsPandasDataFrame'2020-11-07T22:50:57.4325439Z 2020-11-07T22:50:57.4325839Z def get_return_value(answer, gateway_client, target_id=None, name=None):2020-11-07T22:50:57.4326420Z """Converts an answer received from the Java gateway into a Python object.2020-11-07T22:50:57.4326648Z 2020-11-07T22:50:57.4326881Z For example, string representation of integers are converted to Python2020-11-07T22:50:57.4327193Z integer, string representation of objects are converted to JavaObject2020-11-07T22:50:57.4327451Z instances, etc.2020-11-07T22:50:57.4327614Z 2020-11-07T22:50:57.4327819Z :param answer: the string returned by the Java gateway2020-11-07T22:50:57.4328157Z :param gateway_client: the gateway client used to communicate with the Java2020-11-07T22:50:57.4329738Z Gateway. Only necessary if the answer is a reference (e.g., object,2020-11-07T22:50:57.4330018Z list, map)2020-11-07T22:50:57.4330273Z :param target_id: the name of the object from which the answer comes from2020-11-07T22:50:57.4330588Z (e.g., *object1* in `object1.hello()`). Optional.2020-11-07T22:50:57.4330873Z :param name: the name of the member from which the answer comes from2020-11-07T22:50:57.4331170Z (e.g., *hello* in `object1.hello()`). Optional.2020-11-07T22:50:57.4331375Z """2020-11-07T22:50:57.4331542Z if is_error(answer)[0]:2020-11-07T22:50:57.4331761Z if len(answer) &gt; 1:2020-11-07T22:50:57.4331954Z type = answer[1]2020-11-07T22:50:57.4332222Z value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)2020-11-07T22:50:57.4332531Z if answer[1] == REFERENCE_TYPE:2020-11-07T22:50:57.4332757Z raise Py4JJavaError(2020-11-07T22:50:57.4333016Z "An error occurred while calling {0}{1}{2}.\n".2020-11-07T22:50:57.4333303Z &gt; format(target_id, ".", name), value)2020-11-07T22:50:57.4333700Z E py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame.2020-11-07T22:50:57.4334558Z E : java.lang.RuntimeException: Could not remove element ',,,1,hi', should never happen.2020-11-07T22:50:57.4335019Z E at org.apache.flink.table.runtime.arrow.ArrowUtils.filterOutRetractRows(ArrowUtils.java:708)2020-11-07T22:50:57.4335479Z E at org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame(ArrowUtils.java:635)2020-11-07T22:50:57.4336238Z E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-11-07T22:50:57.4336645Z E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-11-07T22:50:57.4337099Z E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-11-07T22:50:57.4337485Z E at java.lang.reflect.Method.invoke(Method.java:498)2020-11-07T22:50:57.4337911Z E at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)2020-11-07T22:50:57.4338410Z E at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)2020-11-07T22:50:57.4338859Z E at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)2020-11-07T22:50:57.4339324Z E at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)2020-11-07T22:50:57.4339810Z E at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)2020-11-07T22:50:57.4340260Z E at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)2020-11-07T22:50:57.4340651Z E at java.lang.Thread.run(Thread.java:748)</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
    </fixedFiles>
  </bug>
  <bug id="2005" opendate="2015-5-12 00:00:00" fixdate="2015-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove dependencies on Record APIs for flink-jdbc module</summary>
      <description>Need to remove dependency on old Record API in the flink-jdbc module.Hopefully we could just move them to use common operators APIs instead.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-jdbc.src.test.java.org.apache.flink.api.java.record.io.jdbc.JDBCOutputFormatTest.java</file>
      <file type="M">flink-staging.flink-jdbc.src.test.java.org.apache.flink.api.java.record.io.jdbc.JDBCInputFormatTest.java</file>
      <file type="M">flink-staging.flink-jdbc.src.test.java.org.apache.flink.api.java.record.io.jdbc.DevNullLogStream.java</file>
      <file type="M">flink-staging.flink-jdbc.src.main.java.org.apache.flink.api.java.record.io.jdbc.JDBCOutputFormat.java</file>
      <file type="M">flink-staging.flink-jdbc.src.main.java.org.apache.flink.api.java.record.io.jdbc.JDBCInputFormat.java</file>
      <file type="M">flink-staging.flink-jdbc.src.main.java.org.apache.flink.api.java.record.io.jdbc.example.JDBCExample.java</file>
      <file type="M">flink-staging.flink-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20050" opendate="2020-11-9 00:00:00" fixdate="2020-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SourceCoordinatorProviderTest.testCheckpointAndReset failed with NullPointerException</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9322&amp;view=logs&amp;j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&amp;t=7c61167f-30b3-5893-cc38-a9e3d057e3922020-11-08T22:24:39.5642544Z [ERROR] testCheckpointAndReset(org.apache.flink.runtime.source.coordinator.SourceCoordinatorProviderTest) Time elapsed: 0.954 s &lt;&lt;&lt; ERROR!2020-11-08T22:24:39.5643055Z java.lang.NullPointerException2020-11-08T22:24:39.5643578Z at org.apache.flink.runtime.source.coordinator.SourceCoordinatorProviderTest.testCheckpointAndReset(SourceCoordinatorProviderTest.java:94)</description>
      <version>1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="20053" opendate="2020-11-9 00:00:00" fixdate="2020-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for file compaction</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.filesystem.zh.md</file>
      <file type="M">docs.dev.table.connectors.filesystem.md</file>
    </fixedFiles>
  </bug>
  <bug id="20059" opendate="2020-11-9 00:00:00" fixdate="2020-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Outdated SQL docs on aggregate functions&amp;#39; merge</summary>
      <description>In the java docs as well as the user docs, the merge method of an aggregation UDF is described as optional, e.g.Merges a group of accumulator instances into one accumulator instance. This function must be implemented for data stream session window grouping aggregates and data set grouping aggregates.However, it seems that nowadays this method is required in more cases (I stumbled on this for a HOP window in streaming):StreamExecGlobalGroupAggregate.scala .needMerge(mergedAccOffset, mergedAccOnHeap, mergedAccExternalTypes)StreamExecGroupWindowAggregateBase.scala generator.needMerge(mergedAccOffset = 0, mergedAccOnHeap = false)StreamExecIncrementalGroupAggregate.scala .needMerge(mergedAccOffset, mergedAccOnHeap = true, mergedAccExternalTypes)StreamExecLocalGroupAggregate.scala .needMerge(mergedAccOffset = 0, mergedAccOnHeap = true)</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.TableAggregateFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.AggregateFunction.java</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="20074" opendate="2020-11-10 00:00:00" fixdate="2020-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix can&amp;#39;t generate plan when joining on changelog source without updates</summary>
      <description>INSTANCE: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9379&amp;view=logs&amp;s=ae4f8708-9994-57d3-c2d7-b892156e7812&amp;j=e25d5e7e-2a9c-5589-4940-0b638d75a414</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
    </fixedFiles>
  </bug>
  <bug id="20077" opendate="2020-11-10 00:00:00" fixdate="2020-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot register a view with MATCH_RECOGNIZE clause</summary>
      <description>TableEnvironment env = TableEnvironment.create(EnvironmentSettings.newInstance() .useBlinkPlanner() .build()); env.executeSql("" + "CREATE TEMPORARY TABLE data (\n" + " id INT,\n" + " ts AS PROCTIME()\n" + ") WITH (\n" + " 'connector' = 'datagen',\n" + " 'rows-per-second' = '3',\n" + " 'fields.id.kind' = 'sequence',\n" + " 'fields.id.start' = '1000000',\n" + " 'fields.id.end' = '2000000'\n" + ")"); env.executeSql("" + "CREATE TEMPORARY VIEW events AS \n" + "SELECT 1 AS key, id, MOD(id, 10) AS measurement, ts \n" + "FROM data"); env.executeSql("" + "CREATE TEMPORARY VIEW foo AS \n" + "SELECT * \n" + "FROM events MATCH_RECOGNIZE (\n" + " PARTITION BY key \n" + " ORDER BY ts ASC \n" + " MEASURES \n" + " this_step.id as startId,\n" + " next_step.id as nextId,\n" + " this_step.ts AS ts1,\n" + " next_step.ts AS ts2,\n" + " next_step.measurement - this_step.measurement AS diff \n" + " AFTER MATCH SKIP TO NEXT ROW \n" + " PATTERN (this_step next_step)\n" + " DEFINE this_step AS TRUE,\n" + " next_step AS TRUE\n" + ")"); env.executeSql("SELECT * FROM foo");fails with: java.lang.AssertionError at org.apache.calcite.sql.SqlMatchRecognize$SqlMatchRecognizeOperator.createCall(SqlMatchRecognize.java:274) at org.apache.calcite.sql.util.SqlShuttle$CallCopyingArgHandler.result(SqlShuttle.java:117) at org.apache.calcite.sql.util.SqlShuttle$CallCopyingArgHandler.result(SqlShuttle.java:101) at org.apache.calcite.sql.util.SqlShuttle.visit(SqlShuttle.java:67) at org.apache.flink.table.planner.utils.Expander$Expanded$1.visit(Expander.java:153) at org.apache.flink.table.planner.utils.Expander$Expanded$1.visit(Expander.java:130) at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) at org.apache.calcite.sql.util.SqlShuttle$CallCopyingArgHandler.visitChild(SqlShuttle.java:134) at org.apache.calcite.sql.util.SqlShuttle$CallCopyingArgHandler.visitChild(SqlShuttle.java:101) at org.apache.calcite.sql.SqlOperator.acceptCall(SqlOperator.java:879) at org.apache.calcite.sql.SqlSelectOperator.acceptCall(SqlSelectOperator.java:133) at org.apache.calcite.sql.util.SqlShuttle.visit(SqlShuttle.java:66) at org.apache.flink.table.planner.utils.Expander$Expanded$1.visit(Expander.java:153) at org.apache.flink.table.planner.utils.Expander$Expanded$1.visit(Expander.java:130) at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) at org.apache.flink.table.planner.utils.Expander$Expanded.substitute(Expander.java:168) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertViewQuery(SqlToOperationConverter.java:728) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertCreateView(SqlToOperationConverter.java:699) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:226) at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:658) at org.apache.flink.table.planner.runtime.stream.table.FunctionITCase.test(FunctionITCase.java:165) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:220) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:53)</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="20086" opendate="2020-11-11 00:00:00" fixdate="2020-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for the open method of UserDefinedFunction</summary>
      <description>According to the questions asked by PyFlink users so far, many users are not aware that there is a open method in UserDefinedFunction where they could perform initialization work. This method is especially useful for ML users where they could perform ML mode initialization.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.python.table.udfs.python.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.python.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="20093" opendate="2020-11-11 00:00:00" fixdate="2020-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a download page for all optional sql client components</summary>
      <description>It would be nice to have a single page that lists all optional binaries for sql client. We could link such a page from the "Optional components" section in the https://flink.apache.org/downloads.html</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.kinesis.md</file>
      <file type="M">docs.dev.table.connectors.kafka.md</file>
      <file type="M">docs.dev.table.connectors.jdbc.md</file>
      <file type="M">docs.dev.table.connectors.hbase.md</file>
      <file type="M">docs.dev.table.connectors.formats.parquet.md</file>
      <file type="M">docs.dev.table.connectors.formats.orc.md</file>
      <file type="M">docs.dev.table.connectors.formats.json.md</file>
      <file type="M">docs.dev.table.connectors.formats.debezium.md</file>
      <file type="M">docs.dev.table.connectors.formats.csv.md</file>
      <file type="M">docs.dev.table.connectors.formats.canal.md</file>
      <file type="M">docs.dev.table.connectors.formats.avro.md</file>
      <file type="M">docs.dev.table.connectors.formats.avro-confluent.md</file>
      <file type="M">docs.dev.table.connectors.elasticsearch.md</file>
    </fixedFiles>
  </bug>
  <bug id="20096" opendate="2020-11-11 00:00:00" fixdate="2020-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up PyFlink documentation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.fig.pyflink.svg</file>
      <file type="M">docs.dev.python.table-api-users-guide.index.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.index.md</file>
      <file type="M">docs.try-flink.python.api.zh.md</file>
      <file type="M">docs.try-flink.python.api.md</file>
      <file type="M">docs.try-flink.index.zh.md</file>
      <file type="M">docs.index.zh.md</file>
      <file type="M">docs.index.md</file>
      <file type="M">docs.dev.python.index.zh.md</file>
      <file type="M">docs.dev.python.index.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.conversion.of.pandas.md</file>
    </fixedFiles>
  </bug>
  <bug id="20098" opendate="2020-11-11 00:00:00" fixdate="2020-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t add flink-connector-files to flink-dist, make dependencies explicit</summary>
      <description>We currently add both flink-connector-files and flink-connector-base to flink-dist. This implies, that users should use the dependency like this:&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-files&lt;/artifactId&gt; &lt;version&gt;${project.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;which differs from other connectors where users don't need to specify &lt;scope&gt;provided&lt;/scope&gt;.Also, flink-connector-files has flink-connector-base as a provided dependency, which means that examples that use this dependency will not run out-of-box in IntelliJ because transitive provided dependencies will not be considered.I propose to just remove the dependencies from flink-dist and let users use the File Connector like any other connector.I believe the initial motivation for "providing" the File Connector in flink-dist was to allow us to use the File Connector under the hood in methods such as StreamExecutionEnvironment.readFile(...). We could decide to deprecate and remove those methods or re-add the File Connector as an explicit (non-provided) dependency again in the future.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-uber.pom.xml</file>
      <file type="M">flink-table.flink-table-uber-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-common.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-files.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20102" opendate="2020-11-12 00:00:00" fixdate="2020-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update HBase connector documentation for HBase 2.x supporting</summary>
      <description>Currently, the HBase connector page says it only supports HBase 1.4.x.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.index.md</file>
      <file type="M">docs..data.sql-connectors.yml</file>
      <file type="M">docs.dev.table.connectors.hbase.zh.md</file>
      <file type="M">docs.dev.table.connectors.hbase.md</file>
    </fixedFiles>
  </bug>
  <bug id="20114" opendate="2020-11-12 00:00:00" fixdate="2020-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix a few KafkaSource-related bugs</summary>
      <description>Feature introduced in https://issues.apache.org/jira/browse/FLINK-18323-------- General Information about the Flink 1.12 release testingWhen testing a feature, consider the following aspects: Is the documentation easy to understand Are the error messages, log messages, APIs etc. easy to understand Is the feature working as expected under normal conditions Is the feature working / failing as expected with invalid input, induced errors etc.If you find a problem during testing, please file a ticket (Priority=Critical; Fix Version = 1.12.0), and link it in this testing ticket.During the testing, and once you are finished, please write a short summary of all things you have tested.</description>
      <version>1.12.0</version>
      <fixedVersion>1.13.0,1.12.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorContextTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SplitEnumeratorContext.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.enumerator.initializer.SpecifiedOffsetsInitializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessorFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.KafkaSourceOptions.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.KafkaSourceBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.reader.KafkaSourceReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.reader.KafkaSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.KafkaSourceITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="20118" opendate="2020-11-12 00:00:00" fixdate="2020-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test New File Source API - Continuous Streaming Execution</summary>
      <description>General Information about the Flink 1.12 release testingWhen testing a feature, consider the following aspects: Is the documentation easy to understand Are the error messages, log messages, APIs etc. easy to understand Is the feature working as expected under normal conditions Is the feature working / failing as expected with invalid input, induced errors etc.If you find a problem during testing, please file a ticket (Priority=Critical; Fix Version = 1.12.0), and link it in this testing ticket.During the testing keep us updated on tests conducted, or please write a short summary of all things you have tested in the end.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.FileSourceTextLinesITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="20130" opendate="2020-11-12 00:00:00" fixdate="2020-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ZStandard format to inputs</summary>
      <description>Allow Flink to read files compressed in ZStandard (.zst)</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.compression.ZStandardInputStreamFactory.java</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.io.GenericCsvInputFormatTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.FileInputFormat.java</file>
      <file type="M">docs.content.zh.docs.dev.dataset.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="20140" opendate="2020-11-13 00:00:00" fixdate="2020-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation of TableResult.collect for Python Table API</summary>
      <description>The documentation of TableResult.collect is missing for Python Table API. This API is very useful for users and we should add clear documentation about it.</description>
      <version>1.12.0</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.table.result.py</file>
      <file type="M">flink-python.pyflink.table.table.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">docs.content.docs.dev.python.table.intro.to.table.api.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.intro.to.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="20144" opendate="2020-11-13 00:00:00" fixdate="2020-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change link format to "{% link ... %}" in docs/ops dir</summary>
      <description>Some documents' link format in docs/ops is "{{site.baseurl}/... }". But it is preferred to use "{% link ... %}" format.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.savepoints.md</file>
      <file type="M">docs.ops.upgrading.zh.md</file>
      <file type="M">docs.ops.upgrading.md</file>
      <file type="M">docs.ops.state.state.backends.zh.md</file>
      <file type="M">docs.ops.state.state.backends.md</file>
      <file type="M">docs.ops.state.savepoints.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.config.md</file>
      <file type="M">docs.ops.config.zh.md</file>
      <file type="M">docs.ops.deployment.cluster.setup.md</file>
      <file type="M">docs.ops.deployment.docker.md</file>
      <file type="M">docs.ops.deployment.docker.zh.md</file>
      <file type="M">docs.ops.deployment.kubernetes.md</file>
      <file type="M">docs.ops.deployment.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.local.md</file>
      <file type="M">docs.ops.deployment.local.zh.md</file>
      <file type="M">docs.ops.deployment.mesos.md</file>
      <file type="M">docs.ops.deployment.mesos.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.zh.md</file>
      <file type="M">docs.ops.filesystems.azure.md</file>
      <file type="M">docs.ops.filesystems.azure.zh.md</file>
      <file type="M">docs.ops.filesystems.index.md</file>
      <file type="M">docs.ops.filesystems.index.zh.md</file>
      <file type="M">docs.ops.filesystems.oss.md</file>
      <file type="M">docs.ops.filesystems.oss.zh.md</file>
      <file type="M">docs.ops.filesystems.s3.md</file>
      <file type="M">docs.ops.filesystems.s3.zh.md</file>
      <file type="M">docs.ops.jobmanager.high.availability.md</file>
      <file type="M">docs.ops.jobmanager.high.availability.zh.md</file>
      <file type="M">docs.ops.memory.mem.migration.md</file>
      <file type="M">docs.ops.memory.mem.migration.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.md</file>
      <file type="M">docs.ops.memory.mem.setup.jobmanager.md</file>
      <file type="M">docs.ops.memory.mem.setup.jobmanager.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.tm.md</file>
      <file type="M">docs.ops.memory.mem.setup.tm.zh.md</file>
      <file type="M">docs.ops.memory.mem.trouble.md</file>
      <file type="M">docs.ops.memory.mem.trouble.zh.md</file>
      <file type="M">docs.ops.memory.mem.tuning.md</file>
      <file type="M">docs.ops.memory.mem.tuning.zh.md</file>
      <file type="M">docs.ops.production.ready.md</file>
      <file type="M">docs.ops.production.ready.zh.md</file>
      <file type="M">docs.ops.python.shell.md</file>
      <file type="M">docs.ops.python.shell.zh.md</file>
      <file type="M">docs.ops.security-kerberos.md</file>
      <file type="M">docs.ops.security-kerberos.zh.md</file>
      <file type="M">docs.ops.security-ssl.md</file>
      <file type="M">docs.ops.security-ssl.zh.md</file>
      <file type="M">docs.ops.state.checkpoints.md</file>
      <file type="M">docs.ops.state.checkpoints.zh.md</file>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
      <file type="M">docs.ops.state.large.state.tuning.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="20145" opendate="2020-11-13 00:00:00" fixdate="2020-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming job fails with IllegalStateException: Should only poll priority events</summary>
      <description>While testing the 1.12 release, I came across the following failure cause:2020-11-13 09:41:52,110 WARN org.apache.flink.runtime.taskmanager.Task [] - dynamic filter (3/4)#0 (b977944851531f96e5324e786f055eb7) switched from RUNNING to FAILED.java.lang.IllegalStateException: Should only poll priority events at org.apache.flink.util.Preconditions.checkState(Preconditions.java:198) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.processPriorityEvents(CheckpointedInputGate.java:116) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:283) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:184) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:577) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:541) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) [flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) [flink-dist_2.11-1.12.0.jar:1.12.0] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_222]I have unaligned checkpointing enabled, the failing operator is a CoFlatMapFunction. The error happend on all four TaskManagers, very soon after job submission. The error doesn't happen when unaligned checkpointing is disabled.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PrioritizedDeque.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.FileBasedBufferIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.java</file>
    </fixedFiles>
  </bug>
  <bug id="20147" opendate="2020-11-13 00:00:00" fixdate="2020-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Invalid lambda deserialization" when trying to read metadata from Kafka in SQL</summary>
      <description>When creating a table to read from Kafka like so using the recently added support for kafka metadata in 1.12:CREATE TABLE test_table ( `foo` STRING, `bar` STRING, `record` STRING, `offset` BIGINT METADATA VIRTUAL) WITH ( 'connector' = 'kafka', 'topic' = 'kewltest.r-test.dummy', 'key.fields' = 'foo;bar', 'key.format' = 'json', 'value.format' = 'json', 'value.fields-include' = 'EXCEPT_KEY', 'scan.startup.mode' = 'earliest-offset', 'properties.bootstrap.servers' = '&lt;HOST:PORT&gt;', 'properties.group.id' = '&lt;CONSUMER-GROUP&gt;')the "offset" column causes queries of the table to fail with an exception: Caused by: java.lang.IllegalArgumentException: Invalid lambda deserializationFull stack trace is attached.Left priority as default as I wasn't sure what to set it to.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.debezium.DebeziumJsonDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.debezium.DebeziumJsonDecodingFormat.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="20180" opendate="2020-11-17 00:00:00" fixdate="2020-11-17 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Translation the FileSink Document into Chinese</summary>
      <description>Translate the newly added FileSink documentation into Chinese</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.filesystems.s3.zh.md</file>
      <file type="M">docs.dev.connectors.index.zh.md</file>
      <file type="M">docs.dev.connectors.file.sink.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="20183" opendate="2020-11-17 00:00:00" fixdate="2020-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the default PYTHONPATH is overwritten in client side</summary>
      <description></description>
      <version>1.10.0,1.11.2,1.12.0</version>
      <fixedVersion>1.10.3,1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonEnvUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="20184" opendate="2020-11-17 00:00:00" fixdate="2020-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update hive streaming read and temporal table documents</summary>
      <description>The hive streaming read and temporal table document has been out of style, we need to update it.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.hive.hive.read.write.zh.md</file>
      <file type="M">docs.dev.table.connectors.hive.hive.read.write.md</file>
    </fixedFiles>
  </bug>
  <bug id="20191" opendate="2020-11-17 00:00:00" fixdate="2020-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for FLIP-95 ability interfaces</summary>
      <description></description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.abilities.SupportsWatermarkPushDown.java</file>
      <file type="M">docs.dev.table.sourceSinks.zh.md</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
      <file type="M">docs.dev.table.connectors.upsert-kafka.zh.md</file>
      <file type="M">docs.dev.table.connectors.upsert-kafka.md</file>
      <file type="M">docs.dev.table.connectors.kafka.zh.md</file>
      <file type="M">docs.dev.table.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="20214" opendate="2020-11-18 00:00:00" fixdate="2020-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unnecessary warning log when starting a k8s session cluster</summary>
      <description>2020-11-18 17:46:36,727 WARN org.apache.flink.kubernetes.kubeclient.decorators.HadoopConfMountDecorator [] - Found 0 files in directory null/etc/hadoop, skip to mount the Hadoop Configuration ConfigMap.2020-11-18 17:46:36,727 WARN org.apache.flink.kubernetes.kubeclient.decorators.HadoopConfMountDecorator [] - Found 0 files in directory null/etc/hadoop, skip to create the Hadoop Configuration ConfigMap.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.parameters.AbstractKubernetesParametersTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.AbstractKubernetesParameters.java</file>
    </fixedFiles>
  </bug>
  <bug id="20216" opendate="2020-11-18 00:00:00" fixdate="2020-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move "Configuration" and "Environment Variables" under "Python API" section</summary>
      <description>"Configuration" and "Environment Variables" are currently under the documentation of "Python Table API". However, these sections apply for both Python DataStream API and Python Table API and so we should move them under "Python API".</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.table.environment.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.table.environment.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.python.config.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.python.config.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.intro.to.table.api.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.intro.to.table.api.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.index.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.index.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.environment.variables.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.environment.variables.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.dependency.management.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.dependency.management.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.conversion.of.pandas.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.conversion.of.pandas.md</file>
      <file type="M">docs.dev.python.installation.zh.md</file>
      <file type="M">docs.dev.python.installation.md</file>
    </fixedFiles>
  </bug>
  <bug id="20235" opendate="2020-11-19 00:00:00" fixdate="2020-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing Hive dependencies</summary>
      <description>I tried following the setup here: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/hive/#dependenciesI put the flink-sql-connector-hive-2.3.6 in the \lib directory and tried running queries (as described in https://ci.apache.org/projects/flink/flink-docs-master/dev/table/hive/hive_streaming.html) via sql-client.SET table.sql-dialect=hive;CREATE TABLE hive_table ( user_id STRING, order_amount DOUBLE) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES ( 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00', 'sink.partition-commit.trigger'='partition-time', 'sink.partition-commit.delay'='1 s', 'sink.partition-commit.policy.kind'='metastore,success-file');SET table.sql-dialect=default;SELECT * FROM hive_table;It fails with:Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.flink.hive.shaded.parquet.format.converter.ParquetMetadataConverter at org.apache.flink.hive.shaded.formats.parquet.ParquetVectorizedInputFormat.createReader(ParquetVectorizedInputFormat.java:112) at org.apache.flink.hive.shaded.formats.parquet.ParquetVectorizedInputFormat.createReader(ParquetVectorizedInputFormat.java:73) at org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter.createReader(HiveBulkFormatAdapter.java:99) at org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter.createReader(HiveBulkFormatAdapter.java:62) at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.checkSplitOrStartNext(FileSourceSplitReader.java:110) at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:68) at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:136) at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:100) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ... 1 more</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-sql-parquet.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-parquet.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20240" opendate="2020-11-19 00:00:00" fixdate="2020-11-19 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add setter in the environment for the `execution.runtime-mode`</summary>
      <description>This is the outcome from the discussion here https://issues.apache.org/jira/browse/FLINK-20115</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.SinkITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.api.datastream.DataStreamBatchExecutionITCase.java</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.RuntimeExecutionMode.java</file>
    </fixedFiles>
  </bug>
  <bug id="20241" opendate="2020-11-19 00:00:00" fixdate="2020-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve exception message when hive deps are missing on JM/TM</summary>
      <description>I followed the setup here: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/hive/#dependenciesI put the flink-sql-connector-hive-2.3.6 in the \lib directoryI tried running queries against an ORC table in hive from sql-client:SET table.sql-dialect=hive; CREATE TABLE hive_table_orc ( user_id STRING, order_amount DOUBLE ) PARTITIONED BY (dt STRING, hr STRING) STORED AS orc TBLPROPERTIES ( 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00', 'sink.partition-commit.trigger'='partition-time', 'sink.partition-commit.delay'='1 s', 'sink.partition-commit.policy.kind'='metastore,success-file', 'streaming-source.enable'='true');SET table.sql-dialect=default;insert into hive_table_orc VALUES ('1', 123.0, '2020-11-11', '12');// or SELECT * FROM hive_table_orc;but I am getting:org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116) at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78) at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:224) at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:217) at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:208) at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:534) at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:89) at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:419) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:286) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:201) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:154) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: org.apache.flink.streaming.runtime.tasks.StreamTaskException: Cannot instantiate user function. at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:325) at org.apache.flink.streaming.runtime.tasks.OperatorChain.&lt;init&gt;(OperatorChain.java:146) at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:485) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:530) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) at java.lang.Thread.run(Thread.java:748)Caused by: java.io.StreamCorruptedException: unexpected block data at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1549) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2125) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2125) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422) at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562) at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550) at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511) at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:310) ... 6 more</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.write.HiveWriterFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.TimestampedHiveInputSplit.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveTableInputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveCompactReaderFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.JobConfWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTablePartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="20245" opendate="2020-11-19 00:00:00" fixdate="2020-11-19 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Document how to create a Hive catalog from DDL</summary>
      <description>I'd appreciate if the documentation contained a description how to create the hive catalog from DDL. What I am missing especially are the options that HiveCatalog expects (type, conf-dir). We should have a table somewhere with a description possible values etc. the same way as we have such tables for other connectors and formats. See e.g. https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/kafka.html#connector-options</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.hive.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="20261" opendate="2020-11-20 00:00:00" fixdate="2020-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Uncaught exception in ExecutorNotifier due to split assignment broken by failed task</summary>
      <description>While trying to extend FileSourceTextLinesITCase::testContinuousTextFileSourceWithTaskManagerFailover with recovery test after TM failure (TestingMiniCluster::terminateTaskExecutor, branch) in FLINK-20118, I encountered the following case: SourceCoordinatorContext::assignSplits schedules async assignment (all reader tasks alive) call TestingMiniCluster::terminateTaskExecutor while doing writeFile in a loop of testContinuousTextFileSource causes graceful TaskExecutor::onStop shutdown causes TM/RM disconnect and failing slot allocations in JM by RM eventually causes SourceCoordinatorContext::unregisterSourceReader actual assignment starts (SourceCoordinatorContext::assignSplits: callInCoordinatorThread) registeredReaders.containsKey(subtaskId) check fails (due to failed task) with IllegalArgumentException which is uncaught in single thread executor forces ThreadPool to recreate the single thread calls CoordinatorExecutorThreadFactory::newThread fails expected condition of single thread creation with IllegalStateException which is uncaught calls FatalExitExceptionHandler and exits JVM abruptly[SourceCoordinator-Source: file-source] ERROR org.apache.flink.runtime.util.FatalExitExceptionHandler - FATAL: Thread 'SourceCoordinator-Source: file-source' produced an uncaught exception. Stopping the process...java.lang.IllegalStateException: Should never happen. This factory should only be used by a SingleThreadExecutor. at org.apache.flink.runtime.source.coordinator.SourceCoordinatorProvider$CoordinatorExecutorThreadFactory.newThread(SourceCoordinatorProvider.java:94) ~[classes/:?] at java.util.concurrent.ThreadPoolExecutor$Worker.&lt;init&gt;(ThreadPoolExecutor.java:619) ~[?:1.8.0_172] at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:932) ~[?:1.8.0_172] at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1025) ~[?:1.8.0_172] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167) ~[?:1.8.0_172] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_172] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_172]Process finished with exit code 239</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.impl.StaticFileSplitEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.impl.ContinuousFileSplitEnumerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="20266" opendate="2020-11-20 00:00:00" fixdate="2020-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>New Sources prevent JVM shutdown when running a job</summary>
      <description>When trying out the new FileSource I noticed that the jobs which I started from my IDE won't properly terminate. To be more precise the spawned JVM for the jobs wouldn't properly terminate. I cannot really tell what the FileSource does differently, but when not using this source, the JVM terminates properly.The stack trace of the hanging JVM is2020-11-20 18:20:02Full thread dump OpenJDK 64-Bit Server VM (11.0.2+9 mixed mode):Threads class SMR info:_java_thread_list=0x00007fb5bc15f1b0, length=19, elements={0x00007fb60d807000, 0x00007fb60d80c000, 0x00007fb60d80f000, 0x00007fb60d809000,0x00007fb60d81a000, 0x00007fb61f00b000, 0x00007fb63d80e000, 0x00007fb61d826800,0x00007fb61d829800, 0x00007fb61e800000, 0x00007fb63d95d800, 0x00007fb63e2f8800,0x00007fb5ba37a800, 0x00007fb5afe1a800, 0x00007fb61dff6800, 0x00007fb63da49800,0x00007fb63e8d0800, 0x00007fb5be001000, 0x00007fb5bb8a4000}"Reference Handler" #2 daemon prio=10 os_prio=31 cpu=10.05ms elapsed=86.35s tid=0x00007fb60d807000 nid=0x4b03 waiting on condition [0x00007000036e9000] java.lang.Thread.State: RUNNABLE at java.lang.ref.Reference.waitForReferencePendingList(java.base@11.0.2/Native Method) at java.lang.ref.Reference.processPendingReferences(java.base@11.0.2/Reference.java:241) at java.lang.ref.Reference$ReferenceHandler.run(java.base@11.0.2/Reference.java:213)"Finalizer" #3 daemon prio=8 os_prio=31 cpu=0.90ms elapsed=86.35s tid=0x00007fb60d80c000 nid=0x3803 in Object.wait() [0x00007000037ec000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(java.base@11.0.2/Native Method) - waiting on &lt;0x0000000600204780&gt; (a java.lang.ref.ReferenceQueue$Lock) at java.lang.ref.ReferenceQueue.remove(java.base@11.0.2/ReferenceQueue.java:155) - waiting to re-lock in wait() &lt;0x0000000600204780&gt; (a java.lang.ref.ReferenceQueue$Lock) at java.lang.ref.ReferenceQueue.remove(java.base@11.0.2/ReferenceQueue.java:176) at java.lang.ref.Finalizer$FinalizerThread.run(java.base@11.0.2/Finalizer.java:170)"Signal Dispatcher" #4 daemon prio=9 os_prio=31 cpu=0.31ms elapsed=86.34s tid=0x00007fb60d80f000 nid=0x4203 runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE"C2 CompilerThread0" #5 daemon prio=9 os_prio=31 cpu=2479.36ms elapsed=86.34s tid=0x00007fb60d809000 nid=0x3f03 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE No compile task"C1 CompilerThread0" #8 daemon prio=9 os_prio=31 cpu=1412.88ms elapsed=86.34s tid=0x00007fb60d81a000 nid=0x3d03 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE No compile task"Sweeper thread" #9 daemon prio=9 os_prio=31 cpu=42.82ms elapsed=86.34s tid=0x00007fb61f00b000 nid=0xa803 runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE"Common-Cleaner" #10 daemon prio=8 os_prio=31 cpu=3.25ms elapsed=86.29s tid=0x00007fb63d80e000 nid=0x5703 in Object.wait() [0x0000700003cfb000] java.lang.Thread.State: TIMED_WAITING (on object monitor) at java.lang.Object.wait(java.base@11.0.2/Native Method) - waiting on &lt;0x0000000600205aa0&gt; (a java.lang.ref.ReferenceQueue$Lock) at java.lang.ref.ReferenceQueue.remove(java.base@11.0.2/ReferenceQueue.java:155) - waiting to re-lock in wait() &lt;0x0000000600205aa0&gt; (a java.lang.ref.ReferenceQueue$Lock) at jdk.internal.ref.CleanerImpl.run(java.base@11.0.2/CleanerImpl.java:148) at java.lang.Thread.run(java.base@11.0.2/Thread.java:834) at jdk.internal.misc.InnocuousThread.run(java.base@11.0.2/InnocuousThread.java:134)"JDWP Transport Listener: dt_socket" #11 daemon prio=10 os_prio=31 cpu=43.46ms elapsed=86.27s tid=0x00007fb61d826800 nid=0xa603 runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE"JDWP Event Helper Thread" #12 daemon prio=10 os_prio=31 cpu=220.06ms elapsed=86.27s tid=0x00007fb61d829800 nid=0x5e03 runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE"JDWP Command Reader" #13 daemon prio=10 os_prio=31 cpu=27.26ms elapsed=86.27s tid=0x00007fb61e800000 nid=0x6103 runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE"Service Thread" #14 daemon prio=9 os_prio=31 cpu=0.06ms elapsed=86.19s tid=0x00007fb63d95d800 nid=0xa203 runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE"ForkJoinPool.commonPool-worker-19" #25 daemon prio=1 os_prio=31 cpu=2.00ms elapsed=84.76s tid=0x00007fb63e2f8800 nid=0x8003 waiting on condition [0x000070000584c000] java.lang.Thread.State: WAITING (parking) at jdk.internal.misc.Unsafe.park(java.base@11.0.2/Native Method) - parking to wait for &lt;0x0000000600a81188&gt; (a java.util.concurrent.ForkJoinPool) at java.util.concurrent.locks.LockSupport.park(java.base@11.0.2/LockSupport.java:194) at java.util.concurrent.ForkJoinPool.runWorker(java.base@11.0.2/ForkJoinPool.java:1628) at java.util.concurrent.ForkJoinWorkerThread.run(java.base@11.0.2/ForkJoinWorkerThread.java:177)"ForkJoinPool.commonPool-worker-23" #58 daemon prio=5 os_prio=31 cpu=0.19ms elapsed=84.12s tid=0x00007fb5ba37a800 nid=0xcc03 waiting on condition [0x0000700007197000] java.lang.Thread.State: WAITING (parking) at jdk.internal.misc.Unsafe.park(java.base@11.0.2/Native Method) - parking to wait for &lt;0x0000000600a81188&gt; (a java.util.concurrent.ForkJoinPool) at java.util.concurrent.locks.LockSupport.park(java.base@11.0.2/LockSupport.java:194) at java.util.concurrent.ForkJoinPool.runWorker(java.base@11.0.2/ForkJoinPool.java:1628) at java.util.concurrent.ForkJoinWorkerThread.run(java.base@11.0.2/ForkJoinWorkerThread.java:177)"ForkJoinPool.commonPool-worker-9" #59 daemon prio=5 os_prio=31 cpu=0.17ms elapsed=84.07s tid=0x00007fb5afe1a800 nid=0xcf03 waiting on condition [0x000070000729a000] java.lang.Thread.State: WAITING (parking) at jdk.internal.misc.Unsafe.park(java.base@11.0.2/Native Method) - parking to wait for &lt;0x0000000600a81188&gt; (a java.util.concurrent.ForkJoinPool) at java.util.concurrent.locks.LockSupport.park(java.base@11.0.2/LockSupport.java:194) at java.util.concurrent.ForkJoinPool.runWorker(java.base@11.0.2/ForkJoinPool.java:1628) at java.util.concurrent.ForkJoinWorkerThread.run(java.base@11.0.2/ForkJoinWorkerThread.java:177)"ForkJoinPool.commonPool-worker-27" #63 daemon prio=5 os_prio=31 cpu=0.72ms elapsed=83.97s tid=0x00007fb61dff6800 nid=0xd503 waiting on condition [0x00007000076a6000] java.lang.Thread.State: TIMED_WAITING (parking) at jdk.internal.misc.Unsafe.park(java.base@11.0.2/Native Method) - parking to wait for &lt;0x0000000600a81188&gt; (a java.util.concurrent.ForkJoinPool) at java.util.concurrent.locks.LockSupport.parkUntil(java.base@11.0.2/LockSupport.java:275) at java.util.concurrent.ForkJoinPool.runWorker(java.base@11.0.2/ForkJoinPool.java:1619) at java.util.concurrent.ForkJoinWorkerThread.run(java.base@11.0.2/ForkJoinWorkerThread.java:177)"Cleaner-0" #185 daemon prio=8 os_prio=31 cpu=10.55ms elapsed=83.39s tid=0x00007fb63da49800 nid=0x1570b in Object.wait() [0x00007000093fd000] java.lang.Thread.State: TIMED_WAITING (on object monitor) at java.lang.Object.wait(java.base@11.0.2/Native Method) - waiting on &lt;0x0000000600c5bd78&gt; (a java.lang.ref.ReferenceQueue$Lock) at java.lang.ref.ReferenceQueue.remove(java.base@11.0.2/ReferenceQueue.java:155) - waiting to re-lock in wait() &lt;0x0000000600c5bd78&gt; (a java.lang.ref.ReferenceQueue$Lock) at jdk.internal.ref.CleanerImpl.run(java.base@11.0.2/CleanerImpl.java:148) at java.lang.Thread.run(java.base@11.0.2/Thread.java:834) at jdk.internal.misc.InnocuousThread.run(java.base@11.0.2/InnocuousThread.java:134)"ComponentClosingUtil" #232 prio=1 os_prio=31 cpu=0.39ms elapsed=81.78s tid=0x00007fb63e8d0800 nid=0x13107 waiting on condition [0x0000700004f31000] java.lang.Thread.State: WAITING (parking) at jdk.internal.misc.Unsafe.park(java.base@11.0.2/Native Method) - parking to wait for &lt;0x00000006008fb930&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(java.base@11.0.2/LockSupport.java:194) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(java.base@11.0.2/AbstractQueuedSynchronizer.java:2081) at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(java.base@11.0.2/ScheduledThreadPoolExecutor.java:1170) at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(java.base@11.0.2/ScheduledThreadPoolExecutor.java:899) at java.util.concurrent.ThreadPoolExecutor.getTask(java.base@11.0.2/ThreadPoolExecutor.java:1054) at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@11.0.2/ThreadPoolExecutor.java:1114) at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@11.0.2/ThreadPoolExecutor.java:628) at java.lang.Thread.run(java.base@11.0.2/Thread.java:834)"DestroyJavaVM" #282 prio=5 os_prio=31 cpu=2125.94ms elapsed=81.19s tid=0x00007fb5be001000 nid=0x1d03 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE"Attach Listener" #291 daemon prio=9 os_prio=31 cpu=0.57ms elapsed=0.09s tid=0x00007fb5bb8a4000 nid=0x14207 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE"VM Thread" os_prio=31 cpu=74.59ms elapsed=86.36s tid=0x00007fb63f807000 nid=0x3603 runnable"GC Thread#0" os_prio=31 cpu=51.88ms elapsed=86.38s tid=0x00007fb62d803800 nid=0x2e03 runnable"GC Thread#1" os_prio=31 cpu=50.32ms elapsed=85.97s tid=0x00007fb61d836800 nid=0x9e03 runnable"GC Thread#2" os_prio=31 cpu=45.79ms elapsed=85.97s tid=0x00007fb61d8b8000 nid=0x9d03 runnable"GC Thread#3" os_prio=31 cpu=46.08ms elapsed=85.97s tid=0x00007fb61d83a800 nid=0x9c03 runnable"GC Thread#4" os_prio=31 cpu=48.88ms elapsed=85.97s tid=0x00007fb61f01c000 nid=0x9a03 runnable"GC Thread#5" os_prio=31 cpu=44.32ms elapsed=85.97s tid=0x00007fb63d8e5000 nid=0x9803 runnable"GC Thread#6" os_prio=31 cpu=49.81ms elapsed=85.97s tid=0x00007fb63d8e6000 nid=0x9703 runnable"GC Thread#7" os_prio=31 cpu=51.55ms elapsed=85.97s tid=0x00007fb61d83b800 nid=0x6503 runnable"GC Thread#8" os_prio=31 cpu=46.05ms elapsed=85.97s tid=0x00007fb62d928000 nid=0x9403 runnable"GC Thread#9" os_prio=31 cpu=50.43ms elapsed=85.97s tid=0x00007fb62d929000 nid=0x9203 runnable"G1 Main Marker" os_prio=31 cpu=0.65ms elapsed=86.37s tid=0x00007fb62d83b000 nid=0x2f03 runnable"G1 Conc#0" os_prio=31 cpu=11.46ms elapsed=86.37s tid=0x00007fb62d83b800 nid=0x3203 runnable"G1 Conc#1" os_prio=31 cpu=11.43ms elapsed=85.39s tid=0x00007fb63da4e800 nid=0x6703 runnable"G1 Conc#2" os_prio=31 cpu=10.03ms elapsed=85.39s tid=0x00007fb63da4f800 nid=0x6803 runnable"G1 Refine#0" os_prio=31 cpu=8.22ms elapsed=86.37s tid=0x00007fb63e102000 nid=0x4e03 runnable"G1 Refine#1" os_prio=31 cpu=0.06ms elapsed=81.10s tid=0x00007fb5aeff4000 nid=0x1d50f runnable"G1 Young RemSet Sampling" os_prio=31 cpu=12.94ms elapsed=86.37s tid=0x00007fb63e102800 nid=0x3403 runnable"VM Periodic Task Thread" os_prio=31 cpu=57.93ms elapsed=86.19s tid=0x00007fb61e833800 nid=0xa003 waiting on conditionJNI global refs: 55, weak refs: 15453My environment is and I used the openjdk version "1.8.0_262" to run the job:IntelliJ IDEA 2020.2.3 (Community Edition)Build #IC-202.7660.26, built on October 6, 2020Runtime version: 11.0.8+10-b944.34 x86_64VM: OpenJDK 64-Bit Server VM by JetBrains s.r.o.macOS 10.14.6GC: ParNew, ConcurrentMarkSweepMemory: 1979MCores: 12Non-Bundled Plugins: CheckStyle-IDEA, com.jetbrains.performancePlugin, org.jetbrains.kotlin, org.intellij.scalacc sewen</description>
      <version>1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.ComponentClosingUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="20275" opendate="2020-11-22 00:00:00" fixdate="2020-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The path delimiter for add_jars and add_classpaths in Python StreamExecutionEnvironment should be ;</summary>
      <description>Currently, the path delimiter for add_jars and add_classpaths in Python StreamExecutionEnvironment is ",", this would cause the rest client fail to upload the specified jars and stuck forever without errors. It should be ";" instead.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="20278" opendate="2020-11-23 00:00:00" fixdate="2020-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Throw a meaningful exception if the Python DataStream API job executes in batch mode</summary>
      <description>Currently, the Python DataStream job still doesn't support batch mode. We should thrown a meaningful exception if it runs in batch mode.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonProcessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonKeyedProcessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="20284" opendate="2020-11-23 00:00:00" fixdate="2020-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error happens in TaskExecutor when closing JobMaster connection if there was a python UDF</summary>
      <description>When a TaskExecutor successfully finished running a python UDF task and disconnecting from JobMaster, errors below will happen. This error, however, seems not affect job execution at the moment.2020-11-20 17:05:21,932 INFO org.apache.beam.runners.fnexecution.logging.GrpcLoggingService [] - 1 Beam Fn Logging clients still connected during shutdown.2020-11-20 17:05:21,938 WARN org.apache.beam.sdk.fn.data.BeamFnDataGrpcMultiplexer [] - Hanged up for unknown endpoint.2020-11-20 17:05:22,126 INFO org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -&gt; select: (f0) -&gt; select: (add_one(f0) AS a) -&gt; to: Tuple2 -&gt; Sink: Streaming select table sink (1/1)#0 (b0c2104dd8f87bb1caf0c83586c22a51) switched from RUNNING to FINISHED.2020-11-20 17:05:22,126 INFO org.apache.flink.runtime.taskmanager.Task [] - Freeing task resources for Source: Custom Source -&gt; select: (f0) -&gt; select: (add_one(f0) AS a) -&gt; to: Tuple2 -&gt; Sink: Streaming select table sink (1/1)#0 (b0c2104dd8f87bb1caf0c83586c22a51).2020-11-20 17:05:22,128 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Un-registering task and sending final execution state FINISHED to JobManager for task Source: Custom Source -&gt; select: (f0) -&gt; select: (add_one(f0) AS a) -&gt; to: Tuple2 -&gt; Sink: Streaming select table sink (1/1)#0 b0c2104dd8f87bb1caf0c83586c22a51.2020-11-20 17:05:22,156 INFO org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ACTIVE, resource profile: ResourceProfile{cpuCores=1.0000000000000000, taskHeapMemory=384.000mb (402653174 bytes), taskOffHeapMemory=0 bytes, managedMemory=512.000mb (536870920 bytes), networkMemory=128.000mb (134217730 bytes)}, allocationId: b67c3307dcf93757adfb4f0f9f7b8c7b, jobId: d05f32162f38ec3ec813c4621bc106d9).2020-11-20 17:05:22,157 INFO org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job d05f32162f38ec3ec813c4621bc106d9 from job leader monitoring.2020-11-20 17:05:22,157 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Close JobManager connection for job d05f32162f38ec3ec813c4621bc106d9.2020-11-20 17:05:23,064 ERROR org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.rejectedExecution [] - Failed to submit a listener notification task. Event loop shut down?java.lang.NoClassDefFoundError: org/apache/beam/vendor/grpc/v1p26p0/io/netty/util/concurrent/GlobalEventExecutor$2 at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.GlobalEventExecutor.startThread(GlobalEventExecutor.java:227) ~[blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT] at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.GlobalEventExecutor.execute(GlobalEventExecutor.java:215) ~[blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT] at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.safeExecute(DefaultPromise.java:841) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT] at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:498) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT] at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT] at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT] at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.setSuccess(DefaultPromise.java:96) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT] at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.SingleThreadEventExecutor$6.run(SingleThreadEventExecutor.java:1089) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT] at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT] at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_261]Caused by: java.lang.ClassNotFoundException: org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.GlobalEventExecutor$2 at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_261] at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_261] at org.apache.flink.util.FlinkUserCodeClassLoader.loadClassWithoutExceptionHandling(FlinkUserCodeClassLoader.java:63) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.util.ChildFirstClassLoader.loadClassWithoutExceptionHandling(ChildFirstClassLoader.java:72) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.util.FlinkUserCodeClassLoader.loadClass(FlinkUserCodeClassLoader.java:49) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_261] ... 11 more</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.SharedResourceHolder.java</file>
    </fixedFiles>
  </bug>
  <bug id="20292" opendate="2020-11-23 00:00:00" fixdate="2020-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the document about table formats overlap in user fat jar</summary>
      <description>When testing the Flink 1.12 in a standalone mode cluster, I found that if the user job jar contains both flink-avro and flink-parquet/flink-orc, the FileSystemTableSink would not be able to load the corresponding format factory correctly. But if only one format is dependent it works.The test project located in here and the test class is FileCompactionTest.The conflict does not seem to affect the local runner, but only has problem when submitted to the standalone cluster.If the problem does exists, we might need to fix it or give user some tips about the conflicts. </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="20295" opendate="2020-11-23 00:00:00" fixdate="2020-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>File Source lost data when reading from directories created by FileSystemTableSink with JSON format</summary>
      <description>When testing the compaction functionality of the FileSystemTableSink, I found that when using json format, the produced directories could not be read correctly by the file source, namely only a part of records are read.By checking the produced directories, the number of the records in it is the same as expected, thus it seems to be the issue of the source side. The issue only exists for JSON format.The data is produced by FileCompactionTest and read by  FileCompactionCheckTest . An example directories tar file of 8000 records are also attached. </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.DeserializationSchemaAdapter.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonBatchFileSystemITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter.java</file>
    </fixedFiles>
  </bug>
  <bug id="20300" opendate="2020-11-23 00:00:00" fixdate="2020-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create Flink 1.12 release notes</summary>
      <description></description>
      <version>1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="20304" opendate="2020-11-23 00:00:00" fixdate="2020-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fail hard when trying to run job with broadcast state in BATCH execution mode</summary>
      <description>Contrary to the documentation it is currently possible to run a job with broadcast state in BATCH execution mode. Since accessing the keyed state from the broadcast side fails, we shouldn't allow the submissions of these kind of jobs in the first place. Hence, I would suggest to fail hard if one tries to run a job using the broadcast state pattern in BATCH execution mode.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.TwoInputTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.BroadcastStateTransformationTranslator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.BroadcastStateITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.AbstractOneInputTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.BroadcastConnectedStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="20307" opendate="2020-11-23 00:00:00" fixdate="2020-11-23 01:00:00" resolution="Invalid">
    <buginformation>
      <summary>Improve the documentation about the temporal table join syntax</summary>
      <description>A query like:SELECT p.name, p.qty * r.rate AS price, p.`tstamp` FROM Products p JOIN versioned_rates r FOR SYSTEM_TIME AS OF p.`tstamp` ON p.currency = r.currency;fails with:[ERROR] Could not execute SQL statement. Reason:org.apache.flink.sql.parser.impl.ParseException: Encountered "FOR" at line 1, column 108.Was expecting one of: &lt;EOF&gt; "EXCEPT" ... "FETCH" ... "GROUP" ... "HAVING" ... "INTERSECT" ... "LIMIT" ... "OFFSET" ... "ON" ... "ORDER" ... "MINUS" ... "TABLESAMPLE" ... "UNION" ... "USING" ... "WHERE" ... "WINDOW" ... "(" ... "NATURAL" ... "JOIN" ... "INNER" ... "LEFT" ... "RIGHT" ... "FULL" ... "CROSS" ... "," ... "OUTER" ...When I do not alias the versioned_rates table everything works as expected. Therefore query like just runs:SELECT p.name, p.qty * versioned_rates.rate AS price, p.`tstamp` FROM Products p JOIN versioned_rates FOR SYSTEM_TIME AS OF p.`tstamp` ON p.currency = versioned_rates.currency;</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
    </fixedFiles>
  </bug>
  <bug id="20310" opendate="2020-11-24 00:00:00" fixdate="2020-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Debezium, Canal, Raw support for Filesystem connector</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.formats.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.md</file>
      <file type="M">docs.dev.table.connectors.filesystem.zh.md</file>
      <file type="M">docs.dev.table.connectors.filesystem.md</file>
    </fixedFiles>
  </bug>
  <bug id="20314" opendate="2020-11-24 00:00:00" fixdate="2020-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Empty Calc is not removed by CalcRemoveRule</summary>
      <description>My DDL：create table if not exists t_order(id int PRIMARY KEY comment '订单id',timestamps bigint comment '订单创建时间',orderInformationId string comment '订单信息ID',userId string comment '用户ID',categoryId int comment '商品类别',productId int comment '商品ID',price decimal(10,2) comment '单价',productCount int comment '购买数量',priceSum decimal(10,2) comment '订单总价',shipAddress string comment '商家地址',receiverAddress string comment '收货地址',ts AS TO_TIMESTAMP(FROM_UNIXTIME(timestamps/1000)),WATERMARK FOR ts AS ts - INTERVAL '3' SECOND)with('connector' = 'kafka','format' = 'debezium-avro-confluent', 'debezium-avro-confluent.schema-registry.url' = 'http://hostname:8081','topic' = 'ods.userAnalysis.order','properties.bootstrap.servers' = 'hostname:9092','properties.group.id' = 'flink-analysis','scan.startup.mode' = 'latest-offset') query is ok  when using the following SQLs：select * from t_orderselect receiverAddress from t_orderselectid,timestamps,orderInformationId,userId,categoryId,productId,price,productCount,priceSum,shipAddressfrom t_orderbut when I add the receiveraddress field to the third sql like:selectid,timestamps,orderInformationId,userId,categoryId,productId,price,productCount,priceSum,shipAddress,receiverAddressfrom t_orderit throws an exception：Exception in thread "main" org.apache.flink.table.api.TableException: This calc has no useful projection and no filter. It should be removed by CalcRemoveRule.Exception in thread "main" org.apache.flink.table.api.TableException: This calc has no useful projection and no filter. It should be removed by CalcRemoveRule. at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateProcessCode(CalcCodeGenerator.scala:166) at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateCalcOperator(CalcCodeGenerator.scala:59) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:84) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:39) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan(ExecNode.scala:59) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan$(ExecNode.scala:57) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalcBase.translateToPlan(StreamExecCalcBase.scala:38) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToTransformation(StreamExecLegacySink.scala:158) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:82) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:48) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan(ExecNode.scala:59) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan$(ExecNode.scala:57) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlan(StreamExecLegacySink.scala:48) at org.apache.flink.table.planner.delegation.StreamPlanner.$anonfun$translateToPlan$1(StreamPlanner.scala:66) at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) at scala.collection.Iterator.foreach(Iterator.scala:937) at scala.collection.Iterator.foreach$(Iterator.scala:937) at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) at scala.collection.IterableLike.foreach(IterableLike.scala:70) at scala.collection.IterableLike.foreach$(IterableLike.scala:69) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike.map(TraversableLike.scala:233) at scala.collection.TraversableLike.map$(TraversableLike.scala:226) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:65) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:167) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1261) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:702) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:1065) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:664) at com.bugboy.analysis.AnalysisCase$.main(AnalysisCase.scala:161) at com.bugboy.analysis.AnalysisCase.main(AnalysisCase.scala)    </description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SourceWatermarkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="20316" opendate="2020-11-24 00:00:00" fixdate="2020-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update the deduplication section of query page</summary>
      <description>We have supported deduplication in row time and deduplicate in mini-batch mode, but the document did not update, we need to update the doc.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
    </fixedFiles>
  </bug>
  <bug id="20317" opendate="2020-11-24 00:00:00" fixdate="2020-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Format Overview page to mention the supported connector for upsert-kafka</summary>
      <description>Currently, the Format Overview page only mentions and links "Apache Kafka". We should update the table for "upsert-kafka" connector.https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/formats/</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.formats.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="20333" opendate="2020-11-25 00:00:00" fixdate="2020-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink standalone cluster throws metaspace OOM after submitting multiple PyFlink UDF jobs.</summary>
      <description>Currently the Flink standalone cluster will throw metaspace OOM after submitting multiple PyFlink UDF jobs. The root cause is that currently the PyFlink classes are running in user classloader and so each job creates a separate user class loader to load PyFlink related classes. There are many soft references and Finalizers in memory (introduced by the underlying Netty), which prevents the garbage collection of the user classloader of already finished PyFlink jobs. Due to their existence, it needs multiple full gc to reclaim the classloader of the completed job. If only one full gc is performed before the metaspace space is insufficient, then OOM will occur. </description>
      <version>None</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2034" opendate="2015-5-18 00:00:00" fixdate="2015-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add vision and roadmap for ML library to docs</summary>
      <description>We should have a document describing the vision of the Machine Learning library in Flink and an up to date roadmap.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.libs.ml.optimization.md</file>
      <file type="M">docs.libs.ml.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="20342" opendate="2020-11-25 00:00:00" fixdate="2020-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revisit page structure in documentation</summary>
      <description>Clean up page structure</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.debugging.classloading.md</file>
      <file type="M">docs.monitoring.README.md</file>
      <file type="M">docs.monitoring.logging.zh.md</file>
      <file type="M">docs.monitoring.logging.md</file>
      <file type="M">docs.monitoring.index.zh.md</file>
      <file type="M">docs.monitoring.index.md</file>
      <file type="M">docs.ops.state.savepoints.zh.md</file>
      <file type="M">docs.ops.state.savepoints.md</file>
      <file type="M">docs.ops.state.checkpoints.zh.md</file>
      <file type="M">docs.ops.state.checkpoints.md</file>
      <file type="M">docs.dev.python.table.api.tutorial.zh.md</file>
      <file type="M">docs.dev.python.table.api.tutorial.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.dependency.management.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.dependency.management.md</file>
      <file type="M">docs.dev.python.faq.zh.md</file>
      <file type="M">docs.dev.python.faq.md</file>
      <file type="M">docs.dev.python.datastream.tutorial.zh.md</file>
      <file type="M">docs.dev.python.datastream.tutorial.md</file>
      <file type="M">docs.dev.python.datastream-api-users-guide.dependency.management.zh.md</file>
      <file type="M">docs.dev.python.datastream-api-users-guide.dependency.management.md</file>
      <file type="M">docs.dev.packaging.zh.md</file>
      <file type="M">docs.dev.packaging.md</file>
      <file type="M">docs.dev.local.execution.zh.md</file>
      <file type="M">docs.dev.local.execution.md</file>
      <file type="M">docs.dev.datastream.api.zh.md</file>
      <file type="M">docs.dev.datastream.api.md</file>
      <file type="M">docs.dev.cluster.execution.zh.md</file>
      <file type="M">docs.dev.cluster.execution.md</file>
      <file type="M">docs.ops.security.security-ssl.zh.md</file>
      <file type="M">docs.ops.security.security-ssl.md</file>
      <file type="M">docs.ops.security.security-kerberos.zh.md</file>
      <file type="M">docs.ops.security.security-kerberos.md</file>
      <file type="M">docs.ops.security.index.zh.md</file>
      <file type="M">docs.ops.security.index.md</file>
      <file type="M">docs.ops.resource-providers.yarn.setup.zh.md</file>
      <file type="M">docs.ops.resource-providers.yarn.setup.md</file>
      <file type="M">docs.ops.resource-providers.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.ops.resource-providers.mesos.zh.md</file>
      <file type="M">docs.ops.resource-providers.mesos.md</file>
      <file type="M">docs.ops.resource-providers.local.zh.md</file>
      <file type="M">docs.ops.resource-providers.local.md</file>
      <file type="M">docs.ops.resource-providers.kubernetes.zh.md</file>
      <file type="M">docs.ops.resource-providers.kubernetes.md</file>
      <file type="M">docs.ops.resource-providers.index.zh.md</file>
      <file type="M">docs.ops.resource-providers.index.md</file>
      <file type="M">docs.ops.resource-providers.hadoop.zh.md</file>
      <file type="M">docs.ops.resource-providers.hadoop.md</file>
      <file type="M">docs.ops.resource-providers.docker.zh.md</file>
      <file type="M">docs.ops.resource-providers.docker.md</file>
      <file type="M">docs.ops.resource-providers.cluster.setup.zh.md</file>
      <file type="M">docs.ops.resource-providers.cluster.setup.md</file>
      <file type="M">docs.ops.filesystems.s3.zh.md</file>
      <file type="M">docs.ops.filesystems.s3.md</file>
      <file type="M">docs.ops.filesystems.oss.zh.md</file>
      <file type="M">docs.ops.filesystems.oss.md</file>
      <file type="M">docs.ops.filesystems.common.zh.md</file>
      <file type="M">docs.ops.filesystems.common.md</file>
      <file type="M">docs.ops.filesystems.azure.zh.md</file>
      <file type="M">docs.ops.filesystems.azure.md</file>
      <file type="M">docs.ops.external.resources.zh.md</file>
      <file type="M">docs.ops.external.resources.md</file>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
      <file type="M">docs.dev.table.connectors.filesystem.zh.md</file>
      <file type="M">docs.dev.table.connectors.filesystem.md</file>
      <file type="M">docs.dev.connectors.streamfile.sink.zh.md</file>
      <file type="M">docs.dev.connectors.streamfile.sink.md</file>
      <file type="M">docs.dev.connectors.file.sink.zh.md</file>
      <file type="M">docs.dev.connectors.file.sink.md</file>
      <file type="M">docs.dev.batch.connectors.zh.md</file>
      <file type="M">docs.dev.batch.connectors.md</file>
      <file type="M">docs.ops.scala.shell.zh.md</file>
      <file type="M">docs.ops.scala.shell.md</file>
      <file type="M">docs.release-notes.flink-1.9.zh.md</file>
      <file type="M">docs.release-notes.flink-1.9.md</file>
      <file type="M">docs.redirects.metrics.md</file>
      <file type="M">docs.ops.monitoring.index.zh.md</file>
      <file type="M">docs.ops.monitoring.index.md</file>
      <file type="M">docs.ops.debugging.index.zh.md</file>
      <file type="M">docs.ops.debugging.index.md</file>
      <file type="M">docs.ops.debugging.debugging.event.time.zh.md</file>
      <file type="M">docs.ops.debugging.debugging.event.time.md</file>
      <file type="M">docs.deployment.metric.reporters.zh.md</file>
      <file type="M">docs.deployment.metric.reporters.md</file>
      <file type="M">docs.redirects.mapr.md</file>
      <file type="M">docs.redirects.gce.setup.md</file>
      <file type="M">docs.redirects.aws.md</file>
      <file type="M">docs.ops.python.shell.zh.md</file>
      <file type="M">docs.ops.python.shell.md</file>
      <file type="M">docs.flinkDev.building.zh.md</file>
      <file type="M">docs.flinkDev.building.md</file>
      <file type="M">docs.dev.table.connectors.hive.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.hive.index.md</file>
      <file type="M">docs.dev.project-configuration.zh.md</file>
      <file type="M">docs.dev.project-configuration.md</file>
      <file type="M">docs.dev.batch.hadoop.compatibility.zh.md</file>
      <file type="M">docs.dev.batch.hadoop.compatibility.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.zh.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
      <file type="M">docs.ops.deployment.mesos.zh.md</file>
      <file type="M">docs.ops.deployment.mesos.md</file>
      <file type="M">docs.ops.deployment.local.zh.md</file>
      <file type="M">docs.ops.deployment.local.md</file>
      <file type="M">docs.ops.deployment.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.kubernetes.md</file>
      <file type="M">docs.ops.deployment.index.zh.md</file>
      <file type="M">docs.ops.deployment.index.md</file>
      <file type="M">docs.ops.deployment.hadoop.zh.md</file>
      <file type="M">docs.ops.deployment.hadoop.md</file>
      <file type="M">docs.ops.deployment.docker.zh.md</file>
      <file type="M">docs.ops.deployment.docker.md</file>
      <file type="M">docs.ops.deployment.cluster.setup.zh.md</file>
      <file type="M">docs.ops.deployment.cluster.setup.md</file>
      <file type="M">docs.deployment.logging.zh.md</file>
      <file type="M">docs.deployment.logging.md</file>
      <file type="M">docs.deployment.historyserver.zh.md</file>
      <file type="M">docs.deployment.historyserver.md</file>
      <file type="M">docs.redirects.scala.shell.md</file>
      <file type="M">docs.redirects.python.shell.md</file>
      <file type="M">docs.redirects.oss.md</file>
      <file type="M">docs.redirects.filesystems.md</file>
      <file type="M">docs.redirects.cli.md</file>
      <file type="M">docs.ops.monitoring.rest.api.zh.md</file>
      <file type="M">docs.ops.monitoring.rest.api.md</file>
      <file type="M">docs.ops.metrics.zh.md</file>
      <file type="M">docs.ops.metrics.md</file>
      <file type="M">docs.monitoring.rest.api.zh.md</file>
      <file type="M">docs.monitoring.rest.api.md</file>
      <file type="M">docs.monitoring.checkpoint.monitoring.zh.md</file>
      <file type="M">docs.monitoring.checkpoint.monitoring.md</file>
      <file type="M">docs.monitoring.back.pressure.zh.md</file>
      <file type="M">docs.monitoring.back.pressure.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.metrics.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.metrics.md</file>
      <file type="M">docs.deployment.security.index.zh.md</file>
      <file type="M">docs.deployment.security.index.md</file>
      <file type="M">docs.deployment.filesystems.plugins.zh.md</file>
      <file type="M">docs.deployment.filesystems.plugins.md</file>
      <file type="M">docs.deployment.plugins.zh.md</file>
      <file type="M">docs.deployment.plugins.md</file>
      <file type="M">docs.deployment.filesystems.index.zh.md</file>
      <file type="M">docs.deployment.filesystems.index.md</file>
      <file type="M">docs.deployment.external.resources.zh.md</file>
      <file type="M">docs.deployment.external.resources.md</file>
      <file type="M">docs.ops.security-ssl.zh.md</file>
      <file type="M">docs.ops.security-ssl.md</file>
      <file type="M">docs.ops.security-kerberos.zh.md</file>
      <file type="M">docs.ops.security-kerberos.md</file>
      <file type="M">docs.ops.jobmanager.high.availability.zh.md</file>
      <file type="M">docs.ops.jobmanager.high.availability.md</file>
      <file type="M">docs.ops.plugins.zh.md</file>
      <file type="M">docs.ops.plugins.md</file>
      <file type="M">docs.ops.filesystems.index.zh.md</file>
      <file type="M">docs.ops.filesystems.index.md</file>
      <file type="M">docs.redirects.windows.local.setup.md</file>
      <file type="M">docs.redirects.setup.quickstart.md</file>
      <file type="M">docs.redirects.local.setup.tutorial.md</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.deployment.resource-providers.local.zh.md</file>
      <file type="M">docs.deployment.resource-providers.local.md</file>
      <file type="M">docs.deployment.resource-providers.kubernetes.zh.md</file>
      <file type="M">docs.deployment.resource-providers.kubernetes.md</file>
      <file type="M">docs.deployment.repls.python.shell.zh.md</file>
      <file type="M">docs.deployment.repls.python.shell.md</file>
      <file type="M">docs.deployment.index.zh.md</file>
      <file type="M">docs.deployment.index.md</file>
      <file type="M">docs.concepts.flink-architecture.zh.md</file>
      <file type="M">docs.concepts.flink-architecture.md</file>
      <file type="M">docs.monitoring.debugging.event.time.zh.md</file>
      <file type="M">docs.monitoring.debugging.event.time.md</file>
      <file type="M">docs.monitoring.application.profiling.zh.md</file>
      <file type="M">docs.monitoring.application.profiling.md</file>
      <file type="M">docs.ops.state.index.zh.md</file>
      <file type="M">docs.ops.state.index.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.index.zh.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.index.md</file>
      <file type="M">docs.release-notes.flink-1.10.zh.md</file>
      <file type="M">docs.release-notes.flink-1.10.md</file>
      <file type="M">docs.ops.memory.mem.tuning.zh.md</file>
      <file type="M">docs.ops.memory.mem.tuning.md</file>
      <file type="M">docs.ops.memory.mem.trouble.zh.md</file>
      <file type="M">docs.ops.memory.mem.trouble.md</file>
      <file type="M">docs.ops.memory.mem.setup.tm.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.tm.md</file>
      <file type="M">docs.ops.memory.mem.setup.jobmanager.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.jobmanager.md</file>
      <file type="M">docs.ops.memory.mem.setup.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.md</file>
      <file type="M">docs.ops.memory.mem.migration.zh.md</file>
      <file type="M">docs.ops.memory.mem.migration.md</file>
      <file type="M">docs.ops.memory.index.zh.md</file>
      <file type="M">docs.ops.memory.index.md</file>
      <file type="M">docs.ops.upgrading.zh.md</file>
      <file type="M">docs.ops.upgrading.md</file>
      <file type="M">docs.ops.production.ready.zh.md</file>
      <file type="M">docs.ops.production.ready.md</file>
      <file type="M">docs.try-flink.flink-operations-playground.zh.md</file>
      <file type="M">docs.try-flink.flink-operations-playground.md</file>
      <file type="M">docs.redirects.rest.api.md</file>
      <file type="M">docs.redirects.back.pressure.md</file>
      <file type="M">docs.ops.state.large.state.tuning.zh.md</file>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
      <file type="M">docs.monitoring.monitoring.rest.api.zh.md</file>
      <file type="M">docs.monitoring.monitoring.rest.api.md</file>
      <file type="M">docs.monitoring.monitoring.index.zh.md</file>
      <file type="M">docs.monitoring.monitoring.index.md</file>
      <file type="M">docs.monitoring.monitoring.checkpoint.monitoring.zh.md</file>
      <file type="M">docs.monitoring.monitoring.checkpoint.monitoring.md</file>
      <file type="M">docs.monitoring.monitoring.back.pressure.zh.md</file>
      <file type="M">docs.monitoring.monitoring.back.pressure.md</file>
      <file type="M">docs.monitoring.debugging.index.zh.md</file>
      <file type="M">docs.monitoring.debugging.index.md</file>
      <file type="M">docs.monitoring.debugging.debugging.event.time.zh.md</file>
      <file type="M">docs.monitoring.debugging.debugging.event.time.md</file>
      <file type="M">docs.monitoring.debugging.debugging.classloading.zh.md</file>
      <file type="M">docs.monitoring.debugging.debugging.classloading.md</file>
      <file type="M">docs.monitoring.debugging.application.profiling.zh.md</file>
      <file type="M">docs.monitoring.debugging.application.profiling.md</file>
      <file type="M">docs.learn-flink.fault.tolerance.zh.md</file>
      <file type="M">docs.learn-flink.fault.tolerance.md</file>
      <file type="M">docs.dev.event.time.zh.md</file>
      <file type="M">docs.dev.event.time.md</file>
      <file type="M">docs.deployment.memory.index.zh.md</file>
      <file type="M">docs.deployment.memory.index.md</file>
      <file type="M">docs.deployment.config.zh.md</file>
      <file type="M">docs.deployment.config.md</file>
      <file type="M">docs.release-notes.flink-1.7.zh.md</file>
      <file type="M">docs.release-notes.flink-1.7.md</file>
      <file type="M">docs.release-notes.flink-1.5.zh.md</file>
      <file type="M">docs.release-notes.flink-1.5.md</file>
      <file type="M">docs.release-notes.flink-1.11.zh.md</file>
      <file type="M">docs.release-notes.flink-1.11.md</file>
      <file type="M">docs.ops.state.state.backends.zh.md</file>
      <file type="M">docs.ops.state.state.backends.md</file>
      <file type="M">docs.ops.config.zh.md</file>
      <file type="M">docs.ops.config.md</file>
      <file type="M">docs.monitoring.metrics.zh.md</file>
      <file type="M">docs.monitoring.metrics.md</file>
      <file type="M">docs.monitoring.historyserver.zh.md</file>
      <file type="M">docs.monitoring.historyserver.md</file>
      <file type="M">docs.monitoring.debugging.classloading.zh.md</file>
      <file type="M">docs.ops.index.md</file>
      <file type="M">docs.ops.index.zh.md</file>
      <file type="M">docs.deployment.cli.md</file>
      <file type="M">docs.deployment.cli.zh.md</file>
      <file type="M">docs.deployment.jobmanager.high.availability.md</file>
      <file type="M">docs.deployment.jobmanager.high.availability.zh.md</file>
      <file type="M">docs.deployment.memory.mem.migration.md</file>
      <file type="M">docs.deployment.memory.mem.migration.zh.md</file>
      <file type="M">docs.deployment.memory.mem.setup.md</file>
      <file type="M">docs.deployment.memory.mem.setup.zh.md</file>
      <file type="M">docs.deployment.memory.mem.setup.jobmanager.md</file>
      <file type="M">docs.deployment.memory.mem.setup.jobmanager.zh.md</file>
      <file type="M">docs.deployment.memory.mem.setup.tm.md</file>
      <file type="M">docs.deployment.memory.mem.setup.tm.zh.md</file>
      <file type="M">docs.deployment.memory.mem.trouble.md</file>
      <file type="M">docs.deployment.memory.mem.trouble.zh.md</file>
      <file type="M">docs.deployment.memory.mem.tuning.md</file>
      <file type="M">docs.deployment.memory.mem.tuning.zh.md</file>
      <file type="M">docs.deployment.resource-providers.cluster.setup.md</file>
      <file type="M">docs.deployment.resource-providers.cluster.setup.zh.md</file>
      <file type="M">docs.deployment.resource-providers.docker.md</file>
      <file type="M">docs.deployment.resource-providers.docker.zh.md</file>
      <file type="M">docs.deployment.resource-providers.mesos.md</file>
      <file type="M">docs.deployment.resource-providers.mesos.zh.md</file>
      <file type="M">docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.deployment.resource-providers.native.kubernetes.zh.md</file>
      <file type="M">docs.deployment.resource-providers.yarn.setup.md</file>
      <file type="M">docs.deployment.resource-providers.yarn.setup.zh.md</file>
      <file type="M">docs.deployment.security.security-kerberos.md</file>
      <file type="M">docs.deployment.security.security-kerberos.zh.md</file>
      <file type="M">docs.dev.connectors.kafka.md</file>
      <file type="M">docs.dev.connectors.kafka.zh.md</file>
      <file type="M">docs.dev.execution.configuration.md</file>
      <file type="M">docs.dev.execution.configuration.zh.md</file>
      <file type="M">docs.dev.parallel.md</file>
      <file type="M">docs.dev.parallel.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.table.environment.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.table.environment.zh.md</file>
      <file type="M">docs.dev.stream.operators.index.md</file>
      <file type="M">docs.dev.stream.operators.index.zh.md</file>
      <file type="M">docs.dev.stream.state.checkpointing.md</file>
      <file type="M">docs.dev.stream.state.checkpointing.zh.md</file>
      <file type="M">docs.dev.stream.state.queryable.state.md</file>
      <file type="M">docs.dev.stream.state.queryable.state.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.index.md</file>
      <file type="M">docs.index.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="20343" opendate="2020-11-25 00:00:00" fixdate="2020-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add overview / reference architecture documentation page</summary>
      <description>To properly guide users, we should add some generic overview of the deployment concepts.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.deployment.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="20347" opendate="2020-11-25 00:00:00" fixdate="2020-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rework YARN deployment documentation page</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnDeploymentTarget.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.DefaultClusterClientServiceLoader.java</file>
      <file type="M">docs.redirects.yarn.setup.md</file>
      <file type="M">docs.flinkDev.building.zh.md</file>
      <file type="M">docs.flinkDev.building.md</file>
      <file type="M">docs.deployment.resource-providers.yarn.setup.zh.md</file>
      <file type="M">docs.deployment.resource-providers.yarn.setup.md</file>
      <file type="M">docs.deployment.resource-providers.hadoop.zh.md</file>
      <file type="M">docs.deployment.resource-providers.hadoop.md</file>
      <file type="M">docs.deployment.memory.mem.tuning.zh.md</file>
      <file type="M">docs.deployment.memory.mem.tuning.md</file>
      <file type="M">docs.deployment.memory.mem.setup.zh.md</file>
      <file type="M">docs.deployment.memory.mem.setup.md</file>
      <file type="M">docs.deployment.memory.mem.migration.zh.md</file>
      <file type="M">docs.deployment.memory.mem.migration.md</file>
      <file type="M">docs.deployment.index.zh.md</file>
      <file type="M">docs.deployment.index.md</file>
      <file type="M">docs.deployment.filesystems.index.zh.md</file>
      <file type="M">docs.deployment.filesystems.index.md</file>
      <file type="M">docs.deployment.config.zh.md</file>
      <file type="M">docs.deployment.config.md</file>
      <file type="M">docs.deployment.cli.zh.md</file>
      <file type="M">docs.deployment.cli.md</file>
      <file type="M">docs.concepts.flink-architecture.zh.md</file>
      <file type="M">docs.concepts.flink-architecture.md</file>
    </fixedFiles>
  </bug>
  <bug id="20349" opendate="2020-11-25 00:00:00" fixdate="2020-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query fails with "A conflict is detected. This is unexpected."</summary>
      <description>The test case to reproduce: @Test public void test() throws Exception { tableEnv.executeSql("create table src(key string,val string)"); tableEnv.executeSql("SELECT sum(char_length(src5.src1_value)) FROM " + "(SELECT src3.*, src4.val as src4_value, src4.key as src4_key FROM src src4 JOIN " + "(SELECT src2.*, src1.key as src1_key, src1.val as src1_value FROM src src1 JOIN src src2 ON src1.key = src2.key) src3 " + "ON src3.src1_key = src4.key) src5").collect(); }</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.MultipleInputITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.MultipleInputCreationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.DeadlockBreakupTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.MultipleInputCreationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DeadlockBreakupTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.processors.utils.InputPriorityGraphGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.processors.utils.InputPriorityGraphGenerator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.processors.utils.InputPriorityConflictResolver.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.processors.utils.InputOrderCalculator.java</file>
    </fixedFiles>
  </bug>
  <bug id="20352" opendate="2020-11-25 00:00:00" fixdate="2020-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rework command line interface documentation page</summary>
      <description>The command line interface documentation page is quite out-dated and not very easy to read. A large part is simply the help message from the CLI which is wall of text. Ideally, we can loosen the page a bit up and update the examples.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.deployment.cli.zh.md</file>
      <file type="M">docs.deployment.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="20354" opendate="2020-11-25 00:00:00" fixdate="2020-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rework standalone deployment documentation page</summary>
      <description>Similar to FLINK-20347 we need to update the standalone deployment documentation page. Additionally, we need to verify that everything we state on the documentation works.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.redirects.windows.local.setup.zh.md</file>
      <file type="M">docs.redirects.windows.local.setup.md</file>
      <file type="M">docs.redirects.windows.zh.md</file>
      <file type="M">docs.redirects.windows.md</file>
      <file type="M">docs.redirects.tutorials.local.setup.zh.md</file>
      <file type="M">docs.redirects.tutorials.local.setup.md</file>
      <file type="M">docs.redirects.tutorials.flink.on.windows.zh.md</file>
      <file type="M">docs.redirects.tutorials.flink.on.windows.md</file>
      <file type="M">docs.redirects.setup.quickstart.zh.md</file>
      <file type="M">docs.redirects.setup.quickstart.md</file>
      <file type="M">docs.redirects.local.setup.tutorial.zh.md</file>
      <file type="M">docs.redirects.local.setup.tutorial.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.local.zh.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.local.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.kubernetes.zh.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.kubernetes.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.index.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.docker.zh.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.deployment.resource-providers.native.kubernetes.zh.md</file>
      <file type="M">docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.deployment.repls.python.shell.zh.md</file>
      <file type="M">docs.deployment.repls.python.shell.md</file>
      <file type="M">docs.deployment.cli.zh.md</file>
      <file type="M">docs.deployment.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="20355" opendate="2020-11-25 00:00:00" fixdate="2020-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rework K8s deployment documentation page</summary>
      <description>Similar to FLINK-20347, we need to update the K8s deployment documentation page. Additionally, we should ensure that everything works which is stated in the documentation.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.deployment.resource-providers.standalone.docker.zh.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.deployment.resource-providers.native.kubernetes.zh.md</file>
      <file type="M">docs.deployment.resource-providers.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug id="20356" opendate="2020-11-25 00:00:00" fixdate="2020-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rework Mesos deployment documentation page</summary>
      <description>Similar to FLINK-20347, we need to rework the Mesos deployment documentation page. Additionally, we should validate that everything which is stated in the documentation actually works.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.deployment.resource-providers.mesos.zh.md</file>
      <file type="M">docs.deployment.resource-providers.mesos.md</file>
    </fixedFiles>
  </bug>
  <bug id="20357" opendate="2020-11-25 00:00:00" fixdate="2020-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rework HA documentation page</summary>
      <description>We need to rework the HA documentation page. The first step is to split the existing documentation into general concepts as an overview page and HA service implementation specific sub pages. For the implementation specific sub pages we need to add Zookeeper and the K8s HA services.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.deployment.resource-providers.standalone.kubernetes.zh.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.kubernetes.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.index.zh.md</file>
      <file type="M">docs.deployment.resource-providers.standalone.index.md</file>
      <file type="M">docs.deployment.resource-providers.native.kubernetes.zh.md</file>
      <file type="M">docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.deployment.ha.zookeeper.ha.zh.md</file>
      <file type="M">docs.deployment.ha.zookeeper.ha.md</file>
      <file type="M">docs.deployment.ha.kubernetes.ha.zh.md</file>
      <file type="M">docs.deployment.ha.kubernetes.ha.md</file>
      <file type="M">docs.deployment.ha.index.zh.md</file>
      <file type="M">docs.deployment.ha.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="20370" opendate="2020-11-26 00:00:00" fixdate="2020-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Result is wrong when sink primary key is not the same with query</summary>
      <description>Both sources are upsert-kafka which synchronizes the changes from MySQL tables (source_city, source_customer). The sink is another MySQL table which is in upsert mode with "city_name" primary key. The join key is "city_id". In this case, the result will be wrong when updating source_city.city_name column in MySQL, as the UPDATE_BEFORE is ignored and the old city_name is retained in the sink table. Sink(table=[default_catalog.default_database.sink_kafka_count_city], fields=[city_name, count_customer, sum_gender], changelogMode=[NONE])+- Calc(select=[city_name, CAST(count_customer) AS count_customer, CAST(sum_gender) AS sum_gender], changelogMode=[I,UA,D]) +- Join(joinType=[InnerJoin], where=[=(city_id, id)], select=[city_id, count_customer, sum_gender, id, city_name], leftInputSpec=[JoinKeyContainsUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey], changelogMode=[I,UA,D]) :- Exchange(distribution=[hash[city_id]], changelogMode=[I,UA,D]) : +- GlobalGroupAggregate(groupBy=[city_id], select=[city_id, COUNT_RETRACT(count1$0) AS count_customer, SUM_RETRACT((sum$1, count$2)) AS sum_gender], changelogMode=[I,UA,D]) : +- Exchange(distribution=[hash[city_id]], changelogMode=[I]) : +- LocalGroupAggregate(groupBy=[city_id], select=[city_id, COUNT_RETRACT(*) AS count1$0, SUM_RETRACT(gender) AS (sum$1, count$2)], changelogMode=[I]) : +- Calc(select=[city_id, gender], changelogMode=[I,UB,UA,D]) : +- ChangelogNormalize(key=[customer_id], changelogMode=[I,UB,UA,D]) : +- Exchange(distribution=[hash[customer_id]], changelogMode=[UA,D]) : +- MiniBatchAssigner(interval=[3000ms], mode=[ProcTime], changelogMode=[UA,D]) : +- TableSourceScan(table=[[default_catalog, default_database, source_customer]], fields=[customer_id, city_id, age, gender, update_time], changelogMode=[UA,D]) +- Exchange(distribution=[hash[id]], changelogMode=[I,UA,D]) +- ChangelogNormalize(key=[id], changelogMode=[I,UA,D]) +- Exchange(distribution=[hash[id]], changelogMode=[UA,D]) +- MiniBatchAssigner(interval=[3000ms], mode=[ProcTime], changelogMode=[UA,D]) +- TableSourceScan(table=[[default_catalog, default_database, source_city]], fields=[id, city_name], changelogMode=[UA,D])We have suggested users to use the same key of the query as the primary key on sink in the documentation: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/queries.html#deduplication. We should make this attention to be more highlight in CREATE TABLE page.</description>
      <version>1.12.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.RankTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.JoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalSink.scala</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaChangelogTableITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.ChangelogSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.ExecutionConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="20371" opendate="2020-11-26 00:00:00" fixdate="2020-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add docs for outer interval join</summary>
      <description>By looking at the docs, it looks like we only support inner interval joins but we also support outer joins according to the tests.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
    </fixedFiles>
  </bug>
  <bug id="20381" opendate="2020-11-26 00:00:00" fixdate="2020-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect use of yarn-session.sh -id vs -yid in log statements.</summary>
      <description>The Yarn per-job modes log about the recommended shutdown of yarn, which doesn't work.See: https://github.com/apache/flink/pull/10964#issuecomment-734166399</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FallbackYarnSessionCli.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.AbstractYarnCli.java</file>
    </fixedFiles>
  </bug>
  <bug id="20387" opendate="2020-11-27 00:00:00" fixdate="2020-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support defining event time attribute on TIMESTAMP_LTZ column</summary>
      <description>Currently, only TIMESTAMP type can be used as rowtime attribute. Would be better to support TIMESTAMP WITH LOCAL ZONE TIME as well.As a workaround, users can cast the TIMESTAMP WITH LOCAL ZONE TIME into TIMESTAMP, CAST(ts AS TIMESTAMP).</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.window.WindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.aggregate.window.SlicingWindowAggOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.TimeWindowUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.TypeCheckUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.types.PlannerTypeUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.WindowOperatorBuilder.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.WindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.triggers.Trigger.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.triggers.ProcessingTimeTriggers.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.triggers.EventTimeTriggers.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.slicing.SlicingWindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.slicing.SliceAssigners.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.internal.MergingWindowProcessFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.internal.InternalWindowProcessFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.combines.WindowCombineFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.window.WindowRankOperatorBuilder.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.window.processors.WindowRankProcessor.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.window.combines.TopNRecordsCombiner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.SlicingWindowAggOperatorBuilder.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.processors.SliceSharedWindowAggProcessor.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.processors.AbstractWindowAggProcessor.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.LocalSlicingWindowAggOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.combines.GlobalAggAccCombiner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.combines.AggRecordsCombiner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.buffers.WindowBuffer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.buffers.RecordsWindowBuffer.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.DateTimeTestUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TestData.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.WindowRankITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.WindowDistinctAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.WindowAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TimeAttributeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SourceWatermarkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.MatchRecognizeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.WindowAggregateUseDaylightTimeHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.WindowAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.validation.TemporalTableJoinValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.UnionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.IntervalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.schema.TimeIndicatorRelDataTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TemporalTableFunctionJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowTableFunctionTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowRankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.UnionTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RelTimeIndicatorConverterTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalFunctionJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.IntervalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ExpandWindowTableFunctionTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupWindowAggregateOperator.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PassThroughPythonStreamGroupWindowAggregateOperator.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.DefaultSchemaResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.SchemaResolutionTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.Schema.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.TableSchema.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeChecks.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.api.TableSchemaTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.CatalogSchemaTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.PlannerRowtimeAttribute.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RelDataTypeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RelDataTypeJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLegacySink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLocalWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanAcrossCalcRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.MatchCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.WatermarkGeneratorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.calcite.WatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalGroupWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalIntervalJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.LegacyCatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.TimeIndicatorRelDataType.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RelDataTypeSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.GroupWindowAggregateJsonITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.TemporalSortJsonITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.WindowAggregateJsonITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.DeduplicationJsonPlanTest.jsonplan.testDeduplication.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testProcessingTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTable.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTableWithProjectionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.MatchRecognizeJsonPlanTest.jsonplan.testMatch.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctWithNonDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortProcessingTime.out</file>
    </fixedFiles>
  </bug>
  <bug id="20391" opendate="2020-11-27 00:00:00" fixdate="2020-11-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Set FORWARD_EDGES_PIPELINED for BATCH ExecutionMode</summary>
      <description>It would be better to treat the rescale operation similar to keyBy or rebalance and make it a possible pipeline border.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.api.datastream.DataStreamBatchExecutionITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorExecutionModeDetectionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="20413" opendate="2020-11-29 00:00:00" fixdate="2020-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sources should add splits back in "resetSubtask()", rather than in "subtaskFailed()".</summary>
      <description>Because "subtaskFailed()" has no strong order guarantees with checkpoint completion, we need to return failed splits in "resetSubtask()" instead.See FLINK-20396 for a detailed explanation of the race condition.</description>
      <version>None</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SplitAssignmentTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SplitAssignmentTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="20417" opendate="2020-11-30 00:00:00" fixdate="2020-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle "Too old resource version" exception in Kubernetes watch more gracefully</summary>
      <description>Currently, when the watcher(pods watcher, configmap watcher) is closed with exception, we will call WatchCallbackHandler#handleFatalError. And this could cause JobManager terminating and then failover.For most cases, this is correct. But not for "too old resource version" exception. See more information here&amp;#91;1&amp;#93;. Usually this exception could happen when the APIServer is restarted. And we just need to create a new watch and continue to do the pods/configmap watching. This could help the Flink cluster reducing the impact of K8s cluster restarting. The issue is inspired by this technical article&amp;#91;2&amp;#93;. Thanks the guys from tencent for the debugging. Note this is a Chinese documentation. &amp;#91;1&amp;#93;. https://stackoverflow.com/questions/61409596/kubernetes-too-old-resource-version&amp;#91;2&amp;#93;. https://cloud.tencent.com/developer/article/1731416</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriverTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.TestingFlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.NoOpWatchCallbackHandler.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesPodsWatcherTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderRetrievalDriverTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionDriverTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityTestBase.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriver.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.AbstractKubernetesWatcher.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderRetrievalDriver.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="20419" opendate="2020-11-30 00:00:00" fixdate="2020-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Insert fails due to failure to generate execution plan</summary>
      <description>Test case to reproduce: @Test public void test() throws Exception { tableEnv.executeSql("create table src(x int)"); tableEnv.executeSql("create table dest(x int) partitioned by (p string,q string)"); tableEnv.executeSql("insert into dest select x,'0','0' from src order by x").await(); }</description>
      <version>None</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecSinkRule.scala</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="20423" opendate="2020-11-30 00:00:00" fixdate="2020-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove usage of {{site.baseurl}} from markdown files</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.streaming.joins.md</file>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.execution.configuration.zh.md</file>
      <file type="M">docs.dev.execution.configuration.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.table.sql.gettingStarted.zh.md</file>
      <file type="M">docs.dev.table.sql.gettingStarted.md</file>
    </fixedFiles>
  </bug>
  <bug id="20444" opendate="2020-12-1 00:00:00" fixdate="2020-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Chain AsyncWaitOperator to new sources</summary>
      <description>For legacy sources, we had to disable chaining because of incompatible threading models.New sources are working fine however and it would give some users massive performance improvements.</description>
      <version>None</version>
      <fixedVersion>1.12.0,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperatorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SimpleOperatorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="20467" opendate="2020-12-3 00:00:00" fixdate="2020-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the Example in Python DataStream Doc</summary>
      <description>Currently the example of MapFunction can't work. We need to fix it.</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.datastream-api-users-guide.operators.zh.md</file>
      <file type="M">docs.dev.python.datastream-api-users-guide.operators.md</file>
    </fixedFiles>
  </bug>
  <bug id="20485" opendate="2020-12-4 00:00:00" fixdate="2020-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Map views are deserialized multiple times</summary>
      <description>The rework of aggregate functions in FLINK-18809 and FLINK-15803 caused a performance regression due to map views being deserialized multiple times. A solution is to wrap the logic back into a RAW type to have lazy binary format semantics.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.DistinctAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.MapDataSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.ExternalSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.ArrayDataSerializer.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.typeutils.DataViewUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="20486" opendate="2020-12-4 00:00:00" fixdate="2020-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive temporal join should allow monitor interval smaller than 1 hour</summary>
      <description>Currently hive temporal join requires the monitor interval to be at least 1h, which may not fit everyone's needs. Although we recommend a relatively large monitor interval, we shouldn't make such decision for users. A warning log is a better option for a small interval.</description>
      <version>None</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveLookupTableSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="20487" opendate="2020-12-4 00:00:00" fixdate="2020-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to consume retractions for window aggregate operator</summary>
      <description>EXCEPTION: org.apache.flink.table.api.TableException: Group Window Aggregate: Retraction on windowed GroupBy Aggregate is not supported yet. org.apache.flink.table.api.TableException: Group Window Aggregate: Retraction on windowed GroupBy Aggregate is not supported yet. please re-check sql grammar. Note: Windowed GroupBy Aggregate should not follow anon-windowed GroupBy aggregation. at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupWindowAggregateBase.translateToPlanInternal(StreamExecGroupWindowAggregateBase.scala:138) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupWindowAggregateBase.translateToPlanInternal(StreamExecGroupWindowAggregateBase.scala:54) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan(ExecNode.scala:58) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan$(ExecNode.scala:56) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupWindowAggregateBase.translateToPlan(StreamExecGroupWindowAggregateBase.scala:54) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:54) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:39) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan(ExecNode.scala:58) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan$(ExecNode.scala:56) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalcBase.translateToPlan(StreamExecCalcBase.scala:38) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToTransformation(StreamExecSink.scala:184) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:91) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:48) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan(ExecNode.scala:58) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode.translateToPlan$(ExecNode.scala:56) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlan(StreamExecSink.scala:48) at org.apache.flink.table.planner.delegation.StreamPlanner.$anonfun$translateToPlan$1(StreamPlanner.scala:60) at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) at scala.collection.Iterator.foreach(Iterator.scala:937) at scala.collection.Iterator.foreach$(Iterator.scala:937) at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) at scala.collection.IterableLike.foreach(IterableLike.scala:70) at scala.collection.IterableLike.foreach$(IterableLike.scala:69) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike.map(TraversableLike.scala:233) at scala.collection.TraversableLike.map$(TraversableLike.scala:226) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:59) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:153) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:682) at org.apache.flink.table.api.internal.TableEnvironmentImpl.insertIntoInternal(TableEnvironmentImpl.java:355) at org.apache.flink.table.api.internal.TableEnvironmentImpl.insertInto(TableEnvironmentImpl.java:334) CASE:SELECT DATE_FORMAT(tumble_end(ROWTIME ,interval '1' hour),'yyyy-MM-dd HH') as stat_time, count(crypto_customer_number) first_phone_numFROM ( SELECT ROWTIME, crypto_customer_number, row_number() over(partition by crypto_customer_number order by ROWTIME ) as rn FROM source_kafka_biz_shuidi_sdb_crm_call_record ) cal where rn =1group by tumble(ROWTIME,interval '1' hour);</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
    </fixedFiles>
  </bug>
  <bug id="20488" opendate="2020-12-4 00:00:00" fixdate="2020-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show checkpoint type in the UI (AC/UC) for each subtask</summary>
      <description>A follow-up ticket after FLINK-19681 to address issues not directly related to checkpointing (see discussion). In the UI, show checkpoint type for each subtask; on a checkpoint level display unaligned if at least one subtask did UC.That should ease debugging of the checkpointing issues.  Disabling propagation moved to FLINK-20548.</description>
      <version>1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.checkpoints.TaskCheckpointStatisticsWithSubtaskDetailsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.TaskStateStatsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.SubtaskStateStatsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointStatsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStatsTrackerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.SubtaskCheckpointStatistics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.SubtaskStateStats.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointMetricsBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointMetrics.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.subtask.job-checkpoints-subtask.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="20496" opendate="2020-12-5 00:00:00" fixdate="2020-3-5 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>RocksDB partitioned index filter option</summary>
      <description>When using RocksDBStateBackend and enabling state.backend.rocksdb.memory.managed and state.backend.rocksdb.memory.fixed-per-slot, flink will strictly limited rocksdb memory usage which contains "write buffer" and "block cache". With these options rocksdb stores index and filters in block cache, because in default options index/filters can grows unlimited. But it's lead another issue, if high-priority cache(configure by state.backend.rocksdb.memory.high-prio-pool-ratio) can't fit all index/filters blocks, it will load all metadata from disk when cache missed, and program went extremely slow. According to Partitioned Index Filters[1], we can enable two-level index having acceptable performance when index/filters cache missed. Enable these options can get over 10x faster in my case&amp;#91;2&amp;#93;, I think we can add an option state.backend.rocksdb.partitioned-index-filters and default value is false, so we can use this feature easily.&amp;#91;1&amp;#93; Partitioned Index Filters: https://github.com/facebook/rocksdb/wiki/Partitioned-Index-Filters&amp;#91;2&amp;#93; Deduplicate scenario, state.backend.rocksdb.memory.fixed-per-slot=256M, SSD, elapsed time 4.91ms -&gt; 0.33ms.</description>
      <version>1.10.2,1.11.2,1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainerTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBMemoryControllerUtilsTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBSharedResources.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMemoryControllerUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMemoryConfiguration.java</file>
      <file type="M">docs.layouts.shortcodes.generated.state.backend.rocksdb.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.configuration.html</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBConfigurableOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.DefaultConfigurableOptionsFactory.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.configurable.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="2050" opendate="2015-5-19 00:00:00" fixdate="2015-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add pipelining mechanism for chainable transformers and estimators</summary>
      <description>The key concept of an easy to use ML library is the quick and simple construction of data analysis pipelines. Scikit-learn's approach to define transformers and estimators seems to be a really good solution to this problem. I propose to follow a similar path, because it makes FlinkML flexible in terms of code reuse as well as easy for people coming from Scikit-learn to use the FlinkML.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.recommendation.ALS.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.SparseVector.scala</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClient.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery9ITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery4ITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery3WithUnionITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery3ITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery10ITCase.java</file>
      <file type="M">flink-tests.src.test.assembly.test-streamingclassloader-assembly.xml</file>
      <file type="M">flink-tests.src.test.assembly.test-kmeans-assembly.xml</file>
      <file type="M">flink-tests.src.test.assembly.test-custominput-assembly.xml</file>
      <file type="M">flink-staging.flink-tez.src.main.java.org.apache.flink.tez.examples.TPCHQuery3.java</file>
      <file type="M">flink-staging.flink-tachyon.src.test.java.org.apache.flink.tachyon.TachyonFileSystemWrapperTest.java</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.runtime.package.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.plan.package.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.package.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.expressions.package.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.scala.table.package.scala</file>
      <file type="M">flink-staging.flink-table.src.main.java.org.apache.flink.api.table.package-info.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.pom.xml</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.regression.MultipleLinearRegressionITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.recommendation.ALSITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.preprocessing.StandardScalerITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.feature.PolynomialBaseITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.experimental.SciKitPipelineSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.classification.CoCoASuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.regression.MultipleLinearRegression.scala</file>
      <file type="M">docs.libs.ml.als.md</file>
      <file type="M">docs.libs.ml.cocoa.md</file>
      <file type="M">docs.libs.ml.multiple.linear.regression.md</file>
      <file type="M">docs.libs.ml.polynomial.base.feature.mapper.md</file>
      <file type="M">docs.libs.ml.standard.scaler.md</file>
      <file type="M">flink-clients.src.test.assembly.test-assembly.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.ConfigurationTest.java</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-examples.flink-java-examples.pom.xml</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.relational.TPCHQuery3.java</file>
      <file type="M">flink-examples.flink-scala-examples.pom.xml</file>
      <file type="M">flink-java8.pom.xml</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.aggregation.AvgAggregationFunction.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.tuple.TupleGenerator.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.traversals.package-info.java</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.java.Job.java</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.Job.scala</file>
      <file type="M">flink-quickstart.flink-tez-quickstart.src.main.resources.archetype-resources.src.main.java.LocalJob.java</file>
      <file type="M">flink-quickstart.flink-tez-quickstart.src.main.resources.archetype-resources.src.main.java.YarnJob.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.checkpoint.package-info.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.package-info.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.JarFileCreator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.JarFileCreatorTest.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.DataSet.scala</file>
      <file type="M">flink-staging.flink-avro.src.test.assembly.test-assembly.xml</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.HadoopFileOutputCommitter.java</file>
      <file type="M">flink-staging.flink-language-binding.flink-python.src.main.java.org.apache.flink.languagebinding.api.java.python.PythonPlanBinder.java</file>
      <file type="M">flink-staging.flink-ml.pom.xml</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.classification.CoCoA.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.ChainedLearner.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.ChainedTransformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.Learner.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.Transformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.ChainedPredictor.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.ChainedTransformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.Estimator.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.KMeans.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.Offset.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.Predictor.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.Scaler.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.experimental.Transformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.feature.PolynomialBase.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Breeze.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.CanCopy.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.package.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Vector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.preprocessing.StandardScaler.scala</file>
    </fixedFiles>
  </bug>
  <bug id="20508" opendate="2020-12-7 00:00:00" fixdate="2020-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce PythonStreamGroupTableAggregateOperator</summary>
      <description>Adds PythonStreamGroupTableAggregateOperator to support running Python TableAggregateFunction</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperatorTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="20517" opendate="2020-12-7 00:00:00" fixdate="2020-1-7 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support mixed keyed/non-keyed operations in BATCH execution mode</summary>
      <description>Using the batch state backend requires that the inputs are sorted by a key. Right now when sorting, we sort either all of the inputs or none. It is because we expect that the same key is consumed from all inputs at the same time.In the process of implementing FLINK-20491 we can also add support for mixed non-keyed/keyed operators.</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.api.datastream.DataStreamBatchExecutionITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.TwoInputTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.MultiInputTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.ConnectedStreams.java</file>
    </fixedFiles>
  </bug>
  <bug id="20521" opendate="2020-12-7 00:00:00" fixdate="2020-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Null result values are being swallowed by RPC system</summary>
      <description>If an RPC method returns a null value, then it seems that the request future won't get completed as reported in FLINK-17921.We should either not allow to return null values as responses or make sure that a null value is properly transmitted to the caller.</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActor.java</file>
    </fixedFiles>
  </bug>
  <bug id="20522" opendate="2020-12-7 00:00:00" fixdate="2020-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make implementing a built-in function straightforward</summary>
      <description>FLIP-65 has introduced new abstractions for functions. They make implementing user-defined functions straightforward. However, currently, these abstraction are not available for built-in functions. Adding a built-in function requires many changes at different locations as shown in FLINK-6810. This is error-prone, limits contributions, and is also not well supported by the current module system.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.LookupJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.BridgingSqlFunctionCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.inference.TypeInferenceReturnInference.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.inference.OperatorBindingCallContext.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.UserDefinedFunctionHelper.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.MiscFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.InputTypeStrategiesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeStrategies.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.InputTypeStrategies.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.BaseExpressions.java</file>
      <file type="M">flink-python.pyflink.table.expression.py</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.converter.FunctionDefinitionConvertRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinition.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">docs.dev.table.functions.systemFunctions.md</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.PythonUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.MinWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWsWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.InternalAggregateFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.CollectAggFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="2053" opendate="2015-5-20 00:00:00" fixdate="2015-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Preregister ML types for Kryo serialization</summary>
      <description>Currently, FlinkML uses interfaces and abstract types to implement generic algorithms. As a consequence we have to use Kryo to serialize the effective subtypes. In order to speed the data transfer up, it's necessary to preregister these types in order to assign them fixed IDs.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.pipeline.PipelineITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.feature.PolynomialBaseITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.recommendation.ALS.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.preprocessing.StandardScaler.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.pipeline.Transformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.pipeline.Predictor.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.pipeline.Estimator.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.BreezeVectorConverter.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Breeze.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.FlinkTools.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.classification.CoCoA.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.PojoSerializer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="20533" opendate="2020-12-8 00:00:00" fixdate="2020-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add histogram support to Datadog reporter</summary>
      <description>The datadog reporter currently ignores Histograms. I think we just saved some time when we added it, but we should rectify that.</description>
      <version>None</version>
      <fixedVersion>1.13.0,1.12.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DMetric.java</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DMeter.java</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DGauge.java</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DCounter.java</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.test.java.org.apache.flink.metrics.datadog.DatadogHttpClientTest.java</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DatadogHttpReporter.java</file>
      <file type="M">docs.deployment.metric.reporters.zh.md</file>
      <file type="M">docs.deployment.metric.reporters.md</file>
    </fixedFiles>
  </bug>
  <bug id="20543" opendate="2020-12-9 00:00:00" fixdate="2020-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix typo in upsert kafka docs</summary>
      <description></description>
      <version>1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.upsert-kafka.zh.md</file>
      <file type="M">docs.dev.table.connectors.upsert-kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="20550" opendate="2020-12-9 00:00:00" fixdate="2020-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong savepoint config in some docs</summary>
      <description>Fix config 'state.savepoint.dir' into 'state.savepoints.dir' in docs</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.try-flink.flink-operations-playground.zh.md</file>
      <file type="M">docs.try-flink.flink-operations-playground.md</file>
      <file type="M">docs.ops.state.savepoints.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="20562" opendate="2020-12-10 00:00:00" fixdate="2020-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support ExplainDetails for EXPLAIN sytnax</summary>
      <description>Currently, EXPLAIN syntax only supports to print the default AST, logical plan, and physical plan. However, it doesn't support to print detailed information such as CHANGELOG_MODE, ESTIMATED_COST, JSON_EXECUTION_PLAN which are defined in ExplainDetail.Allow users to specify the ExplainDetails in statement.EXPLAIN [ExplainDetail[, ExplainDetail]*] &lt;statement&gt;ExplainDetail: { ESTIMATED_COST, CHANGELOG_MODE, JSON_EXECUTION_PLAN}Print the plan for the statement with specified ExplainDetails. ESTIMATED_COSTgenerates cost information on physical node estimated by optimizer, e.g. TableSourceScan(..., cumulative cost = {1.0E8 rows, 1.0E8 cpu, 2.4E9 io, 0.0 network, 0.0 memory})CHANGELOG_MODEgenerates changelog mode for every physical rel node. e.g. GroupAggregate(..., changelogMode=&amp;#91;I,UA,D&amp;#93;)JSON_EXECUTION_PLANgenerates the execution plan in json format of the program.Flink SQL&gt; EXPLAIN ESTIMATED_COST, CHANGELOG SELECT * FROM MyTable;...</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ExplainOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.resources.org.apache.flink.sql.parser.utils.ParserResource.properties</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.utils.ParserResource.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.dql.SqlRichExplain.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.table.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="20563" opendate="2020-12-10 00:00:00" fixdate="2020-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support built-in functions for Hive versions prior to 1.2.0</summary>
      <description>Currently Hive built-in functions are supported only for Hive-1.2.0 and later. We should investigate how to lift this limitation.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV120.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV100.java</file>
    </fixedFiles>
  </bug>
  <bug id="20582" opendate="2020-12-12 00:00:00" fixdate="2020-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix typos in `CREATE Statements` docs</summary>
      <description></description>
      <version>1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
    </fixedFiles>
  </bug>
  <bug id="20615" opendate="2020-12-16 00:00:00" fixdate="2020-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Local recovery and sticky scheduling end-to-end test timeout with "IOException: Stream Closed"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10905&amp;view=logs&amp;j=6caf31d6-847a-526e-9624-468e053467d6&amp;t=0b23652f-b18b-5b6e-6eb6-a11070364610It tried to restart many times, and the final error was following:2020-12-15T23:54:00.5067862Z Dec 15 23:53:42 2020-12-15 23:53:41,538 ERROR org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder [] - Caught unexpected exception.2020-12-15T23:54:00.5068392Z Dec 15 23:53:42 java.io.IOException: Stream Closed2020-12-15T23:54:00.5068767Z Dec 15 23:53:42 at java.io.FileInputStream.readBytes(Native Method) ~[?:?]2020-12-15T23:54:00.5069223Z Dec 15 23:53:42 at java.io.FileInputStream.read(FileInputStream.java:279) ~[?:?]2020-12-15T23:54:00.5070150Z Dec 15 23:53:42 at org.apache.flink.core.fs.local.LocalDataInputStream.read(LocalDataInputStream.java:73) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5071217Z Dec 15 23:53:42 at org.apache.flink.core.fs.FSDataInputStreamWrapper.read(FSDataInputStreamWrapper.java:61) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5072295Z Dec 15 23:53:42 at org.apache.flink.runtime.util.ForwardingInputStream.read(ForwardingInputStream.java:51) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5072967Z Dec 15 23:53:42 at java.io.DataInputStream.readFully(DataInputStream.java:200) ~[?:?]2020-12-15T23:54:00.5073483Z Dec 15 23:53:42 at java.io.DataInputStream.readFully(DataInputStream.java:170) ~[?:?]2020-12-15T23:54:00.5074535Z Dec 15 23:53:42 at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5075847Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:222) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5077187Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:169) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5078495Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:152) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5079802Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:269) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5081013Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:565) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5082215Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:94) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5083500Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:299) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5084899Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5086342Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5087601Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:316) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5088924Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:155) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5090261Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:248) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5091459Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:400) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5092604Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:507) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5093748Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5094866Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:501) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5095912Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:531) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5096875Z Dec 15 23:53:42 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5097814Z Dec 15 23:53:42 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5098373Z Dec 15 23:53:42 at java.lang.Thread.run(Thread.java:834) [?:?]2020-12-15T23:54:00.5099549Z Dec 15 23:53:42 2020-12-15 23:53:41,557 WARN org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Exception while restoring keyed state backend for StreamFlatMap_20ba6b65f97481d5570070de90e4e791_(1/4) from alternative (1/1), will retry while more alternatives are available.2020-12-15T23:54:00.5100556Z Dec 15 23:53:42 org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.2020-12-15T23:54:00.5101480Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:328) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5102669Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:565) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5103763Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:94) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5104723Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:299) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5105700Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5106630Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5107587Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:316) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5108581Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:155) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5109505Z Dec 15 23:53:42 at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:248) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5110456Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:400) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5111316Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:507) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5112175Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5113012Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:501) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5113787Z Dec 15 23:53:42 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:531) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5114521Z Dec 15 23:53:42 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5115209Z Dec 15 23:53:42 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5115635Z Dec 15 23:53:42 at java.lang.Thread.run(Thread.java:834) [?:?]2020-12-15T23:54:00.5115949Z Dec 15 23:53:42 Caused by: java.io.IOException: Stream Closed2020-12-15T23:54:00.5116246Z Dec 15 23:53:42 at java.io.FileInputStream.readBytes(Native Method) ~[?:?]2020-12-15T23:54:00.5116589Z Dec 15 23:53:42 at java.io.FileInputStream.read(FileInputStream.java:279) ~[?:?]2020-12-15T23:54:00.5117284Z Dec 15 23:53:42 at org.apache.flink.core.fs.local.LocalDataInputStream.read(LocalDataInputStream.java:73) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5118080Z Dec 15 23:53:42 at org.apache.flink.core.fs.FSDataInputStreamWrapper.read(FSDataInputStreamWrapper.java:61) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5118894Z Dec 15 23:53:42 at org.apache.flink.runtime.util.ForwardingInputStream.read(ForwardingInputStream.java:51) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5119392Z Dec 15 23:53:42 at java.io.DataInputStream.readFully(DataInputStream.java:200) ~[?:?]2020-12-15T23:54:00.5119808Z Dec 15 23:53:42 at java.io.DataInputStream.readFully(DataInputStream.java:170) ~[?:?]2020-12-15T23:54:00.5120605Z Dec 15 23:53:42 at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5121576Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:222) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5122579Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:169) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5123543Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:152) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5124476Z Dec 15 23:53:42 at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:269) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]2020-12-15T23:54:00.5124994Z Dec 15 23:53:42 ... 16 more </description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactoryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="20626" opendate="2020-12-16 00:00:00" fixdate="2020-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Canceling a job when it is failing will result in job hanging in CANCELING state</summary>
      <description>If user manually cancels a job when the job is failing(here failing means the job encounters unrecoverable failure and is about to fail), the job will hang in CANCELING state and cannot terminate. The cause is that DefaultScheduler currently will always try to transition from `FAILING` to `FAILED` to terminate the job. However, job canceling will change job status to `CANCELING` so that the transition to `FAILED` will not success.</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.11.4,1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphSuspendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphRestartTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="20629" opendate="2020-12-16 00:00:00" fixdate="2020-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Kinesis][EFO] Migrate from DescribeStream to DescribeStreamSummary</summary>
      <description>BackgroundThe Kinesis EFO connector invokes DescribeStream during startup to acquire the stream ARN. This call also includes the shard information and has a TPS of 10. A similar service exists, DescribeStreamSummary that has a TPS of 20 and a lighter response payload size.During startup sources with high parallelism compete to call this service (in LAZY mode), resulting in backoff and retry. Essentially the startup time can grow by 1s for every 10 parallelism, due to the 10 TPS. Migrating to DescribeStreamSummary will improve startup time.ScopeMigrate call to DescribeStream to use DescribeStreamSummary instead.NoteI have targeted 1.12.1, let me know if we should instead target 1.13</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.FakeKinesisFanOutBehavioursFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyV2Test.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyV2Interface.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyV2.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.StreamConsumerRegistrar.java</file>
      <file type="M">docs.dev.connectors.kinesis.zh.md</file>
      <file type="M">docs.dev.connectors.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug id="20639" opendate="2020-12-17 00:00:00" fixdate="2020-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use list to optimize the Row used by Python UDAF intermediate results</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.py</file>
    </fixedFiles>
  </bug>
  <bug id="20648" opendate="2020-12-17 00:00:00" fixdate="2020-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unable to restore job from savepoint when using Kubernetes based HA services</summary>
      <description>When restoring job from savepoint, we always end up with following error:Caused by: org.apache.flink.runtime.client.JobInitializationException: Could not instantiate JobManager. at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$5(Dispatcher.java:463) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1764) ~[?:?] ... 3 moreCaused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Stopped retrying the operation because the error is not retryable. at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) ~[?:?] at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2063) ~[?:?] at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStore.addAndLock(KubernetesStateHandleStore.java:150) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore.addCheckpoint(DefaultCompletedCheckpointStore.java:211) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1479) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.scheduler.SchedulerBase.tryRestoreExecutionGraphFromSavepoint(SchedulerBase.java:325) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:266) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.scheduler.SchedulerBase.&lt;init&gt;(SchedulerBase.java:238) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.scheduler.DefaultScheduler.&lt;init&gt;(DefaultScheduler.java:134) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:108) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:323) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.jobmaster.JobMaster.&lt;init&gt;(JobMaster.java:310) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:96) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:41) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.&lt;init&gt;(JobManagerRunnerImpl.java:141) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.createJobManagerRunner(DefaultJobManagerRunnerFactory.java:80) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$5(Dispatcher.java:450) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1764) ~[?:?] ... 3 moreCaused by: org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Stopped retrying the operation because the error is not retryable. at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperation$1(FutureUtils.java:166) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) ~[?:?] at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) ~[?:?] at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478) ~[?:?] ... 3 moreCaused by: java.util.concurrent.CompletionException: org.apache.flink.kubernetes.kubeclient.resources.KubernetesException: Cannot retry checkAndUpdateConfigMap with configMap pipelines-runner-fulltext-6e99e672-4af29f0768624632839835717898b08d-jobmanager-leader because it does not exist. at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$null$6(Fabric8FlinkKubeClient.java:289) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at java.util.Optional.orElseThrow(Optional.java:401) ~[?:?] at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$null$7(Fabric8FlinkKubeClient.java:289) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1764) ~[?:?] ... 3 moreCaused by: org.apache.flink.kubernetes.kubeclient.resources.KubernetesException: Cannot retry checkAndUpdateConfigMap with configMap pipelines-runner-fulltext-6e99e672-4af29f0768624632839835717898b08d-jobmanager-leader because it does not exist. at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$null$6(Fabric8FlinkKubeClient.java:289) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at java.util.Optional.orElseThrow(Optional.java:401) ~[?:?] at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$null$7(Fabric8FlinkKubeClient.java:289) ~[flink-dist_2.11-1.12.0.jar:1.12.0] at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1764) ~[?:?]Cause of the issue is following: We construct `jobMasterServices` prior starting `leaderElectionService` (in `JobManagerRunnerImpl`) During `jobMasterServices` initialization `tryRestoreExecutionGraphFromSavepoint` gets called. This calls `KubernetesStateHandleStore.addAndLock` interally. `KubernetesStateHandleStore.addAndLock` expects configmap for JM leadership to be already present, which is wrong, because `leaderElectionService` which is responsible for its creation has not started yetPossible fixes: Start `leaderElectionService` before `jobMasterServices` Fix `KubernetesStateHandleStore`, so it can handle the case, when leader hasn't been elected</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.kubernetes.itcases.sh</file>
    </fixedFiles>
  </bug>
  <bug id="20650" opendate="2020-12-17 00:00:00" fixdate="2020-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mark "native-k8s" as deprecated in docker-entrypoint.sh</summary>
      <description>When we are publishing the image 1.12 to docker hub, some docker guys raise up a issue for the docker-entrypoint.sh. They want the images to have a certain standard, because they are the official ones. However the proposed native-k8s command is more like an internal bridge. It is only used for native Kubernetes integration. Another suggestion is removing the "bash -c" wrapper and generate it in the flink codes. Refer here&amp;#91;1&amp;#93; for more information. Note: We mark the native-k8s as deprecated and export the environments for all pass-through mode commands, the flink Kubernetes codes should be adjusted accordingly. &amp;#91;1&amp;#93;. https://github.com/docker-library/official-images/pull/9249</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesTaskManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.JavaCmdTaskManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.JavaCmdJobManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.JavaCmdTaskManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.JavaCmdJobManagerDecorator.java</file>
    </fixedFiles>
  </bug>
  <bug id="20665" opendate="2020-12-18 00:00:00" fixdate="2020-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FileNotFoundException when restore from latest Checkpoint</summary>
      <description>reproduce steps:1.a kafka to hdfs job,open `auto-compaction`2.when the job have done a successful checkpoint then cancel the  job.3.restore from the latest checkpoint.4.after the first checkpoint has done ,the exception will appear2020-12-18 10:40:58java.io.UncheckedIOException: java.io.FileNotFoundException: File does not exist: hdfs://xxxx/day=2020-12-18/hour=10/.uncompacted-part-84db54f8-eda9-4e01-8e85-672144041642-0-0    at org.apache.flink.table.filesystem.stream.compact.CompactCoordinator.lambda$coordinate$1(CompactCoordinator.java:160)    at org.apache.flink.table.runtime.util.BinPacking.pack(BinPacking.java:41)    at org.apache.flink.table.filesystem.stream.compact.CompactCoordinator.lambda$coordinate$2(CompactCoordinator.java:169)    at java.util.HashMap.forEach(HashMap.java:1289)    at org.apache.flink.table.filesystem.stream.compact.CompactCoordinator.coordinate(CompactCoordinator.java:166)    at org.apache.flink.table.filesystem.stream.compact.CompactCoordinator.commitUpToCheckpoint(CompactCoordinator.java:147)    at org.apache.flink.table.filesystem.stream.compact.CompactCoordinator.processElement(CompactCoordinator.java:137)    at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:193)    at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:179)    at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:152)    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:67)    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:372)    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186)    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:575)    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:539)    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)    at java.lang.Thread.run(Thread.java:748)Caused by: java.io.FileNotFoundException: File does not exist: hdfs://xxxx/day=2020-12-18/hour=10/.uncompacted-part-84db54f8-eda9-4e01-8e85-672144041642-0-0    at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1441)    at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)    at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)    at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.getFileStatus(HadoopFileSystem.java:85)    at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.getFileStatus(SafetyNetWrapperFileSystem.java:64)    at org.apache.flink.table.filesystem.stream.compact.CompactCoordinator.lambda$coordinate$1(CompactCoordinator.java:158)    ... 17 moreDDLCREATE TABLE cpc_bd_recall_log_hdfs ( log_timestamp BIGINT, ip STRING, `raw` STRING, `day` STRING, `hour` STRING) PARTITIONED BY (`day` , `hour`) WITH ( 'connector'='filesystem', 'path'='hdfs://xxx', 'format'='parquet', 'parquet.compression'='SNAPPY', 'sink.partition-commit.policy.kind' = 'success-file', 'auto-compaction' = 'true', 'compaction.file-size' = '128MB');</description>
      <version>1.12.0</version>
      <fixedVersion>1.12.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.stream.compact.CompactOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="20666" opendate="2020-12-18 00:00:00" fixdate="2020-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the deserialized Row losing the field_name information in PyFlink</summary>
      <description>Now, the deserialized Row loses the field_name information.@udf(result_type=DataTypes.STRING())def get_string_element(my_list): my_string = 'xxx' for element in my_list: if element.integer_element == 2: # element lost the field_name information my_string = element.string_element return my_stringt = t_env.from_elements( [("1", [Row(3, "flink")]), ("3", [Row(2, "pyflink")]), ("2", [Row(2, "python")])], DataTypes.ROW( [DataTypes.FIELD("Key", DataTypes.STRING()), DataTypes.FIELD("List_element", DataTypes.ARRAY(DataTypes.ROW( [DataTypes.FIELD("integer_element", DataTypes.INT()), DataTypes.FIELD("string_element", DataTypes.STRING())])))]))print(t.select(get_string_element(t.List_element)).to_pandas())element lost the field_name information</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.11.4,1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.fast.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.common.types.py</file>
    </fixedFiles>
  </bug>
  <bug id="2067" opendate="2015-5-21 00:00:00" fixdate="2015-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Chained streaming operators should not throw chained exceptions</summary>
      <description>The exceptions that come from chained operators have an non-intuitive chaining structure, that makes the stack traces harder to understand.For every chained task, there is a "Failed to forward record" exception, before the actual exception comes as a cause.In the Batch API, we use a special "ExceptionInChainedStubException" that is recognized and un-nested to make chained operator exceptions surface as root exceptions. We should do the same for the streaming API.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
    </fixedFiles>
  </bug>
  <bug id="20681" opendate="2020-12-19 00:00:00" fixdate="2020-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support specifying the hdfs path when ship archives or files</summary>
      <description>Currently, our team try to submit flink job that depends extra resource with yarn-application target, and use two options: "yarn.ship-archives" and "yarn.ship-files".But above options only support specifying local resource and shiping them to hdfs, besides if it can support remote resource on distributed filesystem (such as hdfs), then get the following benefits： client will exclude the local resource uploading to accelerate the job submission process yarn will cache them on the nodes so that they doesn't need to be downloaded for application</description>
      <version>1.12.0</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnClusterDescriptorTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.FlinkYarnSessionCliTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNFileReplicationITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnConfigurationITCase.java</file>
      <file type="M">docs.layouts.shortcodes.generated.yarn.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="20695" opendate="2020-12-21 00:00:00" fixdate="2020-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Zookeeper node under leader and leaderlatch is not deleted after job finished</summary>
      <description>I used flink 1.11 in standalone cluster mode for batch job. The enviornment was configured as zookeeper HA mode.After job was commited, flink runtime created nodes under /flink/default/leader and /flink/default/leaderlatch with job id.  Though jobs were finished, these nodes  were remaining in zookeeper path forever. After a period of running, more and more jobs had been executed and there were a greate number of nodes under /flink/default/leader and slowed down the performance of zookeeper. Why not delete the nodes after job finished? Flink runtime could get job status by listeners and delete the leader nodes for job immidiately.</description>
      <version>1.9.0,1.11.3,1.12.0,1.13.0</version>
      <fixedVersion>1.14.0,1.13.1,1.12.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.TestingManualHighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.TestingHighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.AbstractHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherResourceCleanupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.nonha.AbstractNonHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.AbstractHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServicesTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServices.java</file>
    </fixedFiles>
  </bug>
  <bug id="20717" opendate="2020-12-22 00:00:00" fixdate="2020-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create backPressuredTimeMsPerSecond metric</summary>
      <description>Create backPressuredTimeMsPerSecond metric, measured similarly to the existing idleTimeMsPerSecond.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxProcessorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricNames.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskIOMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.java</file>
      <file type="M">docs.ops.metrics.zh.md</file>
      <file type="M">docs.ops.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="20720" opendate="2020-12-22 00:00:00" fixdate="2020-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for ProcessFunction in Python DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.python.datastream.operators.overview.md</file>
      <file type="M">docs.content.docs.dev.datastream.operators.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.python.datastream.operators.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.operators.overview.md</file>
      <file type="M">docs.content.docs.dev.datastream.operators.process.function.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.operators.process.function.md</file>
    </fixedFiles>
  </bug>
  <bug id="20756" opendate="2020-12-24 00:00:00" fixdate="2020-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PythonCalcSplitConditionRule is not working as expected</summary>
      <description>Currently if users write such a SQL:`SELECT pyFunc5(f0, f1) FROM (SELECT e.f0, e.f1 FROM (SELECT pyFunc5(a) as e FROM MyTable) where e.f0 is NULL)`It will be optimized to:`FlinkLogicalCalc(select=&amp;#91;pyFunc5(pyFunc5(a)) AS f0&amp;#93;)+- FlinkLogicalCalc(select=&amp;#91;a&amp;#93;, where=&amp;#91;IS NULL(pyFunc5(a).f0)&amp;#93;) +- FlinkLogicalLegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: &amp;#91;TestTableSource(a, b, c, d)&amp;#93;]], fields=&amp;#91;a, b, c, d&amp;#93;)`The optimized plan is not runnable, we need to fix this.</description>
      <version>None</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.PythonUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="20757" opendate="2020-12-24 00:00:00" fixdate="2020-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize data broadcast for sort-merge shuffle</summary>
      <description>For data broadcast, we can only copy the record once when writing data into SortBuffer. Besides, we can write only one copy of data when spilling data into disk. These optimizations can improve the performance of data broadcast.</description>
      <version>1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriteReadTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="20769" opendate="2020-12-25 00:00:00" fixdate="2020-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support minibatch to optimize Python UDAF</summary>
      <description>Support minibatch to optimize Python UDAF</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.udaf.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.aggregate.fast.pxd</file>
    </fixedFiles>
  </bug>
  <bug id="20770" opendate="2020-12-25 00:00:00" fixdate="2020-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect description for config option kubernetes.rest-service.exposed.type</summary>
      <description>public static final ConfigOption&lt;ServiceExposedType&gt; REST_SERVICE_EXPOSED_TYPE = key("kubernetes.rest-service.exposed.type") .enumType(ServiceExposedType.class) .defaultValue(ServiceExposedType.LoadBalancer) .withDescription("The type of the rest service (ClusterIP or NodePort or LoadBalancer). " + "When set to ClusterIP, the rest service will not be created.");The description of the config option is not correct. We will always create the rest service after refactoring the Kubernetes decorators in FLINK-16194. </description>
      <version>1.11.3,1.12.0</version>
      <fixedVersion>1.11.4,1.12.2,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs..includes.generated.kubernetes.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="20773" opendate="2020-12-25 00:00:00" fixdate="2020-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to parse unescaped control chars in string node</summary>
      <description>Can we add an option `allow-unescaped-control-chars` for json format because of it will throw exception when exist illegal unquoted characters in the data. </description>
      <version>1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonRowDataSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDataDeserializationSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="20783" opendate="2020-12-28 00:00:00" fixdate="2020-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separate the implementation of BatchExec nodes for Join</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.RemoveShuffleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecSortMergeJoinRule.scala</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.data.util.DataFormatTestUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.codegen.SortCodeGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.SortUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.OverAggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecOverAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMatch.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.sort.SortCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.sort.ComparatorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.SortSpec.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecTemporalSort.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSort.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecRank.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortMergeJoin.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortLimit.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSort.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecRank.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.JoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecHashJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecJoinBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.ExecNodeUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecSingleRowJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecNestedLoopJoinRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecNestedLoopJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecNestedLoopJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug id="20832" opendate="2021-1-4 00:00:00" fixdate="2021-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deliver bootstrap resouces ourselves for website and documentation</summary>
      <description>The typesetting of the official website is abnormal. The typesetting seen by users in China is abnormal.Because user can't load https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js CDN.</description>
      <version>1.12.0</version>
      <fixedVersion>1.11.4,1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">NOTICE</file>
      <file type="M">docs..layouts.base.html</file>
    </fixedFiles>
  </bug>
  <bug id="20835" opendate="2021-1-4 00:00:00" fixdate="2021-2-4 01:00:00" resolution="Done">
    <buginformation>
      <summary>Implement FineGrainedSlotManager</summary>
      <description>Introduce SlotManager plugin for fine-grained resource management. Request slot from TaskExecutor with the actual resource profiles. Use ResourceTracker to bookkeep the resource requirements Introduce TaskExecutorTracker, which bookkeep the total / available resource and slot assignment for registered task executor. Bookkeep task manager total and available resources. Bookkeep slot allocations and assignments. Intorduce PendingTaskManager. Map registered task executors to matching PendingTaskManager.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.WorkerResourceSpecTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.WorkerResourceSpec.java</file>
      <file type="M">tools.ci.test.controller.sh</file>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerHATest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRuntimeServicesConfiguration.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.SlotReport.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRuntimeServices.java</file>
    </fixedFiles>
  </bug>
  <bug id="20840" opendate="2021-1-4 00:00:00" fixdate="2021-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Projection pushdown doesn&amp;#39;t work in temporal(lookup) join</summary>
      <description>sql 1: |SELECT T.*, D.id|FROM MyTable AS T|JOIN LookupTable FOR SYSTEM_TIME AS OF T.proctime AS D|ON T.a = D.idoptmized plan:Calc(select=[a, b, c, PROCTIME_MATERIALIZE(proctime) AS proctime, rowtime, id])+- LookupJoin(table=[default_catalog.default_database.LookupTable], joinType=[InnerJoin], async=[false], lookup=[id=a], select=[a, b, c, proctime, rowtime, id, name, age]) +- DataStreamScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c, proctime, rowtime])sql 2:|SELECT T.a, D.id|FROM MyTable AS T|JOIN LookupTable FOR SYSTEM_TIME AS OF T.proctime AS D|ON T.a = D.idoptmized plan:LookupJoin(table=[default_catalog.default_database.LookupTable], joinType=[InnerJoin], async=[false], lookup=[id=a], select=[a, id])+- Calc(select=[a]) +- DataStreamScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c, proctime, rowtime])</description>
      <version>1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TemporalTableFunctionJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.LegacyTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.LegacySinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalFunctionJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.JoinReorderTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.RemoveRedundantLocalRankRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.WindowGroupReorderRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RankNumberColumnRemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkLogicalRankRuleForRangeEndTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkLogicalRankRuleForConstantRangeTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.CalcRankTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SortMergeSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.ShuffledHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.explain.testExecuteSqlWithExplainInsert.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkBatchProgram.scala</file>
    </fixedFiles>
  </bug>
  <bug id="20852" opendate="2021-1-5 00:00:00" fixdate="2021-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enrich back pressure stats per subtask in the WebUI</summary>
      <description>We can enrich the back pressure tab in the WebUI with a couple of more metrics that can help us diagnose the problem, like: backPressuredTimeMsPerSecond busyTimeMsPerSecond idleTimeMsPerSecond inPoolUsage outPoolUsage</description>
      <version>1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.BackPressureITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobVertexBackPressureInfoTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobVertexBackPressureInfo.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-backpressure.ts</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.metrics.VoidMetricFetcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.metrics.MetricFetcherImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.metrics.MetricFetcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.metrics.MetricStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.WebOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs..includes.generated.web.configuration.html</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.slots.TaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobManagerSharedServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.RpcTaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.backpressure.BackPressureRequestCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.backpressure.BackPressureStats.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.backpressure.BackPressureStatsTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.backpressure.BackPressureStatsTrackerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.backpressure.OperatorBackPressureStats.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.backpressure.OperatorBackPressureStatsResponse.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.backpressure.VoidBackPressureStatsTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerNG.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerNGFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.BackPressureSampleableTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.BackPressureSampleService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.RestfulGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.utils.SimpleAckingTaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.TestingJobManagerSharedServicesBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.TestingJobMasterGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.TestingJobMasterGatewayBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.backpressure.BackPressureRequestCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.backpressure.BackPressureStatsTrackerImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SchedulerTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.TestingSchedulerNG.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.TestingSchedulerNGFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.BackPressureSampleServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorExecutionDeploymentReconciliationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorSlotLifetimeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorSubmissionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingDispatcherGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingRestfulGateway.java</file>
    </fixedFiles>
  </bug>
  <bug id="20854" opendate="2021-1-5 00:00:00" fixdate="2021-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce BytesMultiMap to support buffering records</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.aggregate.SumHashAggTestOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.aggregate.BytesHashMapTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.LazyMemorySegmentPool.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashWindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="20862" opendate="2021-1-6 00:00:00" fixdate="2021-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a converter from TypeInformation to DataType</summary>
      <description>Implement a new TypeInfoDataTypeConverter that will no longer produce LegacyTypeInformationType.As mentioned in the FLIP: All types from DataStream API should be supported by this converter. TupleTypeInfoBase will be translated into a proper RowType or StructuredType. BigDecimals will be converted to DECIMAL(38,18) by default. Composite types (tuples, POJOs, rows) will be flattened by default if they are used as top-level records (similar to the old behavior). The order of POJO field's is determined by the DataTypeExtractor and must not be defined manually anymore. GenericTypeInfo is converted to RawType immediately by considering the current configuration. A DataStream that originated from Table API will keep its DataType information due to ExternalTypeInfo implementing DataTypeQueryable.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.utils.LegacyTypeInfoDataTypeConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.extraction.ExtractionUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.DataTypeFactory.java</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.pom.xml</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.DataTypeFactoryImpl.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.data.NestedRowDataTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.utils.DataTypeFactoryMock.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.TypeInfoDataTypeConverterTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.TypeTransformationsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.extraction.DataTypeExtractorTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.DataTypesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.transforms.LegacyRawTypeTransformation.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.DataTypes.java</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.test.scala.org.apache.flink.table.types.TypeInfoDataTypeConverterScalaTest.scala</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvRowDataSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.typeutils.AvroSchemaConverterTest.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.typeutils.AvroSchemaConverter.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkFactoryTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="20882" opendate="2021-1-7 00:00:00" fixdate="2021-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add better error message for illegal import checks</summary>
      <description>We have a list of imports that we currently do not allow, among others for various classes where the flink-shaded variants should be used instead.A developer running into this is just represent with an IllegalImport error, without additional guidance on what to do.We can split the IllegalImport module and provide error messages specific to each case.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.checkstyle.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20885" opendate="2021-1-7 00:00:00" fixdate="2021-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deserialization exception when using &amp;#39;canal-json.table.include&amp;#39; to filter out the binlog of the specified table</summary>
      <description>I found a bug in the canal code. 'canal-json.table.include' does not filter out the binlog of the specified table correctly, which will cause an error in the parsing section. For example, if I want to read the binlog of canal-json.table.include = 'a' table, there is a source field of int in table a, but at this time if table b also has a source field of string, An error will be reported.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.src.test.resources.canal-data-filter-table.txt</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonRowDataSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDataDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.canal.CanalJsonDeserializationSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="20905" opendate="2021-1-9 00:00:00" fixdate="2021-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Format the description of &amp;#39;kubernetes.container.image&amp;#39; option</summary>
      <description>The description of ConfigOption 'kubernetes.container.image' has a URL link, as follows:@Documentation.OverrideDefault( "The default value depends on the actually running version. In general it looks like \"flink:&lt;FLINK_VERSION&gt;-scala_&lt;SCALA_VERSION&gt;\"")public static final ConfigOption&lt;String&gt; CONTAINER_IMAGE = key("kubernetes.container.image") .stringType() .defaultValue(getDefaultFlinkImage()) .withDescription( "Image to use for Flink containers. " + "The specified image must be based upon the same Apache Flink and Scala versions as used by the application. " + "Visit https://hub.docker.com/_/flink?tab=tags for the official docker images provided by the Flink project. The Flink project also publishes docker images here: https://hub.docker.com/r/apache/flink");so the most reasonable way is to use Text Description with Link, as follows:@Documentation.OverrideDefault( "The default value depends on the actually running version. In general it looks like \"flink:&lt;FLINK_VERSION&gt;-scala_&lt;SCALA_VERSION&gt;\"")public static final ConfigOption&lt;String&gt; CONTAINER_IMAGE = key("kubernetes.container.image") .stringType() .defaultValue(getDefaultFlinkImage()) .withDescription( Description.builder() .text( "Image to use for Flink containers. " + "The specified image must be based upon the same Apache Flink and Scala versions as used by the application. " + "Visit %s for the official docker images provided by the Flink project. The Flink project also publishes docker images to %s.", link("https://hub.docker.com/_/flink?tab=tags", "here"), link("https://hub.docker.com/r/apache/flink", "DockerHub")) .build());</description>
      <version>1.11.3,1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs..includes.generated.kubernetes.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="20909" opendate="2021-1-10 00:00:00" fixdate="2021-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MiniBatch Interval derivation does not work well when enable miniBatch optimization in a job which contains deduplicate on row and unbounded aggregate.</summary>
      <description>MiniBatch Interval derivation does not work well when enable miniBatch optimization in a job which contains deduplicate on row and unbounded aggregate.@Testdef testLastRowOnRowtime1(): Unit = { val t = env.fromCollection(rowtimeTestData) .assignTimestampsAndWatermarks(new RowtimeExtractor) .toTable(tEnv, 'a, 'b, 'c, 'rowtime.rowtime()) tEnv.registerTable("T", t) tEnv.executeSql( s""" |CREATE TABLE rowtime_sink ( | cnt BIGINT |) WITH ( | 'connector' = 'values', | 'sink-insert-only' = 'false', | 'changelog-mode' = 'I,UA,D' |) |""".stripMargin) val sql = """ |INSERT INTO rowtime_sink |SELECT COUNT(b) FROM ( | SELECT a, b, c, rowtime | FROM ( | SELECT *, | ROW_NUMBER() OVER (PARTITION BY b ORDER BY rowtime DESC) as rowNum | FROM T | ) | WHERE rowNum = 1 | ) """.stripMargin tEnv.executeSql(sql).await() val rawResult = TestValuesTableFactory.getRawResults("rowtime_sink")}E.g for the above sql, when enable MiniBatch optimization, the optimized plan is as following.Sink(table=[default_catalog.default_database.rowtime_sink], fields=[EXPR$0])+- GlobalGroupAggregate(select=[COUNT_RETRACT(count$0) AS EXPR$0]) +- Exchange(distribution=[single]) +- LocalGroupAggregate(select=[COUNT_RETRACT(b) AS count$0, COUNT_RETRACT(*) AS count1$1]) +- Calc(select=[b]) +- Deduplicate(keep=[LastRow], key=[b], order=[ROWTIME]) +- Exchange(distribution=[hash[b]]) +- Calc(select=[b, rowtime]) +- MiniBatchAssigner(interval=[1000ms], mode=[ProcTime]) +- DataStreamScan(table=[[default_catalog, default_database, T]], fields=[a, b, c, rowtime])A `StreamExecMiniBatchAssigner` will be inserted. The behavior is weird because `Deduplicate` depends on rowTime, however `ProcTimeMiniBatchAssignerOperator` will send watermark every specified interval second depends on process time. For `Deduplicate`, the incoming watermark does not relate to rowTime of incoming record, it cannot indicate rowTime of all following input records are all larger than or equals to the current incoming watermark. </description>
      <version>1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.DeduplicateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalDeduplicate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="20914" opendate="2021-1-10 00:00:00" fixdate="2021-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Format the description of &amp;#39;security.ssl.internal.session-cache-size&amp;#39; option</summary>
      <description>The description of ConfigOption 'security.ssl.internal.session-cache-size' has a URL link, as follows:public static final ConfigOption&lt;Integer&gt; SSL_INTERNAL_SESSION_CACHE_SIZE = key("security.ssl.internal.session-cache-size") .intType() .defaultValue(-1) .withDescription( "The size of the cache used for storing SSL session objects. " + "According to https://github.com/netty/netty/issues/832, you should always set " + "this to an appropriate number to not run into a bug with stalling IO threads " + "during garbage collection. (-1 = use system default).") .withDeprecatedKeys("security.ssl.session-cache-size");so the most reasonable way is to use Text Description with Link, as follows:public static final ConfigOption&lt;Integer&gt; SSL_INTERNAL_SESSION_CACHE_SIZE = key("security.ssl.internal.session-cache-size") .intType() .defaultValue(-1) .withDescription( Description.builder() .text( "The size of the cache used for storing SSL session objects. " + "According to %s, you should always set " + "this to an appropriate number to not run into a bug with stalling IO threads " + "during garbage collection. (-1 = use system default).", link( "https://github.com/netty/netty/issues/832", "here")) .build()) .withDeprecatedKeys("security.ssl.session-cache-size");</description>
      <version>1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
      <file type="M">docs..includes.generated.security.configuration.html</file>
      <file type="M">docs..includes.generated.expert.security.ssl.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="20921" opendate="2021-1-11 00:00:00" fixdate="2021-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Date/Time/Timestamp in Python DataStream</summary>
      <description>Currently the Date/Time/Timestamp type doesn't works in Python DataStream.</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.streaming.api.utils.PythonTypeUtilsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.utils.PythonTypeUtils.java</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="20925" opendate="2021-1-11 00:00:00" fixdate="2021-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separate the implementation of StreamExecLookup and BatchExecLookup</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecLookupJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.common.CommonLookupJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecLookupJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.common.CommonTemporalTableJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeys.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnUniqueness.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.LookupJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.utils.LookupJoinUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="20933" opendate="2021-1-12 00:00:00" fixdate="2021-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Config Python Operator Use Managed Memory In Python DataStream</summary>
      <description>Now the way to set `Python DataStream Operator` to use managed memory is to set a hook in the `execute` method of `Python StreamExecutionEnvironment` to traverse the `StreamGraph` and set the `Python Operator` to use managed memory.But when the user’s job uses `from_data_stream` to convert the `DataStream` to a `Table`, the `TableEnvironment.execute` method is used at the end rather than `StreamExecutionEnvironment.execute`, so the `Python DataStream` related operators will not have `Managed Memory` set.</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.CommonPythonUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCorrelate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCalc.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonGroupAggregate.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="20937" opendate="2021-1-12 00:00:00" fixdate="2021-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fails when DROP a table with illegal watermark definition</summary>
      <description>In sql-client:I create a kafka table with "watermark for proctime()" syntax.Then i try execute a query or drop it, it report fails.I know point watermark with process_time is meaningless, but while i drop it doesn't work , it will affect use.</description>
      <version>1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="20940" opendate="2021-1-12 00:00:00" fixdate="2021-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The LOCALTIME/LOCALTIMSTAMP functions should use session time zone</summary>
      <description>LOCALTIMELOCALTIME TIME(0) NOT NULL #session timezone: UTC 08:52:52 #session timezone: UTC+8 08:52:52wall clock: UTC+8:2020-12-29 08:52:52|LOCALTIMESTAMPLOCALTIMESTAMP TIMESTAMP(0) NOT NULL #session timezone: UTC 2020-12-29T08:52:52 #session timezone: UTC + 8 LOCALTIMESTAMP TIMESTAMP(0) NOT NULL 2020-12-29T08:52:52wall clock: UTC+8:2020-12-29 08:52:52|</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.NonDeterministicTests.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGeneratorContext.scala</file>
    </fixedFiles>
  </bug>
  <bug id="20942" opendate="2021-1-12 00:00:00" fixdate="2021-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Digest of FLOAT literals throws UnsupportedOperationException</summary>
      <description>The recent refactoring of Calcite's digests might have caused a regression for FLOAT literals. org.apache.calcite.rex.RexLiteral#appendAsJava throws a UnsupportedOperationException for the following query:def main(args: Array[String]): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment val source = env.fromElements( (1.0f, 11.0f, 12.0f), (2.0f, 21.0f, 22.0f), (3.0f, 31.0f, 32.0f), (4.0f, 41.0f, 42.0f), (5.0f, 51.0f, 52.0f) ) val settings = EnvironmentSettings.newInstance() .inStreamingMode() .useBlinkPlanner() .build() val tEnv = StreamTableEnvironment.create(env, settings) tEnv.createTemporaryView("myTable", source, $("id"), $("f1"), $("f2")) val query = """ |select * from myTable where id in (1.0, 2.0, 3.0) |""".stripMargin tEnv.executeSql(query).print()}Stack trace:Exception in thread "main" java.lang.UnsupportedOperationException: class org.apache.calcite.sql.type.SqlTypeName: FLOAT at org.apache.calcite.util.Util.needToImplement(Util.java:1075) at org.apache.calcite.rex.RexLiteral.appendAsJava(RexLiteral.java:703) at org.apache.calcite.rex.RexLiteral.toJavaString(RexLiteral.java:408) at org.apache.calcite.rex.RexLiteral.computeDigest(RexLiteral.java:276) at org.apache.calcite.rex.RexLiteral.&lt;init&gt;(RexLiteral.java:223) at org.apache.calcite.rex.RexLiteral.toLiteral(RexLiteral.java:737) at org.apache.calcite.rex.RexLiteral.lambda$printSarg$4(RexLiteral.java:710) at org.apache.calcite.util.RangeSets$Printer.singleton(RangeSets.java:397) at org.apache.calcite.util.RangeSets.forEach(RangeSets.java:237) at org.apache.calcite.util.Sarg.lambda$printTo$0(Sarg.java:110) at org.apache.calcite.linq4j.Ord.forEach(Ord.java:157) at org.apache.calcite.util.Sarg.printTo(Sarg.java:106) at org.apache.calcite.rex.RexLiteral.printSarg(RexLiteral.java:709) at org.apache.calcite.rex.RexLiteral.lambda$appendAsJava$1(RexLiteral.java:652) at org.apache.calcite.util.Util.asStringBuilder(Util.java:2502) at org.apache.calcite.rex.RexLiteral.appendAsJava(RexLiteral.java:651) at org.apache.calcite.rex.RexLiteral.toJavaString(RexLiteral.java:408) at org.apache.calcite.rex.RexLiteral.computeDigest(RexLiteral.java:276) at org.apache.calcite.rex.RexLiteral.&lt;init&gt;(RexLiteral.java:223) at org.apache.calcite.rex.RexBuilder.makeLiteral(RexBuilder.java:971) at org.apache.calcite.rex.RexBuilder.makeSearchArgumentLiteral(RexBuilder.java:1066) at org.apache.calcite.rex.RexSimplify$SargCollector.fix(RexSimplify.java:2786) at org.apache.calcite.rex.RexSimplify.lambda$simplifyOrs$6(RexSimplify.java:1843) at java.util.ArrayList.forEach(ArrayList.java:1257) at org.apache.calcite.rex.RexSimplify.simplifyOrs(RexSimplify.java:1843) at org.apache.calcite.rex.RexSimplify.simplifyOr(RexSimplify.java:1817) at org.apache.calcite.rex.RexSimplify.simplify(RexSimplify.java:313) at org.apache.calcite.rex.RexSimplify.simplifyUnknownAs(RexSimplify.java:282) at org.apache.calcite.rex.RexSimplify.simplify(RexSimplify.java:257) at org.apache.flink.table.planner.plan.utils.FlinkRexUtil$.simplify(FlinkRexUtil.scala:213) at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.simplify(SimplifyFilterConditionRule.scala:63) at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.onMatch(SimplifyFilterConditionRule.scala:46) at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333) at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542) at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407) at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243) at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127) at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202) at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189) at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69) at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:63) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:60) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:60) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:55) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.immutable.Range.foreach(Range.scala:160) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:55) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:163) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:79) at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77) at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:286) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:165) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1329) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:707) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:1107) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:666) at org.apache.flink.table.examples.scala.basics.WordCountTable$.main(WordCountTable.scala:59)</description>
      <version>None</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="20947" opendate="2021-1-13 00:00:00" fixdate="2021-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Idle source doesn&amp;#39;t work when pushing watermark into the source</summary>
      <description>I use table sql to create stream with kafka source, and write data from Kafka into a Hive partitioned table.The following sql to create kafka table:// code placeholdertableEnv.executeSql( "CREATE TABLE stream_tmp.kafka_tmp (`messageId` STRING, `message_type` STRING,`payload` STRING,`timestamp` BIGINT, " + " procTime AS PROCTIME()," + " eventTime AS TO_TIMESTAMP(FROM_UNIXTIME(`timestamp` / 1000,'yyyy-MM-dd HH:mm:ss'))," + " WATERMARK FOR eventTime AS eventTime - INTERVAL '15' SECOND )" + " WITH ('connector' = 'kafka'," + " 'topic' = 'XXX-topic'," + " 'properties.bootstrap.servers'='kafka-server:9092'," + " 'properties.group.id' = 'XXX-group_id'," + " 'scan.startup.mode' = 'latest-offset'," + " 'format' = 'json'," + " 'json.fail-on-missing-field' = 'false'," + " 'json.ignore-parse-errors' = 'true' )" );  And, the following sql to create Hive table:// code placeholdertableEnv.executeSql( "CREATE TABLE hive_tmp.kafka_hive_tmp (`message_id` STRING,`message_type` STRING,`payload` STRING,`event_ts` BIGINT ) " + " PARTITIONED BY (ts_date STRING,ts_hour STRING, ts_minute STRING)" + " STORED AS PARQUET TBLPROPERTIES (" + " 'sink.partition-commit.trigger' = 'partition-time'," + " 'sink.partition-commit.delay' = '1 min'," + " 'sink.partition-commit.policy.kind'='metastore,success-file'," + " 'sink.partition-commit.success-file.name'='_SUCCESS'," + " 'partition.time-extractor.timestamp-pattern' = '$ts_date $ts_hour:$ts_minute:00')"); For the kafka topic used above,  which has multi partitions,  with some of the partitions there's data coming in, while other partitions with no data coming.I noticed that there's config "table.exec.source.idle-timeout" can handle the situation for the "idle" source partition, but event though set this config, it still cannot trigger the Hive partition commit (that means the "_SUCCESS" file will not be created for the partition).After do the analysis for this issue, find the root cause is that the watermark for the "idle" partition will not advance, which cause the Hive partition cannot be committed.  </description>
      <version>1.12.0</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SourceWatermarkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="20961" opendate="2021-1-13 00:00:00" fixdate="2021-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink throws NullPointerException for tables created from DataStream with no assigned timestamps and watermarks</summary>
      <description> Given the following program://import org.apache.flink.api.common.eventtime.{ SerializableTimestampAssigner, WatermarkStrategy }import org.apache.flink.streaming.api.functions.source.SourceFunctionimport org.apache.flink.streaming.api.scala.{StreamExecutionEnvironment, _}import org.apache.flink.streaming.api.watermark.Watermarkimport org.apache.flink.table.annotation.{DataTypeHint, FunctionHint}import org.apache.flink.table.api.bridge.scala.StreamTableEnvironmentimport org.apache.flink.table.api.{$, AnyWithOperations}import org.apache.flink.table.functions.{AggregateFunction, ScalarFunction}import java.time.Instantobject BugRepro { def text: String = s""" |{ | "s": "hello", | "i": ${Random.nextInt()} |} |""".stripMargin def main(args: Array[String]): Unit = { val flink = StreamExecutionEnvironment.createLocalEnvironment() val tableEnv = StreamTableEnvironment.create(flink) val dataStream = flink .addSource { new SourceFunction[(Long, String)] { var isRunning = true override def run(ctx: SourceFunction.SourceContext[(Long, String)]): Unit = while (isRunning) { val x = (Instant.now().toEpochMilli, text) ctx.collect(x) ctx.emitWatermark(new Watermark(x._1)) Thread.sleep(300) } override def cancel(): Unit = isRunning = false } }// .assignTimestampsAndWatermarks(// WatermarkStrategy// .forBoundedOutOfOrderness[(Long, String)](Duration.ofSeconds(30))// .withTimestampAssigner {// new SerializableTimestampAssigner[(Long, String)] {// override def extractTimestamp(element: (Long, String), recordTimestamp: Long): Long =// element._1// }// }// )// tableEnv.createTemporaryView("testview", dataStream, $("event_time").rowtime(), $("json_text")) val res = tableEnv.sqlQuery(""" |SELECT json_text |FROM testview |""".stripMargin) val sink = tableEnv.executeSql( """ |CREATE TABLE SINK ( | json_text STRING |) |WITH ( | 'connector' = 'print' |) |""".stripMargin ) res.executeInsert("SINK").await() () } res.executeInsert("SINK").await() Flink will throw a NullPointerException at runtime:Caused by: java.lang.NullPointerExceptionCaused by: java.lang.NullPointerException at SourceConversion$3.processElement(Unknown Source) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:71) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:46) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:26) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30) at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:305) at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:394) at ai.hunters.pipeline.BugRepro$$anon$1.run(BugRepro.scala:78) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63) at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:215)This is due to the fact that the DataStream did not assign a timestamp to the underlying source. This is the generated code:public class SourceConversion$3 extends org.apache.flink.table.runtime.operators.AbstractProcessStreamOperator implements org.apache.flink.streaming.api.operators.OneInputStreamOperator { private final Object[] references; private transient org.apache.flink.table.data.util.DataFormatConverters.CaseClassConverter converter$0; org.apache.flink.table.data.GenericRowData out = new org.apache.flink.table.data.GenericRowData(2); private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null); public SourceConversion$3( Object[] references, org.apache.flink.streaming.runtime.tasks.StreamTask task, org.apache.flink.streaming.api.graph.StreamConfig config, org.apache.flink.streaming.api.operators.Output output, org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception { this.references = references; converter$0 = (((org.apache.flink.table.data.util.DataFormatConverters.CaseClassConverter) references[0])); this.setup(task, config, output); if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) { ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this) .setProcessingTimeService(processingTimeService); } } @Override public void open() throws Exception { super.open(); } @Override public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception { org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) (org.apache.flink.table.data.RowData) converter$0.toInternal((scala.Tuple2) element.getValue()); org.apache.flink.table.data.TimestampData result$1; boolean isNull$1; org.apache.flink.table.data.binary.BinaryStringData field$2; boolean isNull$2; isNull$2 = in1.isNullAt(1); field$2 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8; if (!isNull$2) { field$2 = ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(1)); } ctx.element = element; result$1 = org.apache.flink.table.data.TimestampData.fromEpochMillis(ctx.timestamp()); if (result$1 == null) { throw new RuntimeException("Rowtime timestamp is null. Please make sure that a " + "proper TimestampAssigner is defined and the stream environment uses the EventTime " + "time characteristic."); } isNull$1 = false; if (isNull$1) { out.setField(0, null); } else { out.setField(0, result$1); } if (isNull$2) { out.setField(1, null); } else { out.setField(1, field$2); } output.collect(outElement.replace(out)); ctx.element = null; } @Override public void close() throws Exception { super.close(); } }The important line is here:result$1 = org.apache.flink.table.data.TimestampData.fromEpochMillis(ctx.timestamp()); if (result$1 == null) { throw new RuntimeException("Rowtime timestamp is null. Please make sure that a " + "proper TimestampAssigner is defined and the stream environment uses the EventTime " + "time characteristic.");`ctx.timestamp` returns null in case no timestamp assigner was created, and `TimestampData.fromEpochMillis` expects a primitive `long`, so a deference fails. The actual check should be:if (!ctx.hasTimestamp) { throw new RuntimeException("Rowtime timestamp is null. Please make sure that a " + "proper TimestampAssigner is defined and the stream environment uses the EventTime " + "time characteristic.");}result$1 = TimestampData.fromEpochMillis(ctx.timestamp()); </description>
      <version>1.12.0</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug id="20963" opendate="2021-1-13 00:00:00" fixdate="2021-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rewrite the example under &amp;#39;flink-python/pyflink/table/examples&amp;#39;</summary>
      <description>Currently the example under 'flink-python/pyflink/table/examples' still uses the deprecated APIs. We need to rewrite it with the latest recommended API.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.examples.batch.word.count.py</file>
    </fixedFiles>
  </bug>
  <bug id="20978" opendate="2021-1-14 00:00:00" fixdate="2021-2-14 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement HeapSavepointRestoreOperation</summary>
      <description>This task includes: extracting common restoring logic from RocksDBRestoreOperation making it possible to restore a HeapKeyedStateBackend from a full checkpoint taken from RocksDB</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapRestoreOperation.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksFullSnapshotStrategy.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.metadata.CheckpointTestUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapSnapshotStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.FullSnapshotAsyncWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.CheckpointStreamWithResultProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="20997" opendate="2021-1-16 00:00:00" fixdate="2021-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>YarnTestBaseTest fails due to NPE</summary>
      <description>YarnTestBase depends on classpaths generated by Maven dependency plugin in `package` phase, but YarnTestBaseTest is a unit test that executed in `test` phase (which is before `package`), so it's unable to find `yarn.classpath` and causes NPE.</description>
      <version>1.11.3,1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21013" opendate="2021-1-18 00:00:00" fixdate="2021-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Blink planner does not ingest timestamp into StreamRecord</summary>
      <description>Currently, the rowtime attribute is not put into the StreamRecord when leaving the Table API to DataStream API. The legacy planner supports this, but the timestamp is null when using the Blink planner.StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); EnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().useBlinkPlanner().build(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, settings); DataStream&lt;Order&gt; orderA = env.fromCollection( Arrays.asList( new Order(1L, "beer", 3), new Order(1L, "diaper", 4), new Order(3L, "rubber", 2))); DataStream&lt;Order&gt; orderB = orderA.assignTimestampsAndWatermarks( new AssignerWithPunctuatedWatermarks&lt;Order&gt;() { @Nullable @Override public Watermark checkAndGetNextWatermark( Order lastElement, long extractedTimestamp) { return new Watermark(extractedTimestamp); } @Override public long extractTimestamp(Order element, long recordTimestamp) { return element.user; } }); Table tableA = tEnv.fromDataStream(orderB, $("user").rowtime(), $("product"), $("amount")); // union the two tables Table result = tEnv.sqlQuery("SELECT * FROM " + tableA); tEnv.toAppendStream(result, Row.class) .process( new ProcessFunction&lt;Row, Row&gt;() { @Override public void processElement(Row value, Context ctx, Collector&lt;Row&gt; out) throws Exception { System.out.println("TIMESTAMP" + ctx.timestamp()); } }) .print(); env.execute();</description>
      <version>1.12.0</version>
      <fixedVersion>1.11.4,1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.SinkCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.OperatorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLegacySink.java</file>
    </fixedFiles>
  </bug>
  <bug id="21017" opendate="2021-1-19 00:00:00" fixdate="2021-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix missing backquote in table connectors docs</summary>
      <description></description>
      <version>1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="21019" opendate="2021-1-19 00:00:00" fixdate="2021-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Netty 4 to 4.1.46</summary>
      <description>Our current Netty version (4.1.44) is vulnerable for at least this CVE:https://nvd.nist.gov/vuln/detail/CVE-2020-11612Bumping to 4.1.46+ should solve it.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2102" opendate="2015-5-27 00:00:00" fixdate="2015-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add predict operation for LabeledVector</summary>
      <description>Currently we can only call predict on DataSet&amp;#91;V &lt;: Vector&amp;#93;.A lot of times though we have a DataSet&amp;#91;LabeledVector&amp;#93; that we split into a train and test set.We should be able to make predictions on the test DataSet&amp;#91;LabeledVector&amp;#93; without having to transform it into a DataSet&amp;#91;Vector&amp;#93;</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.regression.MultipleLinearRegressionITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.classification.SVMITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.regression.MultipleLinearRegression.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.classification.SVM.scala</file>
      <file type="M">docs.libs.ml.svm.md</file>
      <file type="M">docs.libs.ml.multiple.linear.regression.md</file>
    </fixedFiles>
  </bug>
  <bug id="21054" opendate="2021-1-20 00:00:00" fixdate="2021-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement mini-batch optimized slicing window aggregate operator</summary>
      <description>We have supported cumulative windows in FLINK-19605. However, the current cumulative window is not efficient, because the slices are not shared. We leverages the slicing ideas proposed in FLINK-7001 and this design doc &amp;#91;1&amp;#93;. The slicing is an optimized implementation for hopping, cumulative, tumbling windows. Besides of that, we introduced ManagedMemory based mini-batch optimization for the slicing window aggregate operator, this can tremendously reduce the accessing of state and get the higher throughtput without latency loss. &amp;#91;1&amp;#93;: https://docs.google.com/document/d/1ziVsuW_HQnvJr_4a9yKwx_LEnhVkdlde2Z5l6sx5HlY/edit#</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.StateConfigUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.collections.binary.BytesMap.java</file>
    </fixedFiles>
  </bug>
  <bug id="21073" opendate="2021-1-21 00:00:00" fixdate="2021-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mention that RocksDB ignores equals/hashCode because it works on binary data</summary>
      <description>See https://lists.apache.org/thread.html/ra43e2b5d388831290c293b9daf0eee0b0a5d9712543b62c83234a829%40%3Cuser.flink.apache.org%3E</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.state.backends.md</file>
      <file type="M">docs.content.zh.docs.ops.state.state.backends.md</file>
    </fixedFiles>
  </bug>
  <bug id="21108" opendate="2021-1-24 00:00:00" fixdate="2021-7-24 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Flink runtime rest server and history server webmonitor do not require authentication.</summary>
      <description>Flink runtime rest server and history server webmonitor do not require authentication. At certain scenarios, prohibiting unauthorized access is desired. Http basic authentication can be used here.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnConfigurationITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.BatchFineGrainedRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.util.TestRestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.util.DocumentingDispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerSSLAuthITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestClientTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestClientMultipartTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.MultipartUploadResource.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.AbstractHandlerITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.SessionRestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.JobRestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.router.RouterHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.history.HistoryServerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.rest.compatibility.RestAPIStabilityTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.utils.WebFrontendBootstrap.java</file>
      <file type="M">flink-end-to-end-tests.flink-metrics-availability-test.src.test.java.org.apache.flink.metrics.tests.MetricsAvailabilityITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.LocalStandaloneFlinkResource.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientSavepointTriggerTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="21112" opendate="2021-1-25 00:00:00" fixdate="2021-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ValueState/ListState/MapState and corresponding StateDescriptors for Python DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.common.state.py</file>
    </fixedFiles>
  </bug>
  <bug id="21114" opendate="2021-1-25 00:00:00" fixdate="2021-2-25 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add ReducingState and corresponding StateDescriptor for Python DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.state.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
    </fixedFiles>
  </bug>
  <bug id="21115" opendate="2021-1-25 00:00:00" fixdate="2021-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add AggregatingState and corresponding StateDescriptor for Python DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.state.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
    </fixedFiles>
  </bug>
  <bug id="21142" opendate="2021-1-26 00:00:00" fixdate="2021-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink guava Dependence problem</summary>
      <description>We set up a new Hadoop cluster, and we use the flink1.12.0 compiled by the previous release-1.12.0 branch.If I add hive jar to flink/lib/, it will report errors.Operating environment：     flink1.12.0      Hadoop 3.3.0     hive 3.1.2Flink run official demo shell： /tmp/yjb/buildjar/flink1.12.0/bin/flink run -m yarn-cluster /usr/local/flink1.12.0/examples/streaming/WordCount.jarIf I put one of the jar flink-sql-connector-hive-3.1.2_2.11-1.12.0.jar or hive-exec-3.1.2.jar in the Lib directory and execute the above shell, an error will be reported  java.lang.NoSuchMethodError : com.google.common . base.Preconditions.checkArgument (ZLjava/lang/String;Ljava/lang/Object;)V. We can see that it's the dependency conflict of guava.My cluster guava‘s version: /usr/local/hadoop-3.3.0/share/hadoop/yarn/csi/lib/guava-20.0.jar /usr/local/hadoop-3.3.0/share/hadoop/common/lib/guava-27.0-jre.jar /usr/local/apache-hive-3.1.2-bin/lib/guava-20.0.jar /usr/local/apache-hive-3.1.2-bin/lib/jersey-guava-2.25.1.jar /usr/local/spark-3.0.1-bin-hadoop3.2/jars/guava-14.0.1.jarCan you give me some advice？ Thank you！</description>
      <version>1.12.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21174" opendate="2021-1-28 00:00:00" fixdate="2021-4-28 01:00:00" resolution="Done">
    <buginformation>
      <summary>Optimize the performance of ResourceAllocationStrategy</summary>
      <description>In FLINK-20835, we introduce the ResourceAllocationStrategy for fine-grained resource management, which matches resource requirements against available and pending resources and returns the allocation result.We need to optimize the computation logic of it, which is so complicated atm.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.DefaultResourceAllocationStrategy.java</file>
    </fixedFiles>
  </bug>
  <bug id="21178" opendate="2021-1-28 00:00:00" fixdate="2021-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Task failure will not trigger master hook&amp;#39;s reset()</summary>
      <description>In Pravega Flink connector integration with Flink 1.12, we found an issue with our no-checkpoint recovery test case &amp;#91;1&amp;#93;.We expect the recovery will call the ReaderCheckpointHook::reset() function which was the behaviour before 1.12. However FLINK-20222 changes the logic, the reset() call will only be called along with a global recovery. This causes Pravega source data loss when failure happens before the first checkpoint.&amp;#91;1&amp;#93;  https://github.com/crazyzhou/flink-connectors/blob/da9f76d04404071471ebd86bf6889b307c9122ff/src/test/java/io/pravega/connectors/flink/FlinkPravegaReaderRGStateITCase.java#L78</description>
      <version>1.11.3,1.12.0</version>
      <fixedVersion>1.13.0,1.12.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="21239" opendate="2021-2-2 00:00:00" fixdate="2021-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Calcite version to 1.28</summary>
      <description>The following files should be removed from the Flink code base during an upgrade: org.apache.calcite.rex.RexLiteral</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql.validate.SqlValidatorImpl.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql.fun.SqlCastFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql2rel.SqlToRelConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql2rel.RelDecorrelator.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rex.RexSimplify.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.logical.LogicalFilter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.logical.LogicalCorrelate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.core.Filter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.core.Correlate.java</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.CreateTableLikeTest.java</file>
      <file type="M">flink-table.flink-sql-parser.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser-hive.pom.xml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.module.q</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.classloading.ComponentClassLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="21363" opendate="2021-2-11 00:00:00" fixdate="2021-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix baseurl in documentation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.README.md</file>
      <file type="M">docs.layouts.shortcodes.img.html</file>
      <file type="M">docs.layouts.partials.docs.inject.menu-before.html</file>
      <file type="M">docs.layouts.partials.docs.inject.head.html</file>
      <file type="M">docs.config.toml</file>
    </fixedFiles>
  </bug>
  <bug id="21481" opendate="2021-2-24 00:00:00" fixdate="2021-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move git-commit-id-plugin execution to flink-runtime</summary>
      <description>The properties set by the git-commit-id-plugin are only accessed in flink-runtime (see EnvironmentInformation), so we should use the execution of this plugin into flink-runtime to save some build time.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.PseudoRandomValueSelector.java</file>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21627" opendate="2021-3-5 00:00:00" fixdate="2021-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When you insert multiple inserts with statementSet, you modify multiple inserts with OPTIONS(&amp;#39;table-name &amp;#39;=&amp;#39; XXX &amp;#39;), but only the first one takes effect</summary>
      <description>//代码占位符StatementSet statementSet = tableEnvironment.createStatementSet();String sql1 = "insert into test select a,b,c from test_a_12342 /*+OPTIONS('table-name'='test_a_1')*/";String sql2 = "insert into test select a,b,c from test_a_12342 /*+OPTIONS('table-name'='test_a_2')*/";statementSet.addInsertSql(sql1);statementSet.addInsertSql(sql2);statementSet.execute();Sql code as above, in the final after the insert is put test_a_1 table data into the two times, and test_a_2 data did not insert, is excuse me this bug</description>
      <version>1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalLegacySinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.LegacySinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.LegacySinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalSinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalLegacySinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalSinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.connectors.DynamicSinkUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterInCalcIntoTableSourceScanRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushLimitIntoTableSourceScanRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkLogicalRelFactories.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalDataStreamTableScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalLegacyTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalBoundedStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalLegacyTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.common.CommonPhysicalLegacyTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.common.CommonPhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalDataStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalLegacyTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalBoundedStreamScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalLegacyTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalDataStreamScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalLegacyTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.LegacyTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.hint.OptionsHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.CalcRankTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RankNumberColumnRemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.LegacyTableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeysTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.LegacyTableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.testTableSourceSinks.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.calcite.LegacySink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.calcite.LogicalLegacySink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.calcite.LogicalSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.calcite.Sink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalLegacySink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalLegacySink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalLegacySink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalSink.scala</file>
    </fixedFiles>
  </bug>
  <bug id="2175" opendate="2015-6-5 00:00:00" fixdate="2015-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow multiple jobs in single jar file</summary>
      <description>Allow to package multiple jobs into a single jar. extend WebClient to display all available jobs extend WebClient to diplay plan and submit each job</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
      <file type="M">docs.apis.web.client.md</file>
      <file type="M">flink-clients.src.main.resources.web-docs.js.program.js</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.web.PactJobJSONServlet.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.web.JobSubmissionServlet.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.web.JobsServlet.java</file>
    </fixedFiles>
  </bug>
  <bug id="21777" opendate="2021-3-14 00:00:00" fixdate="2021-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace the 4M data writing cache of sort-merge shuffle with writev system call</summary>
      <description>Currently, the sort-merge shuffle implementation uses 4M unmanaged direct memory as cache for data writing. It can be replaced by the writev system call which can reduce the unmanaged direct memory usage without any performance loss.</description>
      <version>1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionSortedBufferTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriteReadTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortBuffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionSortedBuffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BufferReaderWriterUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="21778" opendate="2021-3-14 00:00:00" fixdate="2021-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use heap memory instead of direct memory as index entry cache for sort-merge shuffle</summary>
      <description>Currently, the sort-merge shuffle implementation uses a piece of direct memory as index entry cache for acceleration. We can use heap memory instead to reduce the usage of direct memory which further reduces the possibility of OutOfMemoryError.</description>
      <version>1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="21933" opendate="2021-3-23 00:00:00" fixdate="2021-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[kinesis][efo] EFO consumer treats interrupts as retryable exceptions</summary>
      <description>BackgroundWhen an Flink job is cancelled or failed, the source threads get interrupted. The EFO consumer is treating this exception as retryable and will retry until MAX_RETRIES is exceeded.ScopeUpdate error handling to treat Interrupted exceptions as non-recoverable. The source should gracefully shutdown.</description>
      <version>1.12.0,1.12.1,1.12.2</version>
      <fixedVersion>1.13.0,1.12.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumerTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumerFanOutTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriberTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.RecordPublisher.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="22058" opendate="2021-3-31 00:00:00" fixdate="2021-3-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support FLIP-27 based NumberSequenceSource connector in PyFlink DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.connectors.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.py</file>
    </fixedFiles>
  </bug>
  <bug id="2209" opendate="2015-6-12 00:00:00" fixdate="2015-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document how to use TableAPI, Gelly and FlinkML, StreamingConnectors on a cluster</summary>
      <description>Currently the TableAPI, Gelly, FlinkML and StreamingConnectors are not part of the Flink dist module. Therefore they are not included in the binary distribution. As a consequence, if you want to use one of these libraries the corresponding jar and all their dependencies have to be either manually put on the cluster or the user has to include them in the user code jar.Usually a fat jar is built if the one uses the quickstart archetypes. However if one sets the project manually up this ist not necessarily the case. Therefore, it should be well documented how to run programs using one of these libraries.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.libs.table.md</file>
      <file type="M">docs.libs.ml.index.md</file>
      <file type="M">docs.libs.gelly.guide.md</file>
      <file type="M">docs.apis.streaming.guide.md</file>
      <file type="M">docs.apis.cluster.execution.md</file>
    </fixedFiles>
  </bug>
  <bug id="22133" opendate="2021-4-7 00:00:00" fixdate="2021-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SplitEmumerator does not provide checkpoint id in snapshot</summary>
      <description>In ExternallyInducedSource API, the checkpoint trigger exposes the checkpoint Id for the external client to identify the checkpoint. However, in the FLIP-27 source, the SplitEmumerator::snapshot() is a no-arg method. The connector cannot track the checkpoint ID from Flink</description>
      <version>1.12.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorEventSendingCheckpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.TestingSplitEnumerator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinator.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.connector.source.mocks.MockSplitEnumerator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SplitEnumerator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.lib.util.IteratorSourceEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.enumerator.KafkaEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.ContinuousHiveSplitEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.impl.StaticFileSplitEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.impl.ContinuousFileSplitEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.impl.StaticFileSplitEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.impl.ContinuousFileSplitEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.mocks.MockSplitEnumerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="22233" opendate="2021-4-12 00:00:00" fixdate="2021-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Spelling error of word "constant" in source code</summary>
      <description>When I write the data to MySQL and forget to add the primary key, I report such an error：' org.apache.flink.table.api.ValidationException: Flink doesn't support ENFORCED mode for PRIMARY KEY constaint. 'As mentioned above, The last word 'constant' is misspelled . And the correct spelling is 'constraint'. </description>
      <version>1.12.0</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="2248" opendate="2015-6-19 00:00:00" fixdate="2015-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow disabling of sdtout logging output</summary>
      <description>Currently when a job is submitted through the CLI we get in stdout all the log output about each stage in the job.It would useful to have an easy way to disable this output when submitting the job, as most of the time we are only interested in the log output if something goes wrong.</description>
      <version>None</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.testjar.WordCount.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
      <file type="M">docs.apis.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="22666" opendate="2021-5-14 00:00:00" fixdate="2021-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make structured type&amp;#39;s fields more lenient during casting</summary>
      <description>While writing documentation in FLINK-22537, I found some issues when using the Scala DataStream API. We should add more tests to identify those.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastAvoidanceTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeCasts.java</file>
    </fixedFiles>
  </bug>
  <bug id="22673" opendate="2021-5-17 00:00:00" fixdate="2021-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document about add jar related commands</summary>
      <description>Including ADD JAR, SHOW JAR, REMOVE JAR.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.show.md</file>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.show.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="22733" opendate="2021-5-21 00:00:00" fixdate="2021-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Type mismatch thrown in DataStream.union if parameter is KeyedStream for Python DataStream API</summary>
      <description>See http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/PyFlink-DataStream-union-type-mismatch-td43855.html for more details.</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.13.1,1.12.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="22954" opendate="2021-6-10 00:00:00" fixdate="2021-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t support consuming update and delete changes when use table function that does not contain table field</summary>
      <description>Exception in thread "main" org.apache.flink.table.api.TableException: Table sink 'default_catalog.default_database.kafkaTableSink' doesn't support consuming update and delete changes which is produced by node Join(joinType=[LeftOuterJoin], where=[true], select=[name, word], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])Exception in thread "main" org.apache.flink.table.api.TableException: Table sink 'default_catalog.default_database.kafkaTableSink' doesn't support consuming update and delete changes which is produced by node Join(joinType=[LeftOuterJoin], where=[true], select=[name, word], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.createNewNode(FlinkChangelogModeInferenceProgram.scala:382) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.visit(FlinkChangelogModeInferenceProgram.scala:265) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.org$apache$flink$table$planner$plan$optimize$program$FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor$$visitChild(FlinkChangelogModeInferenceProgram.scala:341) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor$$anonfun$3.apply(FlinkChangelogModeInferenceProgram.scala:330) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor$$anonfun$3.apply(FlinkChangelogModeInferenceProgram.scala:329) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.immutable.Range.foreach(Range.scala:160) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.visitChildren(FlinkChangelogModeInferenceProgram.scala:329) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.visit(FlinkChangelogModeInferenceProgram.scala:279) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.org$apache$flink$table$planner$plan$optimize$program$FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor$$visitChild(FlinkChangelogModeInferenceProgram.scala:341) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor$$anonfun$3.apply(FlinkChangelogModeInferenceProgram.scala:330) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor$$anonfun$3.apply(FlinkChangelogModeInferenceProgram.scala:329) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.immutable.Range.foreach(Range.scala:160) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.visitChildren(FlinkChangelogModeInferenceProgram.scala:329) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.visit(FlinkChangelogModeInferenceProgram.scala:125) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.optimize(FlinkChangelogModeInferenceProgram.scala:50) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.optimize(FlinkChangelogModeInferenceProgram.scala:39) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:63) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:60) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:893) at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:60) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:55) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.immutable.Range.foreach(Range.scala:160) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:55) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:893) at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:163) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:79) at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77) at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:287) at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:100) at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:42) at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainInternal(TableEnvironmentImpl.java:630) at org.apache.flink.table.api.internal.StatementSetImpl.explain(StatementSetImpl.java:92) UDF code:@FunctionHint(output = @DataTypeHint("ROW&lt;word INT&gt;"))public class GenerateSeriesUdf extends TableFunction&lt;Row&gt; { public void eval(int from, int to) { for (int i = from; i &lt; to; i++) { Row row = new Row(1); row.setField(0, i); collect(row); } } @Override public TypeInformation&lt;Row&gt; getResultType() { return Types.ROW(Types.INT()); }} `JOIN` is ok,  `LEFT JOIN` has error.   CREATE TABLE kafkaTableSource ( name string, age int, sex string, address string, pt as PROCTIME()) WITH ( 'connector' = 'kafka', 'topic' = 'hehuiyuan1', 'scan.startup.mode' = 'latest-offset', 'properties.bootstrap.servers' = 'localhost:9092', 'properties.client.id' = 'test-consumer-group', 'properties.group.id' = 'test-consumer-group', 'format' = 'json');CREATE TABLE kafkaTableSink ( name string, sname string, sno string, sclass string, address string) WITH ( 'connector' = 'kafka', 'topic' = 'hehuiyuan2', 'properties.bootstrap.servers' = 'localhost:9092', 'format' = 'json');INSERT INTO kafkaTableSinkSELECT name, name, name, name, wordFROM kafkaTableSourceLEFT JOIN LATERAL TABLE(GENERATE_SERIES(1,5)) AS T(word) ON TRUE;  // UDF is constant , two inutoptimize result:Sink(table=[default_catalog.default_database.kafkaTableSink], fields=[name, name0, name1, name2, word])+- Calc(select=[name, name AS name0, name AS name1, name AS name2, word])   +- Join(joinType=[LeftOuterJoin], where=[true], select=[name, word], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])      :- Exchange(distribution=[single])      :  +- Calc(select=[name])      :     +- TableSourceScan(table=[[default_catalog, default_database, kafkaTableSource]], fields=[name, age, sex, address])      +- Exchange(distribution=[single])         +- Correlate(invocation=[GENERATE_SERIES(1, 5)], correlate=[table(GENERATE_SERIES(1,5))], select=[word], rowType=[RecordType:peek_no_expand(INTEGER word)], joinType=[INNER])            +- Values(tuples=[[{  }]])// UDF that use table field , one inutoptimize result: Sink(table=[default_catalog.default_database.kafkaTableSink], fields=[name, name0, name1, name2, province])+- Calc(select=[name, name AS name0, name AS name1, name AS name2, word AS province])   +- Correlate(invocation=[JSON_TUPLE($cor0.address, _UTF-16LE'province')], correlate=[table(JSON_TUPLE($cor0.address,_UTF-16LE'province'))], select=[name,age,sex,address,pt,word], rowType=[RecordType(VARCHAR(2147483647) name, INTEGER age, VARCHAR(2147483647) sex, VARCHAR(2147483647) address, TIME ATTRIBUTE(PROCTIME) pt, VARCHAR(2147483647) word)], joinType=[LEFT])      +- Calc(select=[name, age, sex, address, PROCTIME() AS pt])         +- TableSourceScan(table=[[default_catalog, default_database, kafkaTableSource]], fields=[name, age, sex, address]) </description>
      <version>1.12.0</version>
      <fixedVersion>1.13.3,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.table.CorrelateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CorrelateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedTableFunctions.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalConstantTableFunctionScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphRestartTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="22971" opendate="2021-6-11 00:00:00" fixdate="2021-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Testcontainers to 1.16.0</summary>
      <description>Testcontainer versions below 1.16.0 are affected by a race condition in the okhttp transport:https://github.com/testcontainers/testcontainers-java/issues/3531The following properties can be observed in many high-frequency test instabilities. They are likely sharing a common root cause. Use the Testcontainers framework Fail due to no outputs in long time A stack similar to the following can be find in the thread dump. Thread is RUNNABLE Testcontainers DockerClient is being created: org.testcontainers.DockerClientFactory.client(DockerClientFactory.java:205) Seems to be downloading something via okhttp "main" #1 prio=5 os_prio=0 tid=0x00007f4fec00b800 nid=0x4f56 runnable [0x00007f4ff536e000] java.lang.Thread.State: RUNNABLE at org.testcontainers.shaded.okio.Buffer.getByte(Buffer.java:312) at org.testcontainers.shaded.okio.Buffer.readUtf8Line(Buffer.java:766) at org.testcontainers.shaded.okio.RealBufferedSource.readUtf8LineStrict(RealBufferedSource.java:231) at org.testcontainers.shaded.okio.RealBufferedSource.readUtf8LineStrict(RealBufferedSource.java:224) at org.testcontainers.shaded.okhttp3.internal.http1.Http1ExchangeCodec$ChunkedSource.readChunkSize(Http1ExchangeCodec.java:489) at org.testcontainers.shaded.okhttp3.internal.http1.Http1ExchangeCodec$ChunkedSource.read(Http1ExchangeCodec.java:471) at org.testcontainers.shaded.okhttp3.internal.Util.skipAll(Util.java:204) at org.testcontainers.shaded.okhttp3.internal.Util.discard(Util.java:186) at org.testcontainers.shaded.okhttp3.internal.http1.Http1ExchangeCodec$ChunkedSource.close(Http1ExchangeCodec.java:511) at org.testcontainers.shaded.okio.ForwardingSource.close(ForwardingSource.java:43) at org.testcontainers.shaded.okhttp3.internal.connection.Exchange$ResponseBodySource.close(Exchange.java:313) at org.testcontainers.shaded.okio.RealBufferedSource.close(RealBufferedSource.java:476) at org.testcontainers.shaded.okhttp3.internal.Util.closeQuietly(Util.java:139) at org.testcontainers.shaded.okhttp3.ResponseBody.close(ResponseBody.java:192) at org.testcontainers.shaded.okhttp3.Response.close(Response.java:290) at org.testcontainers.shaded.com.github.dockerjava.okhttp.OkDockerHttpClient$OkResponse.close(OkDockerHttpClient.java:285) at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder.lambda$null$0(DefaultInvocationBuilder.java:272) at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder$$Lambda$96/1409690119.close(Unknown Source) at com.github.dockerjava.api.async.ResultCallbackTemplate.close(ResultCallbackTemplate.java:77) at org.testcontainers.utility.ResourceReaper.start(ResourceReaper.java:202) at org.testcontainers.DockerClientFactory.client(DockerClientFactory.java:205) - locked &lt;0x000000008980f398&gt; (a [Ljava.lang.Object;) at org.testcontainers.LazyDockerClient.getDockerClient(LazyDockerClient.java:14) at org.testcontainers.LazyDockerClient.listImagesCmd(LazyDockerClient.java:12) at org.testcontainers.images.LocalImagesCache.maybeInitCache(LocalImagesCache.java:68) - locked &lt;0x000000008980f3e8&gt; (a org.testcontainers.images.LocalImagesCache) at org.testcontainers.images.LocalImagesCache.get(LocalImagesCache.java:32) at org.testcontainers.images.AbstractImagePullPolicy.shouldPull(AbstractImagePullPolicy.java:18) at org.testcontainers.images.RemoteDockerImage.resolve(RemoteDockerImage.java:66) at org.testcontainers.images.RemoteDockerImage.resolve(RemoteDockerImage.java:27) at org.testcontainers.utility.LazyFuture.getResolvedValue(LazyFuture.java:17) - locked &lt;0x000000008980f478&gt; (a java.util.concurrent.atomic.AtomicReference) at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:39) at org.testcontainers.containers.GenericContainer.getDockerImageName(GenericContainer.java:1284) at org.testcontainers.containers.GenericContainer.logger(GenericContainer.java:615) at org.testcontainers.elasticsearch.ElasticsearchContainer.&lt;init&gt;(ElasticsearchContainer.java:73) at org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSinkITCase.&lt;clinit&gt;(ElasticsearchSinkITCase.java:48) at sun.misc.Unsafe.ensureClassInitialized(Native Method) at sun.reflect.UnsafeFieldAccessorFactory.newFieldAccessor(UnsafeFieldAccessorFactory.java:43) at sun.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:156) at java.lang.reflect.Field.acquireFieldAccessor(Field.java:1088) at java.lang.reflect.Field.getFieldAccessor(Field.java:1069) at java.lang.reflect.Field.get(Field.java:393) at org.junit.runners.model.FrameworkField.get(FrameworkField.java:73) at org.junit.runners.model.TestClass.getAnnotatedFieldValues(TestClass.java:230) at org.junit.runners.ParentRunner.classRules(ParentRunner.java:255) at org.junit.runners.ParentRunner.withClassRules(ParentRunner.java:244) at org.junit.runners.ParentRunner.classBlock(ParentRunner.java:194) at org.junit.runners.ParentRunner.run(ParentRunner.java:362) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)</description>
      <version>1.12.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23118" opendate="2021-6-23 00:00:00" fixdate="2021-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop Mesos support</summary>
      <description>Following the discussion on the ML , remove Mesos support.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.TaskManagerLocation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.package-info.java</file>
      <file type="M">flink-mesos.src.test.scala.org.apache.flink.runtime.akka.FSMSpec.scala</file>
      <file type="M">flink-mesos.src.test.scala.org.apache.flink.mesos.Utils.scala</file>
      <file type="M">flink-mesos.src.test.scala.org.apache.flink.mesos.scheduler.TasksTest.scala</file>
      <file type="M">flink-mesos.src.test.scala.org.apache.flink.mesos.scheduler.TaskMonitorTest.scala</file>
      <file type="M">flink-mesos.src.test.scala.org.apache.flink.mesos.scheduler.ReconciliationCoordinatorTest.scala</file>
      <file type="M">flink-mesos.src.test.scala.org.apache.flink.mesos.scheduler.LaunchCoordinatorTest.scala</file>
      <file type="M">flink-mesos.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.util.TestingMesosArtifactServer.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.util.MesosResourceAllocationTest.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.scheduler.TestingSchedulerDriver.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.scheduler.OfferTest.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.store.TestingMesosWorkerStore.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.store.MesosWorkerStoreTest.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.services.TestingMesosServices.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParametersTest.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerDriverTest.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.LaunchableMesosWorkerTest.java</file>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.mesos.scheduler.Tasks.scala</file>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.mesos.scheduler.TaskMonitor.scala</file>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.mesos.scheduler.ReconciliationCoordinator.scala</file>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.mesos.scheduler.LaunchCoordinator.scala</file>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.mesos.scheduler.ConnectionMonitor.scala</file>
      <file type="M">flink-mesos.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-mesos.src.main.resources.META-INF.licenses.LICENSE.protobuf</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosUtils.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosResourceAllocation.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosConfiguration.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosArtifactServerImpl.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosArtifactServer.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosArtifactResolver.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.Utils.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.TaskSchedulerBuilder.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.SchedulerProxy.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.Offer.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.StatusUpdate.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.SlaveLost.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.ResourceOffers.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.ReRegistered.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.Registered.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.OfferRescinded.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.FrameworkMessage.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.ExecutorLost.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.Error.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.Disconnected.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.Connected.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.messages.AcceptOffers.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.scheduler.LaunchableTask.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.store.ZooKeeperMesosWorkerStore.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.store.StandaloneMesosWorkerStore.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.store.MesosWorkerStore.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.services.ZooKeeperMesosServices.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.services.StandaloneMesosServices.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.services.MesosServicesUtils.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.services.MesosServices.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.services.AbstractMesosServices.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.RegisteredMesosWorkerNode.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosWorkerResourceSpecFactory.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerFactory.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerDriver.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerActorFactoryImpl.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerActorFactory.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerActions.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosConfigKeys.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.LaunchableMesosWorker.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosTaskExecutorRunner.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosSessionClusterEntrypoint.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosJobClusterEntrypoint.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.configuration.MesosOptions.java</file>
      <file type="M">flink-mesos.pom.xml</file>
      <file type="M">flink-jepsen.test.jepsen.flink.zookeeper.test.clj</file>
      <file type="M">flink-jepsen.src.jepsen.flink.mesos.clj</file>
      <file type="M">flink-jepsen.src.jepsen.flink.flink.clj</file>
      <file type="M">flink-jepsen.src.jepsen.flink.db.clj</file>
      <file type="M">flink-jepsen.README.md</file>
      <file type="M">flink-jepsen.docker.test-specs.mesos-session.edn</file>
      <file type="M">flink-jepsen.docker.run-tests.sh</file>
      <file type="M">flink-jepsen.docker.Dockerfile-db</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.DistributedCacheDfsTest.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.mesos.wordcount.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.mesos.multiple.submissions.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-mesos-cluster.Dockerfile</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-mesos-cluster.docker-compose.yml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.mesos.docker.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-taskmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-jobmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-appmaster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-appmaster-job.sh</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ResourceManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HighAvailabilityOptions.java</file>
      <file type="M">.github.PULL.REQUEST.TEMPLATE.md</file>
      <file type="M">docs.content.zh.docs.concepts.flink-architecture.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kafka.md</file>
      <file type="M">docs.content.zh.docs.deployment.advanced.external.resources.md</file>
      <file type="M">docs.content.zh.docs.deployment.cli.md</file>
      <file type="M">docs.content.zh.docs.deployment.config.md</file>
      <file type="M">docs.content.zh.docs.deployment.elastic.scaling.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.migration.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.setup.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.trouble.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.tuning.md</file>
      <file type="M">docs.content.zh.docs.deployment.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.mesos.md</file>
      <file type="M">docs.content.zh.docs.deployment.security.security-kerberos.md</file>
      <file type="M">docs.content.zh.docs.deployment.security.security-ssl.md</file>
      <file type="M">docs.content.zh.docs.ops.debugging.debugging.classloading.md</file>
      <file type="M">docs.content.zh.docs.ops.state.state.backends.md</file>
      <file type="M">docs.content.zh.docs.ops.upgrading.md</file>
      <file type="M">docs.content.zh.docs.try-flink.flink-operations-playground.md</file>
      <file type="M">docs.content.docs.concepts.flink-architecture.md</file>
      <file type="M">docs.content.docs.connectors.datastream.kafka.md</file>
      <file type="M">docs.content.docs.deployment.advanced.external.resources.md</file>
      <file type="M">docs.content.docs.deployment.cli.md</file>
      <file type="M">docs.content.docs.deployment.config.md</file>
      <file type="M">docs.content.docs.deployment.elastic.scaling.md</file>
      <file type="M">docs.content.docs.deployment.memory.mem.migration.md</file>
      <file type="M">docs.content.docs.deployment.memory.mem.setup.md</file>
      <file type="M">docs.content.docs.deployment.memory.mem.trouble.md</file>
      <file type="M">docs.content.docs.deployment.memory.mem.tuning.md</file>
      <file type="M">docs.content.docs.deployment.overview.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.mesos.md</file>
      <file type="M">docs.content.docs.deployment.security.security-kerberos.md</file>
      <file type="M">docs.content.docs.deployment.security.security-ssl.md</file>
      <file type="M">docs.content.docs.ops.debugging.debugging.classloading.md</file>
      <file type="M">docs.content.docs.ops.state.state.backends.md</file>
      <file type="M">docs.content.docs.ops.upgrading.md</file>
      <file type="M">docs.content.docs.try-flink.flink-operations-playground.md</file>
      <file type="M">docs.layouts.shortcodes.generated.all.taskmanager.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.common.high.availability.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.common.miscellaneous.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.core.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.high.availability.zk.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.high.availability.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.mesos.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.mesos.task.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.resource.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.security.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.task.manager.configuration.html</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.ClusterDescriptor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="23133" opendate="2021-6-24 00:00:00" fixdate="2021-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The dependencies are not handled properly when mixing use of Python Table API and Python DataStream API</summary>
      <description>The reason is that when converting from DataStream to Table, the dependencies should be handled and set correctly for the existing DataStream operators.</description>
      <version>1.12.0,1.13.0</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">docs.content.docs.dev.python.dependency.management.md</file>
      <file type="M">docs.content.zh.docs.dev.python.dependency.management.md</file>
    </fixedFiles>
  </bug>
  <bug id="23838" opendate="2021-8-17 00:00:00" fixdate="2021-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add FLIP-33 metrics to new KafkaSink</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.KafkaWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="23936" opendate="2021-8-24 00:00:00" fixdate="2021-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python UDFs instances are reinitialized if there is no input for more than 1 minute</summary>
      <description>We receive this feedback from some PyFlink users. After some investigation, we find out that the root case is that there is a mechanism in Beam that it will released the BundleProcessors which are inactive for more than 1 minute: https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/worker/sdk_worker.py#L90</description>
      <version>1.10.0,1.11.0,1.12.0,1.13.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.sdk.worker.main.py</file>
    </fixedFiles>
  </bug>
  <bug id="23995" opendate="2021-8-26 00:00:00" fixdate="2021-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive dialect: `insert overwrite table partition if not exists` will throw exception when tablename is like &amp;#39;database.table&amp;#39;</summary>
      <description>when run such hive sql insert overwrite table default.dest2 partition (p1=1,p2='static') if not exists select x from src it will throw exceptionCaused by: org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found default</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="24049" opendate="2021-8-30 00:00:00" fixdate="2021-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TupleTypeInfo doesn&amp;#39;t handle correctly for data types need conversion</summary>
      <description></description>
      <version>1.12.0,1.13.0,1.14.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.common.typeinfo.py</file>
    </fixedFiles>
  </bug>
  <bug id="24228" opendate="2021-9-9 00:00:00" fixdate="2021-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[FLIP-171] Firehose implementation of Async Sink</summary>
      <description>MotivationUser stories:As a Flink user, I’d like to use Kinesis Firehose as sink for my data pipeline.Scope: Implement an asynchronous sink for Kinesis Firehose by inheriting the AsyncSinkBase class. The implementation can for now reside in its own module in flink-connectors. The module and package name can be anything reasonable e.g. flink-connector-aws-kinesis for the module name and org.apache.flink.connector.aws.kinesis for the package name. The implementation must use the Kinesis Java Client. The implementation must allow users to configure the Kinesis Client, with reasonable default settings. Implement an asynchornous sink writer for Firehose by extending the AsyncSinkWriter. The implementation must deal with failed requests and retry them using the requeueFailedRequestEntry method. If possible, the implementation should batch multiple requests (PutRecordsRequestEntry objects) to Firehose for increased throughput. The implemented Sink Writer will be used by the Sink class that will be created as part of this story. Unit/Integration testing. Use Kinesalite (in-memory Kinesis simulation). We already use this in KinesisTableApiITCase. Java / code-level docs. End to end testing: add tests that hits a real AWS instance. (How to best donate resources to the Flink project to allow this to happen?)ReferencesMore details to be found https://cwiki.apache.org/confluence/display/FLINK/FLIP-171%3A+Async+Sink</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkElementConverterTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.examples.SinkIntoFirehose.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkWriter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkElementConverter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-aws-base.src.test.java.org.apache.flink.connector.aws.testutils.AWSServicesTestUtils.java</file>
      <file type="M">docs.content.docs.connectors.datastream.firehose.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.firehose.md</file>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.AwsV2UtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AwsV2Util.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyV2Factory.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyV2.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.sink.writer.AsyncSinkWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.test.java.org.apache.flink.connector.kinesis.util.AWSKinesisDataStreamsUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.util.AWSKinesisDataStreamsUtil.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.sink.KinesisDataStreamsSinkWriter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.config.AWSKinesisDataStreamsConfigConstants.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2423" opendate="2015-7-28 00:00:00" fixdate="2015-8-28 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Properly test checkpoint notifications</summary>
      <description>Checkpoint notifications (via the CheckpointNotifier interface) are currently not properly tested. A test should be included to verify that checkpoint notifications are eventually called on successful checkpoints, and that they are only called once per checkpointID.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StreamCheckpointNotifierITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="24535" opendate="2021-10-13 00:00:00" fixdate="2021-10-13 01:00:00" resolution="Done">
    <buginformation>
      <summary>Update with the latest AWS Glue Schema Registry version</summary>
      <description>This change is to update the flink avro and json schema serializer / deserializer with latest AWS Glue Schema Registry latest version v1.1.5.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24662" opendate="2021-10-27 00:00:00" fixdate="2021-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink sphinx check failed with "node class &amp;#39;meta&amp;#39; is already registered, its visitors will be overridden"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25481&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=8d78fe4f-d658-5c70-12f8-4921589024c3==========mypy checks... [SUCCESS]===========Oct 26 22:08:34 rm -rf _build/*Oct 26 22:08:34 /__w/1/s/flink-python/dev/.conda/bin/sphinx-build -b html -d _build/doctrees -a -W . _build/htmlOct 26 22:08:34 Running Sphinx v2.4.4Oct 26 22:08:34 Oct 26 22:08:34 Warning, treated as error:Oct 26 22:08:34 node class 'meta' is already registered, its visitors will be overriddenOct 26 22:08:34 Makefile:76: recipe for target 'html' failed</description>
      <version>1.12.0,1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug id="24798" opendate="2021-11-5 00:00:00" fixdate="2021-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump commons-cli to v1.5.0</summary>
      <description>Bump commons-cli:commons-cli:1.4 to commons-cli:commons-cli:1.5.0</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="25010" opendate="2021-11-23 00:00:00" fixdate="2021-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Speed up hive&amp;#39;s createMRSplits by multi thread</summary>
      <description>We have thousands of hive partitions and the method createMRSplits will take much time, for example, ten minutes. We can speed up the process by multi thread for different partitions.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReaderITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.PartitionMonitorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveTableInputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSourceFileEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSourceBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveOptions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveLookupTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.ContinuousHiveSplitEnumerator.java</file>
      <file type="M">docs.content.docs.connectors.table.hive.hive.read.write.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.read.write.md</file>
    </fixedFiles>
  </bug>
  <bug id="25011" opendate="2021-11-23 00:00:00" fixdate="2021-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce VertexParallelismDecider</summary>
      <description>Introduce VertexParallelismDecider and provide a default implementation.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.job.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.scheduling.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.jobmanager.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="25091" opendate="2021-11-29 00:00:00" fixdate="2021-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Official website document FileSink orc compression attribute reference error</summary>
      <description>I see the following version is like this &amp;#91;1.12、1.13、1.14 。。。&amp;#93; What should be quoted here is writerProperties Shouldn't be is writerProps docUrl</description>
      <version>1.12.0,1.13.0,1.14.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.connectors.datastream.file.sink.md</file>
      <file type="M">docs.content.docs.connectors.datastream.file.sink.md</file>
    </fixedFiles>
  </bug>
  <bug id="26846" opendate="2022-3-24 00:00:00" fixdate="2022-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Gauge metrics doesn&amp;#39;t work in PyFlink</summary>
      <description>See https://lists.apache.org/thread/w7jkwgpon6qy4p6k1nhhw5k4m81r8c8p for more details.</description>
      <version>1.12.0</version>
      <fixedVersion>1.14.5,1.15.0,1.13.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.metric.FlinkMetricContainer.java</file>
    </fixedFiles>
  </bug>
  <bug id="26847" opendate="2022-3-24 00:00:00" fixdate="2022-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Command line option &amp;#39;-py&amp;#39; doesn&amp;#39;t work in YARN application mode</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.python.PythonDriverTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonDriverOptionsParserFactory.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="26849" opendate="2022-3-24 00:00:00" fixdate="2022-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deduplicate MetricRegistry code for (un)registering metrics with reporters</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricRegistryImpl.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
