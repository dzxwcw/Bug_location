<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="10114" opendate="2018-8-9 00:00:00" fixdate="2018-4-9 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support Orc for StreamingFileSink</summary>
      <description></description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-orc.pom.xml</file>
      <file type="M">docs.dev.connectors.streamfile.sink.md</file>
    </fixedFiles>
  </bug>
  <bug id="10935" opendate="2018-11-19 00:00:00" fixdate="2018-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement KubeClient with Faric8 Kubernetes clients</summary>
      <description>Implement KubeClient with Faric8 Kubernetes clients and add tests</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-docs.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10936" opendate="2018-11-19 00:00:00" fixdate="2018-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement Command line tools</summary>
      <description>Implement command tools to start kubernetes sessions:  k8s-session.sh to start and stop a session like we did in yarn-session.sh customized command line that will be invoked by CliFrontEnd and ./bin/flink run to submit job to kubernetes cluster</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug id="10937" opendate="2018-11-19 00:00:00" fixdate="2018-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add entrypoint scripts for k8s</summary>
      <description>Flink official docker image could be used to active kubernetes integration. An entrypoint script for k8s should be added.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13745" opendate="2019-8-16 00:00:00" fixdate="2019-3-16 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Flink cache on Travis does not exist</summary>
      <description>More and more often I observe that Flink builds fail on Travis because of missing Flink caches:Cached flink dir /home/travis/flink_cache/40072/flink does not exist. Exiting build.It seems as if Travis cannot guarantee that a cache survives as long as the different profiles of a build are running. It would be good to solve this problem because now we have regularly failing builds:https://travis-ci.org/apache/flink/builds/572559629https://travis-ci.org/apache/flink/builds/572523730https://travis-ci.org/apache/flink/builds/571576734</description>
      <version>1.9.0,1.10.0,1.11.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.controller.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13938" opendate="2019-9-2 00:00:00" fixdate="2019-5-2 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Use pre-uploaded libs to accelerate flink submission</summary>
      <description>Currently, every time we start a flink cluster, flink lib jars need to be uploaded to hdfs and then register Yarn local resource so that it could be downloaded to jobmanager and all taskmanager container. I think we could have two optimizations. Use pre-uploaded flink binary to avoid uploading of flink system jars By default, the LocalResourceVisibility is APPLICATION, so they will be downloaded only once and shared for all taskmanager containers of a same application in the same node. However, different applications will have to download all jars every time, including the flink-dist.jar. We could use the yarn public cache to eliminate the unnecessary jars downloading and make launching container faster.  How the feature work? Add yarn.provided.lib.dirs to configure pre-uploaded libs, which contain files that are useful for all the users of the platform(i.e. different applications). When the Flink client wants to ship a local file, it will check the provided libs first. If the provided libs contains a file with the same name, the local ship files will be automatically excluded from uploading. These provided libs needs to be public readable and will be set with PUBLIC visibility for local resources. So they will be cache in the nodes and shared by different applications. How to use the pre-upload feature? 1. First, upload the Flink binary to the HDFS directories 2. Use yarn.provided.lib.dirs to specify the pre-uploaded libs   A final submission command could be issued like following../bin/flink run -m yarn-cluster -d \-yD yarn.provided.lib.dirs=hdfs://myhdfs/flink/lib,hdfs://myhdfs/flink/plugins \examples/streaming/WindowJoin.jar</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnTestUtils.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnFileStageTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnConfigKeys.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnApplicationFileUploader.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.UtilsTest.java</file>
      <file type="M">docs..includes.generated.yarn.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="14311" opendate="2019-10-2 00:00:00" fixdate="2019-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming File Sink end-to-end test failed on Travis</summary>
      <description>The Streaming File Sink end-to-end test fails on Travis because it does not produce output for 10 minutes.https://api.travis-ci.org/v3/job/591992274/log.txt</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="15177" opendate="2019-12-10 00:00:00" fixdate="2019-1-10 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Migrate RocksDB Configurable Options to new type safe config options</summary>
      <description>The RocksDB config options are currently all types to String and are manually parsed and validated. This can be simplified using the new config option classes.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBConfigurableOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.DefaultConfigurableOptionsFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="15286" opendate="2019-12-16 00:00:00" fixdate="2019-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Managed Memory Option for RocksDB not picked up from config</summary>
      <description>This is a missing lookup in the config in the State Backend's configure() method. Or, more precisely, in the State Backend's MemoryConfiguration's configure() method.</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMemoryConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="15347" opendate="2019-12-20 00:00:00" fixdate="2019-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ZooKeeperDefaultDispatcherRunnerTest.testResourceCleanupUnderLeadershipChange failed on Travis</summary>
      <description>The test ZooKeeperDefaultDispatcherRunnerTest.testResourceCleanupUnderLeadershipChange failed on Travis because it got stuck.https://api.travis-ci.org/v3/job/627661879/log.txt</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.akka.StoppingSupervisorWithoutLoggingActorKilledExceptionStrategy.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.SupervisorActorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcService.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.SupervisorActor.java</file>
    </fixedFiles>
  </bug>
  <bug id="15349" opendate="2019-12-20 00:00:00" fixdate="2019-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add "create catalog" DDL to blink planner</summary>
      <description>https://cwiki.apache.org/confluence/display/FLINK/FLIP+69+-+Flink+SQL+DDL+Enhancementsome customers who have internal streaming platform requested this feature, as it's not possible on a platform to load catalogs dynamically at runtime now via sql client yaml. Catalog DDL will come into play</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="15355" opendate="2019-12-21 00:00:00" fixdate="2019-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nightly streaming file sink fails with unshaded hadoop</summary>
      <description>org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph. at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335) at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205) at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:138) at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:664) at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:213) at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:895) at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:968) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:968)Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph. at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:199) at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1751) at org.apache.flink.streaming.api.environment.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:94) at org.apache.flink.streaming.api.environment.StreamContextEnvironment.execute(StreamContextEnvironment.java:63) at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1628) at StreamingFileSinkProgram.main(StreamingFileSinkProgram.java:77) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321) ... 11 moreCaused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph. at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895) at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1746) ... 20 moreCaused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph. at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$7(RestClusterClient.java:326) at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870) at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:852) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:274) at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561) at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:929) at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., &lt;Exception on server side:org.apache.flink.runtime.client.JobSubmissionException: Failed to submit job. at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$internalSubmitJob$3(Dispatcher.java:336) at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:36) at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590) ... 6 moreCaused by: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.&lt;init&gt;(JobManagerRunnerImpl.java:152) at org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.createJobManagerRunner(DefaultJobManagerRunnerFactory.java:84) at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$6(Dispatcher.java:379) at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:34) ... 7 moreCaused by: java.lang.NoSuchMethodError: org.apache.hadoop.conf.Configuration.getTimeDuration(Ljava/lang/String;Ljava/lang/String;Ljava/util/concurrent/TimeUnit;)J at org.apache.hadoop.fs.s3a.S3ARetryPolicy.&lt;init&gt;(S3ARetryPolicy.java:113) at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:257) at org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.create(AbstractS3FileSystemFactory.java:126) at org.apache.flink.core.fs.PluginFileSystemFactory.create(PluginFileSystemFactory.java:61) at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:441) at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:362) at org.apache.flink.core.fs.Path.getFileSystem(Path.java:298) at org.apache.flink.runtime.state.memory.MemoryBackendCheckpointStorage.&lt;init&gt;(MemoryBackendCheckpointStorage.java:85) at org.apache.flink.runtime.state.memory.MemoryStateBackend.createCheckpointStorage(MemoryStateBackend.java:295) at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.&lt;init&gt;(CheckpointCoordinator.java:279) at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.&lt;init&gt;(CheckpointCoordinator.java:205) at org.apache.flink.runtime.executiongraph.ExecutionGraph.enableCheckpointing(ExecutionGraph.java:486) at org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.buildGraph(ExecutionGraphBuilder.java:338) at org.apache.flink.runtime.scheduler.SchedulerBase.createExecutionGraph(SchedulerBase.java:245) at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:217) at org.apache.flink.runtime.scheduler.SchedulerBase.&lt;init&gt;(SchedulerBase.java:205) at org.apache.flink.runtime.scheduler.DefaultScheduler.&lt;init&gt;(DefaultScheduler.java:119) at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:105) at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:278) at org.apache.flink.runtime.jobmaster.JobMaster.&lt;init&gt;(JobMaster.java:266) at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:98) at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:40) at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.&lt;init&gt;(JobManagerRunnerImpl.java:146) ... 10 more</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.CoreOptionsTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.plugin.PluginManagerTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.plugin.PluginLoaderTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.plugin.PluginConfig.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">docs..includes.generated.core.configuration.html</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.plugin.PluginLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="15373" opendate="2019-12-24 00:00:00" fixdate="2019-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update descriptions for framework / task off-heap memory config options</summary>
      <description>Update descriptions for "taskmanager.memory.framework.off-heap.size" and "taskmanager.memory.task.off-heap.size" to explicitly state that: Both direct and native memory are accounted Will be fully counted into MaxDirectMemorySizeDetailed discussion can be found in this ML thread.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs..includes.generated.task.manager.memory.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="15374" opendate="2019-12-24 00:00:00" fixdate="2019-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update descriptions for jvm overhead config options</summary>
      <description>Update descriptions for "taskmanager.memory.jvm-overhead.&amp;#91;min|max|fraction&amp;#93;" to remove "I/O direct memory" and explicitly state that it's not counted into MaxDirectMemorySize.Detailed discussion can be found in this ML thread.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs..includes.generated.task.manager.memory.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="15406" opendate="2019-12-26 00:00:00" fixdate="2019-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RocksDB savepoints with heap timers cannot be restored by non-process functions</summary>
      <description>The savepoint is writted by "State Processor API" can't be restore by map or flatmap. But it can be retored by KeyedProcessFunction.  Following is the error message:java.lang.Exception: Could not write timer service of Flat Map -&gt; Map -&gt; Sink: device_first_user_create (1/8) to checkpoint state stream.java.lang.Exception: Could not write timer service of Flat Map -&gt; Map -&gt; Sink: device_first_user_create (1/8) to checkpoint state stream. at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:466) at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.snapshotState(AbstractUdfStreamOperator.java:89) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:399) at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.checkpointStreamOperator(StreamTask.java:1282) at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1216) at org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:872) at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:777) at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:708) at org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:88) at org.apache.flink.streaming.runtime.io.CheckpointBarrierAligner.processBarrier(CheckpointBarrierAligner.java:177) at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:155) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.pollNextNullable(StreamTaskNetworkInput.java:102) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.pollNextNullable(StreamTaskNetworkInput.java:47) at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:135) at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:279) at org.apache.flink.streaming.runtime.tasks.StreamTask.run(StreamTask.java:301) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:406) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:705) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:530) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.NullPointerException at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:58) at org.apache.flink.streaming.api.operators.InternalTimersSnapshot.&lt;init&gt;(InternalTimersSnapshot.java:52) at org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.snapshotTimersForKeyGroup(InternalTimerServiceImpl.java:291) at org.apache.flink.streaming.api.operators.InternalTimerServiceSerializationProxy.write(InternalTimerServiceSerializationProxy.java:98) at org.apache.flink.streaming.api.operators.InternalTimeServiceManager.snapshotStateForKeyGroup(InternalTimeServiceManager.java:139) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:462) ... 19 more  </description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.operators.KeyedStateBootstrapOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="15414" opendate="2019-12-27 00:00:00" fixdate="2019-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaITCase#prepare failed in travis</summary>
      <description>The travis for release-1.9 failed with the following error:org.apache.kafka.common.KafkaException: Socket server failed to bind to 0.0.0.0:44867: Address already in use. at org.apache.flink.streaming.connectors.kafka.KafkaITCase.prepare(KafkaITCase.java:58)Caused by: java.net.BindException: Address already in use at org.apache.flink.streaming.connectors.kafka.KafkaITCase.prepare(KafkaITCase.java:58)instance: https://api.travis-ci.org/v3/job/629636116/log.txt</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="15455" opendate="2020-1-2 00:00:00" fixdate="2020-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable TCP connection reuse across multiple jobs.</summary>
      <description>Currently, tcp connections can be only reuse by tasks residing in the same TaskManager and consumes the same IntermediateResult. And after job finish or failover, the TCP connections are closed and new connections must be setup latter.As an improvement, we can make tcp connections a cluster level resource which can be reused by multi jobs. The advantages are as follows: Reduce the number of TCP connections so we can save some resources. Reduce the overhead of connection setup and close so restarted jobs after failover and latter jobs submitted to the same session cluster can reuse the previous connections.We use Flink session cluster as a service for ad-hoc queries and the users can produce some statistics or create some statements and reports at any time. Most of the queries finish in 2s and we find tcp connection reuse help a lot to reduce the average execution time which means more queries can be processed using the same resource and time with even better user experience.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.AtomicDisposableReferenceCounterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClientTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyConnectionManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NettyShuffleEnvironmentBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.AtomicDisposableReferenceCounter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.NettyShuffleEnvironmentConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NetworkClientHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyConnectionManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NettyShuffleServiceFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.NettyShuffleEnvironmentOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.netty.shuffle.environment.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.taskmanager.network.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="15584" opendate="2020-1-14 00:00:00" fixdate="2020-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Give nested data type of ROWs in ValidationException</summary>
      <description>In INSERT INTO baz_sinkSELECT  a, ROW(b, c)FROM foo_sourceSchema mismatch mistakes will not get proper detail level, yielding the following:Caused by: org.apache.flink.table.api.ValidationException: Field types of query result and registered TableSink &amp;#91;baz_sink&amp;#93; do not match. Query result schema: &amp;#91;a: Integer, EXPR$2: Row&amp;#93; TableSink schema: &amp;#91;a: Integer, payload: Row&amp;#93;Leaving the user with an opaque 'Row' type to debug. </description>
      <version>1.9.1,1.10.0,1.11.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.validation.InsertIntoValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sinks.TableSinkUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15738" opendate="2020-1-23 00:00:00" fixdate="2020-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump powermock to 2.0.4</summary>
      <description>Kinesis tests are failing with a NullPointerException due to their Whitebox usages.We have to bump powermock from 2.0.2 to at least 2.0.4.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15921" opendate="2020-2-5 00:00:00" fixdate="2020-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Travis-ci error: PYTHON exited with EXIT CODE: 143</summary>
      <description>Currently, some Travis CI failures occur, such as: &amp;#91;1&amp;#93;,&amp;#91;2&amp;#93;. The reason for the failure is that the python dependent `grpcio` released the latest version 1.27.0 &amp;#91;3&amp;#93; today, which resulted in the test cache not having the latest dependency, and the timeout of downloading in the repo. If the problem will be fixed after the first download when the network is in good condition. I am still watching the latest build &amp;#91;4&amp;#93;. If it fails for a long time, we will try to set a lower version of `grpcio` or optimize the current test case. I would like to watch for a while . What do you think?  &amp;#91;1&amp;#93;https://travis-ci.org/apache/flink/builds/646250268 &amp;#91;2&amp;#93;https://travis-ci.org/apache/flink/jobs/646281060&amp;#91;3&amp;#93; https://pypi.org/project/grpcio/#files&amp;#91;4&amp;#93;https://travis-ci.org/apache/flink/builds/646355253</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.tox.ini</file>
    </fixedFiles>
  </bug>
  <bug id="15982" opendate="2020-2-11 00:00:00" fixdate="2020-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>&amp;#39;Quickstarts Java nightly end-to-end test&amp;#39; is failed on travis</summary>
      <description>==============================================================================Running 'Quickstarts Java nightly end-to-end test'==============================================================================TEST_DATA_DIR: /home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-42718423491Flink dist directory: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT22:16:44.021 [INFO] Scanning for projects...22:16:44.095 [INFO] ------------------------------------------------------------------------22:16:44.095 [INFO] BUILD FAILURE22:16:44.095 [INFO] ------------------------------------------------------------------------22:16:44.098 [INFO] Total time: 0.095 s22:16:44.099 [INFO] Finished at: 2020-02-10T22:16:44+00:0022:16:44.143 [INFO] Final Memory: 5M/153M22:16:44.143 [INFO] ------------------------------------------------------------------------22:16:44.144 [ERROR] The goal you specified requires a project to execute but there is no POM in this directory (/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-42718423491). Please verify you invoked Maven from the correct directory. -&gt; [Help 1]22:16:44.144 [ERROR] 22:16:44.145 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.22:16:44.145 [ERROR] Re-run Maven using the -X switch to enable full debug logging.22:16:44.145 [ERROR] 22:16:44.145 [ERROR] For more information about the errors and possible solutions, please read the following articles:22:16:44.145 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MissingProjectException/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/test_quickstarts.sh: line 57: cd: flink-quickstart-java: No such file or directorycp: cannot create regular file '/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-42718423491/flink-quickstart-java/src/main/java/org/apache/flink/quickstart/Elasticsearch5SinkExample.java': No such file or directorysed: can't read /home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-42718423491/flink-quickstart-java/src/main/java/org/apache/flink/quickstart/Elasticsearch5SinkExample.java: No such file or directoryawk: fatal: cannot open file `pom.xml' for reading (No such file or directory)sed: can't read pom.xml: No such file or directorysed: can't read pom.xml: No such file or directory22:16:45.312 [INFO] Scanning for projects...22:16:45.386 [INFO] ------------------------------------------------------------------------22:16:45.386 [INFO] BUILD FAILURE22:16:45.386 [INFO] ------------------------------------------------------------------------22:16:45.391 [INFO] Total time: 0.097 s22:16:45.391 [INFO] Finished at: 2020-02-10T22:16:45+00:0022:16:45.438 [INFO] Final Memory: 5M/153M22:16:45.438 [INFO] ------------------------------------------------------------------------22:16:45.440 [ERROR] The goal you specified requires a project to execute but there is no POM in this directory (/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-42718423491). Please verify you invoked Maven from the correct directory. -&gt; [Help 1]22:16:45.440 [ERROR] 22:16:45.440 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.22:16:45.440 [ERROR] Re-run Maven using the -X switch to enable full debug logging.22:16:45.440 [ERROR] 22:16:45.440 [ERROR] For more information about the errors and possible solutions, please read the following articles:22:16:45.440 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MissingProjectException/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/test_quickstarts.sh: line 73: cd: target: No such file or directoryjava.io.FileNotFoundException: flink-quickstart-java-0.1.jar (No such file or directory) at java.util.zip.ZipFile.open(Native Method) at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:225) at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:155) at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:126) at sun.tools.jar.Main.list(Main.java:1115) at sun.tools.jar.Main.run(Main.java:293) at sun.tools.jar.Main.main(Main.java:1288)Success: There are no flink core classes are contained in the jar.Failure: Since Elasticsearch5SinkExample.class and other user classes are not included in the jar. [FAIL] Test script contains errors.Here are some instances: https://api.travis-ci.org/v3/job/648404584/log.txt https://api.travis-ci.org/v3/job/648404591/log.txt https://api.travis-ci.org/v3/job/648404598/log.txt</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="15999" opendate="2020-2-11 00:00:00" fixdate="2020-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extract “Concepts” material from API/Library sections and start proper concepts section</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.fig.times.clocks.svg</file>
      <file type="M">docs.internals.stream.checkpointing.zh.md</file>
      <file type="M">docs.internals.stream.checkpointing.md</file>
      <file type="M">docs.fig.event.ingestion.processing.time.svg</file>
      <file type="M">docs.fig.processes.svg</file>
      <file type="M">docs.concepts.index.zh.md</file>
      <file type="M">docs.concepts.index.md</file>
      <file type="M">docs.concepts.stream-processing.md</file>
      <file type="M">docs.concepts.programming-model.zh.md</file>
      <file type="M">docs.concepts.glossary.zh.md</file>
      <file type="M">docs.concepts.glossary.md</file>
      <file type="M">docs.dev.event.time.md</file>
      <file type="M">docs.concepts.timely-stream-processing.md</file>
      <file type="M">docs.dev.stream.operators.process.function.zh.md</file>
      <file type="M">docs.dev.stream.operators.process.function.md</file>
      <file type="M">docs.concepts.runtime.zh.md</file>
      <file type="M">docs.concepts.runtime.md</file>
      <file type="M">docs.concepts.flink-architecture.md</file>
      <file type="M">docs.dev.stream.state.state.md</file>
      <file type="M">docs.dev.stream.state.index.md</file>
      <file type="M">docs.dev.stream.state.broadcast.state.md</file>
      <file type="M">docs.concepts.stateful-stream-processing.md</file>
      <file type="M">docs.concepts.programming-model.md</file>
    </fixedFiles>
  </bug>
  <bug id="16014" opendate="2020-2-12 00:00:00" fixdate="2020-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>S3 plugin ClassNotFoundException SAXParser</summary>
      <description>While stress-testing s3 plugin on EMR. org.apache.flink.util.FlinkRuntimeException: Could not perform checkpoint 2 for operator Map (114/160). at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:839) at org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:104) at org.apache.flink.streaming.runtime.io.CheckpointBarrierUnaligner.notifyBarrierReceived(CheckpointBarrierUnaligner.java:149) at org.apache.flink.streaming.runtime.io.InputProcessorUtil$1.lambda$notifyBarrierReceived$0(InputProcessorUtil.java:80) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.run(StreamTaskActionExecutor.java:87) at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:255) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:508) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:492) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.RuntimeException: org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3://emr-unaligned-checkpoints-testing-eu-central-1/inflight/9ae223e41008b17568d7f63c12360268_output/part-file.-1: com.amazonaws.SdkClientException: Couldn't initialize a SAX driver to create an XMLReader: Couldn't initialize a SAX driver to create an XMLReader at org.apache.flink.runtime.io.network.BufferPersisterImpl$Writer.checkErroneousUnsafe(BufferPersisterImpl.java:262) at org.apache.flink.runtime.io.network.BufferPersisterImpl$Writer.add(BufferPersisterImpl.java:137) at org.apache.flink.runtime.io.network.BufferPersisterImpl.addBuffers(BufferPersisterImpl.java:66) at org.apache.flink.streaming.runtime.tasks.StreamTask.prepareInflightDataSnapshot(StreamTask.java:935) at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:898) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94) at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:870) at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:826) ... 12 moreCaused by: org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3://emr-unaligned-checkpoints-testing-eu-central-1/inflight/9ae223e41008b17568d7f63c12360268_output/part-file.-1: com.amazonaws.SdkClientException: Couldn't initialize a SAX driver to create an XMLReader: Couldn't initialize a SAX driver to create an XMLReader at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:177) at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:145) at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2251) at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2149) at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2088) at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1734) at org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:2970) at org.apache.flink.fs.s3hadoop.common.HadoopFileSystem.exists(HadoopFileSystem.java:152) at org.apache.flink.core.fs.PluginFileSystemFactory$ClassLoaderFixingFileSystem.exists(PluginFileSystemFactory.java:143) at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.exists(SafetyNetWrapperFileSystem.java:102) at org.apache.flink.runtime.io.network.BufferPersisterImpl$Writer.get(BufferPersisterImpl.java:213) at org.apache.flink.runtime.io.network.BufferPersisterImpl$Writer.run(BufferPersisterImpl.java:167)Caused by: com.amazonaws.SdkClientException: Couldn't initialize a SAX driver to create an XMLReader at com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.&lt;init&gt;(XmlResponsesSaxParser.java:118) at com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsV2Unmarshaller.unmarshall(Unmarshallers.java:87) at com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsV2Unmarshaller.unmarshall(Unmarshallers.java:77) at com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:62) at com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:31) at com.amazonaws.http.response.AwsResponseHandlerAdapter.handle(AwsResponseHandlerAdapter.java:70) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleResponse(AmazonHttpClient.java:1554) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1272) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1056) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:743) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:717) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667) at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649) at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4325) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4272) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4266) at com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:876) at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$5(S3AFileSystem.java:1262) at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:317) at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:280) at org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:1255) at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2223) ... 9 moreCaused by: org.xml.sax.SAXException: SAX2 driver class org.apache.xerces.parsers.SAXParser not foundjava.lang.ClassNotFoundException: org.apache.xerces.parsers.SAXParser at org.xml.sax.helpers.XMLReaderFactory.loadClass(XMLReaderFactory.java:230) at org.xml.sax.helpers.XMLReaderFactory.createXMLReader(XMLReaderFactory.java:191) at com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.&lt;init&gt;(XmlResponsesSaxParser.java:115) ... 32 moreCaused by: java.lang.ClassNotFoundException: org.apache.xerces.parsers.SAXParser at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at org.apache.flink.core.plugin.PluginLoader$PluginClassLoader.loadClass(PluginLoader.java:149) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at org.xml.sax.helpers.NewInstance.newInstance(NewInstance.java:82) at org.xml.sax.helpers.XMLReaderFactory.loadClass(XMLReaderFactory.java:228) ... 34 more</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16015" opendate="2020-2-12 00:00:00" fixdate="2020-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refine fallback filesystems to only handle specific filesystems</summary>
      <description>Currently, if no s3 plugin is included, hadoop is used as a fallback, which introduces a wide variety of problems. We should probably only white list specific protocols that work well (e.g. hdfs).</description>
      <version>1.10.1,1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.fs.FileSystemTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.FileSystem.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">docs..includes.generated.core.configuration.html</file>
      <file type="M">docs..includes.generated.common.miscellaneous.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="16048" opendate="2020-2-13 00:00:00" fixdate="2020-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support read/write confluent schema registry avro data from Kafka</summary>
      <description>The backgroundI found SQL Kafka connector can not consume avro data that was serialized by `KafkaAvroSerializer` and only can consume Row data with avro schema because we use `AvroRowDeserializationSchema/AvroRowSerializationSchema` to se/de data in  `AvroRowFormatFactory`. I think we should support this because `KafkaAvroSerializer` is very common in Kafka.and someone met same question in stackoverflow&amp;#91;1&amp;#93;.[1]https://stackoverflow.com/questions/56452571/caused-by-org-apache-avro-avroruntimeexception-malformed-data-length-is-negat/56478259The format detailsThe factory identifier (or format id)There are 2 candidates now ~ avro-sr: the pattern borrowed from KSQL JSON_SR format &amp;#91;1&amp;#93; avro-confluent: the pattern borrowed from Clickhouse AvroConfluent &amp;#91;2&amp;#93;Personally i would prefer avro-sr because it is more concise and the confluent is a company name which i think is not that suitable for a format name.The format attributes Options required Remark schema-registry.url true URL to connect to schema registry service schema-registry.subject false Subject name to write to the Schema Registry service, required for sink</description>
      <version>1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.typeutils.AvroSchemaConverterTest.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.typeutils.AvroSchemaConverter.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.RegistryAvroSerializationSchema.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.RegistryAvroDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroSerializationSchema.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroRowDataSerializationSchema.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroRowDataDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroFileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.java.org.apache.flink.formats.avro.registry.confluent.ConfluentRegistryAvroSerializationSchema.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.java.org.apache.flink.formats.avro.registry.confluent.ConfluentRegistryAvroDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16049" opendate="2020-2-13 00:00:00" fixdate="2020-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove outdated "Best Practices" section from Application Development Section</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.best.practices.zh.md</file>
      <file type="M">docs.dev.best.practices.md</file>
    </fixedFiles>
  </bug>
  <bug id="16121" opendate="2020-2-17 00:00:00" fixdate="2020-2-17 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Introduce ArrowReader and ArrowWriter for Arrow format data read and write</summary>
      <description>As the title described, this aim of this JIRA is to introduce classes such as ArrowReader which is used to read the execution results of vectorized Python UDF and ArrowWriter which is used to convert Flink rows to Arrow format before sending them to the Python worker for vectorized Python UDF execution.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.util.StreamRecordUtils.java</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16122" opendate="2020-2-17 00:00:00" fixdate="2020-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>build system: transfer.sh uploads are unstable (February 2020)</summary>
      <description>This issue has been brought up on the dev@ list: https://lists.apache.org/thread.html/rb6661e419b869f040e66a4dd46022fd11961e8e5aebe646b2260f6f8%40%3Cdev.flink.apache.org%3EIssues: timeouts logs not available</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.watchdog.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="16166" opendate="2020-2-19 00:00:00" fixdate="2020-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Javadoc doclint option not working on Java 11</summary>
      <description>The doclint option is overridden in the java 11 profile.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16178" opendate="2020-2-19 00:00:00" fixdate="2020-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prerequisite cleanups and refactorings in the Checkpoint Coordinator</summary>
      <description>Umbrella issue for various small refactorings done as a prerequisite to the implementation of coordinator checkpoints, as well as various small cleanups of tech debt and inconsistencies in the checkpoint coordinator and related components.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointV2Serializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.output.SavepointOutputFormatTest.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.SavepointOutputFormat.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.MergeOperatorStates.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.Checkpoints.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointV1SerializerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointV1Serializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointV1.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointSerializers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorState.java</file>
    </fixedFiles>
  </bug>
  <bug id="16179" opendate="2020-2-20 00:00:00" fixdate="2020-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use configuration from TableFactory in hive connector</summary>
      <description>Now HiveOptions is used for GlobalConfiguration.loadConfiguration() .It is not natural for table, we should use configuration from TableFactory to enable table config.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="16183" opendate="2020-2-20 00:00:00" fixdate="2020-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make identifier parsing in Table API more lenient</summary>
      <description>I suggest to make the parsing logic for identifiers in Table API more lenient. We should not require users to escape any sql identifiers. It will make the identifiers not cross compatible between Table API and SQL, but it will improve user's experience and also will let us support parsing identifiers coming from Java's ExpressionParser (e.g. for a string array(..) it produces a lookup call with an "array" identifier which should be parsed)I suggest doing it by extending the FlinkSqlParserImpl with a new logic for TableApiSqlIdentifier which would be very similar to CompoundIdentifier with slightly adjusted logic which would not discard reserved keywords.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.calcite.CalciteParser.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.calcite.CalciteParser.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableEnvironment.java</file>
      <file type="M">docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  <bug id="1619" opendate="2015-3-2 00:00:00" fixdate="2015-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add pre-aggregator for time windows</summary>
      <description>Currently there is only support for pre-aggregators for tumbling policies.A pre-aggregator should be added for time policies.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.windowbuffer.TumblingPreReducerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.windowbuffer.TumblingGroupedPreReducerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.windowbuffer.BasicWindowBufferTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.WindowBuffer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.TumblingPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.TumblingGroupedPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.BasicWindowBuffer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.TimeTriggerPolicy.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.TimeEvictionPolicy.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.CountTriggerPolicy.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.CountEvictionPolicy.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.windowing.WindowIntegrationTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.WindowUtils.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.WindowedDataStream.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="16233" opendate="2020-2-22 00:00:00" fixdate="2020-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive connector missing log4j1 exclusions against certain hive versions</summary>
      <description>Click to add description</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16237" opendate="2020-2-23 00:00:00" fixdate="2020-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Several places missing Log4j2 configuration property</summary>
      <description>Wordcount on Docker test (custom fs plugin) fails on Travis. https://travis-ci.org/KarmaGYZ/flink/builds/653829954pass WordCountChecking for errors...Found error in log files:Attaching to docker_job-cluster_1job-cluster_1 | Starting the job-clusterjob-cluster_1 | Starting standalonejob as a console application on host 042c3c490edc.job-cluster_1 | ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2Attaching to docker_taskmanager_1taskmanager_1 | Starting the task-managertaskmanager_1 | Starting taskexecutor as a console application on host 059bd37e4232.taskmanager_1 | ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2Checking for exceptions...</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.TestProcessBuilder.java</file>
      <file type="M">flink-table.flink-sql-client.bin.sql-client.sh</file>
      <file type="M">flink-scala-shell.start-script.start-scala-shell.sh</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.TestJvmProcess.java</file>
      <file type="M">flink-python.bin.pyflink-gateway-server.sh</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesUtilsTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
      <file type="M">flink-dist.src.main.flink-bin.yarn-bin.yarn-session.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.kubernetes-bin.kubernetes-session.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink-console.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16245" opendate="2020-2-23 00:00:00" fixdate="2020-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use a delegating classloader as the user code classloader to prevent class leaks.</summary>
      <description>As reported in FLINK-11205, a reference to the user-code ClassLoader can be held by some libraries, causing class leaks.One way to circumvent this class leak is if the ClassLoader that we set as the user-code ClassLoader is a delegating ClassLoader to the real class loader, and when closing the user code ClassLoader we null out the reference.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.FlinkUserCodeClassLoader.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoadersTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.ChildFirstClassLoader.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.BatchFineGrainedRecoveryITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.ClientUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="16257" opendate="2020-2-24 00:00:00" fixdate="2020-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove useless ResultPartitionID from AddCredit message</summary>
      <description>The ResultPartitionID in AddCredit message is never used on upstream side, so we can remove it to cleanup the codes. There would have another two benefits to do so: Reduce the total message size from previous 52 bytes to 20 bytes. Decouple the dependency with `InputChannel#getPartitionId`</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyMessageSerializationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyMessage.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="16263" opendate="2020-2-24 00:00:00" fixdate="2020-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BaseRowArrowReaderWriterTest/RowArrowReaderWriterTest sun.misc.Unsafe or java.nio.DirectByteBuffer.&lt;init&gt;(long, int) not available</summary>
      <description>https://travis-ci.org/apache/flink/jobs/65440936418:17:45.003 [INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.924 s - in org.apache.flink.table.runtime.arrow.ArrowUtilsTest18:17:45.019 [INFO] Running org.apache.flink.table.runtime.arrow.BaseRowArrowReaderWriterTestsun.misc.Unsafe or java.nio.DirectByteBuffer.&lt;init&gt;(long, int) not availablejava.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.&lt;init&gt;(long, int) not available at io.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:399) at io.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:257) at io.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:247) at io.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:248) at org.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:228) at org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:242) at org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:132) at org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:120) at org.apache.flink.table.runtime.arrow.ArrowReaderWriterTestBase.testBasicFunctionality(ArrowReaderWriterTestBase.java:68) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)18:17:45.128 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.102 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.table.runtime.arrow.BaseRowArrowReaderWriterTest18:17:45.128 [ERROR] testBasicFunctionality(org.apache.flink.table.runtime.arrow.BaseRowArrowReaderWriterTest) Time elapsed: 0.097 s &lt;&lt;&lt; FAILURE!java.lang.AssertionError: Exception in test: sun.misc.Unsafe or java.nio.DirectByteBuffer.&lt;init&gt;(long, int) not available18:17:45.143 [INFO] Running org.apache.flink.table.runtime.arrow.RowArrowReaderWriterTestsun.misc.Unsafe or java.nio.DirectByteBuffer.&lt;init&gt;(long, int) not availablejava.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.&lt;init&gt;(long, int) not available at io.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:399) at io.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:257) at io.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:247) at io.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:248) at org.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:228) at org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:242) at org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:132) at org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:120) at org.apache.flink.table.runtime.arrow.ArrowReaderWriterTestBase.testBasicFunctionality(ArrowReaderWriterTestBase.java:68) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)18:17:45.209 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.057 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.table.runtime.arrow.RowArrowReaderWriterTest18:17:45.209 [ERROR] testBasicFunctionality(org.apache.flink.table.runtime.arrow.RowArrowReaderWriterTest) Time elapsed: 0.056 s &lt;&lt;&lt; FAILURE!java.lang.AssertionError: Exception in test: sun.misc.Unsafe or java.nio.DirectByteBuffer.&lt;init&gt;(long, int) not available</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16264" opendate="2020-2-24 00:00:00" fixdate="2020-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQLClientKafkaITCase fails with: Could not map the schema field &amp;#39;rowtime&amp;#39; to a field from source.</summary>
      <description>https://travis-ci.org/apache/flink/jobs/65440936618:42:33.227 [INFO] Running org.apache.flink.tests.util.kafka.SQLClientKafkaITCase18:44:08.035 [ERROR] Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 94.798 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.tests.util.kafka.SQLClientKafkaITCase18:44:08.041 [ERROR] testKafka[0: kafka-version:0.10 kafka-sql-version:.*kafka-0.10.jar](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase) Time elapsed: 32.305 s &lt;&lt;&lt; ERROR!java.io.IOException: Process execution failed due error. Error output:Exception in thread "main" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue. at org.apache.flink.table.client.SqlClient.main(SqlClient.java:190)Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: Could not create execution context. at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:779) at org.apache.flink.table.client.gateway.local.LocalExecutor.openSession(LocalExecutor.java:228) at org.apache.flink.table.client.SqlClient.start(SqlClient.java:98) at org.apache.flink.table.client.SqlClient.main(SqlClient.java:178)Caused by: org.apache.flink.table.api.ValidationException: Could not map the schema field 'rowtime' to a field from source. Please specify the source field from which it can be derived. at org.apache.flink.table.descriptors.SchemaValidator.deriveFieldMapping(SchemaValidator.java:302) at org.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryBase.createStreamTableSource(KafkaTableSourceSinkFactoryBase.java:170) at org.apache.flink.table.factories.StreamTableSourceFactory.createTableSource(StreamTableSourceFactory.java:55) at org.apache.flink.table.factories.TableSourceFactory.createTableSource(TableSourceFactory.java:63) at org.apache.flink.table.factories.TableSourceFactory.createTableSource(TableSourceFactory.java:74) at org.apache.flink.table.client.gateway.local.ExecutionContext.createTableSource(ExecutionContext.java:384) at org.apache.flink.table.client.gateway.local.ExecutionContext.lambda$initializeCatalogs$6(ExecutionContext.java:585) at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684) at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeCatalogs(ExecutionContext.java:583) at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeTableEnvironment(ExecutionContext.java:520) at org.apache.flink.table.client.gateway.local.ExecutionContext.&lt;init&gt;(ExecutionContext.java:165) at org.apache.flink.table.client.gateway.local.ExecutionContext.&lt;init&gt;(ExecutionContext.java:122) at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:768) ... 3 more at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.insertIntoAvroTable(SQLClientKafkaITCase.java:175) at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:148)</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.descriptors.DescriptorProperties.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.descriptors.ConnectTableDescriptor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogTableImpl.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.TestTableSourceFactoryBase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="16287" opendate="2020-2-26 00:00:00" fixdate="2020-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ES6 sql jar relocates log4j2</summary>
      <description>flink-sql-connector-elasticsearch6 still defines a relocation rule for log4j2, but this dependency is no longer bundled and instead provided by flink-dist.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16313" opendate="2020-2-27 00:00:00" fixdate="2020-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-state-processor-api: surefire execution unstable on Azure</summary>
      <description>Log file: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5686&amp;view=logs&amp;j=41cba0bb-1271-5adb-01cc-4768f26a8311&amp;t=44574c85-1cd0-5978-cccf-f0cf7e87a36a2020-02-27T12:36:35.2860111Z [INFO] flink-table-planner ................................ SUCCESS [01:47 min]2020-02-27T12:36:35.2860966Z [INFO] flink-cep-scala .................................... SUCCESS [ 5.041 s]2020-02-27T12:36:35.2861740Z [INFO] flink-sql-client ................................... SUCCESS [03:00 min]2020-02-27T12:36:35.2862503Z [INFO] flink-state-processor-api .......................... FAILURE [ 15.394 s]2020-02-27T12:36:35.2863237Z [INFO] ------------------------------------------------------------------------2020-02-27T12:36:35.2863587Z [INFO] BUILD FAILURE2020-02-27T12:36:35.2864071Z [INFO] ------------------------------------------------------------------------2020-02-27T12:36:35.2864428Z [INFO] Total time: 05:38 min2020-02-27T12:36:35.2866349Z [INFO] Finished at: 2020-02-27T12:36:35+00:002020-02-27T12:36:35.9345815Z [INFO] Final Memory: 147M/2914M2020-02-27T12:36:35.9347238Z [INFO] ------------------------------------------------------------------------2020-02-27T12:36:35.9355362Z [WARNING] The requested profile "skip-webui-build" could not be activated because it does not exist.2020-02-27T12:36:35.9367919Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (integration-tests) on project flink-state-processor-api_2.11: There are test failures.2020-02-27T12:36:35.9368804Z [ERROR] 2020-02-27T12:36:35.9369489Z [ERROR] Please refer to /__w/2/s/flink-libraries/flink-state-processing-api/target/surefire-reports for the individual test results.2020-02-27T12:36:35.9370249Z [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.2020-02-27T12:36:35.9370713Z [ERROR] ExecutionException Error occurred in starting fork, check output in log2020-02-27T12:36:35.9371279Z [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException Error occurred in starting fork, check output in log2020-02-27T12:36:35.9372275Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:510)2020-02-27T12:36:35.9372917Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:457)2020-02-27T12:36:35.9373498Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:298)2020-02-27T12:36:35.9374064Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)2020-02-27T12:36:35.9374636Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)2020-02-27T12:36:35.9375344Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)2020-02-27T12:36:35.9376194Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)2020-02-27T12:36:35.9376791Z [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)2020-02-27T12:36:35.9377375Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)2020-02-27T12:36:35.9377898Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)2020-02-27T12:36:35.9378435Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)2020-02-27T12:36:35.9379063Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)2020-02-27T12:36:35.9379709Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)2020-02-27T12:36:35.9380367Z [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)2020-02-27T12:36:35.9381007Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)2020-02-27T12:36:35.9381510Z [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)2020-02-27T12:36:35.9381973Z [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)2020-02-27T12:36:35.9382404Z [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)2020-02-27T12:36:35.9382839Z [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)2020-02-27T12:36:35.9383248Z [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)2020-02-27T12:36:35.9383661Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-02-27T12:36:35.9384126Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-02-27T12:36:35.9384659Z [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-02-27T12:36:35.9385145Z [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)2020-02-27T12:36:35.9385606Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)2020-02-27T12:36:35.9386293Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)2020-02-27T12:36:35.9386930Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)2020-02-27T12:36:35.9387471Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)2020-02-27T12:36:35.9388056Z [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: Error occurred in starting fork, check output in log2020-02-27T12:36:35.9388731Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:622)2020-02-27T12:36:35.9389289Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)2020-02-27T12:36:35.9389864Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:444)2020-02-27T12:36:35.9390411Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:420)2020-02-27T12:36:35.9390986Z [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)2020-02-27T12:36:35.9391458Z [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2020-02-27T12:36:35.9391991Z [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-02-27T12:36:35.9392419Z [ERROR] at java.lang.Thread.run(Thread.java:748)2020-02-27T12:36:35.9392894Z [ERROR] -&gt; [Help 1]2020-02-27T12:36:35.9393077Z [ERROR] 2020-02-27T12:36:35.9393553Z [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.2020-02-27T12:36:35.9394108Z [ERROR] Re-run Maven using the -X switch to enable full debug logging.2020-02-27T12:36:35.9394392Z [ERROR] 2020-02-27T12:36:35.9394713Z [ERROR] For more information about the errors and possible solutions, please read the following articles:2020-02-27T12:36:35.9395211Z [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException2020-02-27T12:36:35.9395525Z [ERROR] 2020-02-27T12:36:35.9395889Z [ERROR] After correcting the problems, you can resume the build with the command2020-02-27T12:36:35.9396511Z [ERROR] mvn &lt;goals&gt; -rf :flink-state-processor-api_2.112020-02-27T12:36:36.2427441Z MVN exited with EXIT CODE: 1.2020-02-27T12:36:36.2427867Z Trying to KILL watchdog (1633).</description>
      <version>1.10.1,1.11.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.RocksDBStateBackendReaderKeyedStateITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.operator.StateReaderOperator.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.KeyedStateInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="16336" opendate="2020-2-28 00:00:00" fixdate="2020-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support new type inference for temporal table functions</summary>
      <description>Temporal table functions have not been updated to the new type inference yet.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableFunctionRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableFunctionScan.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.functions.TemporalTableFunctionImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="16337" opendate="2020-2-28 00:00:00" fixdate="2020-3-28 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add RelNodes and Rules for vectorized Python UDF execution</summary>
      <description>As the title describes, the aim of this JIRA is to add RelNodes and Rules for vectorized Python UDF execution. </description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.PythonCalcSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.util.python.PythonTableUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.PythonUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.SplitPythonConditionFromJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.SplitPythonConditionFromCorrelateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.PythonCalcSplitRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamPythonCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetPythonCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamMatch.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.PythonFunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.rules.logical.PythonCorrelateSplitRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.rules.datastream.DataStreamPythonCorrelateRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.ExpressionReductionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.ExpressionReductionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.utils.python.PythonTableUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.PythonUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecPythonCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecPythonCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SplitPythonConditionFromJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SplitPythonConditionFromCorrelateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMatch.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.PythonFunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecPythonCorrelateRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecPythonCorrelateRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PythonCorrelateSplitRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.python.SimplePythonFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.python.PythonFunction.java</file>
      <file type="M">flink-python.pyflink.table.udf.py</file>
    </fixedFiles>
  </bug>
  <bug id="16339" opendate="2020-2-28 00:00:00" fixdate="2020-2-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DatadogReporter isn&amp;#39;t logging configuration options</summary>
      <description>To better detect wrongly configured options for reporters it is customary to log the used configured options.The datadog reporter is not doing that at the moment, and this should be fixed.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DatadogHttpReporter.java</file>
    </fixedFiles>
  </bug>
  <bug id="16348" opendate="2020-2-28 00:00:00" fixdate="2020-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add commas to large numeric accumulators</summary>
      <description>Make large numeric accumulator values easier to read.Ex 273232 -&gt; 273,232</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.accumulators.job-overview-drawer-accumulators.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="16370" opendate="2020-3-2 00:00:00" fixdate="2020-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink-dist bundles ZK 3.5 as JDK11-exclusive dependency</summary>
      <description>This is the output from the CI system (https://travis-ci.org/apache/flink/jobs/656931001)16:35:30.798 [ERROR] testKillYarnSessionClusterEntrypoint(org.apache.flink.yarn.YARNHighAvailabilityITCase) Time elapsed: 10.363 s &lt;&lt;&lt; ERROR!org.apache.flink.client.deployment.ClusterDeploymentException: Couldn't deploy Yarn session cluster at org.apache.flink.yarn.YARNHighAvailabilityITCase.deploySessionCluster(YARNHighAvailabilityITCase.java:296) at org.apache.flink.yarn.YARNHighAvailabilityITCase.lambda$testKillYarnSessionClusterEntrypoint$0(YARNHighAvailabilityITCase.java:165) at org.apache.flink.yarn.YARNHighAvailabilityITCase.testKillYarnSessionClusterEntrypoint(YARNHighAvailabilityITCase.java:157)Caused by: org.apache.flink.yarn.YarnClusterDescriptor$YarnDeploymentException: The YARN application unexpectedly switched to state FAILED during deployment. Diagnostics from YARN: Application application_1583080501498_0002 failed 2 times in previous 10000 milliseconds due to AM Container for appattempt_1583080501498_0002_000002 exited with exitCode: 1Failing this attempt.Diagnostics: Exception from container-launch.Container id: container_1583080501498_0002_02_000001Exit code: 1Stack trace: ExitCodeException exitCode=1: at org.apache.hadoop.util.Shell.runCommand(Shell.java:972) at org.apache.hadoop.util.Shell.run(Shell.java:869) at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170) at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236) at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305) at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84) at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at java.base/java.lang.Thread.run(Thread.java:834)... snip ...16:44:14.840 [INFO] Results:16:44:14.840 [INFO] 16:44:14.840 [ERROR] Errors: 16:44:14.840 [ERROR] YARNHighAvailabilityITCase.testJobRecoversAfterKillingTaskManager:187-&gt;YarnTestBase.runTest:242-&gt;lambda$testJobRecoversAfterKillingTaskManager$1:191-&gt;deploySessionCluster:296 Â» ClusterDeployment16:44:14.840 [ERROR] YARNHighAvailabilityITCase.testKillYarnSessionClusterEntrypoint:157-&gt;YarnTestBase.runTest:242-&gt;lambda$testKillYarnSessionClusterEntrypoint$0:165-&gt;deploySessionCluster:296 Â» ClusterDeployment16:44:14.840 [INFO] 16:44:14.840 [ERROR] Tests run: 25, Failures: 0, Errors: 2, Skipped: 4Digging deeper into the problem, this seems to be the root cause:2020-03-01 16:35:14,444 INFO org.apache.flink.shaded.curator4.org.apache.curator.utils.Compatibility [] - Using emulated InjectSessionExpiration2020-03-01 16:35:14,466 WARN org.apache.flink.shaded.curator4.org.apache.curator.CuratorZookeeperClient [] - session timeout [1000] is less than connection timeout [15000]2020-03-01 16:35:14,491 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Shutting YarnSessionClusterEntrypoint down with application status FAILED. Diagnostics java.lang.NoSuchMethodError: org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.quorum.flexible.QuorumMaj.&lt;init&gt;(Ljava/util/Map;)V at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.EnsembleTracker.&lt;init&gt;(EnsembleTracker.java:57) at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.&lt;init&gt;(CuratorFrameworkImpl.java:159) at org.apache.flink.shaded.curator4.org.apache.curator.framework.CuratorFrameworkFactory$Builder.build(CuratorFrameworkFactory.java:165) at org.apache.flink.runtime.util.ZooKeeperUtils.startCuratorFramework(ZooKeeperUtils.java:138) at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:128) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:305) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:263) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:207) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:169) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:168) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:518) at org.apache.flink.yarn.entrypoint.YarnSessionClusterEntrypoint.main(YarnSessionClusterEntrypoint.java:80).2020-03-01 16:35:14,502 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Stopping Akka RPC service.2020-03-01 16:35:14,512 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator [] - Shutting down remote daemon.2020-03-01 16:35:14,514 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator [] - Remote daemon shut down; proceeding with flushing remote transports.2020-03-01 16:35:14,548 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator [] - Remoting shut down.2020-03-01 16:35:14,604 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Stopped Akka RPC service.2020-03-01 16:35:14,592 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Could not start cluster entrypoint YarnSessionClusterEntrypoint.org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint YarnSessionClusterEntrypoint. at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:187) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:518) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.yarn.entrypoint.YarnSessionClusterEntrypoint.main(YarnSessionClusterEntrypoint.java:80) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]Caused by: java.lang.NoSuchMethodError: org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.quorum.flexible.QuorumMaj.&lt;init&gt;(Ljava/util/Map;)V at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.EnsembleTracker.&lt;init&gt;(EnsembleTracker.java:57) ~[flink-shaded-zookeeper-3.4.10.jar:3.4.10-10.0] at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.&lt;init&gt;(CuratorFrameworkImpl.java:159) ~[flink-shaded-zookeeper-3.4.10.jar:3.4.10-10.0] at org.apache.flink.shaded.curator4.org.apache.curator.framework.CuratorFrameworkFactory$Builder.build(CuratorFrameworkFactory.java:165) ~[flink-shaded-zookeeper-3.4.10.jar:3.4.10-10.0] at org.apache.flink.runtime.util.ZooKeeperUtils.startCuratorFramework(ZooKeeperUtils.java:138) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:128) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:305) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:263) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:207) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:169) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at java.security.AccessController.doPrivileged(Native Method) ~[?:?] at javax.security.auth.Subject.doAs(Subject.java:423) ~[?:?] at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0] at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:168) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] ... 2 more</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16374" opendate="2020-3-2 00:00:00" fixdate="2020-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StreamingKafkaITCase: IOException: error=13, Permission denied</summary>
      <description>Build: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5792&amp;view=logs&amp;j=25197a20-5964-5b06-5716-045f87dc0ea9&amp;t=0c53f4dc-c81e-5ebb-13b2-08f1994a2d322020-03-02T05:13:23.4758068Z [INFO] Running org.apache.flink.tests.util.kafka.StreamingKafkaITCase2020-03-02T05:13:55.8260013Z [ERROR] Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 32.346 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.tests.util.kafka.StreamingKafkaITCase2020-03-02T05:13:55.8262664Z [ERROR] testKafka[0: kafka-version:0.10.2.0](org.apache.flink.tests.util.kafka.StreamingKafkaITCase) Time elapsed: 9.217 s &lt;&lt;&lt; ERROR!2020-03-02T05:13:55.8264067Z java.io.IOException: Cannot run program "/tmp/junit5236495846374568650/junit4714535957173883866/bin/start-cluster.sh": error=13, Permission denied2020-03-02T05:13:55.8264733Z at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.testKafka(StreamingKafkaITCase.java:85)2020-03-02T05:13:55.8265242Z Caused by: java.io.IOException: error=13, Permission denied2020-03-02T05:13:55.8265717Z at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.testKafka(StreamingKafkaITCase.java:85)2020-03-02T05:13:55.8266083Z 2020-03-02T05:13:55.8271420Z [ERROR] testKafka[1: kafka-version:0.11.0.2](org.apache.flink.tests.util.kafka.StreamingKafkaITCase) Time elapsed: 11.228 s &lt;&lt;&lt; ERROR!2020-03-02T05:13:55.8272670Z java.io.IOException: Cannot run program "/tmp/junit8038960384540194088/junit1280636219654303027/bin/start-cluster.sh": error=13, Permission denied2020-03-02T05:13:55.8273343Z at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.testKafka(StreamingKafkaITCase.java:85)2020-03-02T05:13:55.8273847Z Caused by: java.io.IOException: error=13, Permission denied2020-03-02T05:13:55.8274418Z at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.testKafka(StreamingKafkaITCase.java:85)2020-03-02T05:13:55.8274768Z 2020-03-02T05:13:55.8275429Z [ERROR] testKafka[2: kafka-version:2.2.0](org.apache.flink.tests.util.kafka.StreamingKafkaITCase) Time elapsed: 11.89 s &lt;&lt;&lt; ERROR!2020-03-02T05:13:55.8276386Z java.io.IOException: Cannot run program "/tmp/junit5500905670445852005/junit4695208010500962520/bin/start-cluster.sh": error=13, Permission denied2020-03-02T05:13:55.8277257Z at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.testKafka(StreamingKafkaITCase.java:85)2020-03-02T05:13:55.8277760Z Caused by: java.io.IOException: error=13, Permission denied2020-03-02T05:13:55.8278228Z at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.testKafka(StreamingKafkaITCase.java:85)</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.watchdog.sh</file>
    </fixedFiles>
  </bug>
  <bug id="16431" opendate="2020-3-5 00:00:00" fixdate="2020-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pass build profile into end to end test script on Azure</summary>
      <description>The nightly tests scripts assumes that it has access to $PROFILE, which does not seem to be true.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="16432" opendate="2020-3-5 00:00:00" fixdate="2020-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Building Hive connector gives problems</summary>
      <description>When building the current Flink source I keep running to problems with the hive connector.The problems focus around dependencies that are not available by default: org.pentaho:pentaho-aggdesigner-algorithm javax.jms:jms</description>
      <version>1.10.2,1.11.0</version>
      <fixedVersion>1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16464" opendate="2020-3-6 00:00:00" fixdate="2020-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>result-mode tableau may shift when content contains Chinese String in SQL CLI</summary>
      <description> result-mode tableau may shift when column content contains Chinese String in SQL CLI  as following: Flink SQL&gt; select * from user_ino;+-----+----------------------+--------+----------------------+| +/- | user_name | is_new | content_col |+-----+----------------------+--------+----------------------+| + | sam | true | content || + | 中文名 | false | content || + | leonard | true | content |We can calculate column widths with UDTF-8 format bytes not length of string to avoid this.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliUtilsTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliTableauResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliTableauResultView.java</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16480" opendate="2020-3-7 00:00:00" fixdate="2020-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve e2e test failure error reporting</summary>
      <description>The purpose of this change is to improve the error reporting for e2e tests: The log upload for e2e tests fails if the bash e2e tests fail coredumps, dumpstreams etc. are not included into the log upload Logs are not scanned for exceptions when exception checking is turned off</description>
      <version>1.11.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="1652" opendate="2015-3-4 00:00:00" fixdate="2015-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong superstep number in VertexCentricIteration in Collection mode</summary>
      <description>When in collection execution mode, the superstep number is not correctly updated for Spargel's and Gelly's VertexCentricIteration. There seems to be to problem with DeltaIteration.See also relevant discussion in dev@ .</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.DegreesWithExceptionITCase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.CollectionExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="16550" opendate="2020-3-11 00:00:00" fixdate="2020-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HadoopS3* tests fail with NullPointerException exceptions</summary>
      <description>Logs: https://travis-ci.org/github/apache/flink/jobs/660975486?utm_medium=notificationAll subsequent builds failed as well. It is likely that this commit / FLINK-16014 introduced the issue, as these tests depend on S3 credentials to be available.09:38:48.022 [INFO] -------------------------------------------------------09:38:48.025 [INFO] T E S T S09:38:48.026 [INFO] -------------------------------------------------------09:38:48.657 [INFO] Running org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase09:38:48.669 [INFO] Running org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase09:38:54.541 [ERROR] Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 5.88 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase09:38:54.542 [ERROR] testResumeAfterCommit(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase) Time elapsed: 3.592 s &lt;&lt;&lt; ERROR!java.lang.Exception: Unexpected exception, expected&lt;java.io.IOException&gt; but was&lt;java.lang.NullPointerException&gt; at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeAfterCommit(HadoopS3RecoverableWriterExceptionITCase.java:162)09:38:54.542 [ERROR] testResumeWithWrongOffset(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase) Time elapsed: 0.24 s &lt;&lt;&lt; ERROR!java.lang.Exception: Unexpected exception, expected&lt;java.io.IOException&gt; but was&lt;java.lang.NullPointerException&gt; at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeWithWrongOffset(HadoopS3RecoverableWriterExceptionITCase.java:182)09:38:54.542 [ERROR] testExceptionWritingAfterCloseForCommit(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase) Time elapsed: 0.448 s &lt;&lt;&lt; ERROR!java.lang.Exception: Unexpected exception, expected&lt;java.io.IOException&gt; but was&lt;java.lang.NullPointerException&gt; at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testExceptionWritingAfterCloseForCommit(HadoopS3RecoverableWriterExceptionITCase.java:144)09:38:55.173 [INFO] Running org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase09:38:58.737 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 10.066 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase09:38:58.737 [ERROR] testDirectoryListing(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase) Time elapsed: 3.448 s &lt;&lt;&lt; ERROR!java.io.FileNotFoundException: No such file or directory: s3://[secure]/temp/tests-f37db36e-c116-4c58-a16b-8ca241baae4b/testdir09:38:59.447 [INFO] Running org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase09:39:01.791 [ERROR] Tests run: 13, Failures: 0, Errors: 13, Skipped: 0, Time elapsed: 6.611 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase09:39:01.797 [ERROR] testCloseWithNoData(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 2.394 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testCloseWithNoData(HadoopS3RecoverableWriterITCase.java:186)09:39:01.798 [ERROR] testCommitAfterPersist(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.191 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testCommitAfterPersist(HadoopS3RecoverableWriterITCase.java:208)09:39:01.799 [ERROR] testRecoverWithEmptyState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.235 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:384) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithSmallData(HadoopS3RecoverableWriterITCase.java:352) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverWithEmptyState(HadoopS3RecoverableWriterITCase.java:302)09:39:01.799 [ERROR] testRecoverFromIntermWithoutAdditionalState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.181 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:384) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithSmallData(HadoopS3RecoverableWriterITCase.java:352) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverFromIntermWithoutAdditionalState(HadoopS3RecoverableWriterITCase.java:316)09:39:01.799 [ERROR] testCallingDeleteObjectTwiceDoesNotThroughException(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.181 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testCallingDeleteObjectTwiceDoesNotThroughException(HadoopS3RecoverableWriterITCase.java:245)09:39:01.801 [ERROR] testCommitAfterNormalClose(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.174 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testCommitAfterNormalClose(HadoopS3RecoverableWriterITCase.java:196)09:39:01.802 [ERROR] testRecoverWithStateWithMultiPart(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.338 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:384) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithMultiPartUploads(HadoopS3RecoverableWriterITCase.java:364) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverWithStateWithMultiPart(HadoopS3RecoverableWriterITCase.java:330)09:39:01.803 [ERROR] testRecoverFromIntermWithoutAdditionalStateWithMultiPart(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.486 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:384) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithMultiPartUploads(HadoopS3RecoverableWriterITCase.java:364) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverFromIntermWithoutAdditionalStateWithMultiPart(HadoopS3RecoverableWriterITCase.java:337)09:39:01.810 [ERROR] testRecoverWithState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.199 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:384) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithSmallData(HadoopS3RecoverableWriterITCase.java:352) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverWithState(HadoopS3RecoverableWriterITCase.java:309)09:39:01.810 [ERROR] testCleanupRecoverableState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.202 s &lt;&lt;&lt; ERROR!java.lang.Exception: Unexpected exception, expected&lt;java.io.FileNotFoundException&gt; but was&lt;java.lang.NullPointerException&gt; at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testCleanupRecoverableState(HadoopS3RecoverableWriterITCase.java:223)09:39:01.810 [ERROR] testCommitAfterRecovery(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.26 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testCommitAfterRecovery(HadoopS3RecoverableWriterITCase.java:270)09:39:01.810 [ERROR] testRecoverAfterMultiplePersistsState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.165 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:384) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithSmallData(HadoopS3RecoverableWriterITCase.java:352) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverAfterMultiplePersistsState(HadoopS3RecoverableWriterITCase.java:323)09:39:01.810 [ERROR] testRecoverAfterMultiplePersistsStateWithMultiPart(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.735 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:384) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithMultiPartUploads(HadoopS3RecoverableWriterITCase.java:364) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverAfterMultiplePersistsStateWithMultiPart(HadoopS3RecoverableWriterITCase.java:344)09:39:14.711 [WARNING] Tests run: 8, Failures: 0, Errors: 0, Skipped: 2, Time elapsed: 15.262 s - in org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase09:39:15.047 [INFO] 09:39:15.047 [INFO] Results:09:39:15.047 [INFO] 09:39:15.047 [ERROR] Errors: 09:39:15.047 [ERROR] HadoopS3FileSystemITCase&gt;AbstractHadoopFileSystemITTest.testDirectoryListing:127 Â» FileNotFound09:39:15.047 [ERROR] HadoopS3RecoverableWriterExceptionITCase.testExceptionWritingAfterCloseForCommit Â» 09:39:15.047 [ERROR] HadoopS3RecoverableWriterExceptionITCase.testResumeAfterCommit Â» Unexpected e...09:39:15.047 [ERROR] HadoopS3RecoverableWriterExceptionITCase.testResumeWithWrongOffset Â» Unexpect...09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testCallingDeleteObjectTwiceDoesNotThroughException:245 Â» NullPointer09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testCleanupRecoverableState Â» Unexpected exce...09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testCloseWithNoData:186 Â» NullPointer09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testCommitAfterNormalClose:196 Â» NullPointer09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testCommitAfterPersist:208 Â» NullPointer09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testCommitAfterRecovery:270 Â» NullPointer09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testRecoverAfterMultiplePersistsState:323-&gt;testResumeAfterMultiplePersistWithSmallData:352-&gt;testResumeAfterMultiplePersist:384 Â» NullPointer09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testRecoverAfterMultiplePersistsStateWithMultiPart:344-&gt;testResumeAfterMultiplePersistWithMultiPartUploads:364-&gt;testResumeAfterMultiplePersist:384 Â» NullPointer09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testRecoverFromIntermWithoutAdditionalState:316-&gt;testResumeAfterMultiplePersistWithSmallData:352-&gt;testResumeAfterMultiplePersist:384 Â» NullPointer09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testRecoverFromIntermWithoutAdditionalStateWithMultiPart:337-&gt;testResumeAfterMultiplePersistWithMultiPartUploads:364-&gt;testResumeAfterMultiplePersist:384 Â» NullPointer09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testRecoverWithEmptyState:302-&gt;testResumeAfterMultiplePersistWithSmallData:352-&gt;testResumeAfterMultiplePersist:384 Â» NullPointer09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testRecoverWithState:309-&gt;testResumeAfterMultiplePersistWithSmallData:352-&gt;testResumeAfterMultiplePersist:384 Â» NullPointer09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testRecoverWithStateWithMultiPart:330-&gt;testResumeAfterMultiplePersistWithMultiPartUploads:364-&gt;testResumeAfterMultiplePersist:384 Â» NullPointer09:39:15.047 [INFO] 09:39:15.047 [ERROR] Tests run: 26, Failures: 0, Errors: 17, Skipped: 2</description>
      <version>1.11.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.main.java.com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="16565" opendate="2020-3-12 00:00:00" fixdate="2020-3-12 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Make Pipeline Json compitable between Java and Python if all Pipelinestage are Java ones</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.ml.tests.test.pipeline.it.case.py</file>
      <file type="M">flink-python.pyflink.ml.tests.test.pipeline.py</file>
      <file type="M">flink-python.pyflink.ml.api.base.py</file>
    </fixedFiles>
  </bug>
  <bug id="16566" opendate="2020-3-12 00:00:00" fixdate="2020-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change the log level of the launching command and dynamic properties from DEBUG to INFO in Mesos integration</summary>
      <description>As discussed in http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Flink-1-10-container-memory-configuration-with-Mesos-td33594.html . It would helpful for debugging to log the launching command and dynamic properties at INFO level. Since such logs occur only when workers started, it would not be massive.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.LaunchableMesosWorker.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosTaskExecutorRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="16590" opendate="2020-3-13 00:00:00" fixdate="2020-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-oss-fs-hadoop: Not all dependencies in NOTICE file are bundled</summary>
      <description>NOTICE file in flink-oss-fs-hadoop lists org.apache.commons:commons-compress as a bundled dependency which is not correct. There are likely other dependencies that are wrongly listed in the NOTICE file.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.licenses.LICENSE.stax2api</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.licenses.LICENSE.re2j</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.licenses.LICENSE.protobuf</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.licenses.LICENSE.paranamer</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.licenses.LICENSE.jzlib</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.licenses.LICENSE.cddlv1.1</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.licenses.LICENSE.cddlv1.0</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.licenses.LICENSE.asm</file>
    </fixedFiles>
  </bug>
  <bug id="16629" opendate="2020-3-17 00:00:00" fixdate="2020-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming bucketing end-to-end test output hash mismatch</summary>
      <description>https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/6298/logs/722Some of the output mismatch failures were reported in another ticket: https://issues.apache.org/jira/browse/FLINK-162272020-03-17T02:04:19.9176915Z Number of produced values 30618/600002020-03-17T02:04:19.9202731Z Truncating buckets2020-03-17T02:04:25.0504959Z Truncating buckets2020-03-17T02:04:30.1731295Z Truncating buckets2020-03-17T02:04:35.3190114Z Truncating buckets2020-03-17T02:04:40.4723887Z Truncating buckets2020-03-17T02:04:45.5984655Z Truncating buckets2020-03-17T02:04:50.7185356Z Truncating buckets2020-03-17T02:04:55.8627129Z Truncating buckets2020-03-17T02:05:01.0715985Z Number of produced values 74008/600002020-03-17T02:05:02.3976850Z Cancelling job dba2fdb79579158295db27d0214fc2ff.2020-03-17T02:05:03.4633541Z Cancelled job dba2fdb79579158295db27d0214fc2ff.2020-03-17T02:05:03.4738270Z Waiting for job (dba2fdb79579158295db27d0214fc2ff) to reach terminal state CANCELED ...2020-03-17T02:05:03.5149228Z Job (dba2fdb79579158295db27d0214fc2ff) reached terminal state CANCELED2020-03-17T02:05:03.5150587Z Job dba2fdb79579158295db27d0214fc2ff was cancelled, time to verify2020-03-17T02:05:03.5590118Z FAIL Bucketing Sink: Output hash mismatch. Got c3787e7a52d913675e620837a7531742, expected 01aba5ff77a0ef5e5cf6a727c248bdc3.2020-03-17T02:05:03.5591888Z head hexdump of actual:2020-03-17T02:05:03.5989908Z 0000000 ( 7 , 1 0 , 0 , S o m e p a y2020-03-17T02:05:03.5991252Z 0000010 l o a d . . . ) \n ( 7 , 1 0 , 12020-03-17T02:05:03.5991923Z 0000020 , S o m e p a y l o a d . . .2020-03-17T02:05:03.5993055Z 0000030 ) \n ( 7 , 1 0 , 2 , S o m e p2020-03-17T02:05:03.5993690Z 0000040 a y l o a d . . . ) \n ( 7 , 1 02020-03-17T02:05:03.5994332Z 0000050 , 3 , S o m e p a y l o a d .2020-03-17T02:05:03.5994967Z 0000060 . . ) \n ( 7 , 1 0 , 4 , S o m e2020-03-17T02:05:03.5995744Z 0000070 p a y l o a d . . . ) \n ( 7 ,2020-03-17T02:05:03.5996359Z 0000080 1 0 , 5 , S o m e p a y l o a2020-03-17T02:05:03.5997133Z 0000090 d . . . ) \n ( 7 , 1 0 , 6 , S o2020-03-17T02:05:03.5997704Z 00000a0 m e p a y l o a d . . . ) \n (2020-03-17T02:05:03.5998295Z 00000b0 7 , 1 0 , 7 , S o m e p a y l2020-03-17T02:05:03.5999087Z 00000c0 o a d . . . ) \n ( 7 , 1 0 , 8 ,2020-03-17T02:05:03.6000243Z 00000d0 S o m e p a y l o a d . . . )2020-03-17T02:05:03.6000880Z 00000e0 \n ( 7 , 1 0 , 9 , S o m e p a2020-03-17T02:05:03.6001494Z 00000f0 y l o a d . . . ) \n 2020-03-17T02:05:03.6001999Z 00000fa2020-03-17T02:05:03.9875220Z Stopping taskexecutor daemon (pid: 49278) on host fv-az668.2020-03-17T02:05:04.2569285Z Stopping standalonesession daemon (pid: 46323) on host fv-az668.2020-03-17T02:05:04.7664418Z Stopping taskexecutor daemon (pid: 46615) on host fv-az668.2020-03-17T02:05:04.7674722Z Skipping taskexecutor daemon (pid: 47009), because it is not running anymore on fv-az668.2020-03-17T02:05:04.7687383Z Skipping taskexecutor daemon (pid: 47299), because it is not running anymore on fv-az668.2020-03-17T02:05:04.7689091Z Skipping taskexecutor daemon (pid: 47619), because it is not running anymore on fv-az668.2020-03-17T02:05:04.7690289Z Stopping taskexecutor daemon (pid: 48538) on host fv-az668.2020-03-17T02:05:04.7691796Z Stopping taskexecutor daemon (pid: 48988) on host fv-az668.2020-03-17T02:05:04.7692365Z [FAIL] Test script contains errors.2020-03-17T02:05:04.7713750Z Checking of logs skipped.2020-03-17T02:05:04.7714249Z 2020-03-17T02:05:04.7715316Z [FAIL] 'Streaming bucketing end-to-end test' failed after 2 minutes and 43 seconds! Test exited with exit code 1</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="16633" opendate="2020-3-17 00:00:00" fixdate="2020-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CI builds without S3 credentials fail</summary>
      <description>Pull request builds on Azure don't have access to the S3 credentials (through environment variables).They fail with the following errors:2020-03-17T02:42:24.4186165Z [ERROR] Tests run: 7, Failures: 0, Errors: 7, Skipped: 0, Time elapsed: 0.645 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase2020-03-17T02:42:24.4187582Z [ERROR] testResumeAfterCommit(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase) Time elapsed: 0.11 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4188593Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4189408Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)2020-03-17T02:42:24.4190152Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.prepare(HadoopS3RecoverableWriterExceptionITCase.java:120)2020-03-17T02:42:24.4190798Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4191529Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)2020-03-17T02:42:24.4192258Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.prepare(HadoopS3RecoverableWriterExceptionITCase.java:120)2020-03-17T02:42:24.4192689Z 2020-03-17T02:42:24.4193119Z [ERROR] testResumeAfterCommit(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase) Time elapsed: 0.11 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4193704Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4194461Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)2020-03-17T02:42:24.4195299Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.cleanup(HadoopS3RecoverableWriterExceptionITCase.java:130)2020-03-17T02:42:24.4195945Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4196588Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)2020-03-17T02:42:24.4197307Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.cleanup(HadoopS3RecoverableWriterExceptionITCase.java:130)2020-03-17T02:42:24.4197697Z 2020-03-17T02:42:24.4198135Z [ERROR] testResumeWithWrongOffset(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase) Time elapsed: 0.001 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4198791Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4199755Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)2020-03-17T02:42:24.4200466Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.prepare(HadoopS3RecoverableWriterExceptionITCase.java:120)2020-03-17T02:42:24.4201169Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4201834Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)2020-03-17T02:42:24.4202578Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.prepare(HadoopS3RecoverableWriterExceptionITCase.java:120)2020-03-17T02:42:24.4203010Z 2020-03-17T02:42:24.4203579Z [ERROR] testResumeWithWrongOffset(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase) Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4204201Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4204884Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)2020-03-17T02:42:24.4205769Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.cleanup(HadoopS3RecoverableWriterExceptionITCase.java:130)2020-03-17T02:42:24.4206387Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4207038Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)2020-03-17T02:42:24.4207731Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.cleanup(HadoopS3RecoverableWriterExceptionITCase.java:130)2020-03-17T02:42:24.4208128Z 2020-03-17T02:42:24.4208574Z [ERROR] testExceptionWritingAfterCloseForCommit(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase) Time elapsed: 0.001 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4209199Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4209846Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)2020-03-17T02:42:24.4210561Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.prepare(HadoopS3RecoverableWriterExceptionITCase.java:120)2020-03-17T02:42:24.4211262Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4211909Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)2020-03-17T02:42:24.4212667Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.prepare(HadoopS3RecoverableWriterExceptionITCase.java:120)2020-03-17T02:42:24.4213270Z 2020-03-17T02:42:24.4213976Z [ERROR] testExceptionWritingAfterCloseForCommit(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase) Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4215187Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4216174Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)2020-03-17T02:42:24.4217335Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.cleanup(HadoopS3RecoverableWriterExceptionITCase.java:130)2020-03-17T02:42:24.4218162Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4218811Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)2020-03-17T02:42:24.4219751Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.cleanup(HadoopS3RecoverableWriterExceptionITCase.java:130)2020-03-17T02:42:24.4220133Z 2020-03-17T02:42:24.4220524Z [ERROR] org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4221167Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4221776Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)2020-03-17T02:42:24.4222539Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.cleanUp(HadoopS3RecoverableWriterExceptionITCase.java:109)2020-03-17T02:42:24.4223186Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4223818Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.getFileSystem(HadoopS3RecoverableWriterExceptionITCase.java:135)2020-03-17T02:42:24.4224649Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.cleanUp(HadoopS3RecoverableWriterExceptionITCase.java:109)2020-03-17T02:42:24.4225138Z 2020-03-17T02:42:24.4244002Z [ERROR] Tests run: 16, Failures: 0, Errors: 16, Skipped: 0, Time elapsed: 0.651 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase2020-03-17T02:42:24.4245484Z [ERROR] testMkdirsReturnsTrueWhenCreatingDirectory(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase) Time elapsed: 0.106 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4246427Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4247410Z at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)2020-03-17T02:42:24.4248029Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4248676Z at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)2020-03-17T02:42:24.4249045Z 2020-03-17T02:42:24.4249509Z [ERROR] testMkdirsReturnsTrueWhenCreatingDirectory(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase) Time elapsed: 0.106 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4250207Z java.lang.NullPointerException2020-03-17T02:42:24.4250371Z 2020-03-17T02:42:24.4250811Z [ERROR] testMkdirsFailsWithExistingParentFile(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase) Time elapsed: 0.001 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4251528Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4252098Z at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)2020-03-17T02:42:24.4252769Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4253384Z at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)2020-03-17T02:42:24.4253748Z 2020-03-17T02:42:24.4254188Z [ERROR] testMkdirsFailsWithExistingParentFile(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase) Time elapsed: 0.001 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4254773Z java.lang.NullPointerException2020-03-17T02:42:24.4254946Z 2020-03-17T02:42:24.4255377Z [ERROR] testMkdirsReturnsTrueForExistingDirectory(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase) Time elapsed: 0.001 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4255992Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4256640Z at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)2020-03-17T02:42:24.4257238Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4258010Z at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)2020-03-17T02:42:24.4258381Z 2020-03-17T02:42:24.4258794Z [ERROR] testMkdirsReturnsTrueForExistingDirectory(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase) Time elapsed: 0.001 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4259265Z java.lang.NullPointerException2020-03-17T02:42:24.4259476Z 2020-03-17T02:42:24.4260069Z [ERROR] testPathAndScheme(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase) Time elapsed: 0.001 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4260921Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4261716Z at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)2020-03-17T02:42:24.4262355Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4262963Z at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)2020-03-17T02:42:24.4263450Z 2020-03-17T02:42:24.4263844Z [ERROR] testPathAndScheme(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase) Time elapsed: 0.001 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4264260Z java.lang.NullPointerException2020-03-17T02:42:24.4264413Z 2020-03-17T02:42:24.4265143Z [ERROR] testHomeAndWorkDir(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase) Time elapsed: 0.001 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4266031Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4267039Z at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)2020-03-17T02:42:24.4267981Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4268921Z at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)2020-03-17T02:42:24.4269308Z 2020-03-17T02:42:24.4269695Z [ERROR] testHomeAndWorkDir(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase) Time elapsed: 0.001 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4270133Z java.lang.NullPointerException2020-03-17T02:42:24.4270288Z 2020-03-17T02:42:24.4270655Z [ERROR] testFileSystemKind(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase) Time elapsed: 0.001 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4271303Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4271841Z at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)2020-03-17T02:42:24.4272728Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4273341Z at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)2020-03-17T02:42:24.4273698Z 2020-03-17T02:42:24.4274064Z [ERROR] testFileSystemKind(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase) Time elapsed: 0.001 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4274498Z java.lang.NullPointerException2020-03-17T02:42:24.4274765Z 2020-03-17T02:42:24.4275176Z [ERROR] testMkdirsFailsForExistingFile(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase) Time elapsed: 0.001 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4275736Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4276290Z at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)2020-03-17T02:42:24.4276995Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4277768Z at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)2020-03-17T02:42:24.4278112Z 2020-03-17T02:42:24.4278526Z [ERROR] testMkdirsFailsForExistingFile(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase) Time elapsed: 0.001 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4278971Z java.lang.NullPointerException2020-03-17T02:42:24.4279123Z 2020-03-17T02:42:24.4279553Z [ERROR] testMkdirsCreatesParentDirectories(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase) Time elapsed: 0.001 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4280192Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4280730Z at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)2020-03-17T02:42:24.4281383Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:24.4281969Z at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.getFileSystem(HadoopS3FileSystemBehaviorITCase.java:60)2020-03-17T02:42:24.4282377Z 2020-03-17T02:42:24.4282797Z [ERROR] testMkdirsCreatesParentDirectories(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase) Time elapsed: 0.001 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:24.4283358Z java.lang.NullPointerException2020-03-17T02:42:24.4283511Z 2020-03-17T02:42:24.6650046Z [INFO] Running org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase2020-03-17T02:42:24.9808490Z [INFO] Running org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase2020-03-17T02:42:25.2595666Z [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.593 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase2020-03-17T02:42:25.2596561Z [ERROR] org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase Time elapsed: 0.593 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.2597100Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.2597675Z at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase.setup(HadoopS3FileSystemITCase.java:55)2020-03-17T02:42:25.2598249Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.2598841Z at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase.setup(HadoopS3FileSystemITCase.java:55)2020-03-17T02:42:25.2599168Z 2020-03-17T02:42:25.6533694Z [ERROR] Tests run: 27, Failures: 0, Errors: 27, Skipped: 0, Time elapsed: 0.668 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase2020-03-17T02:42:25.6534735Z [ERROR] testCloseWithNoData(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.108 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6535359Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6535930Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6536575Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6537219Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6537792Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6538384Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6539009Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6539642Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6540175Z 2020-03-17T02:42:25.6540552Z [ERROR] testCloseWithNoData(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.109 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6541479Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6542046Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6542681Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6543303Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6543886Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6544668Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6545075Z 2020-03-17T02:42:25.6545467Z [ERROR] testCommitAfterPersist(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6546001Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6546671Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6547289Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6547916Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6548470Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6549054Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6549675Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6550350Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6550693Z 2020-03-17T02:42:25.6551139Z [ERROR] testCommitAfterPersist(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6551684Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6552224Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6552850Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6553467Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6554044Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6554948Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6555325Z 2020-03-17T02:42:25.6555730Z [ERROR] testRecoverWithEmptyState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6556276Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6556825Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6557467Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6558074Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6558750Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6559330Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6559962Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6560614Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6560961Z 2020-03-17T02:42:25.6561429Z [ERROR] testRecoverWithEmptyState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6561994Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6562533Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6563183Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6563849Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6564454Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6565279Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6565667Z 2020-03-17T02:42:25.6566084Z [ERROR] testRecoverFromIntermWithoutAdditionalState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6566671Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6567202Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6567836Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6568450Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6569032Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6569621Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6570675Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6571362Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6571694Z 2020-03-17T02:42:25.6572114Z [ERROR] testRecoverFromIntermWithoutAdditionalState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6572712Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6573269Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6573904Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6574650Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6575262Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6575918Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6576389Z 2020-03-17T02:42:25.6576854Z [ERROR] testCallingDeleteObjectTwiceDoesNotThroughException(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6577466Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6578013Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6578636Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6579255Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6579817Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6580473Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6581140Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6581834Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6582161Z 2020-03-17T02:42:25.6582616Z [ERROR] testCallingDeleteObjectTwiceDoesNotThroughException(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6583207Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6583764Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6584393Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6585170Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6585780Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6586416Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6586789Z 2020-03-17T02:42:25.6587189Z [ERROR] testCommitAfterNormalClose(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6587737Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6588271Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6588902Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6589516Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6590121Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6590703Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6591389Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6591996Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6592343Z 2020-03-17T02:42:25.6592733Z [ERROR] testCommitAfterNormalClose(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6593375Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6593904Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6594693Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6595335Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6595920Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6596553Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6596930Z 2020-03-17T02:42:25.6597326Z [ERROR] testRecoverWithStateWithMultiPart(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6597899Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6598574Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6599207Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6599814Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6600443Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6601091Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6601701Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6602336Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6602667Z 2020-03-17T02:42:25.6603066Z [ERROR] testRecoverWithStateWithMultiPart(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6603646Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6604188Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6604982Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6605599Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6606174Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6606826Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6607207Z 2020-03-17T02:42:25.6607663Z [ERROR] testRecoverFromIntermWithoutAdditionalStateWithMultiPart(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.001 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6608277Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6608827Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6609437Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6610075Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6610733Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6611378Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6611992Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6612600Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6612925Z 2020-03-17T02:42:25.6613378Z [ERROR] testRecoverFromIntermWithoutAdditionalStateWithMultiPart(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6613979Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6614617Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6615320Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6616048Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6616634Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6617264Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6617636Z 2020-03-17T02:42:25.6618025Z [ERROR] testRecoverWithState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.001 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6618565Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6619104Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6619746Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6620404Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6620980Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6621597Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6622229Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6622827Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6623172Z 2020-03-17T02:42:25.6623550Z [ERROR] testRecoverWithState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.001 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6624108Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6624780Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6625433Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6626040Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6626633Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6627265Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6627725Z 2020-03-17T02:42:25.6628117Z [ERROR] testCleanupRecoverableState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6628686Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6629217Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6629848Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6630504Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6631134Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6631732Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6632346Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6633035Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6633373Z 2020-03-17T02:42:25.6633768Z [ERROR] testCleanupRecoverableState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6634337Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6635068Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6635709Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6636335Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6636913Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6637578Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6638019Z 2020-03-17T02:42:25.6638426Z [ERROR] testCommitAfterRecovery(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6638976Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6639529Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6640196Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6640822Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6641442Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6642030Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6642665Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6643276Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6643613Z 2020-03-17T02:42:25.6644010Z [ERROR] testCommitAfterRecovery(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6644720Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6645275Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6645932Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6646533Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6647134Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6647762Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6648139Z 2020-03-17T02:42:25.6648547Z [ERROR] testRecoverAfterMultiplePersistsState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.001 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6649134Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6649666Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6650421Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6651076Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6651658Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6652239Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6652879Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6653483Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6653826Z 2020-03-17T02:42:25.6654233Z [ERROR] testRecoverAfterMultiplePersistsState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.001 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6654916Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6655451Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6656110Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6656709Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6657292Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6657950Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6658328Z 2020-03-17T02:42:25.6658760Z [ERROR] testRecoverAfterMultiplePersistsStateWithMultiPart(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6659377Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6659905Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6660590Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6661269Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6661911Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6662500Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6663118Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupLocalDir(HadoopS3RecoverableWriterITCase.java:138)2020-03-17T02:42:25.6663741Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.prepare(HadoopS3RecoverableWriterITCase.java:134)2020-03-17T02:42:25.6664076Z 2020-03-17T02:42:25.6664607Z [ERROR] testRecoverAfterMultiplePersistsStateWithMultiPart(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6665213Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6665753Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6666406Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6667094Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6667679Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6668327Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanupAndCheckTmpCleanup(HadoopS3RecoverableWriterITCase.java:158)2020-03-17T02:42:25.6668690Z 2020-03-17T02:42:25.6669029Z [ERROR] org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase Time elapsed: 0.002 s &lt;&lt;&lt; ERROR!2020-03-17T02:42:25.6669521Z java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6670091Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6670733Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanUp(HadoopS3RecoverableWriterITCase.java:123)2020-03-17T02:42:25.6671361Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T02:42:25.6671927Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getFileSystem(HadoopS3RecoverableWriterITCase.java:174)2020-03-17T02:42:25.6672555Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.cleanUp(HadoopS3RecoverableWriterITCase.java:123)Also, the end 2 end tests are failing:2020-03-17T03:46:29.1079571Z 2020-03-17T03:46:29.1084340Z org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.2020-03-17T03:46:29.1085345Z at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335)2020-03-17T03:46:29.1086049Z at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205)2020-03-17T03:46:29.1086659Z at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:142)2020-03-17T03:46:29.1088954Z at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:664)2020-03-17T03:46:29.1092911Z at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:213)2020-03-17T03:46:29.1093723Z at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:895)2020-03-17T03:46:29.1095120Z at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:968)2020-03-17T03:46:29.1095752Z at java.security.AccessController.doPrivileged(Native Method)2020-03-17T03:46:29.1096250Z at javax.security.auth.Subject.doAs(Subject.java:422)2020-03-17T03:46:29.1096804Z at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)2020-03-17T03:46:29.1097517Z at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)2020-03-17T03:46:29.1098167Z at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:968)2020-03-17T03:46:29.1098932Z Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.2020-03-17T03:46:29.1099822Z at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:199)2020-03-17T03:46:29.1100632Z at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1741)2020-03-17T03:46:29.1101367Z at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:90)2020-03-17T03:46:29.1102043Z at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:58)2020-03-17T03:46:29.1102869Z at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1620)2020-03-17T03:46:29.1103466Z at StreamingFileSinkProgram.main(StreamingFileSinkProgram.java:77)2020-03-17T03:46:29.1103948Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-03-17T03:46:29.1104447Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-03-17T03:46:29.1105005Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-03-17T03:46:29.1105525Z at java.lang.reflect.Method.invoke(Method.java:498)2020-03-17T03:46:29.1106030Z at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321)2020-03-17T03:46:29.1106468Z ... 11 more2020-03-17T03:46:29.1106966Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.2020-03-17T03:46:29.1107615Z at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)2020-03-17T03:46:29.1108421Z at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)2020-03-17T03:46:29.1109196Z at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1736)2020-03-17T03:46:29.1109706Z ... 20 more2020-03-17T03:46:29.1110142Z Caused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.2020-03-17T03:46:29.1110767Z at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$7(RestClusterClient.java:359)2020-03-17T03:46:29.1111370Z at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884)2020-03-17T03:46:29.1111969Z at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:866)2020-03-17T03:46:29.1112556Z at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)2020-03-17T03:46:29.1113126Z at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)2020-03-17T03:46:29.1114124Z at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:274)2020-03-17T03:46:29.1114670Z at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)2020-03-17T03:46:29.1115820Z at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)2020-03-17T03:46:29.1116298Z at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)2020-03-17T03:46:29.1116730Z at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575)2020-03-17T03:46:29.1117188Z at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:943)2020-03-17T03:46:29.1117642Z at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)2020-03-17T03:46:29.1118099Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2020-03-17T03:46:29.1118537Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-03-17T03:46:29.1118915Z at java.lang.Thread.run(Thread.java:748)2020-03-17T03:46:29.1119564Z Caused by: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., &lt;Exception on server side:2020-03-17T03:46:29.1120091Z org.apache.flink.runtime.client.JobSubmissionException: Failed to submit job.2020-03-17T03:46:29.1120568Z at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$internalSubmitJob$3(Dispatcher.java:336)2020-03-17T03:46:29.1121079Z at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)2020-03-17T03:46:29.1122414Z at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)2020-03-17T03:46:29.1122937Z at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)2020-03-17T03:46:29.1123372Z at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)2020-03-17T03:46:29.1123965Z at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)2020-03-17T03:46:29.1124442Z at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)2020-03-17T03:46:29.1124852Z at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)2020-03-17T03:46:29.1125279Z at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)2020-03-17T03:46:29.1125691Z at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)2020-03-17T03:46:29.1126205Z Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager2020-03-17T03:46:29.1126730Z at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:36)2020-03-17T03:46:29.1127210Z at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)2020-03-17T03:46:29.1127512Z ... 6 more2020-03-17T03:46:29.1127830Z Caused by: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager2020-03-17T03:46:29.1128573Z at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.&lt;init&gt;(JobManagerRunnerImpl.java:152)2020-03-17T03:46:29.1129303Z at org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.createJobManagerRunner(DefaultJobManagerRunnerFactory.java:84)2020-03-17T03:46:29.1129881Z at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$6(Dispatcher.java:379)2020-03-17T03:46:29.1130394Z at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:34)2020-03-17T03:46:29.1130710Z ... 7 more2020-03-17T03:46:29.1131072Z Caused by: org.apache.flink.util.FlinkRuntimeException: Failed to create checkpoint storage at checkpoint coordinator side.2020-03-17T03:46:29.1131593Z at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.&lt;init&gt;(CheckpointCoordinator.java:301)2020-03-17T03:46:29.1132112Z at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.&lt;init&gt;(CheckpointCoordinator.java:220)2020-03-17T03:46:29.1132772Z at org.apache.flink.runtime.executiongraph.ExecutionGraph.enableCheckpointing(ExecutionGraph.java:490)2020-03-17T03:46:29.1133357Z at org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.buildGraph(ExecutionGraphBuilder.java:338)2020-03-17T03:46:29.1133916Z at org.apache.flink.runtime.scheduler.SchedulerBase.createExecutionGraph(SchedulerBase.java:263)2020-03-17T03:46:29.1134458Z at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:235)2020-03-17T03:46:29.1134986Z at org.apache.flink.runtime.scheduler.SchedulerBase.&lt;init&gt;(SchedulerBase.java:223)2020-03-17T03:46:29.1135581Z at org.apache.flink.runtime.scheduler.DefaultScheduler.&lt;init&gt;(DefaultScheduler.java:118)2020-03-17T03:46:29.1136099Z at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:103)2020-03-17T03:46:29.1136589Z at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:281)2020-03-17T03:46:29.1137028Z at org.apache.flink.runtime.jobmaster.JobMaster.&lt;init&gt;(JobMaster.java:269)2020-03-17T03:46:29.1137573Z at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:98)2020-03-17T03:46:29.1138251Z at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:40)2020-03-17T03:46:29.1138826Z at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.&lt;init&gt;(JobManagerRunnerImpl.java:146)2020-03-17T03:46:29.1139163Z ... 10 more2020-03-17T03:46:29.1139470Z Caused by: java.io.IOException: null uri host. This can be caused by unencoded / in the password string2020-03-17T03:46:29.1140143Z at org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.create(AbstractS3FileSystemFactory.java:163)2020-03-17T03:46:29.1140672Z at org.apache.flink.core.fs.PluginFileSystemFactory.create(PluginFileSystemFactory.java:61)2020-03-17T03:46:29.1141170Z at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:468)2020-03-17T03:46:29.1141618Z at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:389)2020-03-17T03:46:29.1142008Z at org.apache.flink.core.fs.Path.getFileSystem(Path.java:298)2020-03-17T03:46:29.1142644Z at org.apache.flink.runtime.state.memory.MemoryBackendCheckpointStorage.&lt;init&gt;(MemoryBackendCheckpointStorage.java:85)2020-03-17T03:46:29.1143267Z at org.apache.flink.runtime.state.memory.MemoryStateBackend.createCheckpointStorage(MemoryStateBackend.java:295)2020-03-17T03:46:29.1144068Z at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.&lt;init&gt;(CheckpointCoordinator.java:298)2020-03-17T03:46:29.1144448Z ... 23 more2020-03-17T03:46:29.1144843Z Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string2020-03-17T03:46:29.1145321Z at java.util.Objects.requireNonNull(Objects.java:228)2020-03-17T03:46:29.1145777Z at org.apache.hadoop.fs.s3native.S3xLoginHelper.buildFSURI(S3xLoginHelper.java:69)2020-03-17T03:46:29.1146355Z at org.apache.hadoop.fs.s3a.S3AFileSystem.setUri(S3AFileSystem.java:467)2020-03-17T03:46:29.1147156Z at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:234)2020-03-17T03:46:29.1148468Z at org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.create(AbstractS3FileSystemFactory.java:126)2020-03-17T03:46:29.1148901Z ... 30 more2020-03-17T03:46:29.1149032Z 2020-03-17T03:46:29.1149237Z End of exception on server side&gt;]2020-03-17T03:46:29.1149603Z at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:390)2020-03-17T03:46:29.1150123Z at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:374)2020-03-17T03:46:29.1150761Z at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966)2020-03-17T03:46:29.1151980Z at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)2020-03-17T03:46:29.1156319Z ... 4 more2020-03-17T03:46:29.1288717Z Job could not be submitted.2020-03-17T03:46:29.8870683Z 2020-03-17T03:46:30.6358869Z rm: cannot remove '/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/flink-shaded-netty-tcnative-static-*.jar': No such file or directory2020-03-17T03:46:30.8096233Z ad0803bd62b2c6245dcfc51f4a5345bf69f56d60c1bc4827792dda44f72164f02020-03-17T03:46:30.8578725Z ad0803bd62b2c6245dcfc51f4a5345bf69f56d60c1bc4827792dda44f72164f02020-03-17T03:46:30.8607224Z [FAIL] Test script contains errors.2020-03-17T03:46:30.8614963Z Checking of logs skipped.2020-03-17T03:46:30.8615321Z 2020-03-17T03:46:30.8616405Z [FAIL] 'Streaming File Sink s3 end-to-end test' failed after 0 minutes and 42 seconds! Test exited with exit code 1The YARN tests are also affected[ERROR] Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 1.156 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.yarn.YarnFileStageTestS3ITCase[ERROR] testRecursiveUploadForYarnS3a(org.apache.flink.yarn.YarnFileStageTestS3ITCase) Time elapsed: 0.592 s &lt;&lt;&lt; ERROR!java.io.IOException: Cannot instantiate file system for URI: s3a://$(IT_CASE_S3_BUCKET)/temp/tests-e694ca1f-5578-4b67-b936-7db933e35da0 at org.apache.flink.yarn.YarnFileStageTestS3ITCase.testRecursiveUploadForYarn(YarnFileStageTestS3ITCase.java:159) at org.apache.flink.yarn.YarnFileStageTestS3ITCase.testRecursiveUploadForYarnS3a(YarnFileStageTestS3ITCase.java:199)Caused by: java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string at org.apache.flink.yarn.YarnFileStageTestS3ITCase.testRecursiveUploadForYarn(YarnFileStageTestS3ITCase.java:159) at org.apache.flink.yarn.YarnFileStageTestS3ITCase.testRecursiveUploadForYarnS3a(YarnFileStageTestS3ITCase.java:199)</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.testutils.s3.S3TestCredentials.java</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug id="16663" opendate="2020-3-18 00:00:00" fixdate="2020-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Docs version 1.10 missing from version picker dropdown</summary>
      <description>When going to the latest master docs &amp;#91;1&amp;#93;, the documentation version 1.10 is not shown in the "Pick Docs Version" dropdown menu.&amp;#91;1&amp;#93; https://ci.apache.org/projects/flink/flink-docs-master/</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..config.yml</file>
    </fixedFiles>
  </bug>
  <bug id="16702" opendate="2020-3-20 00:00:00" fixdate="2020-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>develop JDBCCatalogFactory, descriptor, and validator for service discovery</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.catalog.AbstractJDBCCatalog.java</file>
      <file type="M">flink-connectors.flink-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16741" opendate="2020-3-24 00:00:00" fixdate="2020-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web UI: Enable listing TM Logs and displaying Logs by Filename</summary>
      <description>add log list and read log by name for taskmanager in the web</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.refresh-download.refresh-download.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.refresh-download.refresh-download.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.refresh-download.refresh-download.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.navigation.navigation.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.task-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.task-manager.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.task-manager-routing.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.stdout.task-manager-stdout.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.stdout.task-manager-stdout.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.stdout.task-manager-stdout.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.status.task-manager-status.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.logs.task-manager-logs.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.logs.task-manager-logs.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.logs.task-manager-logs.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.stdout.job-manager-stdout.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.logs.job-manager-logs.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.task-manager.ts</file>
    </fixedFiles>
  </bug>
  <bug id="16744" opendate="2020-3-24 00:00:00" fixdate="2020-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement API to persist channel state</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.over.BufferDataOverWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamTaskBuilder.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamTask.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironmentBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DummyInvokable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.CheckpointingOperation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestTaskStateManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskStateManagerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskStateManager.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointTaskStateManager.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.BufferBuilderAndConsumerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.BufferBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="16748" opendate="2020-3-24 00:00:00" fixdate="2020-4-24 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add Python UDTF doc</summary>
      <description>Currently Python UDTF has been supported in coming release 1.11, so we should add relevant docs.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.python.python.udfs.zh.md</file>
      <file type="M">docs.dev.table.python.python.udfs.md</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="16749" opendate="2020-3-24 00:00:00" fixdate="2020-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to set node selector for JM/TM pod</summary>
      <description>The node-selector is a collection of key/value pairs to constrain a pod to only be able to run on particular node(s). Since affinity and anti-affinity are uncommon use case for Flink, so we leave the support in pod template. public static final ConfigOption&lt;Map&lt;String, String&gt;&gt; JOB_MANAGER_NODE_SELECTOR = key("kubernetes.jobmanager.node-selector") .mapType() .noDefaultValue() .withDescription("The node selector to be set for JobManager pod. Specified as key:value pairs separated by " + "commas. For example, environment:production,disk:ssd.");public static final ConfigOption&lt;Map&lt;String, String&gt;&gt; TASK_MANAGER_NODE_SELECTOR = key("kubernetes.taskmanager.node-selector") .mapType() .noDefaultValue() .withDescription("The node selector to be set for TaskManager pods. Specified as key:value pairs separated by " + "commas. For example, environment:production,disk:ssd.");</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.KubernetesTaskManagerTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.KubernetesJobManagerTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesTaskManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs..includes.generated.kubernetes.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="16768" opendate="2020-3-25 00:00:00" fixdate="2020-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HadoopS3RecoverableWriterITCase.testRecoverWithStateWithMultiPart hangs</summary>
      <description>Logs: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6584&amp;view=logs&amp;j=d44f43ce-542c-597d-bf94-b0718c71e5e8&amp;t=d26b3528-38b0-53d2-05f7-37557c2405e42020-03-24T15:52:18.9196862Z "main" #1 prio=5 os_prio=0 tid=0x00007fd36c00b800 nid=0xc21 runnable [0x00007fd3743ce000]2020-03-24T15:52:18.9197235Z java.lang.Thread.State: RUNNABLE2020-03-24T15:52:18.9197536Z at java.net.SocketInputStream.socketRead0(Native Method)2020-03-24T15:52:18.9197931Z at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)2020-03-24T15:52:18.9198340Z at java.net.SocketInputStream.read(SocketInputStream.java:171)2020-03-24T15:52:18.9198749Z at java.net.SocketInputStream.read(SocketInputStream.java:141)2020-03-24T15:52:18.9199171Z at sun.security.ssl.InputRecord.readFully(InputRecord.java:465)2020-03-24T15:52:18.9199840Z at sun.security.ssl.InputRecord.readV3Record(InputRecord.java:593)2020-03-24T15:52:18.9200265Z at sun.security.ssl.InputRecord.read(InputRecord.java:532)2020-03-24T15:52:18.9200663Z at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975)2020-03-24T15:52:18.9201213Z - locked &lt;0x00000000927583d8&gt; (a java.lang.Object)2020-03-24T15:52:18.9201589Z at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933)2020-03-24T15:52:18.9202026Z at sun.security.ssl.AppInputStream.read(AppInputStream.java:105)2020-03-24T15:52:18.9202583Z - locked &lt;0x0000000092758c00&gt; (a sun.security.ssl.AppInputStream)2020-03-24T15:52:18.9203029Z at org.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:137)2020-03-24T15:52:18.9203558Z at org.apache.http.impl.io.SessionInputBufferImpl.read(SessionInputBufferImpl.java:198)2020-03-24T15:52:18.9204121Z at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:176)2020-03-24T15:52:18.9204626Z at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:135)2020-03-24T15:52:18.9205121Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9205679Z at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)2020-03-24T15:52:18.9206164Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9206786Z at com.amazonaws.services.s3.internal.S3AbortableInputStream.read(S3AbortableInputStream.java:125)2020-03-24T15:52:18.9207361Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9207839Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9208327Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9208809Z at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)2020-03-24T15:52:18.9209273Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9210003Z at com.amazonaws.util.LengthCheckInputStream.read(LengthCheckInputStream.java:107)2020-03-24T15:52:18.9210658Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9211154Z at org.apache.hadoop.fs.s3a.S3AInputStream.lambda$read$3(S3AInputStream.java:445)2020-03-24T15:52:18.9211631Z at org.apache.hadoop.fs.s3a.S3AInputStream$$Lambda$42/1936375962.execute(Unknown Source)2020-03-24T15:52:18.9212044Z at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)2020-03-24T15:52:18.9212553Z at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:260)2020-03-24T15:52:18.9212972Z at org.apache.hadoop.fs.s3a.Invoker$$Lambda$23/1457226878.execute(Unknown Source)2020-03-24T15:52:18.9213408Z at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:317)2020-03-24T15:52:18.9213866Z at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:256)2020-03-24T15:52:18.9214273Z at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:231)2020-03-24T15:52:18.9214701Z at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:441)2020-03-24T15:52:18.9215443Z - locked &lt;0x00000000926e88b0&gt; (a org.apache.hadoop.fs.s3a.S3AInputStream)2020-03-24T15:52:18.9215852Z at java.io.DataInputStream.read(DataInputStream.java:149)2020-03-24T15:52:18.9216305Z at org.apache.flink.runtime.fs.hdfs.HadoopDataInputStream.read(HadoopDataInputStream.java:94)2020-03-24T15:52:18.9216781Z at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)2020-03-24T15:52:18.9217187Z at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)2020-03-24T15:52:18.9217571Z at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)2020-03-24T15:52:18.9218108Z - locked &lt;0x00000000926ea000&gt; (a java.io.InputStreamReader)2020-03-24T15:52:18.9218475Z at java.io.InputStreamReader.read(InputStreamReader.java:184)2020-03-24T15:52:18.9218876Z at java.io.BufferedReader.fill(BufferedReader.java:161)2020-03-24T15:52:18.9219261Z at java.io.BufferedReader.readLine(BufferedReader.java:324)2020-03-24T15:52:18.9219890Z - locked &lt;0x00000000926ea000&gt; (a java.io.InputStreamReader)2020-03-24T15:52:18.9220256Z at java.io.BufferedReader.readLine(BufferedReader.java:389)2020-03-24T15:52:18.9220914Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getContentsOfFile(HadoopS3RecoverableWriterITCase.java:423)2020-03-24T15:52:18.9221704Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:411)2020-03-24T15:52:18.9222457Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithMultiPartUploads(HadoopS3RecoverableWriterITCase.java:364)2020-03-24T15:52:18.9223222Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverWithStateWithMultiPart(HadoopS3RecoverableWriterITCase.java:330)2020-03-24T15:52:18.9223817Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-03-24T15:52:18.9224232Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-03-24T15:52:18.9224729Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-03-24T15:52:18.9225160Z at java.lang.reflect.Method.invoke(Method.java:498)2020-03-24T15:52:18.9225675Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)2020-03-24T15:52:18.9226171Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2020-03-24T15:52:18.9226682Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)2020-03-24T15:52:18.9227187Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2020-03-24T15:52:18.9227661Z at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2020-03-24T15:52:18.9228145Z at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2020-03-24T15:52:18.9228718Z at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)2020-03-24T15:52:18.9229112Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-03-24T15:52:18.9229582Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)2020-03-24T15:52:18.9230029Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)2020-03-24T15:52:18.9230525Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)2020-03-24T15:52:18.9230963Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)2020-03-24T15:52:18.9231546Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)2020-03-24T15:52:18.9231999Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)2020-03-24T15:52:18.9232432Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)2020-03-24T15:52:18.9232862Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)2020-03-24T15:52:18.9233307Z at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2020-03-24T15:52:18.9233833Z at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2020-03-24T15:52:18.9234284Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-03-24T15:52:18.9234700Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-03-24T15:52:18.9235076Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2020-03-24T15:52:18.9235599Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)2020-03-24T15:52:18.9236124Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)2020-03-24T15:52:18.9236648Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2020-03-24T15:52:18.9237167Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)2020-03-24T15:52:18.9237688Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2020-03-24T15:52:18.9238244Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2020-03-24T15:52:18.9238745Z at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2020-03-24T15:52:18.9239202Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)2020-03-24T15:52:18.9239549Z 2020-03-24T15:52:18.9239794Z "VM Thread" os_prio=0 tid=0x00007fd36c260800 nid=0xc58 runnable  </description>
      <version>1.10.0,1.11.0,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.watchdog.sh</file>
      <file type="M">tools.ci.test.controller.sh</file>
    </fixedFiles>
  </bug>
  <bug id="16778" opendate="2020-3-25 00:00:00" fixdate="2020-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>the java e2e profile isn&amp;#39;t setting the hadoop switch on Azure</summary>
      <description>Context: https://lists.apache.org/thread.html/r06e597b3dadfee00593989b1cfae0f2b83548f412c8fdca6d4bc3dbe%40%3Cdev.flink.apache.org%3E the azure setup doesn't appear to be equivalent yet since the java e2e profile isn't setting the hadoop switch (-Pe2e-hadoop), as a result of which SQLClientKafkaITCase isn't run</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug id="16789" opendate="2020-3-26 00:00:00" fixdate="2020-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose JMX port via REST API</summary>
      <description>Currently there are no easy way to assign jmxrmi port to a running Flink job.The typical tutorial is to add the following to both TM and JM launch env:-Dcom.sun.management.jmxremote-Dcom.sun.management.jmxremote.port=9999-Dcom.sun.management.jmxremote.local.only=falseHowever, setting the jmxremote port to 9999 is not usually a viable solution when Flink job is running on a shared environment (YARN / K8s / etc).setting -Dcom.sun.management.jmxremote.port=0 is the best option however, there's no easy way to retrieve such port assignment. We proposed to use JMXConnectorServerFactory to explicitly establish a JMXServer inside ClusterEntrypoint &amp; TaskManagerRunner.With the JMXServer explicitly created, we can return the JMXRMI information via either REST API or WebUI.</description>
      <version>1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorToResourceManagerConnectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerDetailsInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.TaskExecutorRegistration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.registration.WorkerRegistration.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerTest.java</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="16795" opendate="2020-3-26 00:00:00" fixdate="2020-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End to end tests timeout on Azure</summary>
      <description>Example: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6650&amp;view=logs&amp;j=08866332-78f7-59e4-4f7e-49a56faa3179 or https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6637&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5##[error]The job running on agent Azure Pipelines 6 ran longer than the maximum time of 200 minutes. For more information, see https://go.microsoft.com/fwlink/?linkid=2077134and ##[error]The operation was canceled.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="16796" opendate="2020-3-26 00:00:00" fixdate="2020-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix The Bug of Python UDTF in SQL Query</summary>
      <description>When executes Python UDTF in sql query, it will cause some problem.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonCorrelate.scala</file>
      <file type="M">flink-python.pyflink.table.tests.test.udtf.py</file>
    </fixedFiles>
  </bug>
  <bug id="16798" opendate="2020-3-26 00:00:00" fixdate="2020-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Logs from BashJavaUtils are not properly preserved and passed into TM logs.</summary>
      <description>With FLINK-15519, in the TM start-up scripts, we have captured logs from BashJavaUtils and passed into the TM JVM process via environment variable. These logs will be merged with other TM logs, writing to same places respecting user's log configurations.This effort was broken in FLINK-15727, where the outputs from BashJavaUtils are thrown away, except for the result JVM parameters and dynamic configurations</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.taskmanager.sh</file>
    </fixedFiles>
  </bug>
  <bug id="16805" opendate="2020-3-26 00:00:00" fixdate="2020-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StreamingKafkaITCase fails with "Could not instantiate instance using default factory."</summary>
      <description>CI: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6654&amp;view=logs&amp;j=fc5181b0-e452-5c8f-68de-1097947f6483&amp;t=27d1d645-cbce-54e2-51c4-d8b45fe246072020-03-26T08:17:42.8881925Z [INFO] T E S T S2020-03-26T08:17:42.8882791Z [INFO] -------------------------------------------------------2020-03-26T08:17:43.6840472Z [INFO] Running org.apache.flink.tests.util.kafka.StreamingKafkaITCase2020-03-26T08:17:43.6933052Z [ERROR] Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 0.006 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.tests.util.kafka.StreamingKafkaITCase2020-03-26T08:17:43.6934567Z [ERROR] testKafka[0: kafka-version:0.10.2.0](org.apache.flink.tests.util.kafka.StreamingKafkaITCase) Time elapsed: 0.004 s &lt;&lt;&lt; ERROR!2020-03-26T08:17:43.6935170Z java.lang.RuntimeException: Could not instantiate instance using default factory.2020-03-26T08:17:43.6935702Z at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.&lt;init&gt;(StreamingKafkaITCase.java:72)2020-03-26T08:17:43.6936024Z 2020-03-26T08:17:43.6936691Z [ERROR] testKafka[1: kafka-version:0.11.0.2](org.apache.flink.tests.util.kafka.StreamingKafkaITCase) Time elapsed: 0 s &lt;&lt;&lt; ERROR!2020-03-26T08:17:43.6937288Z java.lang.RuntimeException: Could not instantiate instance using default factory.2020-03-26T08:17:43.6937789Z at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.&lt;init&gt;(StreamingKafkaITCase.java:72)2020-03-26T08:17:43.6938113Z 2020-03-26T08:17:43.6938890Z [ERROR] testKafka[2: kafka-version:2.2.0](org.apache.flink.tests.util.kafka.StreamingKafkaITCase) Time elapsed: 0 s &lt;&lt;&lt; ERROR!2020-03-26T08:17:43.6939646Z java.lang.RuntimeException: Could not instantiate instance using default factory.2020-03-26T08:17:43.6940153Z at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.&lt;init&gt;(StreamingKafkaITCase.java:72)2020-03-26T08:17:43.6940485Z 2020-03-26T08:17:44.0270048Z [INFO] 2020-03-26T08:17:44.0270457Z [INFO] Results:2020-03-26T08:17:44.0270649Z [INFO] 2020-03-26T08:17:44.0270863Z [ERROR] Errors: 2020-03-26T08:17:44.0271847Z [ERROR] StreamingKafkaITCase.&lt;init&gt;:72 Â» Runtime Could not instantiate instance using ...2020-03-26T08:17:44.0272651Z [ERROR] StreamingKafkaITCase.&lt;init&gt;:72 Â» Runtime Could not instantiate instance using ...2020-03-26T08:17:44.0273487Z [ERROR] StreamingKafkaITCase.&lt;init&gt;:72 Â» Runtime Could not instantiate instance using ...2020-03-26T08:17:44.0274218Z [INFO] 2020-03-26T08:17:44.0274517Z [ERROR] Tests run: 3, Failures: 0, Errors: 3, Skipped: 0</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug id="16834" opendate="2020-3-27 00:00:00" fixdate="2020-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Examples cannot be run from IDE</summary>
      <description>Due to removing the dependency flink-clients from flink-streaming-java, the examples can no longer be executed from the IDE. The problem is that the flink-clients dependency is missing.In order to solve this problem, we need to add the flink-clients dependency to all modules which need it and previously obtained it transitively from flink-streaming-java.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-table-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-table-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16836" opendate="2020-3-27 00:00:00" fixdate="2020-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Losing leadership does not clear rpc connection in JobManagerLeaderListener</summary>
      <description>When losing the leadership the JobManagerLeaderListener closes the current rpcConnection but does not clear the field. This can lead to a failure of JobManagerLeaderListener#reconnect if this method is called after the JobMaster has lost its leadership.I propose to clear the field so that RegisteredRpcConnection#tryReconnect won't be called on a closed rpc connection.</description>
      <version>1.11.0</version>
      <fixedVersion>1.9.3,1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.JobLeaderServiceTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.JobLeaderService.java</file>
    </fixedFiles>
  </bug>
  <bug id="16837" opendate="2020-3-27 00:00:00" fixdate="2020-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable trimStackTrace in surefire plugin</summary>
      <description>Surefire has a trimStackTrace option (enabled by default) which is supposed to cut off the junit parts of stacktraces.However this has various unfortunate side-effects, such as being overzealous and hiding important bits of the stacktrace and hiding suppressed exceptions.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16857" opendate="2020-3-30 00:00:00" fixdate="2020-3-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support partition prune by getPartitions of source</summary>
      <description>Now if a PartitionableTableSource implement the getPartitions, the partition pruner by planner still go to catalog instead of using this method.We need use getPartitions when the PartitionableTableSource implement it.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.testTableSources.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.scala</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="16858" opendate="2020-3-30 00:00:00" fixdate="2020-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose partitioned by grammar</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.resources.org.apache.flink.sql.parser.utils.ParserResource.properties</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.validate.FlinkSqlConformance.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.utils.ParserResource.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="16859" opendate="2020-3-30 00:00:00" fixdate="2020-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce FileSystemTableFactory, FileSystemTableSource, FileSystemTableSink</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TestSinkUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
    </fixedFiles>
  </bug>
  <bug id="1688" opendate="2015-3-11 00:00:00" fixdate="2015-3-11 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add socket sink</summary>
      <description>Add a sink that writes output to socket. I'd consider two options, one which implements a socket server and one which implements a client.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaITCase.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.util.SimpleStringSchema.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.util.SerializationSchema.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.util.RawSchema.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.util.JavaDefaultStringSchema.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.util.DeserializationSchema.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.socket.SocketClientSink.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.RMQTopology.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSource.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSink.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaSimpleConsumerExample.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerExample.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerExample.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.kafka.api.simple.PersistentKafkaSource.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.kafka.api.simple.KafkaTopicUtils.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.kafka.api.KafkaSource.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.kafka.api.KafkaSink.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.flume.FlumeTopology.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.flume.FlumeSource.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.flume.FlumeSink.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.src.main.java.org.apache.flink.streaming.connectors.ConnectorSource.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16885" opendate="2020-3-31 00:00:00" fixdate="2020-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL hive-connector wilcard excludes don&amp;#39;t work on maven 3.1.X</summary>
      <description>The sql-connector-hive modules added in FLINK-16455 use wildcards imports to exclude all transitive dependencies from hive.This is a maven 3.2.1+ feature. This may imply that Flink cannot be properly built anymore with maven 3.1 .</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16887" opendate="2020-3-31 00:00:00" fixdate="2020-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor retraction rules to support inferring ChangelogMode</summary>
      <description>Current retraction machanism only support 2 message kinds (+ and -). However, since FLIP-95, we will introduce more message kinds to users (insert/delete/update_before/update_after). In order to support that, we should first refactor current retraction rules to support ChangelogMode inference. In previous, every node will be attached with a AccMode trait after retraction rule. In the proposed design, we will infer ChangelogMode trait for every node. Design documentation: https://docs.google.com/document/d/1n_iXIQsKT3uiBqENR8j8RdjRhZfzMhhB66QZvx2rFjE/edit?ts=5e8419c1#</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.trait.retractionTraitDefs.scala</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.UpdatableTopNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.TopNFunctionTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.RetractableTopNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.AppendOnlyTopNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepLastRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepFirstRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepLastRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.UpdatableTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.RetractableTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.AppendOnlyTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.AbstractTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepLastRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepLastRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateFunctionHelper.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.MiniBatchGroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.MiniBatchGlobalGroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.GroupTableAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.validation.OverWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.physical.stream.RetractionRulesWithTwoStageAggTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.physical.stream.RetractionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.stream.sql.validation.MatchRecognizeValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SortLimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.ModifiedMonotonicityTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.RetractionRulesWithTwoStageAggTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.RetractionRulesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.sinks.DataStreamTableSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.UpdatingPlanChecker.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelTreeWriterImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RankProcessStrategy.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.ExecNodePlanDumper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.trait.TraitUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.trait.retractionTraits.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.DataStreamQueryOperation.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicity.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonIntermediateTableScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalcBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCorrelateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDeduplicate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGlobalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecIncrementalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecIntermediateTableScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLocalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMatch.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMiniBatchAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTemporalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTemporalSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecValues.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecWatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalRel.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkUpdateAsRetractionTraitInitProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.StreamOptimizeContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.RelNodeBlock.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.FlinkExpandConversionRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecRankRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecRetractionRules.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecSortLimitRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.TwoStageOptimizedAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.DataStreamTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.IntermediateRelTable.scala</file>
    </fixedFiles>
  </bug>
  <bug id="16912" opendate="2020-4-1 00:00:00" fixdate="2020-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce table row write support for parquet writer</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.utils.ParquetSchemaConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="16951" opendate="2020-4-3 00:00:00" fixdate="2020-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate parquet to file system connector</summary>
      <description>Implement ParquetFileSystemFormatFactory</description>
      <version>1.11.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.RowPartitionComputer.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16961" opendate="2020-4-3 00:00:00" fixdate="2020-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Netty 4 to 4.1.44</summary>
      <description>https://nvd.nist.gov/vuln/detail/CVE-2019-20444https://nvd.nist.gov/vuln/detail/CVE-2019-20445</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
      <file type="M">flink-connectors.flink-hbase.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16962" opendate="2020-4-3 00:00:00" fixdate="2020-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Convert DatadogReporter to plugin</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DatadogHttpReporter.java</file>
      <file type="M">docs.monitoring.metrics.zh.md</file>
      <file type="M">docs.monitoring.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="16965" opendate="2020-4-3 00:00:00" fixdate="2020-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Convert Graphite reporter to plugin</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-graphite.src.main.java.org.apache.flink.metrics.graphite.GraphiteReporter.java</file>
      <file type="M">docs.monitoring.metrics.zh.md</file>
      <file type="M">docs.monitoring.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="16966" opendate="2020-4-3 00:00:00" fixdate="2020-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Convert InfluxDB reporter to plugin</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.main.java.org.apache.flink.metrics.influxdb.InfluxdbReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.pom.xml</file>
      <file type="M">docs.monitoring.metrics.zh.md</file>
      <file type="M">docs.monitoring.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="16967" opendate="2020-4-3 00:00:00" fixdate="2020-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Convert Slf4j reporter to plugin</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-slf4j.src.main.java.org.apache.flink.metrics.slf4j.Slf4jReporter.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">docs.monitoring.metrics.zh.md</file>
      <file type="M">docs.monitoring.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="16968" opendate="2020-4-3 00:00:00" fixdate="2020-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Convert StatsD reporter to plugin</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-statsd.src.main.java.org.apache.flink.metrics.statsd.StatsDReporter.java</file>
      <file type="M">docs.monitoring.metrics.zh.md</file>
      <file type="M">docs.monitoring.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="16973" opendate="2020-4-3 00:00:00" fixdate="2020-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Various builds failing with "Corrupted STDOUT by directly writing"</summary>
      <description>https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7028&amp;view=logs&amp;j=c5f0071e-1851-543e-9a45-9ac140befc32&amp;t=f66652e3-384e-5b25-be29-abfea69ea8da (kafka/gelly)https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7021&amp;view=logs&amp;j=b2f046ab-ae17-5406-acdc-240be7e870e4&amp;t=40015e30-d9f1-555e-929f-497bfa903ca8 (libraries)[WARNING] Corrupted STDOUT by directly writing to native stream in forked JVM 1. See FAQ web page and the dump file /__w/3/s/flink-connectors/flink-connector-kafka/target/surefire-reports/2020-04-03T11-40-23_195-jvmRun1.dumpstreamfollowed by:[ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:245)[ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)[ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)[ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)[ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)[ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)[ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)[ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)[ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)[ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)[ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)[ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)[ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)[ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)[ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)[ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)[ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)[ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[ERROR] at java.lang.reflect.Method.invoke(Method.java:498)[ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)[ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)[ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)[ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)[ERROR] -&gt; [Help 1][ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.watchdog.sh</file>
      <file type="M">tools.ci.maven-utils.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.main.java.org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResource.java</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug id="16975" opendate="2020-4-3 00:00:00" fixdate="2020-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add docs for FileSystem connector</summary>
      <description></description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="16976" opendate="2020-4-3 00:00:00" fixdate="2020-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update chinese documentation for ListCheckpointed deprecation</summary>
      <description>The change for the english documentation is in https://github.com/apache/flink/commit/10aadfc6906a1629f7e60eacf087e351ba40d517The original Jira issue is FLINK-6258.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.state.state.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="16979" opendate="2020-4-4 00:00:00" fixdate="2020-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-sql-connector-hive-1.2.2_2.11 doesn&amp;#39;t compile on JDK11</summary>
      <description>Both the jdk11 compile and e2e test failed in this nightly: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7052&amp;view=logs&amp;j=946871de-358d-5815-3994-8175615bc253&amp;t=a4536961-0635-5533-730b-7dc5e128220e[ERROR] Failed to execute goal on project flink-sql-connector-hive-1.2.2_2.11: Could not resolve dependencies for project org.apache.flink:flink-sql-connector-hive-1.2.2_2.11:jar:1.11-SNAPSHOT: Could not find artifact jdk.tools:jdk.tools:jar:1.6 at specified path /usr/lib/jvm/adoptopenjdk-11-hotspot-amd64/../lib/tools.jar -&gt; [Help 1]</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1698" opendate="2015-3-13 00:00:00" fixdate="2015-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add polynomial base feature mapper to ML library</summary>
      <description>Add feature mapper which maps a vector into the polynomial feature space. This can be used as a preprocessing step prior to applying a Learner of Flink's ML library.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.regression.RegressionData.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.regression.MultipleLinearRegressionSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.package.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseMatrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.Transformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.ParameterMap.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.FlinkTools.scala</file>
    </fixedFiles>
  </bug>
  <bug id="16983" opendate="2020-4-4 00:00:00" fixdate="2020-4-4 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support RowType in vectorized Python UDF</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.util.StreamRecordUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.VectorizedColumnBatch.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.ColumnarRow.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.RowArrowReaderWriterTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.BaseRowArrowReaderWriterTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.ArrowUtilsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.ArrowUtils.java</file>
      <file type="M">flink-python.pyflink.table.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udf.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  <bug id="16988" opendate="2020-4-6 00:00:00" fixdate="2020-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add core table source/sink interfaces</summary>
      <description>This will add the most important interfaces for the new source/sink interfaces: DynamicTableSource ScanTableSource extends DynamicTableSource LookupTableSource extends DynamicTableSource DynamicTableSinkAnd some initial ability interfaces: SupportsComputedColumnPushDown SupportsFilterPushDownAll interfaces will have extended JavaDocs.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.TableSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="16989" opendate="2020-4-6 00:00:00" fixdate="2020-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support ScanTableSource in planner</summary>
      <description>Support the ScanTableSource interface in planner.Utility methods for creating type information and the data structure converters might not be implemented yet.Not all changelog modes might be supported initially. This depends on FLINK-16887.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.explain.testStreamTableEnvironmentExplain.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkCalcMergeRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkAggregateRemoveRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TwoStageAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TableAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.PythonCalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.GroupWindowTableAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CorrelateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.ColumnFunctionsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.UnionTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.PartitionableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.JoinReorderTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.TwoStageAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.RemoveRedundantLocalSortAggRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.RemoveRedundantLocalRankRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.RemoveRedundantLocalHashAggRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.EnforceLocalSortAggRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.EnforceLocalHashAggRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SplitPythonConditionFromJoinRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SplitPythonConditionFromCorrelateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonCorrelateSplitRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkLogicalRankRuleForRangeEndTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkLogicalRankRuleForConstantRangeTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkCalcMergeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkAggregateRemoveRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkAggregateExpandDistinctAggregatesRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.ExpressionReductionRulesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.DecomposeGroupingSetsRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.CalcRankTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.CalcPythonCorrelateTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.AggregateReduceGroupingRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.hint.OptionsHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.stringexpr.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.stringexpr.CorrelateStringExpressionTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.PythonCalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.CorrelateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.ColumnFunctionsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.UnionTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SortTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SortLimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveShuffleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.PartitionableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.LimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SortMergeSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SortMergeJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SingleRowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.ShuffledHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.ShuffledHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.JoinReorderTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DeadlockBreakupTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.SortAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.HashAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateReduceGroupingTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.executor.BatchExecutorTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.batch.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sinks.CsvTableSinkFactoryBase.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.factories.TableFactoryUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.CatalogSchemaTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.DatabaseCalciteSchema.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.FlinkCalciteCatalogReader.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.trait.UpdateKind.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicity.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.reuse.SubplanReuser.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecLegacyTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.LegacyTableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.trait.ModifyKindSetTrait.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.trait.UpdateKindTrait.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.explain.testExecuteSqlWithExplainInsert.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.explain.testExecuteSqlWithExplainSelect.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.explain.testExplainSqlWithInsert.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.explain.testExplainSqlWithSelect.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.common.TableFactoryTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TimeAttributeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamingTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamingWithStateTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TestData.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkLogicalRelFactories.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnUniqueness.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeys.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.PhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PushLimitIntoTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.common.CommonLookupJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.MiniBatchIntervalInferRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.explain.testSqlUpdateAndToDataStream.out</file>
    </fixedFiles>
  </bug>
  <bug id="16990" opendate="2020-4-6 00:00:00" fixdate="2020-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support LookupTableSource in planner</summary>
      <description>Support the LookupTableSource interface in planner. Utility methods for the data structure converters might not be implemented yet.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamingTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.InMemoryLookupableTableSource.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TimeAttributeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AsyncLookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecLookupJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.common.CommonLookupJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecLookupJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug id="16992" opendate="2020-4-6 00:00:00" fixdate="2020-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ability interfaces for table source/sink</summary>
      <description>This will add the ability interfaces mentioned in FLIP-95: SupportsWatermarkPushDown SupportsProjectionPushDown + already existing interfaces</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.ScanTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.abilities.SupportsFilterPushDown.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.abilities.SupportsComputedColumnPushDown.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.sink.DynamicTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="16995" opendate="2020-4-6 00:00:00" fixdate="2020-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add new data structure interfaces in table-common</summary>
      <description>This add the new data structure interfaces to table-common.The planner and connector refactoring happens in a separate issue.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.data.RowData.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.ScanTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.sink.DynamicTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="16996" opendate="2020-4-6 00:00:00" fixdate="2020-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor planner and connectors to use new data structures</summary>
      <description>Refactors existing code to use the new data structures interfaces.This issue might be split into smaller subtasks if necessary.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.runners.python.scalar.BaseRowPythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.SplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveVectorizedParquetSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveVectorizedOrcSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveTableInputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveMapredSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableFactory.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.utils.BinaryGenericAsserter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.util.UniformBinaryRowGenerator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.util.StreamRecordUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.util.SegmentsUtilTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.util.ResettableExternalBufferTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.util.GenericRowRecordSortComparator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.util.BinaryRowKeySelector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.util.BaseRowRecordEqualiser.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.util.BaseRowHarnessAssertor.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.typeutils.SqlTimestampSerializerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.typeutils.InternalTypeInfoTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.typeutils.DecimalSerializerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.typeutils.BinaryStringSerializerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.typeutils.BinaryRowTypeInfoTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.typeutils.BinaryRowSerializerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.typeutils.BinaryGenericSerializerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.typeutils.BaseRowSerializerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.typeutils.BaseMapSerializerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.typeutils.BaseArraySerializerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.TypeCheckUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.SqlTimestampTypeInfo.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.SqlTimestampSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.DecimalTypeInfoFactory.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.DecimalTypeInfo.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.DecimalSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.BinaryStringTypeInfoFactory.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.BinaryStringTypeInfo.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.BinaryStringSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.BinaryRowSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.BinaryGenericSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.BaseRowTypeInfo.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.BaseRowSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.BaseMapSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.BaseArraySerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.AbstractRowSerializer.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcColumnarRowSplitReaderTest.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.vector.OrcTimestampColumnVector.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.vector.OrcLongColumnVector.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.vector.OrcDoubleColumnVector.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.vector.OrcDecimalColumnVector.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.vector.OrcBytesColumnVector.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.vector.AbstractOrcColumnVector.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcSplitReaderUtil.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcColumnarRowSplitReader.java</file>
      <file type="M">flink-formats.flink-orc-nohive.src.main.java.org.apache.flink.orc.nohive.vector.OrcNoHiveTimestampVector.java</file>
      <file type="M">flink-formats.flink-orc-nohive.src.main.java.org.apache.flink.orc.nohive.vector.OrcNoHiveLongVector.java</file>
      <file type="M">flink-formats.flink-orc-nohive.src.main.java.org.apache.flink.orc.nohive.vector.OrcNoHiveDoubleVector.java</file>
      <file type="M">flink-formats.flink-orc-nohive.src.main.java.org.apache.flink.orc.nohive.vector.OrcNoHiveDecimalVector.java</file>
      <file type="M">flink-formats.flink-orc-nohive.src.main.java.org.apache.flink.orc.nohive.vector.OrcNoHiveBytesVector.java</file>
      <file type="M">flink-formats.flink-orc-nohive.src.main.java.org.apache.flink.orc.nohive.vector.AbstractOrcNoHiveVector.java</file>
      <file type="M">flink-formats.flink-orc-nohive.src.main.java.org.apache.flink.orc.nohive.OrcNoHiveSplitReaderUtil.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.vector.ParquetColumnarRowSplitReaderTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.row.ParquetRowDataWriterTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.reader.TimestampColumnReader.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.reader.ShortColumnReader.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.reader.RunLengthDecoder.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.reader.LongColumnReader.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.reader.IntColumnReader.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.reader.FloatColumnReader.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.reader.FixedLenBytesColumnReader.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.reader.DoubleColumnReader.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.reader.ColumnReader.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.reader.BytesColumnReader.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.reader.ByteColumnReader.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.reader.BooleanColumnReader.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.reader.AbstractColumnReader.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.ParquetDictionary.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.ParquetDecimalVector.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.ParquetColumnarRowSplitReader.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.row.ParquetRowDataWriter.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.row.ParquetRowDataBuilder.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.ParquetFileSystemFormatFactory.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperatorTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.wmassigners.RowTimeMiniBatchAssginerOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.wmassigners.ProcTimeMiniBatchAssignerOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.window.WindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.window.WindowOperatorContractTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.window.MergingWindowSetTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.window.grouping.HeapWindowsGroupingTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.window.assigners.TumblingWindowAssignerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.window.assigners.SlidingWindowAssignerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.window.assigners.SessionWindowAssignerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.sort.StringRecordComparator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.sort.StringNormalizedKeyComputer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.sort.StreamSortOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.sort.SortUtilTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.sort.RowTimeSortOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.sort.ProcTimeSortOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.sort.IntRecordComparator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.sort.IntNormalizedKeyComputer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.sort.BufferedKVExternalSorterTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.sort.BinaryMergeIteratorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.sort.BinaryExternalSorterTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.UpdatableTopNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.TopNFunctionTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.RetractableTopNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.AppendOnlyTopNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.over.SumAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.over.NonBufferOverWindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.over.BufferDataOverWindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.TimeBoundedStreamJoinTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.String2SortMergeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.String2HashJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.SortMergeJoinIteratorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.RowTimeBoundedStreamJoinTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.RandomSortMergeInnerJoinTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.ProcTimeBoundedStreamJoinTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.LookupJoinHarnessTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.Int2SortMergeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.Int2HashJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.AsyncLookupJoinHarnessTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepLastRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepFirstRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepLastRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepFirstRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateFunctionTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.aggregate.SumHashAggTestOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.aggregate.HashAggTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.aggregate.BytesHashMapTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.hashtable.LongHashTableTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.hashtable.BinaryHashTableTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.RowIterator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.ResettableRowBuffer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.ResettableExternalBuffer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.types.InternalSerializers.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.types.ClassLogicalTypeConverter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.partitioner.BinaryHashPartitioner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperatorFactory.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.wmassigners.RowTimeMiniBatchAssginerOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.wmassigners.ProcTimeMiniBatchAssignerOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.wmassigners.BoundedOutOfOrderWatermarkGenerator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.WindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.TableAggregateWindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.internal.PanedWindowProcessFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.internal.MergingWindowProcessFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.internal.InternalWindowProcessFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.internal.GeneralWindowProcessFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.grouping.WindowsGrouping.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.grouping.HeapWindowsGrouping.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.assigners.WindowAssigner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.assigners.TumblingWindowAssigner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.assigners.SlidingWindowAssigner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.assigners.SessionWindowAssigner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.assigners.CountTumblingWindowAssigner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.assigners.CountSlidingWindowAssigner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.AggregateWindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.values.ValuesInputFormat.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.StreamSortOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.SortUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.SortOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.SortLimitOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.RowTimeSortOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.RankOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.ProcTimeSortOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.LimitOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.BufferedKVExternalSorter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.BinaryMergeIterator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.BinaryKVInMemorySortBuffer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.BinaryKVExternalMerger.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.BinaryInMemorySortBuffer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.BinaryIndexedSortable.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.BinaryExternalSorter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.BinaryExternalMerger.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.BaseTemporalSortOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.UpdatableTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.TopNBuffer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.RetractableTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.AppendOnlyTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.AbstractTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.RowTimeRowsUnboundedPrecedingFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.RowTimeRowsBoundedPrecedingFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.RowTimeRangeUnboundedPrecedingFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.RowTimeRangeBoundedPrecedingFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.ProcTimeUnboundedPrecedingFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.ProcTimeRowsBoundedPrecedingFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.ProcTimeRangeBoundedPrecedingFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.NonBufferOverWindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.frame.UnboundedPrecedingOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.frame.UnboundedOverWindowFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.frame.UnboundedFollowingOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.frame.SlidingOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.frame.RowUnboundedPrecedingOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.frame.RowUnboundedFollowingOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.frame.RowSlidingOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.frame.RangeUnboundedPrecedingOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.frame.RangeUnboundedFollowingOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.frame.RangeSlidingOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.frame.OverWindowFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.frame.OffsetOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.frame.InsensitiveOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.BufferDataOverWindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.over.AbstractRowTimeUnboundedPrecedingOver.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.match.RowtimeProcessFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.match.PatternProcessFunctionRunner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.match.IterativeConditionRunner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.match.BaseRowEventComparator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.TimeBoundedStreamJoin.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.temporal.TemporalRowTimeJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.temporal.TemporalProcessTimeJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.temporal.BaseTwoInputStreamOperatorWithStateRetention.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.StreamingSemiAntiJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.StreamingJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.state.OuterJoinRecordStateViews.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.state.OuterJoinRecordStateView.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.state.JoinRecordStateViews.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.state.JoinRecordStateView.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.state.JoinInputSideSpec.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.AbstractStreamingJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.SortMergeOneSideOuterJoinIterator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.SortMergeJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.SortMergeJoinIterator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.SortMergeInnerJoinIterator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.SortMergeFullOuterJoinIterator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.RowTimeBoundedStreamJoin.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.ProcTimeBoundedStreamJoin.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.OuterJoinPaddingUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.NullAwareJoinHelper.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.lookup.LookupJoinWithCalcRunner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.lookup.AsyncLookupJoinWithCalcRunner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.lookup.AsyncLookupJoinRunner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.HashJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.EmitAwareCollector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepLastRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepFirstRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepLastRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepFirstRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateFunctionHelper.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.RecordCounter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.MiniBatchLocalGroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.MiniBatchIncrementalGroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.MiniBatchGroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.MiniBatchGlobalGroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.GroupTableAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.keyselector.NullBinaryRowKeySelector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.keyselector.BinaryRowKeySelector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.keyselector.BaseRowKeySelector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.io.BinaryRowChannelInputViewIterator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.WrappedRowIterator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.ProbeIterator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.LookupBucketIterator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.LongHybridHashTable.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.LongHashPartition.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.BuildSideIterator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.BinaryHashTable.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.BinaryHashPartition.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.BinaryHashBucketArea.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.generated.WatermarkGenerator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.generated.TableAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.generated.RecordEqualiser.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.generated.RecordComparator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.generated.Projection.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.generated.NormalizedKeyComputer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.generated.NamespaceTableAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.generated.NamespaceAggsHandleFunctionBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.generated.NamespaceAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.generated.JoinCondition.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.generated.HashFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.generated.AggsHandleFunctionBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.generated.AggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlLikeChainChecker.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlFunctionUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlDateTimeUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.context.ExecutionContextImpl.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.context.ExecutionContext.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.RowDataPartitionComputer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSource.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSink.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableFactory.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemFormatFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.StreamTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.BatchTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.PreValidateReWriter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.UserDefinedTableAggFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.DateTimeTestUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.UserDefinedFunctionTestUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TestSinkUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamTestSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamingWithStateTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.ValuesITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SplitAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.TableAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.OverWindowHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.GroupAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.AbstractTwoInputStreamOperatorWithTTLTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.UnionITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.RelTimeIndicatorConverterTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.common.TableFactoryTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.match.PatternTranslatorTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.utils.userDefinedScalarFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.utils.ScalarTypesTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.utils.ScalarOperatorsTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.utils.RowTypeTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.WatermarkGeneratorCodeGenTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.ProjectionCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.HashCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.SortAggCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.BatchAggTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.AggWithoutKeysTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.utils.TestRowDataCsvInputFormat.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.utils.TestCsvFileSystemFormatFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.utils.BaseRowTestUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.RangeInputFormat.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedTableFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.MinWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWsWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunctionWithOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueAggFunctionWithOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunctionWithOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueAggFunctionWithOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstLastValueAggFunctionWithOrderTestBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.AggFunctionTestBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.codegen.SortCodeGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.codegen.LongHashJoinGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.codegen.EqualiserCodeGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.typeutils.TypeInfoCheckUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.sinks.TableSinkUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.ScanUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RexNodeExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.PartitionPruner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRelMdUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecAggRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecWatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecValues.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTemporalSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTemporalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMiniBatchAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMatch.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLocalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecIncrementalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGlobalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDeduplicate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCorrelateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalcBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecValues.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecUnion.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSortAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecNestedLoopJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecJoinBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecHashAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCorrelateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCalcBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.ReturnTypeInference.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.dataview.DataViewUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.WatermarkGeneratorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ValuesCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.sort.SortCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.sort.ComparatorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.SinkCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ProjectionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.over.RangeBoundComparatorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.over.MultiFieldRangeBoundComparatorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.NestedLoopJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.MatchCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.LookupJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.LongHashJoinGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.InputFormatCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.HashCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.GeneratedExpression.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.FunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExpandCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.EqualiserCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CorrelateCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.TimestampDiffCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.TableFunctionCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.StringCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarFunctionCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.MethodCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.FloorCeilCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.DivCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.BridgingSqlFunctionCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CalcCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.ImperativeAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.DistinctAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.DeclarativeAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.WindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.SortWindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.SortAggCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashWindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.AggWithoutKeysCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.AggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.PreValidateReWriter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.utils.KeySelectorUtil.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.MinWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWsWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.converter.ExpressionConverter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.vector.VectorizedColumnBatchTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.vector.ColumnVectorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.SqlTimestampTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.NestedRowTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.DecimalTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.DataFormatTestUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.DataFormatConvertersTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.BinaryStringTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.BinaryRowTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.BinaryArrayTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.BaseRowTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.writable.WritableTimestampVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.writable.WritableShortVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.writable.WritableLongVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.writable.WritableIntVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.writable.WritableFloatVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.writable.WritableDoubleVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.writable.WritableColumnVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.writable.WritableByteVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.writable.WritableBytesVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.writable.WritableBooleanVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.writable.AbstractWritableVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.VectorizedColumnBatch.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.TimestampColumnVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.ShortColumnVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.RowColumnVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.LongColumnVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.IntColumnVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.heap.HeapTimestampVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.heap.HeapShortVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.heap.HeapLongVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.heap.HeapIntVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.heap.HeapFloatVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.heap.HeapDoubleVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.heap.HeapByteVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.heap.HeapBytesVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.heap.HeapBooleanVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.heap.AbstractHeapVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.FloatColumnVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.DoubleColumnVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.Dictionary.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.DecimalColumnVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.ColumnVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.BytesColumnVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.ByteColumnVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.BooleanColumnVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.ArrayColumnVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.util.BinaryRowUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.util.BaseRowUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.UpdatableRow.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.TypeGetterSetters.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.SqlTimestamp.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.ObjectArrayRow.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.NestedRow.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.LazyBinaryFormat.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.JoinedRow.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.GenericRow.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.GenericMap.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.GenericArray.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.Decimal.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.DataFormatConverters.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.ColumnarRow.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.ColumnarArray.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BoxedWrapperRow.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryWriter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryStringUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryString.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinarySection.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryRowWriter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryRow.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryMap.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryGeneric.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryFormat.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryArrayWriter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryArray.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BaseRow.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BaseMap.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BaseArray.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.AbstractBinaryWriter.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.VarCharType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.VarBinaryType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.TypeInformationRawType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.TimestampType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.StructuredType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.RowType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.RawType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.MultisetType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.MapType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.LocalZonedTimestampType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.DecimalType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.CharType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.BinaryType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.ArrayType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.AsyncTableFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.data.StringData.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.data.RowData.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.data.RawValueData.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.data.GenericRowData.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.data.GenericMapData.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.data.GenericArrayData.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.data.DecimalData.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.data.ArrayData.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.typeutils.serializers.python.DecimalSerializerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.typeutils.serializers.python.BaseRowSerializerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.typeutils.serializers.python.BaseMapSerializerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.typeutils.serializers.python.BaseArraySerializerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtilsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.runners.python.table.BaseRowPythonTableFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.runners.python.table.AbstractPythonTableFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.ArrowUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.vectors.ArrowArrayColumnVector.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.vectors.ArrowBigIntColumnVector.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.vectors.ArrowBooleanColumnVector.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.vectors.ArrowDateColumnVector.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.vectors.ArrowDecimalColumnVector.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.vectors.ArrowDoubleColumnVector.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.vectors.ArrowFloatColumnVector.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.vectors.ArrowIntColumnVector.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.vectors.ArrowRowColumnVector.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.vectors.ArrowSmallIntColumnVector.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.vectors.ArrowTimeColumnVector.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.vectors.ArrowTimestampColumnVector.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.vectors.ArrowTinyIntColumnVector.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.vectors.ArrowVarBinaryColumnVector.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.vectors.ArrowVarCharColumnVector.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.vectors.BaseRowArrowReader.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.ArrayWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.BigIntWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.BooleanWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.DateWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.DecimalWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.DoubleWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.FloatWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.IntWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.RowWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.SmallIntWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.TimestampWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.TimeWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.TinyIntWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.VarBinaryWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.writers.VarCharWriter.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.AbstractBaseRowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.BaseRowArrowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.BaseRowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.BaseRowPythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.arrow.BaseRowArrowPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.BaseRowPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.table.BaseRowPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.serializers.python.BaseArraySerializer.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.serializers.python.BaseMapSerializer.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.serializers.python.BaseRowSerializer.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.serializers.python.DecimalSerializer.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.ArrowUtilsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.BaseRowArrowReaderWriterTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.BaseRowArrowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.BaseRowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.table.BaseRowPythonTableFunctionOperatorTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="16997" opendate="2020-4-6 00:00:00" fixdate="2020-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add new factory interfaces and utilities</summary>
      <description>Adds the factory interfaces and necessary utilities for discovering and configuring connectors.This issue will provide a reference implementation how factories, connectors, and formats play together.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.Catalog.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.catalog.completeness.py</file>
    </fixedFiles>
  </bug>
  <bug id="16999" opendate="2020-4-6 00:00:00" fixdate="2020-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Data structure should cover all conversions declared in logical types</summary>
      <description>In order to ensure that we don't loose any type precision or conversion class information in sources and sinks, this issue will add a type integrity test for data structure converters. Also UDFs will benefit from this test.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.connector.source.ScanRuntimeProviderContext.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.connector.source.LookupRuntimeProviderContext.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.connector.source.DataFormatConverterWrapper.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.connector.sink.SinkRuntimeProviderContext.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.connector.sink.DataFormatConverterWrapper.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeTransformations.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.RuntimeConverter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.types.InternalSerializers.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.data.writer.BinaryWriter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.extraction.ExtractionUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.data.RowData.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.data.ArrayData.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.DataTypeFactoryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="17000" opendate="2020-4-6 00:00:00" fixdate="2020-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Type information in sources should cover all data structures</summary>
      <description>In order to ensure that we don't loose any type information in sources, this issue will add a type integrity test for type information converters. See ScanTableSource#Context#createTypeInformation(DataType).</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.typeutils.WrapperTypeInfoTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.WrapperTypeInfo.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.RowDataTypeInfo.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.types.InternalSerializers.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.connector.source.ScanRuntimeProviderContext.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.connector.source.LookupRuntimeProviderContext.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.connector.sink.SinkRuntimeProviderContext.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.BridgingSqlFunctionCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CalcCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.utils.DataTypeUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.utils.DataTypeUtils.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonRowDataSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.debezium.DebeziumJsonFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.debezium.DebeziumJsonDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.canal.CanalJsonFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.canal.CanalJsonDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvRowDataSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroRowDataDeSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroFormatFactoryTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="17002" opendate="2020-4-6 00:00:00" fixdate="2020-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support creating tables using other tables definition</summary>
      <description>We should be able to create a Table based on properties of other tables. This includes merging the properties and creating a new Table based on that.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.MergeTableLikeUtil.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlTableLike.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.TableSchema.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="17004" opendate="2020-4-6 00:00:00" fixdate="2020-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document the CREATE TABLE ... LIKE syntax in english</summary>
      <description>Document the CREATE TABLE ... LIKE syntax in the Flink's documentation.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
    </fixedFiles>
  </bug>
  <bug id="17005" opendate="2020-4-6 00:00:00" fixdate="2020-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate the CREATE TABLE ... LIKE syntax documentation to Chinese</summary>
      <description>Translate the page created in FLINK-17004</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="17009" opendate="2020-4-6 00:00:00" fixdate="2020-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fold API-agnostic documentation into DataStream documentation</summary>
      <description>As per FLIP-42, we want to move most cross-API documentation to the DataStream section and deprecate the DataSet API in the future.We want to go from Project Build Setup Basic API Concepts Streaming (DataStream API) Batch (DataSet API) Table API &amp; SQL Data Types &amp; Serialization Managing Execution Libraries Best Practices API Migration GuidesTo DataStream API Table API / SQL DataSet API</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.scala.api.extensions.zh.md</file>
      <file type="M">docs.dev.scala.api.extensions.md</file>
      <file type="M">docs.dev.java.lambdas.zh.md</file>
      <file type="M">docs.dev.datastream.api.md</file>
      <file type="M">docs.tutorials.datastream.api.md</file>
      <file type="M">docs.redirects.basic.api.concepts.md</file>
      <file type="M">docs.monitoring.metrics.md</file>
      <file type="M">docs.index.md</file>
      <file type="M">docs.dev.table.common.md</file>
      <file type="M">docs.dev.stream.operators.windows.md</file>
      <file type="M">docs.dev.stream.operators.index.md</file>
      <file type="M">docs.dev.parallel.md</file>
      <file type="M">docs.dev.java.lambdas.md</file>
      <file type="M">docs.dev.connectors.cassandra.md</file>
      <file type="M">docs.dev.batch.index.md</file>
      <file type="M">docs.dev.batch.hadoop.compatibility.md</file>
      <file type="M">docs.dev.stream.state.state.md</file>
      <file type="M">docs.dev.user.defined.functions.md</file>
      <file type="M">docs.dev.types.serialization.md</file>
      <file type="M">docs.dev.api.concepts.md</file>
    </fixedFiles>
  </bug>
  <bug id="17010" opendate="2020-4-6 00:00:00" fixdate="2020-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming File Sink s3 end-to-end test fails with "Output hash mismatch"</summary>
      <description>CI: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7099&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f52020-04-06T13:17:38.2460013Z Digest: sha256:a61ed0bca213081b64be94c5e1b402ea58bc549f457c2682a86704dd55231e092020-04-06T13:17:38.2475230Z Status: Downloaded newer image for stedolan/jq:latest2020-04-06T13:18:00.4459693Z Number of produced values 13124/600002020-04-06T13:18:25.3214772Z Number of produced values 18300/600002020-04-06T13:19:06.9767370Z Number of produced values 45366/600002020-04-06T13:20:01.2846102Z Number of produced values 60000/600002020-04-06T13:20:02.5940091Z Cancelling job ff95cd4fd52d10b6540c03cf72b33111.2020-04-06T13:20:03.7862792Z Cancelled job ff95cd4fd52d10b6540c03cf72b33111.2020-04-06T13:20:03.8343709Z Waiting for job (ff95cd4fd52d10b6540c03cf72b33111) to reach terminal state CANCELED ...2020-04-06T13:20:05.8474817Z Job (ff95cd4fd52d10b6540c03cf72b33111) reached terminal state CANCELED2020-04-06T13:20:08.6987955Z FAIL File Streaming Sink: Output hash mismatch. Got 61bb5f161b859759a9829516d96e2bbc, expected 6727342fdd3aae2129e61fc8f433fb6f.2020-04-06T13:20:08.6989364Z head hexdump of actual:2020-04-06T13:20:08.7288989Z 0000000 C o m p l e t e d 2 . 0 K i2020-04-06T13:20:08.7289917Z 0000010 B / 3 4 0 . 7 K i B ( 5 . 32020-04-06T13:20:08.7293001Z 0000020 K i B / s ) w i t h 1 1 02020-04-06T13:20:08.7298661Z 0000030 f i l e ( s ) r e m a i n i2020-04-06T13:20:08.7299371Z 0000040 n g \r d o w n l o a d : s 3 :2020-04-06T13:20:08.7301377Z 0000050 / / f l i n k - i n t e g r a t2020-04-06T13:20:08.7302336Z 0000060 i o n - t e s t s / t e m p / t2020-04-06T13:20:08.7303021Z 0000070 e s t _ s t r e a m i n g _ f i2020-04-06T13:20:08.7303968Z 0000080 l e _ s i n k - 7 b 0 7 7 2 1 22020-04-06T13:20:08.7304790Z 0000090 - d 9 f 8 - 4 0 d 8 - 9 d 0 a -2020-04-06T13:20:08.7305297Z 00000a0 f f 9 f 7 e 9 6 d 7 b d / 0 / p2020-04-06T13:20:08.7306285Z 00000b0 a r t - 2 - 1 t o h o s t d2020-04-06T13:20:08.7307138Z 00000c0 i r / t e m p - t e s t - d i r2020-04-06T13:20:08.7307891Z 00000d0 e c t o r y - 3 5 0 6 5 0 6 7 22020-04-06T13:20:08.7308402Z 00000e0 8 9 / t e m p / t e s t _ s t r2020-04-06T13:20:08.7308870Z 00000f0 e a m i n g _ f i l e _ s i n k2020-04-06T13:20:08.7309579Z 0000100 - 7 b 0 7 7 2 1 2 - d 9 f 8 - 42020-04-06T13:20:08.7310295Z 0000110 0 d 8 - 9 d 0 a - f f 9 f 7 e 92020-04-06T13:20:08.7311022Z 0000120 6 d 7 b d / 0 / p a r t - 2 - 12020-04-06T13:20:08.7311537Z 0000130 \n C o m p l e t e d 2 . 0 K2020-04-06T13:20:08.7312010Z 0000140 i B / 3 4 0 . 7 K i B ( 5 .2020-04-06T13:20:08.7312461Z 0000150 3 K i B / s ) w i t h 1 02020-04-06T13:20:08.7312930Z 0000160 9 f i l e ( s ) r e m a i n2020-04-06T13:20:08.7313393Z 0000170 i n g \r C o m p l e t e d 4 .2020-04-06T13:20:08.7313844Z 0000180 4 K i B / 3 4 0 . 7 K i B 2020-04-06T13:20:08.7314332Z 0000190 ( 9 . 8 K i B / s ) w i t h2020-04-06T13:20:08.7314785Z 00001a0 1 0 9 f i l e ( s ) r e m2020-04-06T13:20:08.7315236Z 00001b0 a i n i n g \r d o w n l o a d :2020-04-06T13:20:08.7315957Z 00001c0 s 3 : / / f l i n k - i n t e2020-04-06T13:20:08.7316672Z 00001d0 g r a t i o n - t e s t s / t e2020-04-06T13:20:08.7317163Z 00001e0 m p / t e s t _ s t r e a m i n2020-04-06T13:20:08.7317869Z 00001f0 g _ f i l e _ s i n k - 7 b 0 72020-04-06T13:20:08.7318579Z 0000200 7 2 1 2 - d 9 f 8 - 4 0 d 8 - 92020-04-06T13:20:08.7319283Z 0000210 d 0 a - f f 9 f 7 e 9 6 d 7 b d2020-04-06T13:20:08.7320032Z 0000220 / 0 / p a r t - 2 - 0 t o h2020-04-06T13:20:08.7320747Z 0000230 o s t d i r / t e m p - t e s t2020-04-06T13:20:08.7321447Z 0000240 - d i r e c t o r y - 3 5 0 6 52020-04-06T13:20:08.7321955Z 0000250 0 6 7 2 8 9 / t e m p / t e s t2020-04-06T13:20:08.7322758Z 0000260 _ s t r e a m i n g _ f i l e _2020-04-06T13:20:08.7323476Z 0000270 s i n k - 7 b 0 7 7 2 1 2 - d 92020-04-06T13:20:08.7324210Z 0000280 f 8 - 4 0 d 8 - 9 d 0 a - f f 92020-04-06T13:20:08.7324690Z 0000290 f 7 e 9 6 d 7 b d / 0 / p a r t2020-04-06T13:20:08.7325360Z 00002a0 - 2 - 0 \n C o m p l e t e d 42020-04-06T13:20:08.7325861Z 00002b0 . 4 K i B / 3 4 0 . 7 K i B2020-04-06T13:20:08.7326493Z 00002c0 ( 9 . 8 K i B / s ) w i t2020-04-06T13:20:08.7327076Z 00002d0 h 1 0 8 f i l e ( s ) r e2020-04-06T13:20:08.7327550Z 00002e0 m a i n i n g \r C o m p l e t e2020-04-06T13:20:08.7328001Z 00002f0 d 8 . 1 K i B / 3 4 0 . 7 2020-04-06T13:20:08.7328465Z 0000300 K i B ( 1 7 . 6 K i B / s )2020-04-06T13:20:08.7328933Z 0000310 w i t h 1 0 8 f i l e ( s2020-04-06T13:20:08.7329401Z 0000320 ) r e m a i n i n g \r d o w n2020-04-06T13:20:08.7329889Z 0000330 l o a d : s 3 : / / f l i n k2020-04-06T13:20:08.7330629Z 0000340 - i n t e g r a t i o n - t e s2020-04-06T13:20:08.7331129Z 0000350 t s / t e m p / t e s t _ s t r2020-04-06T13:20:08.7331608Z 0000360 e a m i n g _ f i l e _ s i n k2020-04-06T13:20:08.7332294Z 0000370 - 7 b 0 7 7 2 1 2 - d 9 f 8 - 42020-04-06T13:20:08.7333106Z 0000380 0 d 8 - 9 d 0 a - f f 9 f 7 e 92020-04-06T13:20:08.7333822Z 0000390 6 d 7 b d / 1 / p a r t - 2 - 12020-04-06T13:20:08.7334310Z 00003a0 2 t o h o s t d i r / t e m2020-04-06T13:20:08.7334982Z 00003b0 p - t e s t - d i r e c t o r y2020-04-06T13:20:08.7335680Z 00003c0 - 3 5 0 6 5 0 6 7 2 8 9 / t e m2020-04-06T13:20:08.7336186Z 00003d0 p / t e s t _ s t r e a m i n g2020-04-06T13:20:08.7336967Z 00003e0 _ f i l e _ s i n k - 7 b 0 7 72020-04-06T13:20:08.7338105Z 00003f0 2 1 2 - d 9 f 8 - 4 0 d 8 - 9 d2020-04-06T13:20:08.7338895Z 0000400 0 a - f f 9 f 7 e 9 6 d 7 b d /2020-04-06T13:20:08.7339640Z 0000410 1 / p a r t - 2 - 1 2 \n C o m p2020-04-06T13:20:08.7340137Z 0000420 l e t e d 8 . 1 K i B / 3 42020-04-06T13:20:08.7340611Z 0000430 0 . 7 K i B ( 1 7 . 6 K i2020-04-06T13:20:08.7341079Z 0000440 B / s ) w i t h 1 0 7 f i2020-04-06T13:20:08.7341534Z 0000450 l e ( s ) r e m a i n i n g \r2020-04-06T13:20:08.7341997Z 0000460 C o m p l e t e d 1 1 . 1 K2020-04-06T13:20:08.7342446Z 0000470 i B / 3 4 0 . 7 K i B ( 2 22020-04-06T13:20:08.7342912Z 0000480 . 9 K i B / s ) w i t h 12020-04-06T13:20:08.7343382Z 0000490 0 7 f i l e ( s ) r e m a i2020-04-06T13:20:08.7343869Z 00004a0 n i n g \r d o w n l o a d : s2020-04-06T13:20:08.7344593Z 00004b0 3 : / / f l i n k - i n t e g r2020-04-06T13:20:08.7345322Z 00004c0 a t i o n - t e s t s / t e m p2020-04-06T13:20:08.7345834Z 00004d0 / t e s t _ s t r e a m i n g _2020-04-06T13:20:08.7346517Z 00004e0 f i l e _ s i n k - 7 b 0 7 7 22020-04-06T13:20:08.7347225Z 00004f0 1 2 - d 9 f 8 - 4 0 d 8 - 9 d 02020-04-06T13:20:08.7348173Z 0000500 a - f f 9 f 7 e 9 6 d 7 b d / 02020-04-06T13:20:08.7348985Z 0000510 / p a r t - 2 - 1 0 t o h o2020-04-06T13:20:08.7349719Z 0000520 s t d i r / t e m p - t e s t -2020-04-06T13:20:08.7350442Z 0000530 d i r e c t o r y - 3 5 0 6 5 02020-04-06T13:20:08.7351115Z 0000540 6 7 2 8 9 / t e m p / t e s t _2020-04-06T13:20:08.7352710Z 0000550 s t r e a m i n g _ f i l e _ s2020-04-06T13:20:08.7353469Z 0000560 i n k - 7 b 0 7 7 2 1 2 - d 9 f2020-04-06T13:20:08.7354013Z 0000570 8 - 4 0 d 8 - 9 d 0 a - f f 9 f2020-04-06T13:20:08.7360064Z 0000580 7 e 9 6 d 7 b d / 0 / p a r t -2020-04-06T13:20:08.7360656Z 0000590 2 - 1 0 \n C o m p l e t e d 12020-04-06T13:20:08.7361032Z 00005a0 1 . 1 K i B / 3 4 0 . 7 K i2020-04-06T13:20:08.7361380Z 00005b0 B ( 2 2 . 9 K i B / s ) w2020-04-06T13:20:08.7361741Z 00005c0 i t h 1 0 6 f i l e ( s ) 2020-04-06T13:20:08.7362090Z 00005d0 r e m a i n i n g \r C o m p l e2020-04-06T13:20:08.7362599Z 00005e0 t e d 1 4 . 1 K i B / 3 4 02020-04-06T13:20:08.7362959Z 00005f0 . 7 K i B ( 2 8 . 9 K i B2020-04-06T13:20:08.7363310Z 0000600 / s ) w i t h 1 0 6 f i l2020-04-06T13:20:08.7363671Z 0000610 e ( s ) r e m a i n i n g \r d2020-04-06T13:20:08.7364043Z 0000620 o w n l o a d : s 3 : / / f l2020-04-06T13:20:08.7364639Z 0000630 i n k - i n t e g r a t i o n -2020-04-06T13:20:08.7364990Z 0000640 t e s t s / t e m p / t e s t _2020-04-06T13:20:08.7365351Z 0000650 s t r e a m i n g _ f i l e _ s2020-04-06T13:20:08.7365884Z 0000660 i n k - 7 b 0 7 7 2 1 2 - d 9 f2020-04-06T13:20:08.7366403Z 0000670 8 - 4 0 d 8 - 9 d 0 a - f f 9 f2020-04-06T13:20:08.7366931Z 0000680 7 e 9 6 d 7 b d / 0 / p a r t -2020-04-06T13:20:08.7367456Z 0000690 2 - 9 t o h o s t d i r / t2020-04-06T13:20:08.7367990Z 00006a0 e m p - t e s t - d i r e c t o2020-04-06T13:20:08.7368507Z 00006b0 r y - 3 5 0 6 5 0 6 7 2 8 9 / t2020-04-06T13:20:08.7368870Z 00006c0 e m p / t e s t _ s t r e a m i2020-04-06T13:20:08.7369399Z 00006d0 n g _ f i l e _ s i n k - 7 b 02020-04-06T13:20:08.7369919Z 00006e0 7 7 2 1 2 - d 9 f 8 - 4 0 d 8 -2020-04-06T13:20:08.7370453Z 00006f0 9 d 0 a - f f 9 f 7 e 9 6 d 7 b2020-04-06T13:20:08.7370969Z 0000700 d / 0 / p a r t - 2 - 9 \n C o m2020-04-06T13:20:08.7371334Z 0000710 p l e t e d 1 4 . 1 K i B /2020-04-06T13:20:08.7371679Z 0000720 3 4 0 . 7 K i B ( 2 8 . 9 2020-04-06T13:20:08.7372043Z 0000730 K i B / s ) w i t h 1 0 5 2020-04-06T13:20:08.7372406Z 0000740 f i l e ( s ) r e m a i n i n2020-04-06T13:20:08.7372751Z 0000750 g \r C o m p l e t e d 1 5 . 12020-04-06T13:20:08.7373109Z 0000760 K i B / 3 4 0 . 7 K i B (2020-04-06T13:20:08.7373603Z 0000770 3 0 . 2 K i B / s ) w i t h2020-04-06T13:20:08.7373961Z 0000780 1 0 5 f i l e ( s ) r e m2020-04-06T13:20:08.7374320Z 0000790 a i n i n g \r C o m p l e t e d2020-04-06T13:20:08.7374664Z 00007a0 1 7 . 5 K i B / 3 4 0 . 7 2020-04-06T13:20:08.7375021Z 00007b0 K i B ( 3 4 . 9 K i B / s )2020-04-06T13:20:08.7375363Z 00007c0 w i t h 1 0 5 f i l e ( s2020-04-06T13:20:08.7375855Z 00007d0 ) r e m a i n i n g \r d o w n2020-04-06T13:20:08.7376278Z 00007e0 l o a d : s 3 : / / f l i n k2020-04-06T13:20:08.7376875Z 00007f0 - i n t e g r a t i o n - t e s2020-04-06T13:20:08.7377242Z 0000800 t s / t e m p / t e s t _ s t r2020-04-06T13:20:08.7377588Z 0000810 e a m i n g _ f i l e _ s i n k2020-04-06T13:20:08.7378125Z 0000820 - 7 b 0 7 7 2 1 2 - d 9 f 8 - 42020-04-06T13:20:08.7378641Z 0000830 0 d 8 - 9 d 0 a - f f 9 f 7 e 92020-04-06T13:20:08.7379170Z 0000840 6 d 7 b d / 0 / p a r t - 2 - 12020-04-06T13:20:08.7379517Z 0000850 3 t o h o s t d i r / t e m2020-04-06T13:20:08.7380042Z 0000860 p - t e s t - d i r e c t o r y2020-04-06T13:20:08.7380579Z 0000870 - 3 5 0 6 5 0 6 7 2 8 9 / t e m2020-04-06T13:20:08.7380931Z 0000880 p / t e s t _ s t r e a m i n g2020-04-06T13:20:08.7381460Z 0000890 _ f i l e _ s i n k - 7 b 0 7 72020-04-06T13:20:08.7381975Z 00008a0 2 1 2 - d 9 f 8 - 4 0 d 8 - 9 d2020-04-06T13:20:08.7382506Z 00008b0 0 a - f f 9 f 7 e 9 6 d 7 b d /2020-04-06T13:20:08.7383035Z 00008c0 0 / p a r t - 2 - 1 3 \n C o m p2020-04-06T13:20:08.7383382Z 00008d0 l e t e d 1 7 . 5 K i B / 32020-04-06T13:20:08.7383740Z 00008e0 4 0 . 7 K i B ( 3 4 . 9 K2020-04-06T13:20:08.7384084Z 00008f0 i B / s ) w i t h 1 0 4 f2020-04-06T13:20:08.7384443Z 0000900 i l e ( s ) r e m a i n i n g2020-04-06T13:20:08.7384817Z 0000910 \r d o w n l o a d : s 3 : / /2020-04-06T13:20:08.7385378Z 0000920 f l i n k - i n t e g r a t i o2020-04-06T13:20:08.7385915Z 0000930 n - t e s t s / t e m p / t e s2020-04-06T13:20:08.7387030Z 0000940 t _ s t r e a m i n g _ f i l e2020-04-06T13:20:08.7387666Z 0000950 _ s i n k - 7 b 0 7 7 2 1 2 - d2020-04-06T13:20:08.7388178Z 0000960 9 f 8 - 4 0 d 8 - 9 d 0 a - f f2020-04-06T13:20:08.7388542Z 0000970 9 f 7 e 9 6 d 7 b d / 1 / p a r2020-04-06T13:20:08.7389057Z 0000980 t - 2 - 1 t o h o s t d i r2020-04-06T13:20:08.7389585Z 0000990 / t e m p - t e s t - d i r e c2020-04-06T13:20:08.7390117Z 00009a0 t o r y - 3 5 0 6 5 0 6 7 2 8 92020-04-06T13:20:08.7390476Z 00009b0 / t e m p / t e s t _ s t r e a2020-04-06T13:20:08.7391006Z 00009c0 m i n g _ f i l e _ s i n k - 72020-04-06T13:20:08.7391591Z 00009d0 b 0 7 7 2 1 2 - d 9 f 8 - 4 0 d2020-04-06T13:20:08.7392129Z 00009e0 8 - 9 d 0 a - f f 9 f 7 e 9 6 d2020-04-06T13:20:08.7392662Z 00009f0 7 b d / 1 / p a r t - 2 - 1 \n C2020-04-06T13:20:08.7393012Z 0000a00 o m p l e t e d 1 7 . 5 K i2020-04-06T13:20:08.7393372Z 0000a10 B / 3 4 0 . 7 K i B ( 3 4 .2020-04-06T13:20:08.7393716Z 0000a20 9 K i B / s ) w i t h 1 02020-04-06T13:20:08.7394073Z 0000a30 3 f i l e ( s ) r e m a i n2020-04-06T13:20:08.7394418Z 0000a40 i n g \r C o m p l e t e d 2 12020-04-06T13:20:08.7394895Z 0000a50 . 3 K i B / 3 4 0 . 7 K i B2020-04-06T13:20:08.7395257Z 0000a60 ( 4 1 . 9 K i B / s ) w i2020-04-06T13:20:08.7395655Z 0000a70 t h 1 0 3 f i l e ( s ) r2020-04-06T13:20:08.7396013Z 0000a80 e m a i n i n g \r d o w n l o a2020-04-06T13:20:08.7396587Z 0000a90 d : s 3 : / / f l i n k - i n2020-04-06T13:20:08.7397265Z 0000aa0 t e g r a t i o n - t e s t s /2020-04-06T13:20:08.7397945Z 0000ab0 t e m p / t e s t _ s t r e a m2020-04-06T13:20:08.7398579Z 0000ac0 i n g _ f i l e _ s i n k - 7 b2020-04-06T13:20:08.7399155Z 0000ad0 0 7 7 2 1 2 - d 9 f 8 - 4 0 d 82020-04-06T13:20:08.7399709Z 0000ae0 - 9 d 0 a - f f 9 f 7 e 9 6 d 72020-04-06T13:20:08.7400276Z 0000af0 b d / 0 / p a r t - 2 - 8 t o2020-04-06T13:20:08.7400835Z 0000b00 h o s t d i r / t e m p - t e2020-04-06T13:20:08.7401405Z 0000b10 s t - d i r e c t o r y - 3 5 02020-04-06T13:20:08.7401876Z 0000b20 6 5 0 6 7 2 8 9 / t e m p / t e2020-04-06T13:20:08.7402338Z 0000b30 s t _ s t r e a m i n g _ f i l2020-04-06T13:20:08.7402888Z 0000b40 e _ s i n k - 7 b 0 7 7 2 1 2 -2020-04-06T13:20:08.7403408Z 0000b50 d 9 f 8 - 4 0 d 8 - 9 d 0 a - f2020-04-06T13:20:08.7403772Z 0000b60 f 9 f 7 e 9 6 d 7 b d / 0 / p a2020-04-06T13:20:08.7404284Z 0000b70 r t - 2 - 8 \n C o m p l e t e d2020-04-06T13:20:08.7404647Z 0000b80 2 1 . 3 K i B / 3 4 0 . 7 2020-04-06T13:20:08.7405008Z 0000b90 K i B ( 4 1 . 9 K i B / s )2020-04-06T13:20:08.7405358Z 0000ba0 w i t h 1 0 2 f i l e ( s2020-04-06T13:20:08.7405721Z 0000bb0 ) r e m a i n i n g \r C o m p2020-04-06T13:20:08.7406065Z 0000bc0 l e t e d 2 5 . 0 K i B / 32020-04-06T13:20:08.7406673Z 0000bd0 4 0 . 7 K i B ( 4 8 . 9 K2020-04-06T13:20:08.7407014Z 0000be0 i B / s ) w i t h 1 0 2 f2020-04-06T13:20:08.7407371Z 0000bf0 i l e ( s ) r e m a i n i n g2020-04-06T13:20:08.7407757Z 0000c00 \r d o w n l o a d : s 3 : / /2020-04-06T13:20:08.7408367Z 0000c10 f l i n k - i n t e g r a t i o2020-04-06T13:20:08.7408893Z 0000c20 n - t e s t s / t e m p / t e s2020-04-06T13:20:08.7409226Z 0000c30 t _ s t r e a m i n g _ f i l e2020-04-06T13:20:08.7409750Z 0000c40 _ s i n k - 7 b 0 7 7 2 1 2 - d2020-04-06T13:20:08.7410268Z 0000c50 9 f 8 - 4 0 d 8 - 9 d 0 a - f f2020-04-06T13:20:08.7410630Z 0000c60 9 f 7 e 9 6 d 7 b d / 0 / p a r2020-04-06T13:20:08.7411158Z 0000c70 t - 2 - 1 1 t o h o s t d i2020-04-06T13:20:08.7411673Z 0000c80 r / t e m p - t e s t - d i r e2020-04-06T13:20:08.7412206Z 0000c90 c t o r y - 3 5 0 6 5 0 6 7 2 82020-04-06T13:20:08.7412553Z 0000ca0 9 / t e m p / t e s t _ s t r e2020-04-06T13:20:08.7413082Z 0000cb0 a m i n g _ f i l e _ s i n k -2020-04-06T13:20:08.7413613Z 0000cc0 7 b 0 7 7 2 1 2 - d 9 f 8 - 4 02020-04-06T13:20:08.7414131Z 0000cd0 d 8 - 9 d 0 a - f f 9 f 7 e 9 62020-04-06T13:20:08.7414815Z 0000ce0 d 7 b d / 0 / p a r t - 2 - 1 12020-04-06T13:20:08.7415245Z 0000cf0 \n C o m p l e t e d 2 5 . 0 2020-04-06T13:20:08.7415604Z 0000d00 K i B / 3 4 0 . 7 K i B ( 42020-04-06T13:20:08.7415949Z 0000d10 8 . 9 K i B / s ) w i t h 2020-04-06T13:20:08.7416309Z 0000d20 1 0 1 f i l e ( s ) r e m a2020-04-06T13:20:08.7416668Z 0000d30 i n i n g \r C o m p l e t e d 2020-04-06T13:20:08.7417013Z 0000d40 2 8 . 6 K i B / 3 4 0 . 7 K2020-04-06T13:20:08.7417370Z 0000d50 i B ( 5 4 . 4 K i B / s ) 2020-04-06T13:20:08.7417714Z 0000d60 w i t h 1 0 1 f i l e ( s )2020-04-06T13:20:08.7418076Z 0000d70 r e m a i n i n g \r d o w n l2020-04-06T13:20:08.7418648Z 0000d80 o a d : s 3 : / / f l i n k -2020-04-06T13:20:08.7419198Z 0000d90 i n t e g r a t i o n - t e s t2020-04-06T13:20:08.7419567Z 0000da0 s / t e m p / t e s t _ s t r e2020-04-06T13:20:08.7420079Z 0000db0 a m i n g _ f i l e _ s i n k -2020-04-06T13:20:08.7420614Z 0000dc0 7 b 0 7 7 2 1 2 - d 9 f 8 - 4 02020-04-06T13:20:08.7421247Z 0000dd0 d 8 - 9 d 0 a - f f 9 f 7 e 9 62020-04-06T13:20:08.7421942Z 0000de0 d 7 b d / 1 / p a r t - 2 - 1 32020-04-06T13:20:08.7422343Z 0000df0 t o h o s t d i r / t e m p2020-04-06T13:20:08.7422945Z 0000e00 - t e s t - d i r e c t o r y -2020-04-06T13:20:08.7423359Z 0000e10 3 5 0 6 5 0 6 7 2 8 9 / t e m p2020-04-06T13:20:08.7423759Z 0000e20 / t e s t _ s t r e a m i n g _2020-04-06T13:20:08.7424370Z 0000e30 f i l e _ s i n k - 7 b 0 7 7 22020-04-06T13:20:08.7424968Z 0000e40 1 2 - d 9 f 8 - 4 0 d 8 - 9 d 02020-04-06T13:20:08.7425577Z 0000e50 a - f f 9 f 7 e 9 6 d 7 b d / 12020-04-06T13:20:08.7426180Z 0000e60 / p a r t - 2 - 1 3 \n 2020-04-06T13:20:08.7426460Z 0000e6b2020-04-06T13:20:13.1615895Z rm: cannot remove '/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/flink-shaded-netty-tcnative-static-*.jar': No such file or directory2020-04-06T13:20:13.3069822Z 5c0f753ccbc8092ee92b422b9efc0e2f8b4895f4512cd28ab6e59b995cabacad2020-04-06T13:20:13.3541676Z 5c0f753ccbc8092ee92b422b9efc0e2f8b4895f4512cd28ab6e59b995cabacad2020-04-06T13:20:13.3570646Z [FAIL] Test script contains errors.2020-04-06T13:20:13.3577141Z Checking of logs skipped.2020-04-06T13:20:13.3577527Z 2020-04-06T13:20:13.3578665Z [FAIL] 'Streaming File Sink s3 end-to-end test' failed after 4 minutes and 38 seconds! Test exited with exit code 1</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.s3.operations.sh</file>
    </fixedFiles>
  </bug>
  <bug id="17014" opendate="2020-4-7 00:00:00" fixdate="2020-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement PipelinedRegionSchedulingStrategy</summary>
      <description>The PipelinedRegionSchedulingStrategy submits one pipelined region to the DefaultScheduler each time. The PipelinedRegionSchedulingStrategy must be aware of the inputs of each pipelined region. It should schedule a region if and only if all the inputs of that region become consumable.PipelinedRegionSchedulingStrategy can implement as below: startScheduling() : schedule all source regions one by one. onPartitionConsumable(partition) : Check all the consumer regions of the notified partition, if all the inputs of a region have turned to be consumable, schedule the region restartTasks(tasksToRestart) : find out all regions which contain the tasks to restart, reschedule those whose inputs are all consumable</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulingResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.SchedulingStrategyUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulingTopology.java</file>
    </fixedFiles>
  </bug>
  <bug id="17021" opendate="2020-4-7 00:00:00" fixdate="2020-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Blink Planner set GlobalDataExchangeMode</summary>
      <description>Blink planner config option "table.exec.shuffle-mode" should be extended to set GlobalDataExchangeMode for a job, values supported are: ALL_EDGES_BLOCKING/batch FORWARD_EDGES_PIPELINED POINTWISE_EDGES_PIPELINED ALL_EDGES_PIPELINED/pipelinedNote that values 'pipelined' and 'batch' are still supported to be compatible: ‘pipelined’ will be treated the same as ‘ALL_EDGES_PIPELINED’ ‘batch’ will be treated the same as as ‘ALL_EDGES_BLOCKING’Blink planner needs to set GlobalDataExchangeMode to StreamGraph according to the config value.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.utils.ExecutorUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.ExecutionConfigOptions.java</file>
      <file type="M">flink-end-to-end-tests.flink-tpcds-test.src.main.java.org.apache.flink.table.tpcds.TpcdsTestProgram.java</file>
      <file type="M">docs..includes.generated.execution.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="17026" opendate="2020-4-7 00:00:00" fixdate="2020-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce a new Kafka connector with new property keys</summary>
      <description>This new Kafka connector should use new interfaces proposed by FLIP-95, e.g. DynamicTableSource, DynamicTableSink, and Factory.The new proposed keys :Old keyNew keyNoteconnector.typeconnector connector.versionN/Amerged into 'connector' keyconnector.topictopic connector.properties.zookeeper.connectproperties.zookeeper.connect connector.properties.bootstrap.serversproperties.bootstrap.servers connector.properties.group.idproperties.group.id connector.startup-modescan.startup.mode connector.specific-offsetsscan.startup.specific-offsets connector.startup-timestamp-millisscan.startup.timestamp-millis connector.sink-partitionersink.partitioner"fixed", or "round-robin", or a class name "org.mycompany.MyPartitioner"connector.sink-partitioner-classN/Amerged into 'sink.partitioner', not needed anymoreformat.typeformat   </description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.TestFormatFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka011TableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010TableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17028" opendate="2020-4-7 00:00:00" fixdate="2020-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce a new HBase connector with new property keys</summary>
      <description>This new hbase connector should use new interfaces proposed by FLIP-95, e.g. DynamicTableSource, DynamicTableSink, and Factory.The new proposed keys :Old keyNew keyNoteconnector.typeconnector connector.versionN/Amerged into 'connector' keyconnector.table-nametable-name connector.zookeeper.quorumzookeeper.quorum connector.zookeeper.znode.parentzookeeper.znode-parent connector.write.buffer-flush.max-sizesink.buffer-flush.max-size connector.write.buffer-flush.max-rowssink.buffer-flush.max-rows connector.write.buffer-flush.intervalsink.buffer-flush.interval   </description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase.src.test.java.org.apache.flink.connector.hbase.util.HBaseTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-hbase.src.test.java.org.apache.flink.connector.hbase.HBaseConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hbase.src.main.java.org.apache.flink.connector.hbase.util.HBaseTypeUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hbase.src.main.java.org.apache.flink.connector.hbase.util.HBaseTableSchema.java</file>
      <file type="M">flink-connectors.flink-connector-hbase.src.main.java.org.apache.flink.connector.hbase.util.HBaseReadWriteHelper.java</file>
      <file type="M">flink-connectors.flink-connector-hbase.src.main.java.org.apache.flink.connector.hbase.source.HBaseRowInputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hbase.src.main.java.org.apache.flink.connector.hbase.sink.HBaseUpsertTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hbase.src.main.java.org.apache.flink.connector.hbase.sink.HBaseUpsertSinkFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="17029" opendate="2020-4-7 00:00:00" fixdate="2020-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce a new JDBC connector with new property keys</summary>
      <description>This new JDBC connector should use new interfaces proposed by FLIP-95, e.g. DynamicTableSource, DynamicTableSink, and Factory.The new proposed keys :Old keyNew keyconnector.typeconnectorconnector.urlurlconnector.tabletable-nameconnector.driverdriverconnector.usernameusernameconnector.passwordpasswordconnector.read.partition.columnscan.partition.columnconnector.read.partition.numscan.partition.numconnector.read.partition.lower-boundscan.partition.lower-boundconnector.read.partition.upper-boundscan.partition.upper-boundconnector.read.fetch-sizescan.fetch-sizeconnector.lookup.cache.max-rowslookup.cache.max-rowsconnector.lookup.cache.ttllookup.cache.ttlconnector.lookup.max-retrieslookup.max-retriesconnector.write.flush.max-rowssink.buffer-flush.max-rowsconnector.write.flush.intervalsink.buffer-flush.intervalconnector.write.max-retriessink.max-retries  </description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcLookupFunctionITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcInputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcDataTypeTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcDataTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.internal.JdbcFullTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCInputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.utils.JdbcTypeUtil.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcUpsertTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.JdbcOutputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.JdbcInputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.TableJdbcUpsertOutputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.options.JdbcReadOptions.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.options.JdbcOptions.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.options.JdbcLookupOptions.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.converter.PostgresRowConverter.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.converter.MySQLRowConverter.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.converter.DerbyRowConverter.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.converter.AbstractJdbcRowConverter.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.AbstractJdbcOutputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.dialect.JdbcDialects.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.dialect.JdbcDialect.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.catalog.JdbcCatalogUtils.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.catalog.AbstractJdbcCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="17030" opendate="2020-4-7 00:00:00" fixdate="2020-5-7 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add primary key syntax to DDL</summary>
      <description>FLIP-87 defines the concept of primary keys and syntax for defining them in the DDL. The new syntax needs to be supported in the DDL.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlTableColumn.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.api.TableSchemaTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.TableSchemaUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.TableSchema.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="17049" opendate="2020-4-8 00:00:00" fixdate="2020-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support implicit conversions of rows</summary>
      <description>@Test public void testCastRowTableFunction() throws Exception { final List&lt;Row&gt; sourceData = Arrays.asList( Row.of(Row.of(1)) ); final List&lt;Row&gt; sinkData = Arrays.asList( Row.of(Row.of(new BigDecimal("1"))) ); TestCollectionTableFactory.reset(); TestCollectionTableFactory.initData(sourceData); tEnv().sqlUpdate("CREATE TABLE SourceTable(s Row(a INT)) WITH ('connector' = 'COLLECTION')"); tEnv().sqlUpdate("CREATE TABLE SinkTable(s ROW(a DECIMAL(10, 2))) WITH ('connector' = 'COLLECTION')"); tEnv().createTemporarySystemFunction("func", RowCastScalarFunction.class); tEnv().sqlUpdate("INSERT INTO SinkTable SELECT func(s) FROM SourceTable"); tEnv().execute("Test Job"); assertThat(TestCollectionTableFactory.getResult(), equalTo(sinkData)); } public static class RowCastScalarFunction extends ScalarFunction { public @DataTypeHint("ROW&lt;f0 INT&gt;") Row eval( @DataTypeHint("ROW&lt;f0 DECIMAL(10, 2)&gt;") Row row) { return Row.of(1); } }fails with:java.lang.AssertionError: use createStructType() instead at org.apache.calcite.sql.type.SqlTypeFactoryImpl.assertBasic(SqlTypeFactoryImpl.java:225) at org.apache.calcite.sql.type.SqlTypeFactoryImpl.createSqlType(SqlTypeFactoryImpl.java:48) at org.apache.flink.table.planner.calcite.FlinkTypeFactory.createSqlType(FlinkTypeFactory.scala:275) at org.apache.calcite.sql.SqlBasicTypeNameSpec.deriveType(SqlBasicTypeNameSpec.java:205) at org.apache.calcite.sql.SqlDataTypeSpec.deriveType(SqlDataTypeSpec.java:222) at org.apache.calcite.sql.SqlDataTypeSpec.deriveType(SqlDataTypeSpec.java:209) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5960) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5845) at org.apache.calcite.sql.SqlDataTypeSpec.accept(SqlDataTypeSpec.java:186) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1800) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1785) at org.apache.calcite.sql.SqlNode.validateExpr(SqlNode.java:260) at org.apache.calcite.sql.SqlOperator.validateCall(SqlOperator.java:423) at org.apache.calcite.sql.SqlFunction.validateCall(SqlFunction.java:199) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateCall(SqlValidatorImpl.java:5552) at org.apache.calcite.sql.SqlCall.validate(SqlCall.java:116) at org.apache.calcite.sql.SqlNode.validateExpr(SqlNode.java:259) at org.apache.calcite.sql.SqlOperator.validateCall(SqlOperator.java:423) at org.apache.calcite.sql.SqlFunction.validateCall(SqlFunction.java:199) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateCall(SqlValidatorImpl.java:5552) at org.apache.calcite.sql.SqlCall.validate(SqlCall.java:116) at org.apache.calcite.sql.SqlNode.validateExpr(SqlNode.java:259) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateExpr(SqlValidatorImpl.java:4306) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelectList(SqlValidatorImpl.java:4284) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3523) at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084) at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059) at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:128) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:107) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:134) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:351) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:149) at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:66) at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:596) at org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.testCastRowTableFunction(FunctionITCase.java:632) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.intellij.rt.execution.application.AppMainV2.main(AppMainV2.java:131)I think the problem is in org.apache.flink.table.planner.functions.inference.TypeInferenceOperandChecker#castTo</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.RowTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="17061" opendate="2020-4-9 00:00:00" fixdate="2020-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unset process/flink memory size from configuration once dynamic worker resource is activated.</summary>
      <description>With FLINK-14106, memory of a TaskExecutor is decided in two steps on active resource managers. SlotManager decides WorkerResourceSpec, including memory used by Flink tasks: task heap, task off-heap, network and managed memory. ResourceManager derives TaskExecutorProcessSpec from WorkerResourceSpec and the configuration, deciding sizes of memory used by Flink framework and JVM: framework heap, framework off-heap, jvm metaspace and jvm overhead.This works fine for now, because both WorkerResourceSpec and TaskExecutorProcessSpec are derived from the same configurations. However, it might cause problem if later we have new SlotManager implementations that decides WorkerResourceSpec dynamically. In such cases, the process/flink sizes in configuration should be ignored, or it may easily lead to configuration conflicts.</description>
      <version>1.11.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="17073" opendate="2020-4-9 00:00:00" fixdate="2020-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Slow checkpoint cleanup causing OOMs</summary>
      <description>A user reported that he sees a decline in checkpoint cleanup speed when upgrading from Flink 1.7.2 to 1.10.0. The result is that a lot of cleanup tasks are waiting in the execution queue occupying memory. Ultimately, the JM process dies with an OOM.Compared to Flink 1.7.2, we introduced a dedicated ioExecutor which is used by the HighAvailabilityServices (FLINK-11851). Before, we use the AkkaRpcService thread pool which was a ForkJoinPool with a max parallelism of 64. Now it is a FixedThreadPool with as many threads as CPU cores. This change might have caused the decline in completed checkpoint discard throughput. This suspicion needs to be validated before trying to fix it!&amp;#91;1&amp;#93; https://lists.apache.org/thread.html/r390e5d775878918edca0b6c9f18de96f828c266a888e34ed30ce8494%40%3Cuser.flink.apache.org%3E</description>
      <version>1.7.3,1.8.0,1.9.0,1.10.0,1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RegionFailoverITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.NotifyCheckpointAbortedITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.CheckpointsUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.RecoverableCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.SerializableRunnable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointsCleaningRunner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreMockitoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.FailoverStrategyCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ExecutionGraphCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDeciderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointMetadataLoadingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorRestoringTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorMasterHooksTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.Checkpoints.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDecider.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorCoordinatorCheckpoints.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointsCleaner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="17081" opendate="2020-4-10 00:00:00" fixdate="2020-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Batch test classes in Blink planner does not extend TestLogger</summary>
      <description>Currently streaming test classes in Blink planner has extended TestLogger while batch test classes hasn't. They should also extend TestLogger for better debugging.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.BatchAbstractTestBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.api.TableUtilsStreamingITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="17082" opendate="2020-4-10 00:00:00" fixdate="2020-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove mocking from SQL client tests</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliTableauResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="17084" opendate="2020-4-10 00:00:00" fixdate="2020-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement type inference for ROW/ARRAY/MAP constructors</summary>
      <description>We can already implement input type inference and output type strategies for functions:BuiltinFunctionDefinitions.ROWBuiltinFunctionDefinitions.ARRAYBuiltinFunctionDefinitions.MAP</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.validation.MapTypeValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.validation.ArrayTypeValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.ArrayTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.PlannerTypeInferenceUtilImpl.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.validation.ArrayTypeValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.MapTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ArrayTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.collection.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.converter.FunctionDefinitionConvertRule.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.TypeStrategiesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.InputTypeStrategiesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeStrategies.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.strategies.WildcardInputTypeStrategy.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.InputTypeStrategies.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.FunctionLookupMock.java</file>
    </fixedFiles>
  </bug>
  <bug id="17096" opendate="2020-4-13 00:00:00" fixdate="2020-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mini-batch group aggregation doesn&amp;#39;t expire state even if state ttl is enabled</summary>
      <description>At the moment, MiniBatch Group Agg include Local/Global doesn`t support State TTL, for streaming job, it will lead to OOM in long time running, so we need to make state data expire after ttl, the solution is that use incremental cleanup feature refer to FLINK-16581</description>
      <version>1.9.0,1.10.0,1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.StreamingSemiAntiJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.StreamingJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.stream.AbstractStreamingJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.MiniBatchIncrementalGroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.MiniBatchGroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.MiniBatchGlobalGroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.GroupTableAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.dataview.StateListView.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.dataview.PerKeyStateDataViewStore.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.TableAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.GroupAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecIncrementalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGlobalGroupAggregate.scala</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.eventtime.WatermarkOutputMultiplexer.java</file>
    </fixedFiles>
  </bug>
  <bug id="17103" opendate="2020-4-13 00:00:00" fixdate="2020-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Supports dynamic table options for Blink planner</summary>
      <description></description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.testTableSources.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.delegation.PlannerContext.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.TableSinkFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.CatalogTable.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.CatalogSinkModifyOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.ConnectorCatalogTable.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogTableImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.dml.RichSqlInsert.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
    </fixedFiles>
  </bug>
  <bug id="17130" opendate="2020-4-14 00:00:00" fixdate="2020-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web UI: Enable listing JM Logs and displaying Logs by Filename</summary>
      <description>add log list and read log by name for jobmanager in the web</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.log-detail.task-manager-log-detail.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager-routing.module.ts</file>
    </fixedFiles>
  </bug>
  <bug id="17131" opendate="2020-4-14 00:00:00" fixdate="2020-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Javadocs broken for master</summary>
      <description>The javadocs for the master branch aren't being displayed on the website for some reason.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17136" opendate="2020-4-14 00:00:00" fixdate="2020-4-14 01:00:00" resolution="Done">
    <buginformation>
      <summary>Rename toplevel DataSet/DataStream section titles</summary>
      <description>According to FLIP-42: Streaming (DataStream API) -&gt; DataStream API Batch (DataSet API) -&gt; DataSet API</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.datastream.api.zh.md</file>
      <file type="M">docs.dev.datastream.api.md</file>
      <file type="M">docs.dev.batch.index.zh.md</file>
      <file type="M">docs.dev.batch.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="17142" opendate="2020-4-14 00:00:00" fixdate="2020-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump ORC version</summary>
      <description>The current dependency version of ORC is 1.4.3 which is little outdated so bump the version to 1.5.6.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-formats.flink-orc.pom.xml</file>
      <file type="M">flink-formats.flink-orc-nohive.src.main.java.org.apache.flink.orc.nohive.vector.AbstractOrcNoHiveVector.java</file>
      <file type="M">flink-formats.flink-orc-nohive.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17148" opendate="2020-4-15 00:00:00" fixdate="2020-4-15 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support converting pandas dataframe to flink table</summary>
      <description>As the title described, the aim of this Jira is to add support of converting a pandas dataframe to a flink table in PyFlink.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.RowDataArrowReaderWriterTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.RowArrowReaderWriterTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.ArrowUtilsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.arrow.AbstractArrowPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.RowDataArrowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.ArrowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.arrow.ArrowPythonScalarFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.ArrowUtils.java</file>
      <file type="M">flink-python.pyflink.table.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.types.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.py</file>
      <file type="M">docs.dev.table.python.index.zh.md</file>
      <file type="M">docs.dev.table.python.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="17149" opendate="2020-4-15 00:00:00" fixdate="2020-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce Debezium format to support reading debezium changelogs</summary>
      <description>Introduce DebeziumFormatFactory and DebeziumRowDeserializationSchema to read debezium changelogs.CREATE TABLE my_table ( ...) WITH ( 'connector'='...', -- e.g. 'kafka' 'format'='debezium-json', 'debezium-json.schema-include'='true' -- default false, Debeizum can be configured to include or exclude the message schema 'debezium-json.ignore-parse-errors'='true' -- default false);</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-formats.flink-json.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
    </fixedFiles>
  </bug>
  <bug id="1716" opendate="2015-3-18 00:00:00" fixdate="2015-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CoCoA algorithm to flink-ml</summary>
      <description>Add the communication efficient distributed dual coordinate ascent algorithm to the flink machine learning library. See CoCoA for the implementation details.I propose to first implement it with hinge loss and l2-norm. This way, it will allow us to train SVMs in parallel.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.recommendation.ALSITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.SparseVectorSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.SparseMatrixSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.regression.MultipleLinearRegression.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.recommendation.ALS.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Vector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.SparseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.SparseMatrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Matrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseMatrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.Transformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.Learner.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.LabeledVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.FlinkTools.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.ChainedTransformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.ChainedLearner.scala</file>
    </fixedFiles>
  </bug>
  <bug id="1717" opendate="2015-3-18 00:00:00" fixdate="2015-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support to read libSVM and SVMLight input files</summary>
      <description>In order to train SVMs, the machine learning library should be able to read standard SVM input file formats. A widespread format is used by libSVM and SMVLight which has the following format:&lt;line&gt; .=. &lt;target&gt; &lt;feature&gt;:&lt;value&gt; &lt;feature&gt;:&lt;value&gt; ... &lt;feature&gt;:&lt;value&gt; # &lt;info&gt;&lt;target&gt; .=. +1 | -1 | 0 | &lt;float&gt; &lt;feature&gt; .=. &lt;integer&gt; | "qid"&lt;value&gt; .=. &lt;float&gt;&lt;info&gt; .=. &lt;string&gt;Details can be found here and here</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.regression.RegressionData.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.feature.PolynomialBaseITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.regression.MultipleLinearRegression.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.SparseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.SparseMatrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseMatrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.feature.PolynomialBase.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.LabeledVector.scala</file>
      <file type="M">flink-staging.flink-ml.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17172" opendate="2020-4-15 00:00:00" fixdate="2020-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-enable debug level logging in Jepsen Tests</summary>
      <description>Since log4j2 was enabled, logs in Jepsen tests are on INFO level. We should re-enable debug level logging in Jepsen Tests.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.db.clj</file>
    </fixedFiles>
  </bug>
  <bug id="17209" opendate="2020-4-17 00:00:00" fixdate="2020-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow users to specify dialect in sql-client yaml</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableConfig.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.TableConfigOptions.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.ExecutionEntry.java</file>
      <file type="M">docs..includes.generated.table.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="17218" opendate="2020-4-17 00:00:00" fixdate="2020-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add recovery to integration tests of unaligned checkpoints</summary>
      <description>Current tests are just smoke tests that spills data without recovery. The tests need to be extended to perform correctness tests and force recovery in some way.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.RecordingChannelStateWriter.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.MockChannelStateWriter.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="17223" opendate="2020-4-17 00:00:00" fixdate="2020-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>System.IO.IOException: No space left on device in misc profile on free Azure builders</summary>
      <description>Builds on the free Azure builders are failing with##[error]Unhandled exception. System.IO.IOException: No space left on device at System.IO.FileStream.WriteNative(ReadOnlySpan`1 source) at System.IO.FileStream.FlushWriteBuffer() at System.IO.FileStream.Flush(Boolean flushToDisk) at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder) at System.Diagnostics.TextWriterTraceListener.Flush() at System.Diagnostics.TraceSource.Flush() at Microsoft.VisualStudio.Services.Agent.TraceManager.Dispose(Boolean disposing) at Microsoft.VisualStudio.Services.Agent.TraceManager.Dispose() at Microsoft.VisualStudio.Services.Agent.HostContext.Dispose(Boolean disposing) at Microsoft.VisualStudio.Services.Agent.HostContext.Dispose() at Microsoft.VisualStudio.Services.Agent.Worker.Program.Main(String[] args)Error reported in diagnostic logs. Please examine the log for more details. - /home/vsts/agents/2.165.2/_diag/Worker_20200414-093250-utc.logSystem.IO.IOException: No space left on device at System.IO.FileStream.WriteNative(ReadOnlySpan`1 source) at System.IO.FileStream.FlushWriteBuffer() at System.IO.FileStream.Flush(Boolean flushToDisk) at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder) at System.Diagnostics.TextWriterTraceListener.Flush() at Microsoft.VisualStudio.Services.Agent.HostTraceListener.WriteHeader(String source, TraceEventType eventType, Int32 id) at Microsoft.VisualStudio.Services.Agent.HostTraceListener.TraceEvent(TraceEventCache eventCache, String source, TraceEventType eventType, Int32 id, String message) at System.Diagnostics.TraceSource.TraceEvent(TraceEventType eventType, Int32 id, String message) at Microsoft.VisualStudio.Services.Agent.Worker.Worker.RunAsync(String pipeIn, String pipeOut) at Microsoft.VisualStudio.Services.Agent.Worker.Program.MainAsync(IHostContext context, String[] args)System.IO.IOException: No space left on device at System.IO.FileStream.WriteNative(ReadOnlySpan`1 source) at System.IO.FileStream.FlushWriteBuffer() at System.IO.FileStream.Flush(Boolean flushToDisk) at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder) at System.Diagnostics.TextWriterTraceListener.Flush() at Microsoft.VisualStudio.Services.Agent.HostTraceListener.WriteHeader(String source, TraceEventType eventType, Int32 id) at Microsoft.VisualStudio.Services.Agent.HostTraceListener.TraceEvent(TraceEventCache eventCache, String source, TraceEventType eventType, Int32 id, String message) at System.Diagnostics.TraceSource.TraceEvent(TraceEventType eventType, Int32 id, String message) at Microsoft.VisualStudio.Services.Agent.Tracing.Error(Exception exception) at Microsoft.VisualStudio.Services.Agent.Worker.Program.MainAsync(IHostContext context, String[] args),##[error]The job running on agent Azure Pipelines 9 ran longer than the maximum time of 240 minutes. For more information, see https://go.microsoft.com/fwlink/?linkid=2077134,##[warning]Agent Azure Pipelines 9 did not respond to a cancelation request with 00:01:00.CI run: https://dev.azure.com/chesnay/flink/_build/results?buildId=205&amp;view=logs&amp;j=764762df-f65b-572b-3d5c-65518c777be4</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="17226" opendate="2020-4-17 00:00:00" fixdate="2020-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Prometheus relocations</summary>
      <description>Now that we load the Prometheus reporters as plugins we should remove the shade-plugin configuration/relocations.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-prometheus.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17227" opendate="2020-4-17 00:00:00" fixdate="2020-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Datadog relocations</summary>
      <description>Now that we load the Datadog reporter as a plugin we should remove the shade-plugin configuration/relocations.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-datadog.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17236" opendate="2020-4-19 00:00:00" fixdate="2020-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add new Training section to Documentation</summary>
      <description>This section will contain pages of content contributed from Ververica's Flink training website.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.concepts.stream-processing.md</file>
      <file type="M">docs.concepts.index.zh.md</file>
      <file type="M">docs.concepts.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="17237" opendate="2020-4-19 00:00:00" fixdate="2020-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Intro to DataStream API page to Training section</summary>
      <description>This page should contain a basic introduction, a complete example, and a pointer to the RideCleansing exercise.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.redirects.tutorials.datastream.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="17286" opendate="2020-4-21 00:00:00" fixdate="2020-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate json to file system connector</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.filesystem.RowPartitionComputerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.SingleDirectoryWriter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.PartitionTempFileManager.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.PartitionPathUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.PartitionLoader.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.GroupedPartitionWriter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSource.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSink.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.DynamicPartitionWriter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.FileSystemITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.utils.TestRowDataCsvInputFormat.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.utils.TestCsvFileSystemFormatFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.ParquetFileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcFileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-formats.flink-json.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.DelimitedInputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="17287" opendate="2020-4-21 00:00:00" fixdate="2020-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable merge commit button</summary>
      <description>Make use of the .asf.yaml feature to disable the GitHub merge commit button.Ideally we just drop this into all repos for consistency.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.create.source.release.sh</file>
    </fixedFiles>
  </bug>
  <bug id="17322" opendate="2020-4-22 00:00:00" fixdate="2020-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable latency tracker would corrupt the broadcast state</summary>
      <description>This bug is reported from user mail list: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Latency-tracking-together-with-broadcast-state-can-cause-job-failure-td34013.htmlExecute BroadcastStateIT#broadcastStateWorksWithLatencyTracking would easily reproduce this problem.From current information, the broadcast element would be corrupt once we enable env.getConfig().setLatencyTrackingInterval(2000). The exception stack trace would be: (based on current master branch)Caused by: java.io.IOException: Corrupt stream, found tag: 84 at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:217) ~[classes/:?] at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:46) ~[classes/:?] at org.apache.flink.runtime.plugable.NonReusingDeserializationDelegate.read(NonReusingDeserializationDelegate.java:55) ~[classes/:?] at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.getNextRecord(SpillingAdaptiveSpanningRecordDeserializer.java:157) ~[classes/:?] at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:123) ~[classes/:?] at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processInput(StreamTwoInputProcessor.java:181) ~[classes/:?] at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:332) ~[classes/:?] at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxStep(MailboxProcessor.java:206) ~[classes/:?] at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:196) ~[classes/:?] at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:505) ~[classes/:?] at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:485) ~[classes/:?] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:720) ~[classes/:?] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:544) ~[classes/:?] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_144]</description>
      <version>1.9.3,1.10.1,1.11.0,1.12.0</version>
      <fixedVersion>1.10.2,1.11.0,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.BroadcastRecordWriterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.BufferConsumer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.BroadcastRecordWriter.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.BufferBuilderAndConsumerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.RecordWriterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.RecordWriterDelegateTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.BufferBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.RecordWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="17323" opendate="2020-4-22 00:00:00" fixdate="2020-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ChannelStateReader rejects requests about unkown channels (Unaligned checkpoints)</summary>
      <description>ChannelStateReader expects requests only for channels or subpartitions that have state.In case of upscaling or starting from scratch this behavior is incorrect. It should return NO_MORE_DATA.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateReaderImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="17325" opendate="2020-4-22 00:00:00" fixdate="2020-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate orc to file system connector</summary>
      <description>Integrate orc to file system connector, so in the sql world, users can create file system table with orc format by DDL, do some reading, writing and streaming writing. And the RowData is the sql data format. The works are: Introduce OrcRowDataInputFormat with partition support. Introduce RowDataVectorizer. Introduce OrcFileSystemFormatFactory.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-orc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17330" opendate="2020-4-23 00:00:00" fixdate="2020-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid scheduling deadlocks caused by cyclic input dependencies between regions</summary>
      <description>Imagine a job like this:A &amp;#8211; (pipelined FORWARD) --&gt; B &amp;#8211; (blocking ALL-to-ALL) --&gt; DA &amp;#8211; (pipelined FORWARD) --&gt; C &amp;#8211; (pipelined FORWARD) --&gt; Dparallelism=2 for all vertices.We will have 2 execution pipelined regions:R1 = {A1, B1, C1, D1}R2 = {A2, B2, C2, D2}R1 has a cross-region input edge (B2-&gt;D1).R2 has a cross-region input edge (B1-&gt;D2).Scheduling deadlock will happen since we schedule a region only when all its inputs are consumable (i.e. blocking partitions to be finished). This is because R1 can be scheduled only if R2 finishes, while R2 can be scheduled only if R1 finishes.To avoid this, one solution is to force a logical pipelined region with intra-region ALL-to-ALL blocking edges to form one only execution pipelined region, so that there would not be cyclic input dependency between regions.Besides that, we should also pay attention to avoid cyclic cross-region POINTWISE blocking edges.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.PipelinedRegionComputeUtilTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.PipelinedRegionComputeUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="17339" opendate="2020-4-23 00:00:00" fixdate="2020-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default planner to blink and update test cases in both planners</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkILoop.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.EnvironmentSettings.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamingTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.TableAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.OverWindowHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.GroupAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.validation.SetOperatorsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.validation.JoinValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.AggTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogViewITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.catalog.CatalogStatisticsTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.catalog.CatalogITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.catalog.CatalogConstraintTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.api.TableUtilsStreamingITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.api.EnvironmentTest.java</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.StreamSQLExample.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.StreamWindowSQLExample.java</file>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.StreamSQLExample.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.api.StreamTableEnvironmentTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.stream.sql.JavaSqlITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.stream.table.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.stream.table.ValuesITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.validation.InsertIntoValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.StreamTableEnvironmentValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.InsertIntoValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.JoinValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.SetOperatorsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.validation.UnsupportedOpsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.TableSourceValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.match.PatternTranslatorTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.AggFunctionHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.GroupAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.MatchHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.harness.TableAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.InsertIntoITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.MatchRecognizeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.SortITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.GroupWindowTableAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.RetractionITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.ExecutionEntry.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-stream-sql-test.src.main.java.org.apache.flink.sql.tests.StreamSQLTestProgram.java</file>
      <file type="M">flink-ml-parent.flink-ml-lib.src.main.java.org.apache.flink.ml.common.MLEnvironment.java</file>
      <file type="M">flink-ml-parent.flink-ml-lib.src.test.java.org.apache.flink.ml.common.MLEnvironmentTest.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.environment.settings.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.testing.test.case.utils.py</file>
    </fixedFiles>
  </bug>
  <bug id="17352" opendate="2020-4-23 00:00:00" fixdate="2020-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>All doc links w/ site.baseurl &amp; link tag are broken</summary>
      <description>Using {{ site.baseurl }}{% link foo.md %}creates a link containing something likehttps://ci.apache.org/projects/flink/flink-docs-master//ci.apache.org/projects/flink/flink-docs-master/foo.htmlThe link tag includes site.baseurl, so no need to include it again.</description>
      <version>1.11.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.tutorials.streaming.analytics.zh.md</file>
      <file type="M">docs.tutorials.streaming.analytics.md</file>
      <file type="M">docs.tutorials.fault.tolerance.zh.md</file>
      <file type="M">docs.tutorials.fault.tolerance.md</file>
      <file type="M">docs.tutorials.event.driven.zh.md</file>
      <file type="M">docs.tutorials.event.driven.md</file>
      <file type="M">docs.tutorials.etl.zh.md</file>
      <file type="M">docs.tutorials.etl.md</file>
      <file type="M">docs.tutorials.datastream.api.zh.md</file>
      <file type="M">docs.tutorials.datastream.api.md</file>
      <file type="M">docs.dev.stream.state.state.md</file>
      <file type="M">docs.dev.stream.state.index.md</file>
      <file type="M">docs.dev.stream.state.broadcast.state.md</file>
      <file type="M">docs.dev.event.time.md</file>
      <file type="M">docs.concepts.timely-stream-processing.zh.md</file>
      <file type="M">docs.concepts.timely-stream-processing.md</file>
      <file type="M">docs.concepts.stateful-stream-processing.zh.md</file>
      <file type="M">docs.concepts.stateful-stream-processing.md</file>
      <file type="M">docs.concepts.index.zh.md</file>
      <file type="M">docs.concepts.index.md</file>
      <file type="M">docs.concepts.flink-architecture.md</file>
    </fixedFiles>
  </bug>
  <bug id="1737" opendate="2015-3-18 00:00:00" fixdate="2015-1-18 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add statistical whitening transformation to machine learning library</summary>
      <description>The statistical whitening transformation &amp;#91;1&amp;#93; is a preprocessing step for different ML algorithms. It decorrelates the individual dimensions and sets its variance to 1.Statistical whitening should be implemented as a Transfomer.Resources:&amp;#91;1&amp;#93; http://en.wikipedia.org/wiki/Whitening_transformation</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.SparseVectorSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.DenseVectorSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Vector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.SparseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseVector.scala</file>
    </fixedFiles>
  </bug>
  <bug id="17370" opendate="2020-4-24 00:00:00" fixdate="2020-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure deterministic type inference extraction</summary>
      <description>The extracted type inference might differ depending on the used JVM. Since order of candidates matters when deriving an implicit type, we need to make the extraction deterministic.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.InputTypeStrategiesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.extraction.TypeInferenceExtractorTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.strategies.OrInputTypeStrategy.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.InputTypeStrategies.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.extraction.utils.TemplateUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.extraction.utils.FunctionMappingExtractor.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.extraction.utils.ExtractionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="17373" opendate="2020-4-24 00:00:00" fixdate="2020-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support the NULL type for function calls</summary>
      <description>According to FLIP-37, the null type is used to represent untyped NULL literals. Those are in particular useful when it comes to passing them to a function call. The new type inference of FLIP-65 is able to process call such as `f(NULL)`.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactoryTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeGeneralizationTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.InputTypeStrategiesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeGeneralization.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.NullType.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeInferenceUtil.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.DataTypes.java</file>
    </fixedFiles>
  </bug>
  <bug id="17383" opendate="2020-4-26 00:00:00" fixdate="2020-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink legacy planner should not use CollectionEnvironment any more</summary>
      <description>As discussed in https://github.com/apache/flink/pull/11794，CollectionEnvironment is not a good practice, as it is not going through all the steps that a regular user program would go. We should change the tests to use LocalEnvironment. commit "Introduce CollectionPipelineExecutor for CollectionEnvironment (c983ac9)" should also be reverted at that moment.</description>
      <version>1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.utils.TableProgramsCollectionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.TableEnvironmentITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="17387" opendate="2020-4-26 00:00:00" fixdate="2020-5-26 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement LookupableTableSource for Hive connector</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemOptions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="17391" opendate="2020-4-26 00:00:00" fixdate="2020-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>sink.rolling-policy.time.interval default value should be bigger</summary>
      <description>Otherwise there is a lot of small files.We should also consider sin.rolling-policy.file.size</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="17395" opendate="2020-4-27 00:00:00" fixdate="2020-4-27 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add the sign and shasum logic for PyFlink wheel packages</summary>
      <description>Add the sign and sha logic for PyFlink wheel packages</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.create.binary.release.sh</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17398" opendate="2020-4-27 00:00:00" fixdate="2020-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filesystem support regex path reading</summary>
      <description>Like: Single file reading wildcard path reading (regex)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.connector.file.table.FileSystemTableSinkStreamingITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.enumerate.NonSplittingRecursiveEnumerator.java</file>
      <file type="M">docs.content.docs.connectors.table.filesystem.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.filesystem.md</file>
    </fixedFiles>
  </bug>
  <bug id="1740" opendate="2015-3-18 00:00:00" fixdate="2015-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StreamExecutionEnvironment is not respecting the setNumberOfExecutionRetries()</summary>
      <description>The value is not passed to the runtime.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.StreamGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="17401" opendate="2020-4-27 00:00:00" fixdate="2020-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Labels to Mesos TM taskinfo object</summary>
      <description>Currently labels are not set in the task info object. A lot of companies can have logic specific to labels on TM. For example in criteo, based on labels we set kerberos env variables and mesos debug capabilities. It is critical for us to be able to pass these label values to the task manager.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParametersTest.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerDriverTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.LaunchableMesosWorker.java</file>
      <file type="M">docs.layouts.shortcodes.generated.mesos.task.manager.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="17404" opendate="2020-4-27 00:00:00" fixdate="2020-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Running HA per-job cluster (rocks, non-incremental) gets stuck killing a non-existing pid in Hadoop 3 build profile</summary>
      <description>CI log: https://api.travis-ci.org/v3/job/678609505/log.txtWaiting for text Completed checkpoint [1-9]* for job 00000000000000000000000000000000 to appear 2 of times in logs...grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directorygrep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directoryStarting standalonejob daemon on host travis-job-e606668f-b674-49c0-8590-e3508e22b99d.grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directorygrep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directoryKilled TM @ 18864kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]Killed TM @ No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#build-times-out-because-no-output-was-receivedThe build has been terminated</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17406" opendate="2020-4-27 00:00:00" fixdate="2020-6-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add documentation about dynamic table options</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
      <file type="M">docs.dev.table.sql.index.zh.md</file>
      <file type="M">docs.dev.table.sql.index.md</file>
      <file type="M">docs.dev.table.config.zh.md</file>
      <file type="M">docs.dev.table.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="17408" opendate="2020-4-27 00:00:00" fixdate="2020-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce GPUDriver</summary>
      <description>Introduce GPUDriver for GPU resource.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17423" opendate="2020-4-28 00:00:00" fixdate="2020-5-28 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Python UDTF in blink planner under batch mode</summary>
      <description>Currently, Python UDTF has been supported under flink planner and blink planner(only stream). This jira dedicates to add Python UDTF support for blink planner under batch mode.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-python.pyflink.table.tests.test.udtf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">docs.dev.table.python.python.udfs.zh.md</file>
      <file type="M">docs.dev.table.python.python.udfs.md</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="17424" opendate="2020-4-28 00:00:00" fixdate="2020-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client end-to-end test (Old planner) Elasticsearch (v7.5.1) failed due to download error</summary>
      <description>`SQL Client end-to-end test (Old planner) Elasticsearch (v7.5.1)` failed in release-1.10 crone job with below error:Preparing Elasticsearch(version=7)...Downloading Elasticsearch from https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.5.1-linux-x86_64.tar.gz ... % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 4 276M 4 13.3M 0 0 28.8M 0 0:00:09 --:--:-- 0:00:09 28.8M 42 276M 42 117M 0 0 80.7M 0 0:00:03 0:00:01 0:00:02 80.7M 70 276M 70 196M 0 0 79.9M 0 0:00:03 0:00:02 0:00:01 79.9M 89 276M 89 248M 0 0 82.3M 0 0:00:03 0:00:03 --:--:-- 82.4Mcurl: (56) GnuTLS recv error (-54): Error in the pull function. % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0curl: (7) Failed to connect to localhost port 9200: Connection refused[FAIL] Test script contains errors.https://api.travis-ci.org/v3/job/680222168/log.txt</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.elasticsearch-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="17425" opendate="2020-4-28 00:00:00" fixdate="2020-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support SupportsFilterPushDown in planner</summary>
      <description>Support the SupportsFilterPushDown interface for ScanTableSource.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.testTableSourceSinks.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.LegacyTableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.LegacyTableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.LegacyTableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.LegacyTableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="17426" opendate="2020-4-28 00:00:00" fixdate="2020-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support SupportsLimitPushDown in planner</summary>
      <description>Support the SupportsLimitPushDown interface for ScanTableSource.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TestLimitableTableSource.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.LimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.LimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.LimitTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.LimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="17427" opendate="2020-4-28 00:00:00" fixdate="2020-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support SupportsPartitionPushDown in planner</summary>
      <description>Support the SupportsPartitionPushDown interface for ScanTableSource.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoLegacyTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoLegacyTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushLimitIntoTableSourceScanRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="17428" opendate="2020-4-28 00:00:00" fixdate="2020-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support SupportsProjectionPushDown in planner</summary>
      <description>Support the SupportsProjectionPushDown interface for ScanTableSource.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.testTableSourceSinks.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamingTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.ScanTableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.ScanTableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoLegacyTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="17431" opendate="2020-4-28 00:00:00" fixdate="2020-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement table DDLs for Hive dialect part 1</summary>
      <description>Will cover CREATE, DROP, DESCRIBE, SHOW table in this ticket.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.type.ExtendedSqlRowTypeNameSpec.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.type.ExtendedSqlCollectionTypeNameSpec.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.dql.SqlRichDescribeTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.constraint.SqlTableConstraint.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlCreateHiveDatabase.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlAlterHiveDatabaseOwner.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlAlterHiveDatabase.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.HiveDDLUtils.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="17432" opendate="2020-4-28 00:00:00" fixdate="2020-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename Tutorials to Training</summary>
      <description>Change Tutorials to Training in the sidebar navigation and headings, and change the URL path as well. The motivation for this change is SEO – folks looking for this kind of content are more likely to be searching for training. </description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.tutorials.streaming.analytics.zh.md</file>
      <file type="M">docs.tutorials.streaming.analytics.md</file>
      <file type="M">docs.tutorials.index.zh.md</file>
      <file type="M">docs.tutorials.index.md</file>
      <file type="M">docs.tutorials.fault.tolerance.zh.md</file>
      <file type="M">docs.tutorials.fault.tolerance.md</file>
      <file type="M">docs.tutorials.event.driven.zh.md</file>
      <file type="M">docs.tutorials.event.driven.md</file>
      <file type="M">docs.tutorials.etl.zh.md</file>
      <file type="M">docs.tutorials.etl.md</file>
      <file type="M">docs.tutorials.datastream.api.zh.md</file>
      <file type="M">docs.tutorials.datastream.api.md</file>
      <file type="M">docs.concepts.index.zh.md</file>
      <file type="M">docs.concepts.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="17434" opendate="2020-4-28 00:00:00" fixdate="2020-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive partitioned source support streaming read</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamTestSink.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.PartitionPathUtils.java</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.src.main.java.org.apache.flink.api.java.hadoop.mapred.wrapper.HadoopInputSplit.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveTableInputSplit.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveTableInputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.JobConfWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTablePartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="17435" opendate="2020-4-28 00:00:00" fixdate="2020-5-28 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Hive non-partitioned source support streaming read</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveTableFileInputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.ConsumeOrder.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemOptions.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveTableInputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveContinuousMonitoringFunction.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="17465" opendate="2020-4-29 00:00:00" fixdate="2020-6-29 01:00:00" resolution="Done">
    <buginformation>
      <summary>Update Chinese user documentation for job manager memory model</summary>
      <description>This is a follow-up for FLINK-16946.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.memory.mem.tuning.zh.md</file>
      <file type="M">docs.ops.memory.mem.trouble.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.tm.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.jobmanager.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.zh.md</file>
      <file type="M">docs.ops.memory.mem.migration.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="17467" opendate="2020-4-29 00:00:00" fixdate="2020-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement aligned savepoint in UC mode</summary>
      <description></description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-stream-stateful-job-upgrade-test.src.main.java.org.apache.flink.streaming.tests.StatefulStreamJobUpgradeTestProgram.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MockSubtaskCheckpointCoordinatorBuilder.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.InputProcessorUtil.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierTracker.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierAligner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.TestInputChannel.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.EventSerializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointOptionsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.Buffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.CheckpointBarrier.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointOptions.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointedInputGate.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierUnaligner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="17471" opendate="2020-4-30 00:00:00" fixdate="2020-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move LICENSE and NOTICE files to root directory of python distribution</summary>
      <description>This is observed and proposed by Robert during 1.10.1 RC1 check:Another question that I had while checking the release was the"apache-flink-1.10.1.tar.gz" binary, which I suppose is the pythondistribution.It does not contain a LICENSE and NOTICE file at the root level (which isokay [1] for binary releases), but in the "pyflink/" directory. There isalso a "deps/" directory, which contains a full distribution of Flink,without any license files.I believe it would be a little bit nicer to have the LICENSE and NOTICEfile in the root directory (if the python wheels format permits) to makesure it is obvious that all binary release contents are covered by thesefiles.http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/VOTE-Release-1-10-1-release-candidate-1-tp40724p40910.html</description>
      <version>1.9.3,1.10.0,1.11.0</version>
      <fixedVersion>1.9.4,1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.MANIFEST.in</file>
    </fixedFiles>
  </bug>
  <bug id="17483" opendate="2020-4-30 00:00:00" fixdate="2020-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update flink-sql-connector-elasticsearch7 NOTICE file to correctly reflect bundled dependencies</summary>
      <description>This issue is found during 1.10.1 RC1 check by Robert, that `com.carrotsearch:hppc` and `com.github:mustachejava` were included into the shaded binary to fix FLINK-16170 but not added into the NOTICE file of flink-sql-connector-elasticsearch7 module. More details please refer to the ML discussion thread.</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="17497" opendate="2020-5-3 00:00:00" fixdate="2020-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Quickstarts Java nightly end-to-end test fails with "class file has wrong version 55.0, should be 52.0"</summary>
      <description>CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=540&amp;view=logs&amp;j=08866332-78f7-59e4-4f7e-49a56faa3179&amp;t=931b3127-d6ee-5f94-e204-48d51cd1c334[ERROR] -&gt; [Help 1][ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureExceptionjava.io.FileNotFoundException: flink-quickstart-java-0.1.jar (No such file or directory) at java.util.zip.ZipFile.open(Native Method) at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:230) at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:160) at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:131) at sun.tools.jar.Main.list(Main.java:1115) at sun.tools.jar.Main.run(Main.java:293) at sun.tools.jar.Main.main(Main.java:1288)Success: There are no flink core classes are contained in the jar.Failure: Since Elasticsearch5SinkExample.class and other user classes are not included in the jar. [FAIL] Test script contains errors.Checking for errors...No errors in log files.Checking for exceptions...No exceptions in log files.Checking for non-empty .out files...grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*.out: No such file or directoryNo non-empty .out files.[FAIL] 'Quickstarts Java nightly end-to-end test' failed after 0 minutes and 6 seconds! Test exited with exit code 1</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.quickstarts.sh</file>
    </fixedFiles>
  </bug>
  <bug id="17526" opendate="2020-5-5 00:00:00" fixdate="2020-5-5 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support AVRO serialization and deseriazation schema for RowData type</summary>
      <description>Add support AvroRowDataDeserializationSchema and AvroRowDataSerializationSchema for the new data structure RowData.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-avro.src.test.resources.avro.user.avsc</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.typeutils.AvroSchemaConverter.java</file>
      <file type="M">flink-formats.flink-avro.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17536" opendate="2020-5-6 00:00:00" fixdate="2020-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change the config option of slot max limitation to "slotmanager.number-of-slots.max"</summary>
      <description></description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ResourceManagerOptions.java</file>
      <file type="M">docs..includes.generated.resource.manager.configuration.html</file>
      <file type="M">docs..includes.generated.expert.scheduling.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="17543" opendate="2020-5-6 00:00:00" fixdate="2020-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rerunning failed azure jobs fails when uploading logs</summary>
      <description>No LastRequestResponse on exception ArtifactExistsException: Artifact logs-ci-tests already exists for build 677.</description>
      <version>None</version>
      <fixedVersion>1.11.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.watchdog.sh</file>
    </fixedFiles>
  </bug>
  <bug id="17547" opendate="2020-5-6 00:00:00" fixdate="2020-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support unaligned checkpoints for records spilled to files</summary>
      <description></description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.HybridMemorySegment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.DataOutputSerializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierUnaligner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ChannelPersistenceITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.RecordingChannelStateWriter.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.MockChannelStateWriter.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.CheckpointInProgressRequestTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.RecordDeserializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.NonSpanningWrapper.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriter.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.IOUtils.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.main.java.org.apache.flink.fs.s3.common.utils.RefCountedFSOutputStream.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.main.java.org.apache.flink.fs.s3.common.utils.RefCountedFileWithStream.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.main.java.org.apache.flink.fs.s3.common.utils.RefCounted.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.SpanningWrapper.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.CloseableIterator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.MemorySegmentFactory.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.test.java.org.apache.flink.fs.s3.common.writer.S3RecoverableFsDataOutputStreamTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.test.java.org.apache.flink.fs.s3.common.writer.RecoverableMultiPartUploadImplTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.test.java.org.apache.flink.fs.s3.common.utils.RefCountedFileTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.test.java.org.apache.flink.fs.s3.common.utils.RefCountedBufferingFileStreamTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.main.java.org.apache.flink.fs.s3.common.writer.S3RecoverableWriter.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.main.java.org.apache.flink.fs.s3.common.writer.S3RecoverableMultipartUploadFactory.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.main.java.org.apache.flink.fs.s3.common.writer.S3RecoverableFsDataOutputStream.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.main.java.org.apache.flink.fs.s3.common.utils.RefCountedTmpFileCreator.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.main.java.org.apache.flink.fs.s3.common.utils.RefCountedFile.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.main.java.org.apache.flink.fs.s3.common.utils.RefCountedBufferingFileStream.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.main.java.org.apache.flink.fs.s3.common.FlinkS3FileSystem.java</file>
    </fixedFiles>
  </bug>
  <bug id="17581" opendate="2020-5-8 00:00:00" fixdate="2020-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update translation of S3 documentation</summary>
      <description>The change in https://github.com/apache/flink/commit/7c5ac3584e42a0e7ebc5e78c532887bf4d383d9d needs to be added to the Chinese variant of the documentation page.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.filesystems.s3.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="17582" opendate="2020-5-8 00:00:00" fixdate="2020-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update quickstarts to use universal Kafka connector</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17587" opendate="2020-5-9 00:00:00" fixdate="2020-5-9 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Filesystem streaming sink support partition commit (success file)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSink.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableFactory.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.java</file>
    </fixedFiles>
  </bug>
  <bug id="17599" opendate="2020-5-10 00:00:00" fixdate="2020-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documents due to FLIP-84</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.index.md</file>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.streaming.query.configuration.zh.md</file>
      <file type="M">docs.dev.table.streaming.query.configuration.md</file>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
      <file type="M">docs.dev.table.sql.insert.zh.md</file>
      <file type="M">docs.dev.table.sql.insert.md</file>
      <file type="M">docs.dev.table.sql.index.zh.md</file>
      <file type="M">docs.dev.table.sql.drop.zh.md</file>
      <file type="M">docs.dev.table.sql.drop.md</file>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.sql.alter.zh.md</file>
      <file type="M">docs.dev.table.sql.alter.md</file>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
      <file type="M">docs.dev.table.common.zh.md</file>
      <file type="M">docs.dev.table.common.md</file>
      <file type="M">docs.dev.table.catalogs.zh.md</file>
      <file type="M">docs.dev.table.catalogs.md</file>
    </fixedFiles>
  </bug>
  <bug id="17604" opendate="2020-5-11 00:00:00" fixdate="2020-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement format factory for CSV serialization and deseriazation schema of RowData type</summary>
      <description></description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-csv.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17606" opendate="2020-5-11 00:00:00" fixdate="2020-5-11 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce DataGen connector in table</summary>
      <description>CREATE TABLE user (    id BIGINT,    age INT,    description STRING) WITH (    'connector' = 'datagen',    'rows-per-second'='100',    ' fields.id.kind' = 'sequence',    'fields.id.start' = '1',    'fields.age.kind' = 'random',    'fields.age.min' = '0',    'fields.age.max' = '100',    'fields.description.kind' = 'random',    'fields.description.length' = '100')-- Default is random generator.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.pom.xml</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.StatefulSequenceSourceTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="17608" opendate="2020-5-11 00:00:00" fixdate="2020-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add TM log and stdout page back</summary>
      <description>According to the discussion in https://github.com/apache/flink/pull/11731#issuecomment-620048458TM log and stdout page should be added in order not to break the previous user experience.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.task-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.thread-dump.task-manager-thread-dump.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.task-manager.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.task-manager-routing.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.status.task-manager-status.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.log-detail.task-manager-log-detail.component.ts</file>
    </fixedFiles>
  </bug>
  <bug id="17614" opendate="2020-5-11 00:00:00" fixdate="2020-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add project ITCase for partition table in filesystem connector</summary>
      <description>create table partitionedTable ( x string, y int, a int, b bigint ) partitioned by (a, b) with ( 'connector' = 'filesystem', ... ）Add ITCase that project field from general field(x, y) and partition key field(a, b) to validate project and default partition value works well like:check( "select y, b, x from partitionedTable where a=3", Seq( row(17, 1, "x17"), row(18, 2, "x18"), row(19, 3, "x19") ))</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.FileSystemITCaseBase.scala</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="17616" opendate="2020-5-11 00:00:00" fixdate="2020-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Temporarily increase akka.ask.timeout in TPC-DS e2e test</summary>
      <description>Until FLINK-17558 is fixed, we should increase the akka.ask.timeout in the e2e test to mitigate FLINK-17194</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
    </fixedFiles>
  </bug>
  <bug id="17617" opendate="2020-5-11 00:00:00" fixdate="2020-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink SQL CLI Autocomplete does not return correct hints</summary>
      <description>The SQL Client does not properly return the hints for commands such as:USE CATALOG,SHOW CATALOGS,SHOW MODULESOr whenever it does not actually fall back to the table planner.The reason being is that the hint returned does not take into account the current cursor position but returns the complete text.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCompleter.java</file>
    </fixedFiles>
  </bug>
  <bug id="17621" opendate="2020-5-11 00:00:00" fixdate="2020-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use default akka.ask.timeout in TPC-DS e2e test</summary>
      <description>Revert the changes in FLINK-17616</description>
      <version>1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
    </fixedFiles>
  </bug>
  <bug id="17622" opendate="2020-5-11 00:00:00" fixdate="2020-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove useless switch for decimal in PostresCatalog</summary>
      <description>Remove the useless switch for decimal fields. The Postgres JDBC connector translate them to numeric</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalogTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="17626" opendate="2020-5-12 00:00:00" fixdate="2020-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fs connector should use FLIP-122 format options style</summary>
      <description>format.parquet.compression -&gt; parquet.compressionformat.field-delimiter -&gt; csv.field-delimiter</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSource.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSink.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableFactory.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.StreamFileSystemTestCsvITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.FsStreamingSinkTestCsvITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.FileSystemTestCsvITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.utils.TestCsvFileSystemFormatFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecSinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecLegacySinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecSinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecLegacySinkRule.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.ParquetFileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-orc.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcFileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonBatchFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-json.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-formats.flink-json.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonFileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvFilesystemBatchITCase.java</file>
      <file type="M">flink-formats.flink-csv.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-formats.flink-csv.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroFilesystemITCase.java</file>
      <file type="M">flink-formats.flink-avro.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-formats.flink-avro.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroFileSystemFormatFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="17628" opendate="2020-5-12 00:00:00" fixdate="2020-5-12 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Remove the unnecessary py4j log information</summary>
      <description>Currently the py4j will print the INFO level logging information to the console. It is unnecessary for users. We should set the level to WARN. </description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.java.gateway.py</file>
    </fixedFiles>
  </bug>
  <bug id="17629" opendate="2020-5-12 00:00:00" fixdate="2020-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement format factory for JSON serialization and deserialization schema</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDataSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDataDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17632" opendate="2020-5-12 00:00:00" fixdate="2020-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to specify a remote path for job jar</summary>
      <description>After FLINK-13938, we could support to register remote shared libs as Yarn local resources and prevent unnecessary uploading and downloading for system Flink jars.However, we still need to specify a local user jar to run a Flink job on Yarn. This ticket aims to add the remote path support. It have at least two purposes. Accelerate the submission Better integration with application mode. Since the user main code is executed in the cluster(jobmanager), we do not need the user jar exists locally.A very typical use case is like following../bin/flink run-application -p 10 -t yarn-application \-yD yarn.provided.lib.dirs="hdfs://myhdfs/flink/lib" \hdfs://myhdfs/jars/WindowJoin.jar</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnApplicationFileUploaderTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnFileStageTestS3ITCase.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnFileStageTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnLocalResourceDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnApplicationFileUploader.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNApplicationITCase.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnClusterDescriptorTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnApplicationClusterEntryPoint.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug id="17634" opendate="2020-5-12 00:00:00" fixdate="2020-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reject multiple handler registrations under the same URL</summary>
      <description>In FLINK-11853 a handler was added the is being registered under the same URL as another handler. This should never happen, and we should add a check to ensure this doesn't happen again.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="17635" opendate="2020-5-12 00:00:00" fixdate="2020-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about view support</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
      <file type="M">docs.dev.table.sql.index.zh.md</file>
      <file type="M">docs.dev.table.sql.index.md</file>
      <file type="M">docs.dev.table.sql.drop.zh.md</file>
      <file type="M">docs.dev.table.sql.drop.md</file>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
    </fixedFiles>
  </bug>
  <bug id="17646" opendate="2020-5-13 00:00:00" fixdate="2020-5-13 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Reduce the python package size of PyFlink</summary>
      <description>Currently the python package size of PyFlink has increased to about 320MB, which exceeds the size limit of pypi.org (300MB). We need to remove unnecessary jars to reduce the package size.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  <bug id="17664" opendate="2020-5-13 00:00:00" fixdate="2020-5-13 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce print, blackhole connector in table</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.DataGenTableSourceFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sources.datagen.DataGenTableSourceFactory.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sources.datagen.DataGenTableSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="17666" opendate="2020-5-13 00:00:00" fixdate="2020-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Insert into partitioned table can fail with select *</summary>
      <description>The following test @Test public void test() throws Exception { hiveShell.execute("create table src (x int,y string)"); hiveShell.insertInto("default", "src").addRow(1, "a").commit(); hiveShell.execute("create table dest (x int) partitioned by (p1 int,p2 string)"); TableEnvironment tableEnvironment = getTableEnvWithHiveCatalog(); tableEnvironment.executeSql("insert into dest partition (p1=1) select * from src") .getJobClient().get().getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get(); }Fails withorg.apache.flink.table.api.ValidationException: Field types of query result and registered TableSink test-catalog.default.dest do not match.Query schema: [x: INT, y: VARCHAR(2147483647), EXPR$2: INT NOT NULL]Sink schema: [x: INT, p1: INT, p2: VARCHAR(2147483647)] at org.apache.flink.table.planner.sinks.TableSinkUtils$.validateSchemaAndApplyImplicitCast(TableSinkUtils.scala:95) at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(PlannerBase.scala:199) at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(PlannerBase.scala:191) at scala.Option.map(Option.scala:146) at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:191) at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:150) at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:150) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:150) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1202) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:687) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:773) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:677)......However, the same DML passes if I change "select *" to "select x,y" in the query.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.PreValidateReWriter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.dql.SqlShowCatalogs.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="17667" opendate="2020-5-13 00:00:00" fixdate="2020-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement INSERT for Hive dialect</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.calcite.CalciteParser.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="17669" opendate="2020-5-13 00:00:00" fixdate="2020-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use new WatermarkStrategy/WatermarkGenerator in Kafka connector</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionStateWithPunctuatedWatermarks.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionStateWithPeriodicWatermarks.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThreadTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka010FetcherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer010.java</file>
    </fixedFiles>
  </bug>
  <bug id="17672" opendate="2020-5-13 00:00:00" fixdate="2020-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>OperatorCoordinators receive failure notifications on task failure instead of on task restarts</summary>
      <description>Currently, the OperatorCoordinators receive failure notifications on task restart. That follows the same approach as the InputSplit assigners from the legacy sources (after which the integration of the Coordinators with the Scheduler was modeled).However, propagating the failure notifications during the actual failure is more intuitive, and also improve situations where tasks fail but don't get restarted for a while (this can happen for batch tasks when a TM dies and no spare resources are available). In those cases, the coordinator can react much earlier to the failure.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
    </fixedFiles>
  </bug>
  <bug id="17674" opendate="2020-5-13 00:00:00" fixdate="2020-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>OperatorCoordinator state in checkpoints should always be a ByteStreamStateHandle</summary>
      <description>State restore to the task vertices and coordinators (even after loading the Checkpoint Metadata) happens in the JobManager's main thread and must consequently not do any potentially blocking I/O operations.The OperatorCoordinator state is a generic StreamStateHandle whose state might require I/O to retrieve. This never happens in the current implementation (we always use ByteStreamStateHandle) the signatures and contracts don't guarantee that and leave this open for a potential future bug.Typing the OperatorCoordinator state to ByteStreamStateHandle makes sure that we can always retrieve the data directly without I/O and clarifies that no arbitrary StreamStateHandle is supported at that point. If state restoring becomes an asynchronous operation we can relax this restriction.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestingStreamStateHandle.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.metadata.CheckpointTestUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorCoordinatorCheckpoints.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.metadata.MetadataV3Serializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="17675" opendate="2020-5-14 00:00:00" fixdate="2020-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Resolve CVE-2019-11358 from jquery</summary>
      <description>https://nvd.nist.gov/vuln/detail/CVE-2019-11358</description>
      <version>None</version>
      <fixedVersion>1.11.0,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">NOTICE</file>
      <file type="M">docs.page.js.jquery.min.js</file>
      <file type="M">docs.page.js.flink.js</file>
    </fixedFiles>
  </bug>
  <bug id="17678" opendate="2020-5-14 00:00:00" fixdate="2020-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create flink-sql-connector-hbase module to shade HBase</summary>
      <description>Currently, flink doesn't contains a hbase uber jar, so users have to add hbase dependency manually.Could I create new module called flink-sql-connector-hbase like elasticsaerch and kafka sql -connector.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.TestUtils.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.StreamingKafkaITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.java</file>
      <file type="M">flink-connectors.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17679" opendate="2020-5-14 00:00:00" fixdate="2020-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the bug of encoding bytes in cython coder</summary>
      <description>The python bytes b'x\x00\x00\x00' will be transposed to b'\x'. If we use strlen() of c function to compute the length of char*, we will get wrong length. So we need to use the Python function of len() to compute the length. </description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.fast.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.fast.coder.impl.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.fast.coder.impl.pxd</file>
    </fixedFiles>
  </bug>
  <bug id="17681" opendate="2020-5-14 00:00:00" fixdate="2020-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TableEnvironment fromValues not work with map type and SQL</summary>
      <description>Map&lt;Integer, Integer&gt; mapData = new HashMap&lt;&gt;(); mapData.put(1, 1); mapData.put(2, 2); Row row = Row.of(mapData); tEnv().createTemporaryView("values_t", tEnv().fromValues(Collections.singletonList(row))); Iterator&lt;Row&gt; iter = tEnv().executeSql("select * from values_t").collect(); List&lt;Row&gt; results = new ArrayList&lt;&gt;(); while (iter.hasNext()) { results.add(iter.next()); } System.out.println(results);Not work, will occur exception:java.lang.AssertionError: Conversion to relational algebra failed to preserve datatypes:validated type:RecordType((INTEGER NOT NULL, INTEGER NOT NULL) MAP f0) NOT NULLconverted type:RecordType((INTEGER NOT NULL, INTEGER NOT NULL) MAP NOT NULL f0) NOT NULLIf change to Iterator&lt;Row&gt; iter = tEnv().from("values_t").execute().collect(); will work.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.types.PlannerTypeUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.ValuesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.table.ValuesITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.converter.CustomizedConvertRule.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.TypeStrategiesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeStrategies.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.operations.utils.ValuesOperationTreeBuilderTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="17692" opendate="2020-5-14 00:00:00" fixdate="2020-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"flink-end-to-end-tests/test-scripts/hadoop/yarn.classpath" present after building Flink</summary>
      <description>Some changes introduced in FLINK-11086 cause the "flink-end-to-end-tests/test-scripts/hadoop/yarn.classpath" file to be generated and present in the source tree after building Flink.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17707" opendate="2020-5-15 00:00:00" fixdate="2020-6-15 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support configuring replica of Deployment based HA setups</summary>
      <description>At the moment, in the native K8s setups, we hard code the replica of Deployment to 1. However, when users enable the ZooKeeper HighAvailabilityServices, they would like to configure the replica of Deployment also for faster failover.This ticket proposes to make replica of Deployment configurable in the ZooKeeper based HA setups.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParametersTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.kubernetes.config.configuration.html</file>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug id="17717" opendate="2020-5-15 00:00:00" fixdate="2020-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Throws for DDL create temporary system function with composite table path</summary>
      <description>Currently, we support syntax create temporary system function catalog.db.func_name as function_classBut actually we drop the catalog and db silently, the temporary system function never has custom table paths, it belongs always to the system and current session, so, we should limit the table path to simple identifier.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.BatchTableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="17718" opendate="2020-5-15 00:00:00" fixdate="2020-5-15 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Integrate avro to file system connector</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroRowDataSerializationSchema.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroRowDataDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-avro.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17722" opendate="2020-5-15 00:00:00" fixdate="2020-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Failed to find the file" in "build_wheels" stage</summary>
      <description>CI https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1343&amp;view=logs&amp;j=fe7ebddc-3e2f-5c50-79ee-226c8653f218&amp;t=b2830442-93c7-50ff-36f4-5b3e2dca8c83Successfully built dill crcmod httplib2 hdfs oauth2client future avro-python3Installing collected packages: six, pbr, mock, dill, typing, crcmod, numpy, pyarrow, python-dateutil, typing-extensions, fastavro, httplib2, protobuf, pymongo, docopt, idna, chardet, urllib3, requests, hdfs, pyparsing, pydot, pyasn1, pyasn1-modules, rsa, oauth2client, grpcio, future, avro-python3, pytz, apache-beam, cythonSuccessfully installed apache-beam-2.19.0 avro-python3-1.9.2.1 chardet-3.0.4 crcmod-1.7 cython-0.29.16 dill-0.3.1.1 docopt-0.6.2 fastavro-0.21.24 future-0.18.2 grpcio-1.29.0 hdfs-2.5.8 httplib2-0.12.0 idna-2.9 mock-2.0.0 numpy-1.18.4 oauth2client-3.0.0 pbr-5.4.5 protobuf-3.11.3 pyarrow-0.15.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydot-1.4.1 pymongo-3.10.1 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2020.1 requests-2.23.0 rsa-4.0 six-1.14.0 typing-3.7.4.1 typing-extensions-3.7.4.2 urllib3-1.25.9+ (( i++ ))+ (( i&lt;3 ))+ (( i=0 ))+ (( i&lt;3 ))+ /home/vsts/work/1/s/flink-python/dev/.conda/envs/3.5/bin/python setup.py bdist_wheelCompiling pyflink/fn_execution/fast_coder_impl.pyx because it changed.Compiling pyflink/fn_execution/fast_operations.pyx because it changed.[1/2] Cythonizing pyflink/fn_execution/fast_coder_impl.pyx[2/2] Cythonizing pyflink/fn_execution/fast_operations.pyxFailed to find the file /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/opt/flink-sql-client_*.jar.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure.controller.sh</file>
    </fixedFiles>
  </bug>
  <bug id="17727" opendate="2020-5-15 00:00:00" fixdate="2020-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can&amp;#39;t subsume checkpoint with no channel state in UC mode</summary>
      <description>When there are no channel state handles, the underlying FS stream is still created.On discard it is not deleted because it's not referenced by any state handles.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-stateful-job-upgrade-test.src.main.java.org.apache.flink.streaming.tests.StatefulStreamJobUpgradeTestProgram.java</file>
    </fixedFiles>
  </bug>
  <bug id="17728" opendate="2020-5-15 00:00:00" fixdate="2020-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>sql client supports parser statements via sql parser</summary>
      <description>Current, all statements in sql client are parsed via regex matching, which has many limitations, such as it can't handle comments. To avoid that limitations, we should try best to use sql parser to parse a statement. There are many statement can't be handle by sql parser, such as: set, reset. so they are still handle through regex matching.statements handled through regex matching:quit, exit, clear, help, desc, explain, set, reset source, show modulesstatements handled through sql parser:show catalogs, show databases, show tables, show functions, use catalog, use, describe, explain plan for, select, insert, DDLsnote: we keep `explain xx`, and also support `explain plan for xx`</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCompleter.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="17729" opendate="2020-5-15 00:00:00" fixdate="2020-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make mandatory to have lib/, plugin/ and dist in yarn.provided.lib.dirs</summary>
      <description>This will make mandatory for users of the "yarn.provided.lib.dirs" to also include the lib/, plugin/ and flink-dist jar. If these are not included in the shared resources, then the feature cannot be used and all the dependencies of an application are expected to be shipped from the client (as it was the previous behaviour). If they are provided, then these are going to be used and NOT what the user may have locally (e.g. different flink or log4j versions).The reason for this requirement is to avoid unpleasant surprises with classloading issues.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnApplicationFileUploader.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="17734" opendate="2020-5-15 00:00:00" fixdate="2020-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add specialized collecting sink function</summary>
      <description>A specialized collecting sink is needed to implement the algorithms in FLINK-14807</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamingRuntimeContext.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.TestUtils.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.TestingTaskManagerRuntimeInfo.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.JvmExitOnFatalErrorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorSlotLifetimeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.TaskManagerRuntimeInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointTaskManagerRuntimeInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="17735" opendate="2020-5-15 00:00:00" fixdate="2020-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add specialized collecting iterator</summary>
      <description>Add specialized collecting iterator is needed to implement the algorithms in FLINK-14807As legacy planner does not have data structures like ResettableExternalBuffer in Blink planner, we're not implementing the algorithms in legacy planner. Legacy planner just use the current collect implementation.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.collect.utils.TestCollectClient.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.collect.utils.CollectRequestSender.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.collect.CollectSinkFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorCoordinator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectSinkFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="17737" opendate="2020-5-15 00:00:00" fixdate="2020-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KeyedStateCheckpointingITCase fails in UnalignedCheckpoint mode</summary>
      <description></description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.AlternatingCheckpointBarrierHandlerTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.AlternatingCheckpointBarrierHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="17753" opendate="2020-5-16 00:00:00" fixdate="2020-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>watermark defined in ddl does not work in Table api</summary>
      <description>the following code will get org.apache.flink.table.api.ValidationException: A group window expects a time attribute for grouping in a stream environment.@Test def testRowTimeTableSourceGroupWindow(): Unit = { val ddl = s""" |CREATE TABLE rowTimeT ( | id int, | rowtime timestamp(3), | val bigint, | name varchar(32), | watermark for rowtime as rowtime |) WITH ( | 'connector' = 'projectable-values', | 'bounded' = 'false' |) """.stripMargin util.tableEnv.executeSql(ddl) val t = util.tableEnv.from("rowTimeT") .where($"val" &gt; 100) .window(Tumble over 10.minutes on 'rowtime as 'w) .groupBy('name, 'w) .select('name, 'w.end, 'val.avg) util.verifyPlan(t) }The reason is planner does not convert watermarkSpecs in TableSchema to correct type when calling tableEnv.from</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.DatabaseCalciteSchemaTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.CatalogManagerTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.ParserImpl.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.LegacyTableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.WatermarkGeneratorCodeGenTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.LegacyTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.utils.PlannerMocks.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.FlinkCalciteCatalogReaderTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.sources.TableSourceUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.delegation.ParserImpl.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.DatabaseCalciteSchema.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.CatalogSchemaTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.CatalogManagerCalciteSchema.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.CatalogCalciteSchema.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.ParserMock.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.delegation.Parser.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogTableImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="17756" opendate="2020-5-16 00:00:00" fixdate="2020-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop table/view shouldn&amp;#39;t take effect on each other</summary>
      <description>Currently "DROP VIEW" can successfully drop a table, and "DROP TABLE" can successfully a view. We should disable this.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="17764" opendate="2020-5-16 00:00:00" fixdate="2020-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update tips about the default planner when the planner parameter value is not recognized</summary>
      <description>This default planner has been set to blink in the code.However, when the planner parameter value is not recognized, the default planner is prompted to be flink.  </description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.StreamSQLExample.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.StreamSQLExample.java</file>
    </fixedFiles>
  </bug>
  <bug id="1777" opendate="2015-3-24 00:00:00" fixdate="2015-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Java 8 Lambdas with Eclipse documentation</summary>
      <description>The Eclipse JDT compiler team has introduced a compiler flag for us, which is not covered in the Flink documentation yet.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">docs.java8.programming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="17771" opendate="2020-5-17 00:00:00" fixdate="2020-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"PyFlink end-to-end test" fails with "The output result: [] is not as expected: [2, 3, 4]!" on Java11</summary>
      <description>Java 11 nightly profile: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1579&amp;view=logs&amp;j=6caf31d6-847a-526e-9624-468e053467d6&amp;t=679407b1-ea2c-5965-2c8d-1467777fff88Job has been submitted with JobID ef78030becb3bfd6415d3de2e06420b4java.lang.AssertionError: The output result: [] is not as expected: [2, 3, 4]! at org.apache.flink.python.tests.FlinkStreamPythonUdfSqlJob.main(FlinkStreamPythonUdfSqlJob.java:55) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288) at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198) at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:148) at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:689) at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:227) at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:906) at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:982) at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30) at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:982)Stopping taskexecutor daemon (pid: 2705) on host fv-az670.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
    </fixedFiles>
  </bug>
  <bug id="17772" opendate="2020-5-17 00:00:00" fixdate="2020-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>test_pandas_udf.py: NoClassDefFoundError RowDataArrowPythonScalarFunctionRunner</summary>
      <description>Java 11 nightly profile: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1579&amp;view=logs&amp;j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&amp;t=9739ebd8-9cbd-5d3f-d48a-1fac792a86792020-05-16T23:12:27.0921553Z pyflink/table/tests/test_pandas_udf.py:63: 2020-05-16T23:12:27.0921999Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 2020-05-16T23:12:27.0922424Z pyflink/table/table_environment.py:1049: in execute2020-05-16T23:12:27.0923081Z return JobExecutionResult(self._j_tenv.execute(job_name))2020-05-16T23:12:27.0923876Z .tox/py35-cython/lib/python3.5/site-packages/py4j/java_gateway.py:1286: in __call__2020-05-16T23:12:27.0924419Z answer, self.gateway_client, self.target_id, self.name)2020-05-16T23:12:27.0924800Z pyflink/util/exceptions.py:147: in deco2020-05-16T23:12:27.0925086Z return f(*a, **kw)2020-05-16T23:12:27.0925662Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 2020-05-16T23:12:27.0926095Z 2020-05-16T23:12:27.0926604Z answer = 'xro11689'2020-05-16T23:12:27.0927067Z gateway_client = &lt;py4j.java_gateway.GatewayClient object at 0x7f3788994c18&gt;2020-05-16T23:12:27.0927746Z target_id = 'o11627', name = 'execute'2020-05-16T23:12:27.0927931Z 2020-05-16T23:12:27.0929789Z def get_return_value(answer, gateway_client, target_id=None, name=None):2020-05-16T23:12:27.0930237Z """Converts an answer received from the Java gateway into a Python object.2020-05-16T23:12:27.0930505Z 2020-05-16T23:12:27.0931277Z For example, string representation of integers are converted to Python2020-05-16T23:12:27.0931748Z integer, string representation of objects are converted to JavaObject2020-05-16T23:12:27.0932173Z instances, etc.2020-05-16T23:12:27.0932449Z 2020-05-16T23:12:27.0932773Z :param answer: the string returned by the Java gateway2020-05-16T23:12:27.0933272Z :param gateway_client: the gateway client used to communicate with the Java2020-05-16T23:12:27.0933820Z Gateway. Only necessary if the answer is a reference (e.g., object,2020-05-16T23:12:27.0934255Z list, map)2020-05-16T23:12:27.0934677Z :param target_id: the name of the object from which the answer comes from2020-05-16T23:12:27.0935187Z (e.g., *object1* in `object1.hello()`). Optional.2020-05-16T23:12:27.0935692Z :param name: the name of the member from which the answer comes from2020-05-16T23:12:27.0936344Z (e.g., *hello* in `object1.hello()`). Optional.2020-05-16T23:12:27.0936614Z """2020-05-16T23:12:27.0936840Z if is_error(answer)[0]:2020-05-16T23:12:27.0937186Z if len(answer) &gt; 1:2020-05-16T23:12:27.0937696Z type = answer[1]2020-05-16T23:12:27.0938164Z value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)2020-05-16T23:12:27.0938688Z if answer[1] == REFERENCE_TYPE:2020-05-16T23:12:27.0939177Z raise Py4JJavaError(2020-05-16T23:12:27.0939530Z "An error occurred while calling {0}{1}{2}.\n".2020-05-16T23:12:27.0939943Z &gt; format(target_id, ".", name), value)2020-05-16T23:12:27.0940706Z E py4j.protocol.Py4JJavaError: An error occurred while calling o11627.execute.2020-05-16T23:12:27.0941428Z E : java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.2020-05-16T23:12:27.0942239Z E at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)2020-05-16T23:12:27.0942936Z E at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)2020-05-16T23:12:27.0943688Z E at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1665)2020-05-16T23:12:27.0944563Z E at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:74)2020-05-16T23:12:27.0945520Z E at org.apache.flink.table.planner.delegation.ExecutorBase.execute(ExecutorBase.java:52)2020-05-16T23:12:27.0946337Z E at org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1088)2020-05-16T23:12:27.0947024Z E at jdk.internal.reflect.GeneratedMethodAccessor164.invoke(Unknown Source)2020-05-16T23:12:27.0947887Z E at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-05-16T23:12:27.0948609Z E at java.base/java.lang.reflect.Method.invoke(Method.java:566)2020-05-16T23:12:27.0949382Z E at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)2020-05-16T23:12:27.0950131Z E at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)2020-05-16T23:12:27.0950905Z E at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)2020-05-16T23:12:27.0951617Z E at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)2020-05-16T23:12:27.0952420Z E at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)2020-05-16T23:12:27.0953156Z E at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)2020-05-16T23:12:27.0953800Z E at java.base/java.lang.Thread.run(Thread.java:834)2020-05-16T23:12:27.0954407Z E Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.2020-05-16T23:12:27.0955017Z E at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)2020-05-16T23:12:27.0955942Z E at org.apache.flink.client.program.PerJobMiniClusterFactory$PerJobMiniClusterJobClient.lambda$getJobExecutionResult$2(PerJobMiniClusterFactory.java:186)2020-05-16T23:12:27.0956876Z E at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642)2020-05-16T23:12:27.0957600Z E at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)2020-05-16T23:12:27.0958233Z E at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)2020-05-16T23:12:27.0958955Z E at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:229)2020-05-16T23:12:27.0959665Z E at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)2020-05-16T23:12:27.0960453Z E at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)2020-05-16T23:12:27.0961042Z E at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)2020-05-16T23:12:27.0961782Z E at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)2020-05-16T23:12:27.0962451Z E at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:890)2020-05-16T23:12:27.0963100Z E at akka.dispatch.OnComplete.internal(Future.scala:264)2020-05-16T23:12:27.0963693Z E at akka.dispatch.OnComplete.internal(Future.scala:261)2020-05-16T23:12:27.0964313Z E at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)2020-05-16T23:12:27.0964894Z E at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)2020-05-16T23:12:27.0965542Z E at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)2020-05-16T23:12:27.0966301Z E at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)2020-05-16T23:12:27.0967132Z E at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)2020-05-16T23:12:27.0967721Z E at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)2020-05-16T23:12:27.0968205Z E at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)2020-05-16T23:12:27.0968876Z E at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)2020-05-16T23:12:27.0969445Z E at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)2020-05-16T23:12:27.0973679Z E at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)2020-05-16T23:12:27.0974397Z E at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)2020-05-16T23:12:27.0974869Z E at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)2020-05-16T23:12:27.0975572Z E at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)2020-05-16T23:12:27.0976426Z E at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)2020-05-16T23:12:27.0977267Z E at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)2020-05-16T23:12:27.0978062Z E at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)2020-05-16T23:12:27.0978724Z E at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)2020-05-16T23:12:27.0979414Z E at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)2020-05-16T23:12:27.0980049Z E at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)2020-05-16T23:12:27.0980751Z E at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)2020-05-16T23:12:27.0981450Z E at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)2020-05-16T23:12:27.0982089Z E at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)2020-05-16T23:12:27.0982783Z E at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)2020-05-16T23:12:27.0983437Z E at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)2020-05-16T23:12:27.0984215Z E Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy2020-05-16T23:12:27.0985010Z E at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)2020-05-16T23:12:27.0985974Z E at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)2020-05-16T23:12:27.0986939Z E at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)2020-05-16T23:12:27.0987857Z E at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)2020-05-16T23:12:27.0988704Z E at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)2020-05-16T23:12:27.0989644Z E at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:501)2020-05-16T23:12:27.0990412Z E at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:386)2020-05-16T23:12:27.0991030Z E at jdk.internal.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)2020-05-16T23:12:27.0991961Z E at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-05-16T23:12:27.0992658Z E at java.base/java.lang.reflect.Method.invoke(Method.java:566)2020-05-16T23:12:27.0993343Z E at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284)2020-05-16T23:12:27.0994263Z E at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)2020-05-16T23:12:27.0995051Z E at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)2020-05-16T23:12:27.0995836Z E at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)2020-05-16T23:12:27.0996505Z E at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)2020-05-16T23:12:27.0997070Z E at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)2020-05-16T23:12:27.0997785Z E at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)2020-05-16T23:12:27.0998383Z E at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)2020-05-16T23:12:27.0999019Z E at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)2020-05-16T23:12:27.0999619Z E at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)2020-05-16T23:12:27.1000272Z E at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)2020-05-16T23:12:27.1000832Z E at akka.actor.Actor$class.aroundReceive(Actor.scala:517)2020-05-16T23:12:27.1001328Z E at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)2020-05-16T23:12:27.1001932Z E at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)2020-05-16T23:12:27.1002448Z E at akka.actor.ActorCell.invoke(ActorCell.scala:561)2020-05-16T23:12:27.1002972Z E at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)2020-05-16T23:12:27.1003550Z E at akka.dispatch.Mailbox.run(Mailbox.scala:225)2020-05-16T23:12:27.1004071Z E at akka.dispatch.Mailbox.exec(Mailbox.scala:235)2020-05-16T23:12:27.1004442Z E ... 4 more2020-05-16T23:12:27.1005025Z E Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.flink.table.runtime.runners.python.scalar.arrow.RowDataArrowPythonScalarFunctionRunner2020-05-16T23:12:27.1006197Z E at org.apache.flink.table.runtime.operators.python.scalar.arrow.RowDataArrowPythonScalarFunctionOperator.createPythonFunctionRunner(RowDataArrowPythonScalarFunctionOperator.java:98)2020-05-16T23:12:27.1007310Z E at org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.createPythonFunctionRunner(AbstractStatelessFunctionOperator.java:149)2020-05-16T23:12:27.1008468Z E at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.open(AbstractPythonFunctionOperator.java:141)2020-05-16T23:12:27.1009335Z E at org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.open(AbstractStatelessFunctionOperator.java:131)2020-05-16T23:12:27.1010068Z E at org.apache.flink.table.runtime.operators.python.scalar.AbstractPythonScalarFunctionOperator.open(AbstractPythonScalarFunctionOperator.java:88)2020-05-16T23:12:27.1010843Z E at org.apache.flink.table.runtime.operators.python.scalar.AbstractRowDataPythonScalarFunctionOperator.open(AbstractRowDataPythonScalarFunctionOperator.java:80)2020-05-16T23:12:27.1011627Z E at org.apache.flink.table.runtime.operators.python.scalar.arrow.RowDataArrowPythonScalarFunctionOperator.open(RowDataArrowPythonScalarFunctionOperator.java:78)2020-05-16T23:12:27.1012463Z E at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:289)2020-05-16T23:12:27.1013071Z E at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:469)2020-05-16T23:12:27.1013739Z E at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92)2020-05-16T23:12:27.1014473Z E at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:465)2020-05-16T23:12:27.1015020Z E at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:518)2020-05-16T23:12:27.1015526Z E at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:713)2020-05-16T23:12:27.1016069Z E at org.apache.flink.runtime.taskmanager.Task.run(Task.java:539)2020-05-16T23:12:27.1016484Z E at java.base/java.lang.Thread.run(Thread.java:834)2020-05-16T23:12:27.1016721Z 2020-05-16T23:12:27.1017622Z .tox/py35-cython/lib/python3.5/site-packages/py4j/protocol.py:328: Py4JJavaError[...]2020-05-16T23:19:04.5010620Z ___________________________________ summary ____________________________________2020-05-16T23:19:04.5011582Z ERROR: py35-cython: commands failed2020-05-16T23:19:04.5012036Z ERROR: py36-cython: commands failed2020-05-16T23:19:04.5012455Z ERROR: py37-cython: commands failed2020-05-16T23:19:04.5353287Z ============tox checks... [FAILED]============2020-05-16T23:19:04.5371085Z PYTHON exited with EXIT CODE: 1.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.ArrowUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="17773" opendate="2020-5-17 00:00:00" fixdate="2020-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation for new WatermarkGenerator/WatermarkStrategies</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.event.timestamp.extractors.md</file>
      <file type="M">docs.dev.event.timestamps.watermarks.md</file>
      <file type="M">docs.dev.event.time.md</file>
    </fixedFiles>
  </bug>
  <bug id="17774" opendate="2020-5-17 00:00:00" fixdate="2020-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>supports all kinds of changes for select result</summary>
      <description>FLINK-17252 has supported select query, however only append change is supported. because FLINK-16998 is not finished. This issue aims to support all kinds of changes based on FLINK-16998.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamingRuntimeContext.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.collect.utils.TestCoordinationRequestHandler.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.collect.CollectSinkFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorCoordinator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectSinkOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectSinkFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectCoordinationResponse.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sinks.StreamSelectTableSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sinks.BatchSelectTableSink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.AggregationITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.sinks.StreamSelectTableSink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.sinks.SelectTableSinkSchemaConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.sinks.SelectTableSinkBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.sinks.BatchSelectTableSink.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.PrintUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.PrintUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.PlannerMock.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ModifyOperationVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.delegation.Planner.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableResultImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.SelectTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="17777" opendate="2020-5-17 00:00:00" fixdate="2020-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Mesos Jepsen Tests pass with Hadoop-free Flink</summary>
      <description>Since FLINK-11086, we can no longer build a Flink distribution with Hadoop. Therefore, we need to set the HADOOP_CLASSPATH environment variable for the TM processes.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.db.clj</file>
    </fixedFiles>
  </bug>
  <bug id="17779" opendate="2020-5-17 00:00:00" fixdate="2020-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Orc file format support filter push down</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcFileSystemFormatFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="17788" opendate="2020-5-18 00:00:00" fixdate="2020-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>scala shell in yarn mode is broken</summary>
      <description>When I start scala shell in yarn mode, one yarn app will be launched, and after I write some flink code and trigger a flink job, another yarn app will be launched but would failed to launch due to some conflicts.</description>
      <version>1.10.1,1.11.0</version>
      <fixedVersion>1.10.2,1.11.0,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
    </fixedFiles>
  </bug>
  <bug id="1779" opendate="2015-3-24 00:00:00" fixdate="2015-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename the function name from "getCurrentyActiveConnections" to "getCurrentActiveConnections" in org.apache.flink.runtime.blob</summary>
      <description>I think the function name "getCurrentyActiveConnections" in ' org.apache.flink.runtime.blob' is a wrong spelling, it should be "getCurrentActiveConnections" is more better, and also I add some comments about the function and the Tests.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobServerGetTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="17791" opendate="2020-5-18 00:00:00" fixdate="2020-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Combine collecting sink and iterator to support collecting query results under all execution and network environments</summary>
      <description>After introducing specialized collecting sink and iterator, the last thing we need to do is to combine them together in Table / DataSteram API so that the whole collecting mechanism works for the users.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sinks.StreamSelectTableSink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.ScalarQueryITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.sinks.StreamSelectTableSink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.sinks.BatchSelectTableSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="17792" opendate="2020-5-18 00:00:00" fixdate="2020-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failing to invoking jstack on TM processes should not fail Jepsen Tests</summary>
      <description>jstack can fail if the JVM process exits prematurely while or before we invoke jstack. If jstack fails, the exception propagates and exits the Jepsen Tests prematurely.</description>
      <version>1.10.1,1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.utils.clj</file>
    </fixedFiles>
  </bug>
  <bug id="17795" opendate="2020-5-18 00:00:00" fixdate="2020-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an example to show how to leverage GPU resources</summary>
      <description>Add an example to show how to leverage GPU resources.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17797" opendate="2020-5-18 00:00:00" fixdate="2020-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Align the behavior between the new and legacy HBase table source</summary>
      <description>The legacy HBase table source, i.e. HBaseTableSource, supports projection push down. In order to make the user experience consistent. We should align the behavior and add tests.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase.src.main.java.org.apache.flink.connector.hbase.util.HBaseTableSchema.java</file>
      <file type="M">flink-connectors.flink-connector-hbase.src.main.java.org.apache.flink.connector.hbase.util.HBaseSerde.java</file>
      <file type="M">flink-connectors.flink-connector-hbase.src.main.java.org.apache.flink.connector.hbase.source.HBaseDynamicTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hbase.src.main.java.org.apache.flink.connector.hbase.HBaseDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hbase.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17798" opendate="2020-5-18 00:00:00" fixdate="2020-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Align the behavior between the new and legacy JDBC table source</summary>
      <description>The legacy JDBC table source, i.e. JdbcTableSource, supports projection push down. In order to make the user experience consistent. We should align the behavior and add tests.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcLookupFunctionITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17799" opendate="2020-5-18 00:00:00" fixdate="2020-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression in all network benchmarks</summary>
      <description>Performance regression took place between May 2nd (commit 18af2a1) and May 15th (commit 282da0dd3e). Unfortunately in this period benchmarking infrastructure was broken, so we do not know on which day has it happened.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="17802" opendate="2020-5-19 00:00:00" fixdate="2020-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set offset commit only if group id is configured for new Kafka Table source</summary>
      <description>As https://issues.apache.org/jira/browse/FLINK-17619 described,the new Kafka Table source exits same problem and should fix too.note: this  fix both for master and release-1.11 </description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSourceBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="17809" opendate="2020-5-19 00:00:00" fixdate="2020-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BashJavaUtil script logic does not work for paths with spaces</summary>
      <description>Multiple paths aren't quoted (class path, conf_dir) resulting in errors if they contain spaces.</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.taskmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
    </fixedFiles>
  </bug>
  <bug id="17810" opendate="2020-5-19 00:00:00" fixdate="2020-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for K8s application mode</summary>
      <description>Add document for how to start/stop K8s application cluster.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.0,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug id="17814" opendate="2020-5-19 00:00:00" fixdate="2020-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate native kubernetes document to Chinese</summary>
      <description>https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/native_kubernetes.html Translate the native kubernetes document to Chinese.English updated in 7723774a0402e10bc914b1fa6128e3c80678dafe</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="17816" opendate="2020-5-19 00:00:00" fixdate="2020-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change Latency Marker to work with "scheduleAtFixedDelay" instead of "scheduleAtFixedRate"</summary>
      <description>Latency Markers and other periodic timers are scheduled with scheduleAtFixedRate. That means every X time the callable is called. If it blocks (backpressure) is can be called immediately again.I would suggest to switch this to scheduleAtFixedDelay to avoid calling for a lot of latency marker injections when there is no way to actually execute the injection call.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="17817" opendate="2020-5-19 00:00:00" fixdate="2020-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CollectResultFetcher fails with EOFException in AggregateReduceGroupingITCase</summary>
      <description>CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1826&amp;view=logs&amp;j=e25d5e7e-2a9c-5589-4940-0b638d75a414&amp;t=f83cd372-208c-5ec4-12a8-3374624571292020-05-19T10:34:18.3224679Z [ERROR] testSingleAggOnTable_SortAgg(org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase) Time elapsed: 7.537 s &lt;&lt;&lt; ERROR!2020-05-19T10:34:18.3225273Z java.lang.RuntimeException: Failed to fetch next result2020-05-19T10:34:18.3227634Z at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:92)2020-05-19T10:34:18.3228518Z at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:63)2020-05-19T10:34:18.3229170Z at org.apache.flink.shaded.guava18.com.google.common.collect.Iterators.addAll(Iterators.java:361)2020-05-19T10:34:18.3229863Z at org.apache.flink.shaded.guava18.com.google.common.collect.Lists.newArrayList(Lists.java:160)2020-05-19T10:34:18.3230586Z at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:300)2020-05-19T10:34:18.3231303Z at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:141)2020-05-19T10:34:18.3231996Z at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:107)2020-05-19T10:34:18.3232847Z at org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase.testSingleAggOnTable(AggregateReduceGroupingITCase.scala:176)2020-05-19T10:34:18.3233694Z at org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase.testSingleAggOnTable_SortAgg(AggregateReduceGroupingITCase.scala:122)2020-05-19T10:34:18.3234461Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-05-19T10:34:18.3234983Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-05-19T10:34:18.3235632Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-05-19T10:34:18.3236615Z at java.lang.reflect.Method.invoke(Method.java:498)2020-05-19T10:34:18.3237256Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)2020-05-19T10:34:18.3237965Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2020-05-19T10:34:18.3238750Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)2020-05-19T10:34:18.3239314Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2020-05-19T10:34:18.3239838Z at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2020-05-19T10:34:18.3240362Z at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2020-05-19T10:34:18.3240803Z at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)2020-05-19T10:34:18.3243624Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-05-19T10:34:18.3244531Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)2020-05-19T10:34:18.3245325Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)2020-05-19T10:34:18.3246086Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)2020-05-19T10:34:18.3246765Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)2020-05-19T10:34:18.3247390Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)2020-05-19T10:34:18.3248012Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)2020-05-19T10:34:18.3248779Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)2020-05-19T10:34:18.3249417Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)2020-05-19T10:34:18.3250357Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-05-19T10:34:18.3251021Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-05-19T10:34:18.3251597Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-05-19T10:34:18.3252141Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2020-05-19T10:34:18.3252798Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)2020-05-19T10:34:18.3253527Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)2020-05-19T10:34:18.3254458Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2020-05-19T10:34:18.3255420Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)2020-05-19T10:34:18.3256207Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2020-05-19T10:34:18.3257025Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2020-05-19T10:34:18.3257719Z at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2020-05-19T10:34:18.3258447Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)2020-05-19T10:34:18.3258948Z Caused by: java.io.EOFException2020-05-19T10:34:18.3259379Z at java.io.DataInputStream.readUnsignedByte(DataInputStream.java:290)2020-05-19T10:34:18.3259826Z at org.apache.flink.api.java.typeutils.runtime.MaskUtils.readIntoMask(MaskUtils.java:73)2020-05-19T10:34:18.3260307Z at org.apache.flink.api.java.typeutils.runtime.RowSerializer.deserialize(RowSerializer.java:200)2020-05-19T10:34:18.3261044Z at org.apache.flink.api.java.typeutils.runtime.RowSerializer.deserialize(RowSerializer.java:58)2020-05-19T10:34:18.3261535Z at org.apache.flink.api.common.typeutils.base.ListSerializer.deserialize(ListSerializer.java:133)2020-05-19T10:34:18.3262105Z at org.apache.flink.streaming.api.operators.collect.CollectCoordinationResponse.getResults(CollectCoordinationResponse.java:91)2020-05-19T10:34:18.3262752Z at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher$ResultBuffer.dealWithResponse(CollectResultFetcher.java:291)2020-05-19T10:34:18.3263385Z at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher$ResultBuffer.access$200(CollectResultFetcher.java:249)2020-05-19T10:34:18.3264077Z at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:142)2020-05-19T10:34:18.3264666Z at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:89)2020-05-19T10:34:18.3265050Z ... 40 more</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectSinkFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="17820" opendate="2020-5-19 00:00:00" fixdate="2020-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Memory threshold is ignored for channel state</summary>
      <description>Config parameter state.backend.fs.memory-threshold is ignored for channel state. Causing each subtask to have a file per checkpoint. Regardless of the size of channel state (of this subtask).This also causes slow cleanup and delays the next checkpoint. The problem is that ChannelStateCheckpointWriter.finishWriteAndResult calls flush(); which actually flushes the data on disk. From FSDataOutputStream.flush Javadoc:A completed flush does not mean that the data is necessarily persistent. Data persistence can is only assumed after calls to close() or sync(). Possible solutions:1. not to flush in ChannelStateCheckpointWriter.finishWriteAndResult (which can lead to data loss in a wrapping stream).2. change FsCheckpointStateOutputStream.flush behavior3. wrap FsCheckpointStateOutputStream to prevent flush}}{{</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.FsStateBackendEntropyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStorageTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="17827" opendate="2020-5-20 00:00:00" fixdate="2020-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>scala-shell.sh should fail early if no mode is specified, or have default logging settings</summary>
      <description>The scala-shell has multiple modes it can run in: local, remote and yarn.It is mandatory to specify such a mode, but this is only enforced on the scala side, not in the bash script.The problem is that the scala-shell script derives the log4j properties from the mode, and if no mode is set, then the log4j properties are empty.This leads to a warning from slf4j that no logger was defined and all that.Either scala-shell.sh should fail early if no mode is specified, or it should have some default logging settings (e.g., the ones for local/remote).</description>
      <version>1.11.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.start-script.start-scala-shell.sh</file>
    </fixedFiles>
  </bug>
  <bug id="17836" opendate="2020-5-20 00:00:00" fixdate="2020-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for Hive dim join</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.hive.streaming.zh.md</file>
      <file type="M">docs.dev.table.hive.hive.streaming.md</file>
    </fixedFiles>
  </bug>
  <bug id="17842" opendate="2020-5-20 00:00:00" fixdate="2020-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression on 19.05.2020</summary>
      <description>There is a noticeable performance regression in many benchmarks:http://codespeed.dak8s.net:8000/timeline/?ben=serializerHeavyString&amp;env=2http://codespeed.dak8s.net:8000/timeline/?ben=networkThroughput.1000,1ms&amp;env=2http://codespeed.dak8s.net:8000/timeline/?ben=networkThroughput.100,100ms&amp;env=2http://codespeed.dak8s.net:8000/timeline/?ben=globalWindow&amp;env=2that happened on May 19th, probably between 260ef2c and 2f18138</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.NonSpanningWrapper.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.SpanningWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="17843" opendate="2020-5-20 00:00:00" fixdate="2020-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check for RowKind when converting Row to expression</summary>
      <description>A row ctor does not allow for a rowKind thus we should check if the rowKind is set when converting from Row to expression.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.expressions.ObjectToExpressionTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.ApiExpressionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="17847" opendate="2020-5-20 00:00:00" fixdate="2020-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ArrayIndexOutOfBoundsException happens when codegen StreamExec operator</summary>
      <description>user case://source table create table json_table( w_es BIGINT, w_type STRING, w_isDdl BOOLEAN, w_data ARRAY&lt;ROW&lt;pay_info STRING, online_fee DOUBLE, sign STRING, account_pay_fee DOUBLE&gt;&gt;, w_ts TIMESTAMP(3), w_table STRING) WITH ( 'connector.type' = 'kafka', 'connector.version' = '0.10', 'connector.topic' = 'json-test2', 'connector.properties.zookeeper.connect' = 'localhost:2181', 'connector.properties.bootstrap.servers' = 'localhost:9092', 'connector.properties.group.id' = 'test-jdbc', 'connector.startup-mode' = 'earliest-offset', 'format.type' = 'json', 'format.derive-schema' = 'true')// real data:{"w_es":1589870637000,"w_type":"INSERT","w_isDdl":false,"w_data":[{"pay_info":"channelId=82&amp;onlineFee=89.0&amp;outTradeNo=0&amp;payId=0&amp;payType=02&amp;rechargeId=4&amp;totalFee=89.0&amp;tradeStatus=success&amp;userId=32590183789575&amp;sign=00","online_fee":"89.0","sign":"00","account_pay_fee":"0.0"}],"w_ts":"2020-05-20T13:58:37.131Z","w_table":"cccc111"}//queryselect w_ts, 'test' as city1_id, w_data[0].pay_info AS cate3_id, w_data as pay_order_id from json_tableexception://Caused by: java.lang.ArrayIndexOutOfBoundsException: 1427848Caused by: java.lang.ArrayIndexOutOfBoundsException: 1427848 at org.apache.flink.table.runtime.util.SegmentsUtil.getByteMultiSegments(SegmentsUtil.java:598) at org.apache.flink.table.runtime.util.SegmentsUtil.getByte(SegmentsUtil.java:590) at org.apache.flink.table.runtime.util.SegmentsUtil.bitGet(SegmentsUtil.java:534) at org.apache.flink.table.dataformat.BinaryArray.isNullAt(BinaryArray.java:117) at StreamExecCalc$10.processElement(Unknown Source) Looks like in the codegen StreamExecCalc$10 operator some operation visit a '-1' index which should be wrong, this bug exits both in 1.10 and 1.11 public class StreamExecCalc$10 extends org.apache.flink.table.runtime.operators.AbstractProcessStreamOperator implements org.apache.flink.streaming.api.operators.OneInputStreamOperator { private final Object[] references; private final org.apache.flink.table.dataformat.BinaryString str$3 = org.apache.flink.table.dataformat.BinaryString.fromString("test"); private transient org.apache.flink.table.runtime.typeutils.BaseArraySerializer typeSerializer$5; final org.apache.flink.table.dataformat.BoxedWrapperRow out = new org.apache.flink.table.dataformat.BoxedWrapperRow(4); private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null); public StreamExecCalc$10( Object[] references, org.apache.flink.streaming.runtime.tasks.StreamTask task, org.apache.flink.streaming.api.graph.StreamConfig config, org.apache.flink.streaming.api.operators.Output output) throws Exception { this.references = references; typeSerializer$5 = (((org.apache.flink.table.runtime.typeutils.BaseArraySerializer) references[0])); this.setup(task, config, output); } @Override public void open() throws Exception { super.open(); } @Override public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception { org.apache.flink.table.dataformat.BaseRow in1 = (org.apache.flink.table.dataformat.BaseRow) element.getValue(); org.apache.flink.table.dataformat.SqlTimestamp field$2; boolean isNull$2; org.apache.flink.table.dataformat.BaseArray field$4; boolean isNull$4; org.apache.flink.table.dataformat.BaseArray field$6; org.apache.flink.table.dataformat.BinaryString field$8; boolean isNull$8; org.apache.flink.table.dataformat.BinaryString result$9; boolean isNull$9; isNull$2 = in1.isNullAt(4); field$2 = null; if (!isNull$2) { field$2 = in1.getTimestamp(4, 3); } isNull$4 = in1.isNullAt(3); field$4 = null; if (!isNull$4) { field$4 = in1.getArray(3); } field$6 = field$4; if (!isNull$4) { field$6 = (org.apache.flink.table.dataformat.BaseArray) (typeSerializer$5.copy(field$6)); } out.setHeader(in1.getHeader()); if (isNull$2) { out.setNullAt(0); } else { out.setNonPrimitiveValue(0, field$2); } if (false) { out.setNullAt(1); } else { out.setNonPrimitiveValue(1, ((org.apache.flink.table.dataformat.BinaryString) str$3)); } boolean isNull$7 = isNull$4 || false || field$6.isNullAt(((int) 0) - 1); org.apache.flink.table.dataformat.BaseRow result$7 = isNull$7 ? null : field$6.getRow(((int) 0) - 1, 4); if (isNull$7) { result$9 = org.apache.flink.table.dataformat.BinaryString.EMPTY_UTF8; isNull$9 = true; } else { isNull$8 = result$7.isNullAt(0); field$8 = org.apache.flink.table.dataformat.BinaryString.EMPTY_UTF8; if (!isNull$8) { field$8 = result$7.getString(0); } result$9 = field$8; isNull$9 = isNull$8; } if (isNull$9) { out.setNullAt(2); } else { out.setNonPrimitiveValue(2, result$9); } if (isNull$4) { out.setNullAt(3); } else { out.setNonPrimitiveValue(3, field$6); } output.collect(outElement.replace(out)); } @Override public void close() throws Exception { super.close(); }}     </description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ArrayTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="1785" opendate="2015-3-26 00:00:00" fixdate="2015-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Master tests in flink-tachyon fail with java.lang.NoSuchFieldError: IBM_JAVA</summary>
      <description>The master fail in flink-tachyon test when running mvn test:------------------------------------------------------- T E S T S-------------------------------------------------------------------------------------------------------------- T E S T S-------------------------------------------------------Running org.apache.flink.tachyon.HDFSTestRunning org.apache.flink.tachyon.TachyonFileSystemWrapperTestjava.lang.NoSuchFieldError: IBM_JAVAat org.apache.hadoop.security.UserGroupInformation.getOSLoginModuleName(UserGroupInformation.java:303)at org.apache.hadoop.security.UserGroupInformation.&lt;clinit&gt;(UserGroupInformation.java:348)at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:807)at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:266)at org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:122)at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:775)at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:642)at org.apache.hadoop.hdfs.MiniDFSCluster.&lt;init&gt;(MiniDFSCluster.java:334)at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:316)at org.apache.flink.tachyon.HDFSTest.createHDFS(HDFSTest.java:62)at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)at java.lang.reflect.Method.invoke(Method.java:606)at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)...Results :Failed tests: HDFSTest.createHDFS:76 Test failed IBM_JAVA HDFSTest.createHDFS:76 Test failed Could not initialize classorg.apache.hadoop.security.UserGroupInformationTests in error: HDFSTest.destroyHDFS:83 NullPointer HDFSTest.destroyHDFS:83 NullPointer TachyonFileSystemWrapperTest.testHadoopLoadability:116 »NoClassDefFound Could...Tests run: 6, Failures: 3, Errors: 3, Skipped: 0</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-tachyon.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17861" opendate="2020-5-21 00:00:00" fixdate="2020-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Channel state handles, when inlined, duplicate underlying data</summary>
      <description>If Unaligned checkpoints are enabled, channel state is written as state handles. Each channel has a handle and each such handle references the same underlying streamStateHandle (this is done to have a single file per subtask).But, if the state is less then state.backend.fs.memory-threshold, the data is sent directly to JM as a byteStreamHandle. This causes each channel state handle to hold the whole subtask state.This PR solves this by extracting relevant potions of the underlying handles if they are {{byteStreamHandle}}s.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateSerializerImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateReaderImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.RefCountingFSDataInputStream.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateStreamReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateReaderImpl.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.InterruptSensitiveRestoreTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateDownloaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.testutils.EmptyStreamStateHandle.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.SharedStateRegistryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.messages.CheckpointMessagesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StreamStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.RetrievableStreamStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.PlaceholderStreamStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.OperatorStreamStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.ByteStreamStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyGroupsStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FileStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="17867" opendate="2020-5-21 00:00:00" fixdate="2020-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Hive-3.1.1 test</summary>
      <description>TableEnvHiveConnectorTest::testOrcSchemaEvol fails due to "ClassNotFoundException: org.apache.hadoop.hdfs.client.HdfsDataOutputStream$SyncFlag"</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17868" opendate="2020-5-21 00:00:00" fixdate="2020-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Proctime in DDL can not work in batch mode</summary>
      <description>The data of this proctime column will be all null. Should same to current timestamp.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.over.NonBufferOverWindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.over.BufferDataOverWindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.TableStreamOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.AbstractProcessStreamOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableScanITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.BatchLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.StreamOptimizeContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkRelTimeIndicatorProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkOptimizeContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkBatchProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLegacyTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecDataStreamScan.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecCorrelate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecCalc.java</file>
    </fixedFiles>
  </bug>
  <bug id="17870" opendate="2020-5-22 00:00:00" fixdate="2020-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>dependent jars are missing to be shipped to cluster in scala shell</summary>
      <description></description>
      <version>1.11.0</version>
      <fixedVersion>1.10.2,1.11.0,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.main.java.org.apache.flink.api.java.ScalaShellStreamEnvironment.java</file>
      <file type="M">flink-scala-shell.src.main.java.org.apache.flink.api.java.ScalaShellEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="17872" opendate="2020-5-22 00:00:00" fixdate="2020-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update StreamingFileSink documents to add avro formats</summary>
      <description>We added Avro-format for StreamingFileSink in FLINK-11395 , but did not update the document to reflect that.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.streamfile.sink.zh.md</file>
      <file type="M">docs.dev.connectors.streamfile.sink.md</file>
    </fixedFiles>
  </bug>
  <bug id="17880" opendate="2020-5-22 00:00:00" fixdate="2020-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use new type inference for SQL table and scalar functions</summary>
      <description>We urgently need to reduce the friction around different type systems and types of function. Therefore, we should update table/scalar functions in SQL to the new type inference.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.stream.table.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.FunctionCatalogOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.PythonUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.python.PythonTableFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.python.PythonScalarFunction.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.functions.FunctionDefinitionUtilTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.functions.FunctionDefinitionUtil.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.bridge.java.StreamTableEnvironment.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.ExecutionEntry.java</file>
    </fixedFiles>
  </bug>
  <bug id="17881" opendate="2020-5-22 00:00:00" fixdate="2020-5-22 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add documentation for PyFlink&amp;#39;s Windows support</summary>
      <description>Currently PyFlink already supports running on windows in Flink 1.11. But as we drop the bat script in Flink 1.11, submitting Python job on windows is not supported. We should add documentation for this.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.python.installation.zh.md</file>
      <file type="M">docs.dev.table.python.installation.md</file>
    </fixedFiles>
  </bug>
  <bug id="17882" opendate="2020-5-22 00:00:00" fixdate="2020-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t allow self referencing structured type</summary>
      <description>Currently, the logical constraint "A type cannot be defined so that one of its attribute types (transitively) uses itself." is not enforced during extraction and leads to a stack overflow. We should throw a helpful exception instead.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.extraction.DataTypeExtractorTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.extraction.ExtractionUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.extraction.DataTypeExtractor.java</file>
    </fixedFiles>
  </bug>
  <bug id="17886" opendate="2020-5-22 00:00:00" fixdate="2020-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Chinese documentation for new WatermarkGenerator/WatermarkStrategies</summary>
      <description>We need to update the Chinese documentation according to FLINK-17773.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.event.timestamp.extractors.zh.md</file>
      <file type="M">docs.dev.event.timestamps.watermarks.zh.md</file>
      <file type="M">docs.dev.event.time.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="17889" opendate="2020-5-22 00:00:00" fixdate="2020-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-connector-hive jar contains wrong class in its SPI config file org.apache.flink.table.factories.TableFactory</summary>
      <description>These 2 classes are in flink-connector-hive jar's SPI config fileorg.apache.flink.orc.OrcFileSystemFormatFactoryLicense.org.apache.flink.formats.parquet.ParquetFileSystemFormatFactory Due to this issue, I get the following exception in zeppelin side.Caused by: java.util.ServiceConfigurationError: org.apache.flink.table.factories.TableFactory: Provider org.apache.flink.orc.OrcFileSystemFormatFactory not a subtypeCaused by: java.util.ServiceConfigurationError: org.apache.flink.table.factories.TableFactory: Provider org.apache.flink.orc.OrcFileSystemFormatFactory not a subtype at java.util.ServiceLoader.fail(ServiceLoader.java:239) at java.util.ServiceLoader.access$300(ServiceLoader.java:185) at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:376) at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) at java.util.ServiceLoader$1.next(ServiceLoader.java:480) at java.util.Iterator.forEachRemaining(Iterator.java:116) at org.apache.flink.table.factories.TableFactoryService.discoverFactories(TableFactoryService.java:214) ... 35 more</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17893" opendate="2020-5-23 00:00:00" fixdate="2020-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL-CLI no exception stack</summary>
      <description>If write a wrong DDL, only "&amp;#91;ERROR&amp;#93; Unknown or invalid SQL statement" message.No exception stack in client and logs.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="17894" opendate="2020-5-23 00:00:00" fixdate="2020-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RowGenerator in datagen connector should be serializable</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.DataGenTableSourceFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.DataGenTableSourceFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="17895" opendate="2020-5-23 00:00:00" fixdate="2020-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Default value of rows-per-second in datagen can be limited</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.DataGenTableSourceFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="17897" opendate="2020-5-23 00:00:00" fixdate="2020-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Resolve stability annotations discussion for FLIP-27 in 1.11</summary>
      <description>Currently the interfaces from the FLIP-27 sources are all labeled as @Public.Given that FLIP-27 is going to be in a "beta" version in the 1.11 release, we are discussing to downgrade the stability to @PublicEvolving.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SplitsAssignment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SplitEnumeratorContext.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SplitEnumerator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SourceSplit.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SourceReaderContext.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SourceReader.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SourceOutput.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SourceEvent.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.Source.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.ReaderInfo.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.Boundedness.java</file>
    </fixedFiles>
  </bug>
  <bug id="17905" opendate="2020-5-24 00:00:00" fixdate="2020-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Docs for JDBC connector show licence and markup</summary>
      <description></description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.jdbc.zh.md</file>
      <file type="M">docs.dev.connectors.jdbc.md</file>
    </fixedFiles>
  </bug>
  <bug id="17918" opendate="2020-5-25 00:00:00" fixdate="2020-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LIMIT queries are failed when adding sleeping time of async checkpoint</summary>
      <description>When we change the timing of operations (sleep after emit first record and sleep for async operation of checkpoint) with this commit, the test org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase#testDifferentTypesSumWithRetract in flink-table-planner-blink is failed.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.AppendOnlyTopNFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.FailingCollectionSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="17923" opendate="2020-5-25 00:00:00" fixdate="2020-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>It will throw MemoryAllocationException if rocksdb statebackend and Python UDF are used in the same slot</summary>
      <description>For the following job:import loggingimport osimport shutilimport sysimport tempfilefrom pyflink.datastream import StreamExecutionEnvironmentfrom pyflink.table import TableConfig, StreamTableEnvironment, DataTypesfrom pyflink.table.udf import udfdef word_count(): content = "line Licensed to the Apache Software Foundation ASF under one " \ "line or more contributor license agreements See the NOTICE file " \ "line distributed with this work for additional information " \ "line regarding copyright ownership The ASF licenses this file " \ "to you under the Apache License Version the " \ "License you may not use this file except in compliance " \ "with the License" t_config = TableConfig() env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(env, t_config) # register Results table in table environment tmp_dir = tempfile.gettempdir() result_path = tmp_dir + '/result' if os.path.exists(result_path): try: if os.path.isfile(result_path): os.remove(result_path) else: shutil.rmtree(result_path) except OSError as e: logging.error("Error removing directory: %s - %s.", e.filename, e.strerror) logging.info("Results directory: %s", result_path) sink_ddl = """ create table Results( word VARCHAR, `count` BIGINT ) with ( 'connector' = 'blackhole' ) """ t_env.sql_update(sink_ddl) @udf(input_types=[DataTypes.BIGINT()], result_type=DataTypes.BIGINT()) def inc(count): return count + 1 t_env.register_function("inc", inc) elements = [(word, 1) for word in content.split(" ")] t_env.from_elements(elements, ["word", "count"]) \ .group_by("word") \ .select("word, count(1) as count") \ .select("word, inc(count) as count") \ .insert_into("Results") t_env.execute("word_count")if __name__ == '__main__': logging.basicConfig(stream=sys.stdout, level=logging.INFO, format="%(message)s") word_count()It will throw the following exception if rocksdb state backend is used:Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for KeyedProcessOperator_c27dcf7b54ef6bfd6cff02ca8870b681_(1/1) from any of the 1 provided restore options. at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:317) at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:144) ... 9 moreCaused by: java.io.IOException: Failed to acquire shared cache resource for RocksDB at org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.allocateSharedCachesIfConfigured(RocksDBOperationUtils.java:212) at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:516) at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:301) at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142) at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121) ... 11 moreCaused by: org.apache.flink.runtime.memory.MemoryAllocationException: Could not created the shared memory resource of size 536870920. Not enough memory left to reserve from the slot's managed memory. at org.apache.flink.runtime.memory.MemoryManager.lambda$getSharedMemoryResourceForManagedMemory$8(MemoryManager.java:603) at org.apache.flink.runtime.memory.SharedResources.createResource(SharedResources.java:130) at org.apache.flink.runtime.memory.SharedResources.getOrAllocateSharedResource(SharedResources.java:72) at org.apache.flink.runtime.memory.MemoryManager.getSharedMemoryResourceForManagedMemory(MemoryManager.java:617) at org.apache.flink.runtime.memory.MemoryManager.getSharedMemoryResourceForManagedMemory(MemoryManager.java:566) at org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.allocateSharedCachesIfConfigured(RocksDBOperationUtils.java:208) ... 15 moreCaused by: org.apache.flink.runtime.memory.MemoryReservationException: Could not allocate 536870920 bytes. Only 454033416 bytes are remaining. at org.apache.flink.runtime.memory.MemoryManager.reserveMemory(MemoryManager.java:461) at org.apache.flink.runtime.memory.MemoryManager.lambda$getSharedMemoryResourceForManagedMemory$8(MemoryManager.java:601) ... 20 more</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonBase.scala</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.PythonConfigTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.python.PythonFunctionFactoryTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonConfig.java</file>
      <file type="M">flink-python.pyflink.testing.test.case.utils.py</file>
      <file type="M">docs..includes.generated.python.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="17931" opendate="2020-5-25 00:00:00" fixdate="2020-6-25 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Document fromValues clause</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="17936" opendate="2020-5-26 00:00:00" fixdate="2020-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement type inference for AS</summary>
      <description>Type information gets lost due to the legacy planner expressions. The user might experience unexpected exceptions.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.fieldExpression.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
    </fixedFiles>
  </bug>
  <bug id="17937" opendate="2020-5-26 00:00:00" fixdate="2020-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change some hive connector tests to IT cases</summary>
      <description>Hive connector tests that use FlinkStandaloneHiveRunner should be IT cases. Besides, changing them to IT cases can avoid lib conflicts because we'll be using the shaded jars, e.g. guava.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveLookupJoinTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="1794" opendate="2015-3-27 00:00:00" fixdate="2015-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add test base for scalatest and adapt flink-ml test cases</summary>
      <description>Currently, the flink-ml test cases use the standard ExecutionEnvironment which can cause problems in parallel test executions as they happen on Travis. For these tests it would be helpful to have an appropriate Scala test base which instantiates a ForkableFlinkMiniCluster and sets the ExecutionEnvironment appropriately.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils.pom.xml</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.regression.MultipleLinearRegressionITCase.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.recommendation.ALSITCase.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.feature.PolynomialBaseITCase.scala</file>
      <file type="M">flink-staging.flink-ml.pom.xml</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.Client.java</file>
    </fixedFiles>
  </bug>
  <bug id="17944" opendate="2020-5-26 00:00:00" fixdate="2020-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong output in sql client&amp;#39;s table mode</summary>
      <description>When I run the following sql example, I get the wrong outputSELECT name, COUNT(*) AS cnt FROM (VALUES ('Bob'), ('Alice'), ('Greg'), ('Bob')) AS NameTable(name) GROUP BY name;   Bob 1 Alice 1 Greg 1 Bob 2 This is due to we add kind in Row, so the sematics of equals method changes</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResult.java</file>
    </fixedFiles>
  </bug>
  <bug id="17957" opendate="2020-5-27 00:00:00" fixdate="2020-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Forbidden syntax "CREATE SYSTEM FUNCTION" for sql parser</summary>
      <description>This syntax is invalid, but the parser still works.</description>
      <version>1.11.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.utils.ParserResource.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
    </fixedFiles>
  </bug>
  <bug id="17958" opendate="2020-5-27 00:00:00" fixdate="2020-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kubernetes session constantly allocates taskmanagers after cancel a job</summary>
      <description>When i am testing the kubernetes-session.sh, i find that the KubernetesResourceManager will constantly allocate taskmanager after cancel a job. I think it may be caused by a bug of the following code. When the dividend is 0 and divisor is bigger than 1, the return value will be 1. However, we expect it to be 0./** * Divide and rounding up to integer. * E.g., divideRoundUp(3, 2) returns 2. * @param dividend value to be divided by the divisor * @param divisor value by which the dividend is to be divided * @return the quotient rounding up to integer */public static int divideRoundUp(int dividend, int divisor) { return (dividend - 1) / divisor + 1;} How to reproduce this issue? Start a Kubernetes session Submit a Flink job to the existing session Cancel the job and wait for the TaskManager released via idle timeout More and more TaskManagers will be allocated</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.WorkerSpecContainerResourceAdapter.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.MathUtilTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.MathUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="17960" opendate="2020-5-27 00:00:00" fixdate="2020-6-27 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Improve commands in the "Common Questions" document for PyFlink</summary>
      <description>Currently, in the "Common Questions" document, we have the command `$ setup-pyflink-virtual-env.sh` to run the script. However, the script is not executable. It would be better to replace the command with `$ sh setup-pyflink-virtual-env.sh` and add download command.$ curl -O https://ci.apache.org/projects/flink/flink-docs-master/downloads/setup-pyflink-virtual-env.sh$ sh setup-pyflink-virtual-env.sh</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.python.common.questions.zh.md</file>
      <file type="M">docs.dev.table.python.common.questions.md</file>
    </fixedFiles>
  </bug>
  <bug id="17974" opendate="2020-5-27 00:00:00" fixdate="2020-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test new Flink Docker image</summary>
      <description>Test Flink's new Docker image and the corresponding Dockerfile: Try to build custom image Try to run different Flink processes (Master (session, per-job), TaskManager) Try custom configuration and log properties</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.docker.zh.md</file>
      <file type="M">docs.ops.deployment.docker.md</file>
    </fixedFiles>
  </bug>
  <bug id="17976" opendate="2020-5-27 00:00:00" fixdate="2020-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test native K8s integration</summary>
      <description>Test Flink's native K8s integration: session mode application mode custom Flink image custom configuration and log properties</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
      <file type="M">docs.ops.deployment.index.zh.md</file>
      <file type="M">docs.ops.deployment.index.md</file>
      <file type="M">docs.ops.deployment.docker.zh.md</file>
      <file type="M">docs.ops.deployment.docker.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="17977" opendate="2020-5-27 00:00:00" fixdate="2020-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check log sanity</summary>
      <description>Run a normal Flink workload (e.g. job with fixed number of failures on session cluster) and check that the produced Flink logs make sense and don't contain confusing statements.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.ConnectionUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.SecurityUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.registration.RetryingRegistration.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="17981" opendate="2020-5-27 00:00:00" fixdate="2020-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add new Flink docs homepage content</summary>
      <description>Flink docs homepage requires a serious redesign to better guide users through the different sections of the documentation. This ticket is focused soley on updating the text.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..config.yml</file>
      <file type="M">docs.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="17982" opendate="2020-5-27 00:00:00" fixdate="2020-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Resolve TODO&amp;#39;s in concepts documentation</summary>
      <description>The concepts section in the documentation contains several TODO's. These should be replaced with proper content or the TODO's removed for 1.11.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.concepts.stateful-stream-processing.md</file>
      <file type="M">docs.concepts.timely-stream-processing.md</file>
    </fixedFiles>
  </bug>
  <bug id="17986" opendate="2020-5-27 00:00:00" fixdate="2020-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Erroneous check in FsCheckpointStateOutputStream#write(int)</summary>
      <description>When fixing FLINK-17820 a flush call was accidentally introduced on every single byte/int write to FsCheckpointStateOutputStream, which could significantly affect performance.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactoryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="17988" opendate="2020-5-27 00:00:00" fixdate="2020-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkpointing slows down after reaching state.checkpoints.num-retained</summary>
      <description>With Unaligned checkpoints, happens always (new checkpoint is never started or triggered).With Aligned checkpoints - to some degree - depending on state size and thresholds: delayed by 1 minute. Delay grows very slowly with state size. Filesystems: s3p and s3aParallelism: 176, repartition (num stages): 5.Number of files in checkpoint is about 1K (depends on state.backend.fs.memory-threshold and their size). Size doesn't matter (100K..10G).</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractChannelStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorSubtaskState.java</file>
    </fixedFiles>
  </bug>
  <bug id="1799" opendate="2015-3-30 00:00:00" fixdate="2015-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala API does not support generic arrays</summary>
      <description>The Scala API does not support generic arrays at the moment. It throws a rather unhelpful error message ```InvalidTypesException: The given type is not a valid object array```.Code to reproduce the problem is given below:def main(args: Array[String]) { foobar[Double]}def foobar[T: ClassTag: TypeInformation]: DataSet[Block[T]] = { val tpe = createTypeInformation[Array[T]] null}</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.types.TypeInformationGenTest.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.codegen.TypeInformationGen.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="1800" opendate="2015-3-30 00:00:00" fixdate="2015-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a "Beta" badge in the documentation to components in flink-staging</summary>
      <description>As per mailing list discussion: http://apache-flink-incubator-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Add-a-quot-Beta-quot-badge-in-the-documentation-to-components-in-flink-staging-td4801.html</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..layouts.default.html</file>
      <file type="M">docs..includes.sidenav.html</file>
      <file type="M">docs..includes.navbar.html</file>
      <file type="M">docs.table.md</file>
      <file type="M">docs.streaming.guide.md</file>
      <file type="M">docs.hadoop.compatibility.md</file>
      <file type="M">docs.gelly.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="18004" opendate="2020-5-28 00:00:00" fixdate="2020-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update checkpoint UI related pictures in documentation</summary>
      <description>After FLINK-13390 which clarifies what the "state size" means on incremental checkpoint, the checkpoint UI has already changed, and we should also update related documentation to not mislead users.</description>
      <version>1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.fig.checkpoint.monitoring-summary.png</file>
      <file type="M">docs.fig.checkpoint.monitoring-history.png</file>
      <file type="M">docs.fig.checkpoint.monitoring-details.summary.png</file>
      <file type="M">docs.fig.checkpoint.monitoring-details.subtasks.png</file>
      <file type="M">docs.fig.checkpoint.monitoring-details.png</file>
    </fixedFiles>
  </bug>
  <bug id="18005" opendate="2020-5-28 00:00:00" fixdate="2020-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement type inference for CAST</summary>
      <description>CAST has high priority because it looses a lot of information about types.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.RowTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.DecimalTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.time.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.cast.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.call.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.aggregations.scala</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.utils.CallContextMock.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.TypeStrategiesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.InputTypeStrategiesTestBase.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.InputTypeStrategiesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.TypeStrategies.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.strategies.MapInputTypeStrategy.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.strategies.ComparableTypeStrategy.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.strategies.ArrayInputTypeStrategy.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.InputTypeStrategies.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.ConstantArgumentCount.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="18006" opendate="2020-5-28 00:00:00" fixdate="2020-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>It will throw Invalid lambda deserialization Exception when writing to elastic search with new format</summary>
      <description>My job follows:// create table csv( pageId VARCHAR, eventId VARCHAR, recvTime VARCHAR) with ( 'connector' = 'filesystem', 'path' = '/Users/ohmeatball/Work/flink-sql-etl/data-generator/src/main/resources/user3.csv', 'format' = 'csv' )-----------------------------------------CREATE TABLE es_table ( aggId varchar , pageId varchar , ts varchar , expoCnt int , clkCnt int) WITH ('connector' = 'elasticsearch','hosts' = 'http://localhost:9200','index' = 'cli_test','document-id.key-delimiter' = '$','sink.bulk-flush.interval' = '1000','format' = 'json')-----------------------------------------INSERT INTO es_tableSELECT pageId,eventId,cast(recvTime as varchar) as ts, 1, 1 from csv;The full exception follows:Sink(table=[default_catalog.default_database.es_table], fields=[aggId, pageId, ts, expoCnt, clkCnt]) (1/1) (b51209fac96948c20e85b8df137287d3) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@bb5ab41.Sink(table=[default_catalog.default_database.es_table], fields=[aggId, pageId, ts, expoCnt, clkCnt]) (1/1) (b51209fac96948c20e85b8df137287d3) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@bb5ab41.org.apache.flink.streaming.runtime.tasks.StreamTaskException: Cannot instantiate user function. at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:291) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:471) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:393) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:459) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:393) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.&lt;init&gt;(OperatorChain.java:155) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:518) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:720) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:545) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_151]Caused by: java.io.IOException: unexpected exception type at java.io.ObjectStreamClass.throwMiscException(ObjectStreamClass.java:1682) ~[?:1.8.0_151] at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1254) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2073) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] ... 10 moreCaused by: java.lang.reflect.InvocationTargetExceptionCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1248) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2073) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] ... 10 moreCaused by: java.lang.IllegalArgumentException: Invalid lambda deserialization at org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSink$Builder.$deserializeLambda$(ElasticsearchSink.java:80) ~[flink-sql-connector-elasticsearch7_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1248) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2073) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] ... 10 moreNotice: everything works fine with former connector grammer.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="18012" opendate="2020-5-28 00:00:00" fixdate="2020-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deactivate slot timeout if TaskSlotTable.tryMarkSlotActive is called</summary>
      <description>With FLINK-9932 we loosened the slot allocation protocol in a way that the JobMaster can deploy Tasks into a slot which has not been ACTIVATED but only ALLOCATED for a given job. This allowed to better handle the case where the JobMasterGateway#offerSlots response was late so that it timed out. The way it was solved is to offer a TaskSlotTable#tryMarkSlotActive method which, in contrast to TaskSlotTable#markSlotActive, would not fail if the requested slot was not available.However, the problem is that the former method does not deactivate the slot timeout. Hence, it can happen if the offerSlots response never arrives at the TaskExecutor that an ACTIVATED slot times out.In order to fix the problem, we should also deactivate the slot timeout when TaskSlotTable#tryMarkSlotActive is being called.</description>
      <version>1.9.3,1.10.1,1.11.0</version>
      <fixedVersion>1.9.4,1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="18027" opendate="2020-5-29 00:00:00" fixdate="2020-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ROW value constructor cannot deal with complex expressions</summary>
      <description>create table my_source (my_row row&lt;a int, b int, c int&gt;) with (...);create table my_sink (my_row row&lt;a int, b int&gt;) with (...);insert into my_sinkselect ROW(my_row.a, my_row.b) from my_source;will throw excepions:Exception in thread "main" org.apache.flink.table.api.SqlParserException: SQL parse failed. Encountered "." at line 1, column 18.Exception in thread "main" org.apache.flink.table.api.SqlParserException: SQL parse failed. Encountered "." at line 1, column 18.Was expecting one of:    ")" ...    "," ...     at org.apache.flink.table.planner.calcite.CalciteParser.parse(CalciteParser.java:56) at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:64) at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:627) at com.bytedance.demo.KafkaTableSource.main(KafkaTableSource.java:76)Caused by: org.apache.calcite.sql.parser.SqlParseException: Encountered "." at line 1, column 18.Was expecting one of:    ")" ...    "," ...     at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.convertException(FlinkSqlParserImpl.java:416) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.normalizeException(FlinkSqlParserImpl.java:201) at org.apache.calcite.sql.parser.SqlParser.handleException(SqlParser.java:148) at org.apache.calcite.sql.parser.SqlParser.parseQuery(SqlParser.java:163) at org.apache.calcite.sql.parser.SqlParser.parseStmt(SqlParser.java:188) at org.apache.flink.table.planner.calcite.CalciteParser.parse(CalciteParser.java:54) ... 3 moreCaused by: org.apache.flink.sql.parser.impl.ParseException: Encountered "." at line 1, column 18.Was expecting one of:    ")" ...    "," ...     at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.generateParseException(FlinkSqlParserImpl.java:36161) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.jj_consume_token(FlinkSqlParserImpl.java:35975) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.ParenthesizedSimpleIdentifierList(FlinkSqlParserImpl.java:21432) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.Expression3(FlinkSqlParserImpl.java:17164) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.Expression2b(FlinkSqlParserImpl.java:16820) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.Expression2(FlinkSqlParserImpl.java:16861) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.Expression(FlinkSqlParserImpl.java:16792) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SelectExpression(FlinkSqlParserImpl.java:11091) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SelectItem(FlinkSqlParserImpl.java:10293) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SelectList(FlinkSqlParserImpl.java:10267) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlSelect(FlinkSqlParserImpl.java:6943) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.LeafQuery(FlinkSqlParserImpl.java:658) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.LeafQueryOrExpr(FlinkSqlParserImpl.java:16775) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.QueryOrExpr(FlinkSqlParserImpl.java:16238) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.OrderedQueryOrExpr(FlinkSqlParserImpl.java:532) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlStmt(FlinkSqlParserImpl.java:3761) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlStmtEof(FlinkSqlParserImpl.java:3800) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.parseSqlStmtEof(FlinkSqlParserImpl.java:248) at org.apache.calcite.sql.parser.SqlParser.parseQuery(SqlParser.java:161) ... 5 more </description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.RowFunctionITCase.java</file>
      <file type="M">docs.dev.table.functions.systemFunctions.zh.md</file>
      <file type="M">docs.dev.table.functions.systemFunctions.md</file>
    </fixedFiles>
  </bug>
  <bug id="18029" opendate="2020-5-29 00:00:00" fixdate="2020-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more ITCases for Kafka with new formats</summary>
      <description>Add ITCase for Kafka read/write CSV Add ITCase for Kafka read/write Avro Add ITCase for Kafka read/write JSON</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1803" opendate="2015-3-30 00:00:00" fixdate="2015-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add type hints to the streaming api</summary>
      <description>The streaming operators currently don't support type hints as the batch api does with the `returns(...)` method. The functionality is essentially provided already by the setType method, but it should be replaced and modified to fully provide the necessary functionality.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamJoinOperator.scala</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamCrossOperator.scala</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.temporaloperator.StreamJoinOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.temporaloperator.StreamCrossOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="18035" opendate="2020-5-29 00:00:00" fixdate="2020-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Executors#newCachedThreadPool could not work as expected</summary>
      <description>In FLINK-17558, we introduce Executors#newCachedThreadPool to create dedicated thread pool for TaskManager io. However, it could not work as expected.The root cause is about the following constructor of ThreadPoolExecutor. Only when the workQueue is full, new thread will be started then. So if we set a LinkedBlockingQueue with Integer.MAX_VALUE capacity, only one thread will be started. It never grows up. public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory)</description>
      <version>1.10.2,1.11.0</version>
      <fixedVersion>1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="18042" opendate="2020-5-31 00:00:00" fixdate="2020-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump flink-shaded version to 11.0</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.TestUtils.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.SecureTestEnvironment.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.LocalStandaloneFlinkResourceFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.LocalStandaloneFlinkResource.java</file>
    </fixedFiles>
  </bug>
  <bug id="18045" opendate="2020-6-1 00:00:00" fixdate="2020-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Kerberos credentials checking to unblock Flink on secured MapR</summary>
      <description>I was not able to run Flink 1.10.1 on YARN on a a secured MapR cluster, but the previous version (1.10.0) works fine.After some investigation it looks like during some refactoring, checking if the enabled security method is kerberos was removed, effectively reintroducing https://issues.apache.org/jira/browse/FLINK-5949 Refactoring commit: https://github.com/apache/flink/commit/8751e69037d8a9b1756b75eed62a368c3ef29137 My proposal would be to bring back the kerberos check:loginUser.getAuthenticationMethod() == UserGroupInformation.AuthenticationMethod.KERBEROSand add an unit test for that case to prevent it from happening againI'm happy to prepare a PR after reaching consensus</description>
      <version>1.10.1,1.11.0</version>
      <fixedVersion>1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.modules.HadoopModule.java</file>
      <file type="M">flink-filesystems.flink-hadoop-fs.src.main.java.org.apache.flink.runtime.util.HadoopUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="18046" opendate="2020-6-1 00:00:00" fixdate="2020-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Decimal column stats not supported for Hive table</summary>
      <description>For now, we can just return CatalogColumnStatisticsDataDouble for decimal columns.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogHiveMetadataTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveStatsUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="18065" opendate="2020-6-2 00:00:00" fixdate="2020-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for new scalar/table functions</summary>
      <description>Write documentation for scalar/table functions of FLIP-65.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="18066" opendate="2020-6-2 00:00:00" fixdate="2020-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for how to develop a new table source/sink</summary>
      <description>Covers how to write a custom source/sink and format using FLIP-95 interfaces.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sourceSinks.md</file>
    </fixedFiles>
  </bug>
  <bug id="18068" opendate="2020-6-2 00:00:00" fixdate="2020-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Job scheduling stops but not exits after throwing non-fatal exception</summary>
      <description>The batch job will stop but still be alive with doing nothing for a long time (maybe forever?) if any non fatal exception is thrown from interacting with YARN. Here is the example :java.lang.IllegalStateException: The RMClient's and YarnResourceManagers internal state about the number of pending container requests for resource &lt;memory:10240, vCores:1.0&gt; has diverged. Number client's pending container requests 40 != Number RM's pending container requests 0. at org.apache.flink.util.Preconditions.checkState(Preconditions.java:217) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.yarn.YarnResourceManager.getPendingRequestsAndCheckConsistency(YarnResourceManager.java:518) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.yarn.YarnResourceManager.onContainersOfResourceAllocated(YarnResourceManager.java:431) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.yarn.YarnResourceManager.lambda$onContainersAllocated$1(YarnResourceManager.java:395) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-ljy-1.0.jar:?] at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-ljy-1.0.jar:?] at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-ljy-1.0.jar:?] at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-ljy-1.0.jar:?] at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]</description>
      <version>1.10.1,1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerDriverTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManagerDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="1807" opendate="2015-3-31 00:00:00" fixdate="2015-2-31 01:00:00" resolution="Won&amp;#39;t Do">
    <buginformation>
      <summary>Stochastic gradient descent optimizer for ML library</summary>
      <description>Stochastic gradient descent (SGD) is a widely used optimization technique in different ML algorithms. Thus, it would be helpful to provide a generalized SGD implementation which can be instantiated with the respective gradient computation. Such a building block would make the development of future algorithms easier.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.regression.RegressionData.scala</file>
      <file type="M">flink-staging.flink-ml.pom.xml</file>
      <file type="M">docs.libs.ml.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="18074" opendate="2020-6-2 00:00:00" fixdate="2020-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Confirm checkpoint completed on task side would not fail the task if exception thrown out</summary>
      <description>FLINK-17350 let the task fail immediately once sync phase of checkpoint failed. However, the included commit 'Simplify checkpoint exception handling' actually would not fail the task if the runnable of () -&gt; notifyCheckpointComplete throwing exception out.In a nutshell, this actually changes previous checkpoint exception handling.Moreover, that part of code also affect the implemented code of notifyCheckpointAbortAsync when I introduce notifyCheckpointAborted on task side.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="18075" opendate="2020-6-2 00:00:00" fixdate="2020-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka connector does not call open method of (de)serialization schema</summary>
      <description>The Kafka consumer and producer do not call the open methods of plain (De)SerializationSchema interfaces. Only the Keyed and Kafka specific interfaces. The updated SQL implementations such as AvroRowDataSeriailzationSchema use these methods and so SQL queries using avro and kafka will fail in a null pointer exception. cc aljoscha</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.table.Kafka011DynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka011TableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.DataGenerators.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaShortRetentionTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="18076" opendate="2020-6-2 00:00:00" fixdate="2020-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sql client uses wrong class loader when parsing queries</summary>
      <description>Sql-client when parsing queries does not use the user class loader from ExecutionContext. This makes it impossible to query any sources if the dependencies are added with -j flag.In order to reproduce it try querying e.g. KafkaDynamicSource withCREATE TABLE MyUserTable ( f0 BIGINT) WITH ( 'connector' = 'kafka', 'topic' = 'topic_name', -- required: topic name from which the table is read -- required: specify the Kafka server connection string 'properties.bootstrap.servers' = 'localhost:9092', -- required for Kafka source, optional for Kafka sink, specify consumer group 'properties.group.id' = 'testGroup', -- optional: valid modes are "earliest-offset", "latest-offset", "group-offsets", "specific-offsets" or "timestamp"'scan.startup.mode' = 'earliest-offset', 'format' = 'avro');SELECT * FROM MyUserTable;It give exception:Exception in thread "main" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue. at org.apache.flink.table.client.SqlClient.main(SqlClient.java:213)Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: Invalidate SQL statement. at org.apache.flink.table.client.cli.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:95) at org.apache.flink.table.client.cli.SqlCommandParser.parse(SqlCommandParser.java:79) at org.apache.flink.table.client.cli.CliClient.parseCommand(CliClient.java:256) at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:212) at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:142) at org.apache.flink.table.client.SqlClient.start(SqlClient.java:114) at org.apache.flink.table.client.SqlClient.main(SqlClient.java:201)Caused by: org.apache.flink.table.api.ValidationException: Unable to create a source for reading table 'default_catalog.default_database.MyUserTable'.Table options are:'connector'='kafka''format'='avro''properties.bootstrap.servers'='localhost:9092''properties.group.id'='testGroup''scan.startup.mode'='earliest-offset''topic'='topic_name' at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:125) at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.buildTableScan(CatalogSourceTable.scala:135) at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.scala:78) at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3492) at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2415) at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2102) at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051) at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661) at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642) at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345) at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151) at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:773) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:745) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:238) at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:66) at org.apache.flink.table.client.cli.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:90) ... 6 moreCaused by: org.apache.flink.table.api.ValidationException: Cannot discover a connector using option ''connector'='kafka''. at org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:329) at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:118) ... 23 moreCaused by: org.apache.flink.table.api.ValidationException: Could not find any factory for identifier 'kafka' that implements 'org.apache.flink.table.factories.DynamicTableSourceFactory' in the classpath.Available factory identifiers are:datagen at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:240) at org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:326) ... 24 moreShutting down the session...done.Because the factories are present only in the user classloader.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="18080" opendate="2020-6-3 00:00:00" fixdate="2020-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate the "Kerberos Authentication Setup and Configuration" page into Chinese</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.deployment.security.security-kerberos.md</file>
    </fixedFiles>
  </bug>
  <bug id="18081" opendate="2020-6-3 00:00:00" fixdate="2020-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix broken links in "Kerberos Authentication Setup and Configuration" doc</summary>
      <description>The config.html#kerberos-based-security is not valid now.</description>
      <version>1.10.1,1.11.0,1.12.0</version>
      <fixedVersion>1.10.3,1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.security-kerberos.zh.md</file>
      <file type="M">docs.ops.security-kerberos.md</file>
    </fixedFiles>
  </bug>
  <bug id="18083" opendate="2020-6-3 00:00:00" fixdate="2020-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve exception message of TIMESTAMP/TIME out of the HBase connector supported precision</summary>
      <description></description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase.src.test.java.org.apache.flink.connector.hbase.HBaseDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hbase.src.main.java.org.apache.flink.connector.hbase.util.HBaseTypeUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hbase.src.main.java.org.apache.flink.connector.hbase.util.HBaseSerde.java</file>
    </fixedFiles>
  </bug>
  <bug id="18084" opendate="2020-6-3 00:00:00" fixdate="2020-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create documentation for the Application Mode</summary>
      <description>This ticket aims at documenting the application mode and its new capabilities. This should also include the documentation for Yarn (for Kubernetes the documentation is tracked by another ticket).</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.FlinkYarnSessionCliTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.cli.KubernetesSessionCli.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.ExecutorCLITest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ExecutorCLI.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.zh.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
      <file type="M">docs.ops.deployment.index.zh.md</file>
      <file type="M">docs.ops.deployment.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="18087" opendate="2020-6-3 00:00:00" fixdate="2020-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Uploading user artifact for Yarn job cluster could not work</summary>
      <description>In FLINK-17632, we add the support remote user jar. However, uploading user artifact for Yarn job cluster is broken exceptionally. In the following code, we should only upload local files. Now it has the contrary behavior.// only upload local filesif (Utils.isRemotePath(entry.getValue().filePath)) { Path localPath = new Path(entry.getValue().filePath); Tuple2&lt;Path, Long&gt; remoteFileInfo = fileUploader.uploadLocalFileToRemote(localPath, entry.getKey()); jobGraph.setUserArtifactRemotePath(entry.getKey(), remoteFileInfo.f0.toString());} Another problem is the related tests testPerJobModeWithDistributedCache does not fail because we do not fetch the artifact from remote filesystem(i.e. HDFS). We directly get it from local file. It also needs to be enhanced.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.0,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="18089" opendate="2020-6-3 00:00:00" fixdate="2020-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the network zero-copy test into the azure E2E pipeline</summary>
      <description>The zero-copy E2E test added in Flink-10742 is only added to the deprecated travis pipeline previously. It should be added into the Azure test pipeline.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="1809" opendate="2015-3-31 00:00:00" fixdate="2015-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add standard scaler to ML library</summary>
      <description>Many ML algorithms require the input data to have mean 0 and a variance 1 for each individual feature &amp;#91;1&amp;#93;. Therefore, a Transformer which achieves exactly that would be a valuable addition to the machine learning library.Resources:&amp;#91;1&amp;#93; http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.sidenav.html</file>
    </fixedFiles>
  </bug>
  <bug id="18090" opendate="2020-6-3 00:00:00" fixdate="2020-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the Row.toString method</summary>
      <description>This updates the Row.toString method to provide a good summary string.In particular it fixes the following issues:Changeflag: According to FLIP-95, a row describes an entry in achangelog. Therefore, it should visible whether a row is an insert,delete, or update change. Now indicated with +I, -D, +U, -U.Nested rows: In the old implementation it was not visible whether nestedrows exist or not due to missing start/end boundaries. Now indicated with&amp;#91;...&amp;#93; or {...}.Positioned rows vs. named rows: According to FLIP-136, it should be visiblewhether a row operates in name-based or position-based field mode. Nowindicated with &amp;#91;...&amp;#93; or {...}.Nested arrays in maps and lists: In the old implementation arrays in mapsor lists could not be represented.Wrong formatting: Most programming languages use a space after a comma.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.AggregationITCase.scala</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.types.RowTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.RowUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.filesystem.PartitionWriterTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.filesystem.FileSystemOutputFormatTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.utils.TableProgramsTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.utils.StreamingWithStateTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.GroupWindowTableAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.stream.table.ValuesITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.stream.sql.JavaSqlITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.LegacyTableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.UnnestITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SourceWatermarkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.PruneAggregateCallITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.DeduplicateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.ChangelogSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateRemoveITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.SortITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.LegacyTableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.CalcITCase.scala</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.src.test.java.org.apache.flink.connector.hbase1.HBaseConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.src.test.java.org.apache.flink.connector.hbase2.HBaseConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveLookupJoinITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogUseBlinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcLookupFunctionTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcLookupTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.UnsignedTypeConversionITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaChangelogTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaTableITCase.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.sql.sh</file>
      <file type="M">flink-examples.flink-examples-table.src.test.java.org.apache.flink.table.examples.java.basics.UpdatingTopCityExampleITCase.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvFilesystemBatchITCase.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.debezium.DebeziumJsonFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonBatchFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcTableSourceITCase.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.ml.tests.test.pipeline.it.case.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.calc.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.dependency.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.conversion.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udaf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.row.based.operation.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.sql.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udtf.py</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.PrintTableSinkFactory.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.expressions.ObjectToExpressionTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.table.PrintConnectorITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSinkITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="18110" opendate="2020-6-4 00:00:00" fixdate="2020-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bucket Listener in StreamingFileSink should notify for buckets detected to be inactive at recovery</summary>
      <description>Current streaming file sink using bucket lifecycle listener to support the global commit of Hive sink, and the listener sends notification when buckets are created and get inactive. However, after failover, a bucket may be directly detected to be inactive and deserted directly, in this case the listener should also send the inactive notification.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketsTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.java</file>
    </fixedFiles>
  </bug>
  <bug id="18117" opendate="2020-6-4 00:00:00" fixdate="2020-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Kerberized YARN per-job on Docker test" fails with "Could not start hadoop cluster."</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2683&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f52020-06-04T06:03:53.2844296Z Creating slave1 ... [32mdone [0m2020-06-04T06:03:53.4981251Z [1BWaiting for hadoop cluster to come up. We have been trying for 0 seconds, retrying ...2020-06-04T06:03:58.5980181Z Waiting for hadoop cluster to come up. We have been trying for 5 seconds, retrying ...2020-06-04T06:04:03.6997087Z Waiting for hadoop cluster to come up. We have been trying for 10 seconds, retrying ...2020-06-04T06:04:08.7910791Z Waiting for hadoop cluster to come up. We have been trying for 15 seconds, retrying ...2020-06-04T06:04:13.8921621Z Waiting for hadoop cluster to come up. We have been trying for 20 seconds, retrying ...2020-06-04T06:04:18.9648844Z Waiting for hadoop cluster to come up. We have been trying for 25 seconds, retrying ...2020-06-04T06:04:24.0381851Z Waiting for hadoop cluster to come up. We have been trying for 31 seconds, retrying ...2020-06-04T06:04:29.1220264Z Waiting for hadoop cluster to come up. We have been trying for 36 seconds, retrying ...2020-06-04T06:04:34.1882187Z Waiting for hadoop cluster to come up. We have been trying for 41 seconds, retrying ...2020-06-04T06:04:39.2784948Z Waiting for hadoop cluster to come up. We have been trying for 46 seconds, retrying ...2020-06-04T06:04:44.3843337Z Waiting for hadoop cluster to come up. We have been trying for 51 seconds, retrying ...2020-06-04T06:04:49.4703561Z Waiting for hadoop cluster to come up. We have been trying for 56 seconds, retrying ...2020-06-04T06:04:54.5463207Z Waiting for hadoop cluster to come up. We have been trying for 61 seconds, retrying ...2020-06-04T06:04:59.6650405Z Waiting for hadoop cluster to come up. We have been trying for 66 seconds, retrying ...2020-06-04T06:05:04.7500168Z Waiting for hadoop cluster to come up. We have been trying for 71 seconds, retrying ...2020-06-04T06:05:09.8177904Z Waiting for hadoop cluster to come up. We have been trying for 76 seconds, retrying ...2020-06-04T06:05:14.9751297Z Waiting for hadoop cluster to come up. We have been trying for 81 seconds, retrying ...2020-06-04T06:05:20.0336417Z Waiting for hadoop cluster to come up. We have been trying for 87 seconds, retrying ...2020-06-04T06:05:25.1627704Z Waiting for hadoop cluster to come up. We have been trying for 92 seconds, retrying ...2020-06-04T06:05:30.2583315Z Waiting for hadoop cluster to come up. We have been trying for 97 seconds, retrying ...2020-06-04T06:05:35.3283678Z Waiting for hadoop cluster to come up. We have been trying for 102 seconds, retrying ...2020-06-04T06:05:40.4184029Z Waiting for hadoop cluster to come up. We have been trying for 107 seconds, retrying ...2020-06-04T06:05:45.5388372Z Waiting for hadoop cluster to come up. We have been trying for 112 seconds, retrying ...2020-06-04T06:05:50.6155334Z Waiting for hadoop cluster to come up. We have been trying for 117 seconds, retrying ...2020-06-04T06:05:55.7225186Z Command: start_hadoop_cluster failed. Retrying...2020-06-04T06:05:55.7237999Z Starting Hadoop cluster2020-06-04T06:05:56.5188293Z kdc is up-to-date2020-06-04T06:05:56.5292716Z master is up-to-date2020-06-04T06:05:56.5301735Z slave2 is up-to-date2020-06-04T06:05:56.5306179Z slave1 is up-to-date2020-06-04T06:05:56.6800566Z Waiting for hadoop cluster to come up. We have been trying for 0 seconds, retrying ...2020-06-04T06:06:01.7668291Z Waiting for hadoop cluster to come up. We have been trying for 5 seconds, retrying ...2020-06-04T06:06:06.8620265Z Waiting for hadoop cluster to come up. We have been trying for 10 seconds, retrying ...2020-06-04T06:06:11.9753596Z Waiting for hadoop cluster to come up. We have been trying for 15 seconds, retrying ...2020-06-04T06:06:17.0402846Z Waiting for hadoop cluster to come up. We have been trying for 21 seconds, retrying ...2020-06-04T06:06:22.1650005Z Waiting for hadoop cluster to come up. We have been trying for 26 seconds, retrying ...2020-06-04T06:06:27.2500179Z Waiting for hadoop cluster to come up. We have been trying for 31 seconds, retrying ...2020-06-04T06:06:32.3133809Z Waiting for hadoop cluster to come up. We have been trying for 36 seconds, retrying ...2020-06-04T06:06:37.4432923Z Waiting for hadoop cluster to come up. We have been trying for 41 seconds, retrying ...2020-06-04T06:06:42.5658250Z Waiting for hadoop cluster to come up. We have been trying for 46 seconds, retrying ...2020-06-04T06:06:47.6682536Z Waiting for hadoop cluster to come up. We have been trying for 51 seconds, retrying ...2020-06-04T06:06:52.7810371Z Waiting for hadoop cluster to come up. We have been trying for 56 seconds, retrying ...2020-06-04T06:06:57.8860269Z Waiting for hadoop cluster to come up. We have been trying for 61 seconds, retrying ...2020-06-04T06:07:03.0337979Z Waiting for hadoop cluster to come up. We have been trying for 67 seconds, retrying ...2020-06-04T06:07:08.1080310Z Waiting for hadoop cluster to come up. We have been trying for 72 seconds, retrying ...2020-06-04T06:07:13.2297578Z Waiting for hadoop cluster to come up. We have been trying for 77 seconds, retrying ...2020-06-04T06:07:18.3779034Z Waiting for hadoop cluster to come up. We have been trying for 82 seconds, retrying ...2020-06-04T06:07:23.4789495Z Waiting for hadoop cluster to come up. We have been trying for 87 seconds, retrying ...2020-06-04T06:07:28.6063062Z Waiting for hadoop cluster to come up. We have been trying for 92 seconds, retrying ...2020-06-04T06:07:33.8220409Z Waiting for hadoop cluster to come up. We have been trying for 97 seconds, retrying ...2020-06-04T06:07:38.9439231Z Waiting for hadoop cluster to come up. We have been trying for 102 seconds, retrying ...2020-06-04T06:07:44.0193849Z Waiting for hadoop cluster to come up. We have been trying for 108 seconds, retrying ...2020-06-04T06:07:49.1241642Z Waiting for hadoop cluster to come up. We have been trying for 113 seconds, retrying ...2020-06-04T06:07:54.2425087Z Waiting for hadoop cluster to come up. We have been trying for 118 seconds, retrying ...2020-06-04T06:07:59.3835321Z Command: start_hadoop_cluster failed. Retrying...2020-06-04T06:07:59.3847275Z Starting Hadoop cluster2020-06-04T06:08:00.1959109Z kdc is up-to-date2020-06-04T06:08:00.1968717Z master is up-to-date2020-06-04T06:08:00.1982811Z slave1 is up-to-date2020-06-04T06:08:00.1988143Z slave2 is up-to-date2020-06-04T06:08:00.4014781Z Waiting for hadoop cluster to come up. We have been trying for 0 seconds, retrying ...2020-06-04T06:08:05.5168483Z Waiting for hadoop cluster to come up. We have been trying for 5 seconds, retrying ...2020-06-04T06:08:10.6759355Z Waiting for hadoop cluster to come up. We have been trying for 10 seconds, retrying ...2020-06-04T06:08:15.8307550Z Waiting for hadoop cluster to come up. We have been trying for 15 seconds, retrying ...2020-06-04T06:08:21.0143341Z Waiting for hadoop cluster to come up. We have been trying for 21 seconds, retrying ...2020-06-04T06:08:26.0932297Z Waiting for hadoop cluster to come up. We have been trying for 26 seconds, retrying ...2020-06-04T06:08:31.2526775Z Waiting for hadoop cluster to come up. We have been trying for 31 seconds, retrying ...2020-06-04T06:08:36.4356124Z Waiting for hadoop cluster to come up. We have been trying for 36 seconds, retrying ...2020-06-04T06:08:41.5607530Z Waiting for hadoop cluster to come up. We have been trying for 41 seconds, retrying ...2020-06-04T06:08:46.6407963Z Waiting for hadoop cluster to come up. We have been trying for 46 seconds, retrying ...2020-06-04T06:08:51.8464789Z Waiting for hadoop cluster to come up. We have been trying for 51 seconds, retrying ...2020-06-04T06:08:56.9735817Z Waiting for hadoop cluster to come up. We have been trying for 56 seconds, retrying ...2020-06-04T06:09:02.1023842Z Waiting for hadoop cluster to come up. We have been trying for 62 seconds, retrying ...2020-06-04T06:09:07.2390427Z Waiting for hadoop cluster to come up. We have been trying for 67 seconds, retrying ...2020-06-04T06:09:12.4433329Z Waiting for hadoop cluster to come up. We have been trying for 72 seconds, retrying ...2020-06-04T06:09:17.5390800Z Waiting for hadoop cluster to come up. We have been trying for 77 seconds, retrying ...2020-06-04T06:09:22.7020537Z Waiting for hadoop cluster to come up. We have been trying for 82 seconds, retrying ...2020-06-04T06:09:27.8754909Z Waiting for hadoop cluster to come up. We have been trying for 87 seconds, retrying ...2020-06-04T06:09:33.0447274Z Waiting for hadoop cluster to come up. We have been trying for 93 seconds, retrying ...2020-06-04T06:09:38.1804596Z Waiting for hadoop cluster to come up. We have been trying for 98 seconds, retrying ...2020-06-04T06:09:43.3636590Z Waiting for hadoop cluster to come up. We have been trying for 103 seconds, retrying ...2020-06-04T06:09:48.4975410Z Waiting for hadoop cluster to come up. We have been trying for 108 seconds, retrying ...2020-06-04T06:09:53.6117328Z Waiting for hadoop cluster to come up. We have been trying for 113 seconds, retrying ...2020-06-04T06:09:58.7785946Z Waiting for hadoop cluster to come up. We have been trying for 118 seconds, retrying ...2020-06-04T06:10:03.9748663Z Command: start_hadoop_cluster failed. Retrying...2020-06-04T06:10:03.9808244Z Command: start_hadoop_cluster failed 3 times.2020-06-04T06:10:03.9823071Z ERROR: Could not start hadoop cluster. Aborting...Frequent, suspicious logs2020-06-04T06:10:04.5032658Z 20/06/04 06:05:42 WARN ipc.Client: Failed to connect to server: master.docker-hadoop-cluster-network/172.19.0.3:9000: try once and fail.2020-06-04T06:10:04.5033211Z java.net.ConnectException: Connection refused...2020-06-04T06:10:04.6867876Z 20/06/04 06:04:11 ERROR namenode.NameNode: Failed to start namenode.2020-06-04T06:10:04.6868640Z java.net.BindException: Port in use: 0.0.0.0:504702020-06-04T06:10:04.6869062Z at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:998)2020-06-04T06:10:04.6869702Z at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:935)2020-06-04T06:10:04.6870199Z at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:171)2020-06-04T06:10:04.6870740Z at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:842)2020-06-04T06:10:04.6871235Z at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:693)2020-06-04T06:10:04.6871728Z at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:906)2020-06-04T06:10:04.6872202Z at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:885)2020-06-04T06:10:04.6872699Z at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1626)2020-06-04T06:10:04.6873701Z at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1694)2020-06-04T06:10:04.6874100Z Caused by: java.net.BindException: Address already in use2020-06-04T06:10:04.6901805Z at sun.nio.ch.Net.bind0(Native Method)2020-06-04T06:10:04.6902168Z at sun.nio.ch.Net.bind(Net.java:433)2020-06-04T06:10:04.6902478Z at sun.nio.ch.Net.bind(Net.java:425)2020-06-04T06:10:04.6902847Z at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)2020-06-04T06:10:04.6903296Z at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)2020-06-04T06:10:04.6903744Z at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)2020-06-04T06:10:04.6904395Z at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:993)2020-06-04T06:10:04.6904727Z ... 8 more2020-06-04T06:10:04.6905005Z 20/06/04 06:04:11 INFO util.ExitUtil: Exiting with status 12020-06-04T06:10:04.6905401Z 20/06/04 06:04:11 INFO namenode.NameNode: SHUTDOWN_MSG:</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.config.hdfs-site.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.Dockerfile</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.bootstrap.sh</file>
    </fixedFiles>
  </bug>
  <bug id="18130" opendate="2020-6-4 00:00:00" fixdate="2020-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>File name conflict for different jobs in filesystem/hive sink</summary>
      <description>The sink of different jobs (or the same SQL runs multiple times) will produce the same file name, the rename will fail, and different file systems will produce different results: HadoopFileSystem: ignore new data. LocalFileSystem: overwrite old data.We can add UUID to prefix, make sure there is no conflict between jobs.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.PartitionTempFileManager.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.PartitionLoader.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.StreamFileSystemITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.FileSystemITCaseBase.scala</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="18131" opendate="2020-6-4 00:00:00" fixdate="2020-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for the new JSON format</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.formats.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="18132" opendate="2020-6-4 00:00:00" fixdate="2020-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for the new CSV format</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.formats.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="18134" opendate="2020-6-4 00:00:00" fixdate="2020-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for the Debezium format</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.kafka.zh.md</file>
      <file type="M">docs.dev.table.connectors.kafka.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="18137" opendate="2020-6-4 00:00:00" fixdate="2020-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JobMasterTriggerSavepointITCase.testStopJobAfterSavepoint fails with AskTimeoutException</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2747&amp;view=logs&amp;j=5c8e7682-d68f-54d1-16a2-a09310218a49&amp;t=45cc9205-bdb7-5b54-63cd-89fdc09833232020-06-04T16:17:20.4404189Z [ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 14.352 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase2020-06-04T16:17:20.4405548Z [ERROR] testStopJobAfterSavepoint(org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase) Time elapsed: 10.058 s &lt;&lt;&lt; ERROR!2020-06-04T16:17:20.4407342Z java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.triggerSavepoint(org.apache.flink.api.common.JobID,java.lang.String,boolean,org.apache.flink.api.common.time.Time) timed out.2020-06-04T16:17:20.4409562Z at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)2020-06-04T16:17:20.4410333Z at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)2020-06-04T16:17:20.4411259Z at org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase.cancelWithSavepoint(JobMasterTriggerSavepointITCase.java:264)2020-06-04T16:17:20.4412292Z at org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase.testStopJobAfterSavepoint(JobMasterTriggerSavepointITCase.java:127)2020-06-04T16:17:20.4413163Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-06-04T16:17:20.4413990Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-06-04T16:17:20.4414783Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-06-04T16:17:20.4415936Z at java.lang.reflect.Method.invoke(Method.java:498)2020-06-04T16:17:20.4416693Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)2020-06-04T16:17:20.4417632Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2020-06-04T16:17:20.4418637Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)2020-06-04T16:17:20.4419367Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2020-06-04T16:17:20.4420118Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-06-04T16:17:20.4420742Z at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)2020-06-04T16:17:20.4421909Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-06-04T16:17:20.4422493Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)2020-06-04T16:17:20.4423247Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)2020-06-04T16:17:20.4424263Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)2020-06-04T16:17:20.4424876Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)2020-06-04T16:17:20.4426346Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)2020-06-04T16:17:20.4427052Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)2020-06-04T16:17:20.4427772Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)2020-06-04T16:17:20.4428562Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)2020-06-04T16:17:20.4429158Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-06-04T16:17:20.4429861Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-06-04T16:17:20.4430448Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-06-04T16:17:20.4431060Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2020-06-04T16:17:20.4431678Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)2020-06-04T16:17:20.4432513Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)2020-06-04T16:17:20.4433396Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2020-06-04T16:17:20.4434298Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)2020-06-04T16:17:20.4440904Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2020-06-04T16:17:20.4443425Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2020-06-04T16:17:20.4444349Z at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2020-06-04T16:17:20.4445160Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)2020-06-04T16:17:20.4446389Z Caused by: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.triggerSavepoint(org.apache.flink.api.common.JobID,java.lang.String,boolean,org.apache.flink.api.common.time.Time) timed out.2020-06-04T16:17:20.4447610Z at com.sun.proxy.$Proxy31.triggerSavepoint(Unknown Source)2020-06-04T16:17:20.4448545Z at org.apache.flink.runtime.minicluster.MiniCluster.lambda$triggerSavepoint$8(MiniCluster.java:595)2020-06-04T16:17:20.4449259Z at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)2020-06-04T16:17:20.4449990Z at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)2020-06-04T16:17:20.4450789Z at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)2020-06-04T16:17:20.4451584Z at org.apache.flink.runtime.minicluster.MiniCluster.runDispatcherCommand(MiniCluster.java:621)2020-06-04T16:17:20.4452473Z at org.apache.flink.runtime.minicluster.MiniCluster.triggerSavepoint(MiniCluster.java:595)2020-06-04T16:17:20.4453572Z at org.apache.flink.client.program.MiniClusterClient.cancelWithSavepoint(MiniClusterClient.java:89)2020-06-04T16:17:20.4454746Z at org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase.cancelWithSavepoint(JobMasterTriggerSavepointITCase.java:262)2020-06-04T16:17:20.4455517Z ... 32 more2020-06-04T16:17:20.4457589Z Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/dispatcher_2#830345697]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.2020-06-04T16:17:20.4459164Z at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)2020-06-04T16:17:20.4460107Z at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)2020-06-04T16:17:20.4460819Z at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)2020-06-04T16:17:20.4461613Z at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)2020-06-04T16:17:20.4462444Z at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)2020-06-04T16:17:20.4463203Z at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)2020-06-04T16:17:20.4464089Z at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)2020-06-04T16:17:20.4464833Z at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)2020-06-04T16:17:20.4465800Z at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)2020-06-04T16:17:20.4466746Z at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)2020-06-04T16:17:20.4467579Z at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)2020-06-04T16:17:20.4468467Z at java.lang.Thread.run(Thread.java:748)</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTriggeringTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="18140" opendate="2020-6-5 00:00:00" fixdate="2020-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for ORC format</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.formats.parquet.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.parquet.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="18141" opendate="2020-6-5 00:00:00" fixdate="2020-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Parquet format</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.formats.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="18149" opendate="2020-6-5 00:00:00" fixdate="2020-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Taskmanager logs could not show up in native K8s deployment</summary>
      <description>In FLINK-17935, we use flinkConfig.get(DeploymentOptionsInternal.CONF_DIR) to replace CliFrontend.getConfigurationDirectoryFromEnv. It will cause problem in native K8s integration. The root cause we set the DeploymentOptionsInternal.CONF_DIR in flink-conf.yaml to a local path. However, it does not exist in JobManager pod.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesTestUtils.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.parameters.AbstractKubernetesParametersTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.FlinkConfMountDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.AbstractKubernetesParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.FlinkConfMountDecorator.java</file>
    </fixedFiles>
  </bug>
  <bug id="18157" opendate="2020-6-5 00:00:00" fixdate="2020-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jobstore size check compares against offHeapMemory</summary>
      <description>Setting jobmanager.memory.off-heap.size to 0 results in this confusing error:[] - Loading configuration property: jobmanager.memory.process.size, 2000m[] - Loading configuration property: jobmanager.memory.heap.size, 1500m[] - Loading configuration property: jobmanager.memory.jvm-overhead.min, 100m[] - Loading configuration property: jobmanager.memory.jvm-overhead.max, 350m[] - Loading configuration property: jobmanager.memory.off-heap.size, 0m[] - The configured or derived JVM heap memory size (jobstore.cache-size: 0 bytes) is less than the configured or default size of the job store cache (jobmanager.memory.heap.size: 50.000mb (52428800 bytes))According to the documentation the jobstore uses the heap though.private static JobManagerFlinkMemory createJobManagerFlinkMemory( Configuration config, MemorySize jvmHeap, MemorySize offHeapMemory) { verifyJvmHeapSize(jvmHeap); verifyJobStoreCacheSize(config, offHeapMemory); return new JobManagerFlinkMemory(jvmHeap, offHeapMemory);}private static void verifyJvmHeapSize(MemorySize jvmHeapSize) { if (jvmHeapSize.compareTo(JobManagerOptions.MIN_JVM_HEAP_SIZE) &lt; 1) { LOG.warn( "The configured or derived JVM heap memory size ({}) is less than its recommended minimum value ({})", jvmHeapSize.toHumanReadableString(), JobManagerOptions.MIN_JVM_HEAP_SIZE); }}</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerProcessUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.config.memory.jobmanager.JobManagerFlinkMemoryUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="18161" opendate="2020-6-5 00:00:00" fixdate="2020-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Changing parallelism is not possible in sql-client.sh</summary>
      <description>I tried using SET execution.parallelism=12and changing the parallelism in the configuration file.My SQL queries were always running with p=1 for all operators.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="18173" opendate="2020-6-8 00:00:00" fixdate="2020-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bundle flink-csv and flink-json jars in lib</summary>
      <description>The biggest problem for distributions I see is the variety of problems caused by users' lack of format dependency. These three formats are very small and no third party dependence, and they are widely used by table users. Actually, we don't have any other built-in table formats now.. flink-csv-1.10.0.jar flink-json-1.10.0.jar We can just bundle them in "flink/lib/". It not solve all problems and it is independent of "fat" and "slim". But also improve usability.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.release-notes.flink-1.11.zh.md</file>
      <file type="M">docs.release-notes.flink-1.11.md</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">docs.dev.table.connectors.formats.avro.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.avro.md</file>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
    </fixedFiles>
  </bug>
  <bug id="18175" opendate="2020-6-8 00:00:00" fixdate="2020-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add human readable summary for configured and derived memory sizes.</summary>
      <description>FLIP-49 &amp; FLIP-116 introduces sophisticated memory configurations for TaskManager and Master processes. Before the JVM processes are started, Flink derives the accurate sizes for all necessary components, based on both explicit user configurations and implicit defaults.To make the configuration results (especially those implicitly derived) clear to users, it would be helpful to print them in the beginning of the process logs. Currently, we only have printed JVM parameters (TM &amp; Master) dynamic memory configurations (TM only). They are incomplete (jvm overhead for both processes and off-heap memory for the master process are not presented) and unfriendly (in bytes).Therefore, I propose to add a human readable summary at the beginning of process logs.See also this PR discussion.</description>
      <version>1.10.1,1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.bash.BashJavaUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="18176" opendate="2020-6-8 00:00:00" fixdate="2020-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add supplement for file system connector document</summary>
      <description>Illustate difference between old file system connector and new file system connector.Add supplement for rolling policy.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.filesystem.zh.md</file>
      <file type="M">docs.dev.table.connectors.filesystem.md</file>
    </fixedFiles>
  </bug>
  <bug id="18188" opendate="2020-6-8 00:00:00" fixdate="2020-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document asymmetric allocation of Flink memory</summary>
      <description>I stumbled on some non-documented and surprising behavior while configuring the heap/off-heap memory sizes.Let's start with a base line, where we configure the total process memory to 1024mb:Final Master Memory configuration: Total Process Memory: 1024.000mb (1073741824 bytes) Total Flink Memory: 576.000mb (603979776 bytes) Flink Heap Memory: 448.000mb (469762048 bytes) Flink Off-heap Memory: 128.000mb (134217728 bytes) JVM Metaspace Memory: 256.000mb (268435456 bytes) JVM Overhead Memory: 192.000mb (201326592 bytes)If we reduce the off-heap memory to 16mb, then the gained memory is allocated to the heap memory:Final Master Memory configuration: Total Process Memory: 1024.000mb (1073741824 bytes) Total Flink Memory: 576.000mb (603979776 bytes) Flink Heap Memory: 560.000mb (587202560 bytes) Flink Off-heap Memory: 16.000mb (16777216 bytes) JVM Metaspace Memory: 256.000mb (268435456 bytes) JVM Overhead Memory: 192.000mb (201326592 bytes)Conversely however, if we reduce the heap memory to 128mb, then the gained memory is allocated to the JVM overhead.Final Master Memory configuration: Total Process Memory: 1024.000mb (1073741824 bytes) Total Flink Memory: 256.000mb (268435456 bytes) Flink Heap Memory: 128.000mb (134217728 bytes) Flink Off-heap Memory: 128.000mb (134217728 bytes) JVM Metaspace Memory: 256.000mb (268435456 bytes) JVM Overhead Memory: 512.000mb (536870912 bytes)I'm not sure whether this behavior is correct.The documentation for Capped Fractionated Components does that "It can also happen that the fraction is ignored if the sizes of the total memory and its other components are defined. In this case, the JVM Overhead is the rest of the total memory."This however only explains one of the above cases (the last one).</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerProcessUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.config.memory.jobmanager.JobManagerFlinkMemoryUtils.java</file>
      <file type="M">docs.ops.memory.mem.setup.master.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.master.md</file>
    </fixedFiles>
  </bug>
  <bug id="18214" opendate="2020-6-9 00:00:00" fixdate="2020-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect warning if jobstore.cache-size exceeds heap size</summary>
      <description>The logging parameters are mixed up.The configured or derived JVM heap memory size (jobstore.cache-size: 128.000mb (134217728 bytes)) is less than the configured or default size of the job store cache (jobmanager.memory.heap.size: 1.000gb (1073741825 bytes))</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerProcessUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.config.memory.jobmanager.JobManagerFlinkMemoryUtils.java</file>
      <file type="M">docs.ops.memory.mem.setup.master.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.master.md</file>
    </fixedFiles>
  </bug>
  <bug id="18215" opendate="2020-6-9 00:00:00" fixdate="2020-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BashJavaUtils logging message should include log level</summary>
      <description>The BashJavaUtils currently log things like this:[] - The derived from fraction jvm overhead memory (160.000mb (167772162 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used insteadThis means that users cannot differentiate between info/warn/error messages.An example where this might be helpful is FLINK-18214.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.resources.log4j-bash-utils.properties</file>
    </fixedFiles>
  </bug>
  <bug id="18222" opendate="2020-6-9 00:00:00" fixdate="2020-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Avro Confluent Schema Registry nightly end-to-end test" unstable with "Kafka cluster did not start after 120 seconds"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3045&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f52020-06-09T15:16:48.1427795Z ==============================================================================2020-06-09T15:16:48.1428609Z Running 'Avro Confluent Schema Registry nightly end-to-end test'2020-06-09T15:16:48.1429204Z ==============================================================================2020-06-09T15:16:48.1438117Z TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-481432981702020-06-09T15:16:48.2985167Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT2020-06-09T15:16:48.3157575Z Downloading Kafka from https://archive.apache.org/dist/kafka/0.10.2.0/kafka_2.11-0.10.2.0.tgz2020-06-09T15:16:48.3214487Z % Total % Received % Xferd Average Speed Time Time Time Current2020-06-09T15:16:48.3215154Z Dload Upload Total Spent Left Speed2020-06-09T15:16:48.3215597Z 2020-06-09T15:16:48.3528820Z 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 02020-06-09T15:16:49.3421526Z 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 02020-06-09T15:16:50.3415678Z 8 35.8M 8 2960k 0 0 2896k 0 0:00:12 0:00:01 0:00:11 2896k2020-06-09T15:16:51.3406836Z 23 35.8M 23 8544k 0 0 4226k 0 0:00:08 0:00:02 0:00:06 4225k2020-06-09T15:16:51.6553485Z 70 35.8M 70 25.2M 0 0 8550k 0 0:00:04 0:00:03 0:00:01 8548k2020-06-09T15:16:51.6555606Z 100 35.8M 100 35.8M 0 0 10.7M 0 0:00:03 0:00:03 --:--:-- 10.7M2020-06-09T15:16:51.9818041Z Downloading confluent from http://packages.confluent.io/archive/3.2/confluent-oss-3.2.0-2.11.tar.gz2020-06-09T15:16:51.9880242Z % Total % Received % Xferd Average Speed Time Time Time Current2020-06-09T15:16:51.9880983Z Dload Upload Total Spent Left Speed2020-06-09T15:16:51.9914252Z 2020-06-09T15:16:52.3398614Z 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 02020-06-09T15:16:53.3399552Z 9 398M 9 39.5M 0 0 111M 0 0:00:03 --:--:-- 0:00:03 111M2020-06-09T15:16:53.9149276Z 47 398M 47 188M 0 0 139M 0 0:00:02 0:00:01 0:00:01 138M2020-06-09T15:16:53.9150980Z 100 398M 100 398M 0 0 206M 0 0:00:01 0:00:01 --:--:-- 206M2020-06-09T15:17:04.3565942Z Waiting for broker...2020-06-09T15:17:12.4215170Z Waiting for broker...2020-06-09T15:17:14.3012835Z Waiting for broker...2020-06-09T15:17:16.1965074Z Waiting for broker...2020-06-09T15:17:18.1102274Z Waiting for broker...2020-06-09T15:17:19.9929632Z Waiting for broker...2020-06-09T15:17:21.8607172Z Waiting for broker...2020-06-09T15:17:23.7802949Z Waiting for broker...2020-06-09T15:17:25.6695260Z Waiting for broker...2020-06-09T15:17:27.5536417Z Waiting for broker...2020-06-09T15:17:29.4327778Z Waiting for broker...2020-06-09T15:17:31.3203091Z Waiting for broker...2020-06-09T15:17:33.1987150Z Waiting for broker...2020-06-09T15:17:35.0694860Z Waiting for broker...2020-06-09T15:17:36.9595576Z Waiting for broker...2020-06-09T15:17:38.9243558Z Waiting for broker...2020-06-09T15:17:40.7984064Z Waiting for broker...2020-06-09T15:17:42.6676095Z Waiting for broker...2020-06-09T15:17:44.5628797Z Waiting for broker...2020-06-09T15:17:46.4374532Z Waiting for broker...2020-06-09T15:17:48.3086761Z Waiting for broker...2020-06-09T15:17:50.1574336Z Waiting for broker...2020-06-09T15:17:52.0432952Z Waiting for broker...2020-06-09T15:17:53.9406541Z Waiting for broker...2020-06-09T15:17:55.8162052Z Waiting for broker...2020-06-09T15:17:57.7090015Z Waiting for broker...2020-06-09T15:17:59.5747770Z Waiting for broker...2020-06-09T15:18:01.4601854Z Waiting for broker...2020-06-09T15:18:03.3332039Z Waiting for broker...2020-06-09T15:18:05.2210453Z Waiting for broker...2020-06-09T15:18:07.1133675Z Waiting for broker...2020-06-09T15:18:09.0132417Z Waiting for broker...2020-06-09T15:18:10.8769511Z Waiting for broker...2020-06-09T15:18:12.7601639Z Waiting for broker...2020-06-09T15:18:14.6389770Z Waiting for broker...2020-06-09T15:18:16.5210725Z Waiting for broker...2020-06-09T15:18:18.4088216Z Waiting for broker...2020-06-09T15:18:20.2732225Z Waiting for broker...2020-06-09T15:18:22.1558390Z Waiting for broker...2020-06-09T15:18:24.0400570Z Waiting for broker...2020-06-09T15:18:25.9134038Z Waiting for broker...2020-06-09T15:18:27.7922350Z Waiting for broker...2020-06-09T15:18:29.6748679Z Waiting for broker...2020-06-09T15:18:31.5340996Z Waiting for broker...2020-06-09T15:18:33.3998472Z Waiting for broker...2020-06-09T15:18:35.2718135Z Waiting for broker...2020-06-09T15:18:37.1426082Z Waiting for broker...2020-06-09T15:18:39.1282264Z Waiting for broker...2020-06-09T15:18:41.0029183Z Waiting for broker...2020-06-09T15:18:42.8700037Z Waiting for broker...2020-06-09T15:18:44.7531621Z Waiting for broker...2020-06-09T15:18:46.6465173Z Waiting for broker...2020-06-09T15:18:48.9504192Z Waiting for broker...2020-06-09T15:18:50.4165383Z Waiting for broker...2020-06-09T15:18:52.2931688Z Waiting for broker...2020-06-09T15:18:54.1669857Z Waiting for broker...2020-06-09T15:18:56.0238505Z Waiting for broker...2020-06-09T15:18:57.8931143Z Waiting for broker...2020-06-09T15:18:59.7607751Z Kafka cluster did not start after 120 seconds. Printing Kafka logs:There's a lot of log output I didn't analyze yet.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="18229" opendate="2020-6-10 00:00:00" fixdate="2020-1-10 01:00:00" resolution="Done">
    <buginformation>
      <summary>Pending worker requests should be properly cleared</summary>
      <description>Currently, if Kubernetes/Yarn does not have enough resources to fulfill Flink's resource requirement, there will be pending pod/container requests on Kubernetes/Yarn. These pending resource requirements are never cleared until either fulfilled or the Flink cluster is shutdown.However, sometimes Flink no longer needs the pending resources. E.g., the slot request is then fulfilled by another slots that become available, or the job failed due to slot request timeout (in a session cluster). In such cases, Flink does not remove the resource request until the resource is allocated, then it discovers that it no longer needs the allocated resource and release them. This would affect the underlying Kubernetes/Yarn cluster, especially when the cluster is under heavy workload.It would be good for Flink to cancel pod/container requests as earlier as possible if it can discover that some of the pending workers are no longer needed.There are several approaches potentially achieve this. We can always check whether there's a pending worker that can be canceled when a PendingTaskManagerSlot is unassigned. We can have a separate timeout for requesting new worker. If the resource cannot be allocated within the given time since requested, we should cancel that resource request and claim a resource allocation failure. We can share the same timeout for starting new worker (proposed in FLINK-13554). This is similar to 2), but it requires the worker to be registered, rather than allocated, before timeout.</description>
      <version>1.9.3,1.10.1,1.11.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerDriverTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManagerDriver.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.TaskExecutorManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedTaskManagerTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.active.ResourceManagerDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriverTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="18238" opendate="2020-6-10 00:00:00" fixdate="2020-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RemoteChannelThroughputBenchmark deadlocks</summary>
      <description>In the last couple of days RemoteChannelThroughputBenchmark.remoteRebalance deadlocked for the second time:http://codespeed.dak8s.net:8080/job/flink-master-benchmarks/6019/</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="18240" opendate="2020-6-10 00:00:00" fixdate="2020-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct modulus function usage in documentation or allow % operator</summary>
      <description>In the documentation: https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/sql/queries.html#scan-projection-and-filterThere is an example:SELECT * FROM Orders WHERE a % 2 = 0But % operator is not allowed in Flink:org.apache.calcite.sql.parser.SqlParseException: Percent remainder '%' is not allowed under the current SQL conformance levelEither we correct the documentation to use MOD function, or allow % operator. This is reported in user-zh ML: http://apache-flink.147419.n8.nabble.com/FLINK-SQL-td3822.html</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.validate.FlinkSqlConformance.java</file>
    </fixedFiles>
  </bug>
  <bug id="18248" opendate="2020-6-11 00:00:00" fixdate="2020-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update data type documentation for 1.11</summary>
      <description>Update the data type documentation for 1.11.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.types.md</file>
    </fixedFiles>
  </bug>
  <bug id="18258" opendate="2020-6-11 00:00:00" fixdate="2020-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement SHOW PARTITIONS for Hive dialect</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ShowOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.utils.SqlParserHelper.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="18264" opendate="2020-6-12 00:00:00" fixdate="2020-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate the "External Resource Framework" page into Chinese</summary>
      <description></description>
      <version>1.11.0</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.external.resources.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="18265" opendate="2020-6-12 00:00:00" fixdate="2020-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hidden files should be ignored when the filesystem table searches for partitions</summary>
      <description>If there are some hidden files in the path of filesystem partitioned table, query this table will occur:Caused by: org.apache.flink.table.api.TableException: Partition keys are: [j], incomplete partition spec: {} at org.apache.flink.table.filesystem.FileSystemTableSource.toFullLinkedPartSpec(FileSystemTableSource.java:209) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.table.filesystem.FileSystemTableSource.access$800(FileSystemTableSource.java:62) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.table.filesystem.FileSystemTableSource$1.lambda$getPaths$0(FileSystemTableSource.java:174) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_152] at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ~[?:1.8.0_152] at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ~[?:1.8.0_152] at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ~[?:1.8.0_152] at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545) ~[?:1.8.0_152] at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260) ~[?:1.8.0_152] at java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:438) ~[?:1.8.0_152] at org.apache.flink.table.filesystem.FileSystemTableSource$1.getPaths(FileSystemTableSource.java:177) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]Hidden files should be ignored when the filesystem table searches for partitions. This is not correct partition.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.FileSystemITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.PartitionPathUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.filesystem.FileSystemOutputFormatTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="18268" opendate="2020-6-12 00:00:00" fixdate="2020-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct Table API in Temporal table docs</summary>
      <description>see user's feedback&amp;#91;1&amp;#93;:The  getTableEnvironment method has been dropped, but the documentation still use it val tEnv = TableEnvironment.getTableEnvironment(env)&amp;#91;1&amp;#93;http://apache-flink.147419.n8.nabble.com/flink-TableEnvironment-can-not-call-getTableEnvironment-api-tt3871.html </description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.streaming.temporal.tables.zh.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.md</file>
      <file type="M">docs.dev.table.connectors.hbase.zh.md</file>
      <file type="M">docs.dev.table.connectors.hbase.md</file>
    </fixedFiles>
  </bug>
  <bug id="18279" opendate="2020-6-12 00:00:00" fixdate="2020-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Simplify table overview page</summary>
      <description>The table overview page contains an overwhelming amount of information. We should simplify the page so users quickly know: What dependencies they need to add in their user code Which planner to use</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="18290" opendate="2020-6-15 00:00:00" fixdate="2020-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tests are crashing with exit code 239</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3467&amp;view=logs&amp;j=d44f43ce-542c-597d-bf94-b0718c71e5e8&amp;t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8Kafka011ProducerExactlyOnceITCase 2020-06-15T03:24:28.4677649Z [WARNING] The requested profile "skip-webui-build" could not be activated because it does not exist.2020-06-15T03:24:28.4692049Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (integration-tests) on project flink-connector-kafka-0.11_2.11: There are test failures.2020-06-15T03:24:28.4692585Z [ERROR] 2020-06-15T03:24:28.4693170Z [ERROR] Please refer to /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire-reports for the individual test results.2020-06-15T03:24:28.4693928Z [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.2020-06-15T03:24:28.4694423Z [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?2020-06-15T03:24:28.4696762Z [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target &amp;&amp; /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dlog4j.configurationFile=log4j2-test.properties -Dmvn.forkNumber=2 -XX:-UseGCOverheadLimit -jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire/surefirebooter617700788970993266.jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire 2020-06-15T03-07-01_381-jvmRun2 surefire2676050245109796726tmp surefire_602825791089523551074tmp2020-06-15T03:24:28.4698486Z [ERROR] Error occurred in starting fork, check output in log2020-06-15T03:24:28.4699066Z [ERROR] Process Exit Code: 2392020-06-15T03:24:28.4699458Z [ERROR] Crashed tests:2020-06-15T03:24:28.4699960Z [ERROR] org.apache.flink.streaming.connectors.kafka.Kafka011ProducerExactlyOnceITCase2020-06-15T03:24:28.4700849Z [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?2020-06-15T03:24:28.4703760Z [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target &amp;&amp; /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dlog4j.configurationFile=log4j2-test.properties -Dmvn.forkNumber=2 -XX:-UseGCOverheadLimit -jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire/surefirebooter617700788970993266.jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire 2020-06-15T03-07-01_381-jvmRun2 surefire2676050245109796726tmp surefire_602825791089523551074tmp2020-06-15T03:24:28.4705501Z [ERROR] Error occurred in starting fork, check output in log2020-06-15T03:24:28.4706297Z [ERROR] Process Exit Code: 2392020-06-15T03:24:28.4706592Z [ERROR] Crashed tests:2020-06-15T03:24:28.4706895Z [ERROR] org.apache.flink.streaming.connectors.kafka.Kafka011ProducerExactlyOnceITCase2020-06-15T03:24:28.4707386Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:510)2020-06-15T03:24:28.4708053Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:457)2020-06-15T03:24:28.4708908Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:298)2020-06-15T03:24:28.4709720Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)2020-06-15T03:24:28.4710497Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)2020-06-15T03:24:28.4711448Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)2020-06-15T03:24:28.4712395Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)2020-06-15T03:24:28.4712997Z [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)2020-06-15T03:24:28.4713524Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)2020-06-15T03:24:28.4714079Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)2020-06-15T03:24:28.4714560Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)2020-06-15T03:24:28.4715096Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)2020-06-15T03:24:28.4715672Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)2020-06-15T03:24:28.4716445Z [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)2020-06-15T03:24:28.4717024Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)2020-06-15T03:24:28.4717478Z [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)2020-06-15T03:24:28.4717939Z [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)2020-06-15T03:24:28.4718378Z [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)2020-06-15T03:24:28.4718852Z [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)2020-06-15T03:24:28.4719230Z [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)2020-06-15T03:24:28.4719676Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-06-15T03:24:28.4720309Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-06-15T03:24:28.4720882Z [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-06-15T03:24:28.4721339Z [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)2020-06-15T03:24:28.4721888Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)2020-06-15T03:24:28.4722658Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)2020-06-15T03:24:28.4723430Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)2020-06-15T03:24:28.4724062Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)2020-06-15T03:24:28.4724657Z [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?2020-06-15T03:24:28.4726770Z [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target &amp;&amp; /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dlog4j.configurationFile=log4j2-test.properties -Dmvn.forkNumber=2 -XX:-UseGCOverheadLimit -jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire/surefirebooter617700788970993266.jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire 2020-06-15T03-07-01_381-jvmRun2 surefire2676050245109796726tmp surefire_602825791089523551074tmp2020-06-15T03:24:28.4728582Z [ERROR] Error occurred in starting fork, check output in log2020-06-15T03:24:28.4729202Z [ERROR] Process Exit Code: 2392020-06-15T03:24:28.4729612Z [ERROR] Crashed tests:2020-06-15T03:24:28.4730247Z [ERROR] org.apache.flink.streaming.connectors.kafka.Kafka011ProducerExactlyOnceITCase2020-06-15T03:24:28.4730781Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:669)2020-06-15T03:24:28.4731292Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)2020-06-15T03:24:28.4731829Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:444)2020-06-15T03:24:28.4732353Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:420)2020-06-15T03:24:28.4732792Z [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)2020-06-15T03:24:28.4733235Z [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2020-06-15T03:24:28.4733718Z [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-15T03:24:28.4734170Z [ERROR] at java.lang.Thread.run(Thread.java:748)2020-06-15T03:24:28.4734682Z [ERROR] -&gt; [Help 1]2020-06-15T03:24:28.4734859Z [ERROR] 2020-06-15T03:24:28.4735312Z [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.2020-06-15T03:24:28.4735927Z [ERROR] Re-run Maven using the -X switch to enable full debug logging.2020-06-15T03:24:28.4736439Z [ERROR] 2020-06-15T03:24:28.4736952Z [ERROR] For more information about the errors and possible solutions, please read the following articles:2020-06-15T03:24:28.4737706Z [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException2020-06-15T03:24:28.4738167Z [ERROR] 2020-06-15T03:24:28.4738553Z [ERROR] After correcting the problems, you can resume the build with the command2020-06-15T03:24:28.4739663Z [ERROR] mvn &lt;goals&gt; -rf :flink-connector-kafka-0.11_2.112020-06-15T03:24:29.0980029Z MVN exited with EXIT CODE: 1.This could be a CI environment issue...When did it start?</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="18294" opendate="2020-6-15 00:00:00" fixdate="2020-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log java processes and disk space after each e2e test</summary>
      <description>To debug interferences between e2e test it would be helpful to log disk usages and leftover java processes.I've seen instances where, right before the java e2e tests are run, there is still a kafka process running, and in one abnormal case we use 13gb more disk space.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="18326" opendate="2020-6-16 00:00:00" fixdate="2020-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kubernetes NOTICE issues</summary>
      <description>snakeyaml: 1.23 -&gt; 1.24logging-interceptor: 3.12.0 -&gt; 3.12.6generex is actually excluded</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="18328" opendate="2020-6-16 00:00:00" fixdate="2020-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Blink planner NOTICE issues</summary>
      <description>not actually bundled: asm, json-smart, accessors-smart, joda-time</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="18330" opendate="2020-6-16 00:00:00" fixdate="2020-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python NOTICE issues</summary>
      <description>beam-runners-core-java / beam-vendor-bytebuddy bundled but not listedprotobuf-java-util listed but not bundledThe NOTICE file additionally lists various dependencies that are bundled by beam. While this is fine, the lack of separation makes verification difficult.These would be the entries for directly bundled dependencies:This project bundles the following dependencies under the Apache Software License 2.0 (http://www.apache.org/licenses/LICENSE-2.0.txt)- com.fasterxml.jackson.core:jackson-annotations:2.10.1- com.fasterxml.jackson.core:jackson-core:2.10.1- com.fasterxml.jackson.core:jackson-databind:2.10.1- com.google.flatbuffers:flatbuffers-java:1.9.0- io.netty:netty-buffer:4.1.44.Final- io.netty:netty-common:4.1.44.Final- joda-time:joda-time:2.5- org.apache.arrow:arrow-format:0.16.0- org.apache.arrow:arrow-memory:0.16.0- org.apache.arrow:arrow-vector:0.16.0- org.apache.beam:beam-model-fn-execution:2.19.0- org.apache.beam:beam-model-job-management:2.19.0- org.apache.beam:beam-model-pipeline:2.19.0- org.apache.beam:beam-runners-core-construction-java:2.19.0- org.apache.beam:beam-runners-java-fn-execution:2.19.0- org.apache.beam:beam-sdks-java-core:2.19.0- org.apache.beam:beam-sdks-java-fn-execution:2.19.0- org.apache.beam:beam-vendor-grpc-1_21_0:0.1- org.apache.beam:beam-vendor-guava-26_0-jre:0.1- org.apache.beam:beam-vendor-sdks-java-extensions-protobuf:2.19.0This project bundles the following dependencies under the BSD license.See bundled license files for details- net.sf.py4j:py4j:0.10.8.1- com.google.protobuf:protobuf-java:3.7.1This project bundles the following dependencies under the MIT license. (https://opensource.org/licenses/MIT)See bundled license files for details.- net.razorvine:pyrolite:4.13These are the ones that are (supposedly) bundled by beam, which would need additional verification:The bundled Apache Beam dependencies bundle the following dependencies under the Apache Software License 2.0 (http://www.apache.org/licenses/LICENSE-2.0.txt)- com.google.api.grpc:proto-google-common-protos:1.12.0- com.google.code.gson:gson:2.7- com.google.guava:guava:26.0-jre- io.grpc:grpc-auth:1.21.0- io.grpc:grpc-core:1.21.0- io.grpc:grpc-context:1.21.0- io.grpc:grpc-netty:1.21.0- io.grpc:grpc-protobuf:1.21.0- io.grpc:grpc-stub:1.21.0- io.grpc:grpc-testing:1.21.0- io.netty:netty-buffer:4.1.34.Final- io.netty:netty-codec:4.1.34.Final- io.netty:netty-codec-http:4.1.34.Final- io.netty:netty-codec-http2:4.1.34.Final- io.netty:netty-codec-socks:4.1.34.Final- io.netty:netty-common:4.1.34.Final- io.netty:netty-handler:4.1.34.Final- io.netty:netty-handler-proxy:4.1.34.Final- io.netty:netty-resolver:4.1.34.Final- io.netty:netty-transport:4.1.34.Final- io.netty:netty-transport-native-epoll:4.1.34.Final- io.netty:netty-transport-native-unix-common:4.1.34.Final- io.netty:netty-tcnative-boringssl-static:2.0.22.Final- io.opencensus:opencensus-api:0.21.0- io.opencensus:opencensus-contrib-grpc-metrics:0.21.0The bundled Apache Beam dependencies bundle the following dependencies under the BSD license.See bundled license files for details- com.google.protobuf:protobuf-java-util:3.7.1- com.google.auth:google-auth-library-credentials:0.13.0</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="18331" opendate="2020-6-16 00:00:00" fixdate="2020-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive NOTICE issues</summary>
      <description>1.2/2.2 NOTICE entries are not sorted alphabetically.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="18380" opendate="2020-6-19 00:00:00" fixdate="2020-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a table source example</summary>
      <description>We introduce a lot of interfaces in FLIP-95. We should add a little example for demoing.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.WordCountTable.java</file>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.WordCountSQL.java</file>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.StreamWindowSQLExample.java</file>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.StreamSQLExample.java</file>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18403" opendate="2020-6-22 00:00:00" fixdate="2020-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure that only exactly once checkpointing can be unaligned</summary>
      <description>Currently, it's possible to configure at least once and unaligned checkpoint enabled with undesired side-effect.We should sanitize the configuration; potentially with warnings.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.InputProcessorUtil.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="1844" opendate="2015-4-8 00:00:00" fixdate="2015-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Normaliser to ML library</summary>
      <description>In many algorithms in ML, the features' values would be better to lie between a given range of values, usually in the range (0,1) &amp;#91;1&amp;#93;. Therefore, a Transformer could be implemented to achieve that normalisation.Resources: &amp;#91;1&amp;#93;http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.libs.ml.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="18447" opendate="2020-6-29 00:00:00" fixdate="2020-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add &amp;#39;flink-connector-base&amp;#39; to &amp;#39;flink-dist&amp;#39;</summary>
      <description>Source connectors will rely mostly on 'flink-connector-base'. It is like a high-level Source API.Including it in flink-dist would avoid that each connector has to package that into a fat-jar. It would then be used similarly to other APIs.</description>
      <version>1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18449" opendate="2020-6-29 00:00:00" fixdate="2020-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make topic discovery and partition discovery configurable for FlinkKafkaConsumer in Table API</summary>
      <description>In streaming api, we can use regex to find topic and enable partiton discovery by setting non-negative value for `flink.partition-discovery.interval-millis`. However, it's not work in table api. We propose the following new connector options:'topic' = 'topic' // for single topic'topic' = 'topic-1, topic-2,..., topic-n', // for list topic'topic-pattern' = 'topic*' // for use of regex, "topic" and "topic-pattern" can't coexist'scan.topic-partition-discovery.interval' = '5s' // for both topic discovery</description>
      <version>1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaOptions.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSourceBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.java.org.apache.flink.streaming.connectors.kafka.table.Kafka011DynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.table.Kafka011DynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.table.Kafka011DynamicSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.table.Kafka010DynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.table.Kafka010DynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.table.Kafka010DynamicSource.java</file>
      <file type="M">docs.dev.table.connectors.kafka.zh.md</file>
      <file type="M">docs.dev.table.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="18461" opendate="2020-7-2 00:00:00" fixdate="2020-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Changelog source can&amp;#39;t be insert into upsert sink</summary>
      <description>CREATE TABLE t_pick_order ( order_no VARCHAR, status INT) WITH ( 'connector' = 'kafka', 'topic' = 'example', 'scan.startup.mode' = 'latest-offset', 'properties.bootstrap.servers' = '172.19.78.32:9092', 'format' = 'canal-json');CREATE TABLE order_status ( order_no VARCHAR, status INT, PRIMARY KEY (order_no) NOT ENFORCED) WITH ( 'connector' = 'jdbc', 'url' = 'jdbc:mysql://xxx:3306/flink_test', 'table-name' = 'order_status', 'username' = 'dev', 'password' = 'xxxx');INSERT INTO order_status SELECT order_no, status FROM t_pick_order ;The above queries throw the following exception:[ERROR] Could not execute SQL statement. Reason:org.apache.flink.table.api.TableException: Provided trait [BEFORE_AND_AFTER] can't satisfy required trait [ONLY_UPDATE_AFTER]. This is a bug in planner, please file an issue. Current node is TableSourceScan(table=[[default_catalog, default_database, t_pick_order]], fields=[order_no, status])It is a bug in planner that we didn't fallback to BEFORE_AND_AFTER trait when ONLY_UPDATE_AFTER can't be satisfied. This results in Changelog source can't be used to written into upsert sink.</description>
      <version>None</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcRowDataOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcRowDataOutputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="18469" opendate="2020-7-2 00:00:00" fixdate="2020-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Application Mode to release notes.</summary>
      <description></description>
      <version>1.11.0</version>
      <fixedVersion>1.11.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.release-notes.flink-1.11.zh.md</file>
      <file type="M">docs.release-notes.flink-1.11.md</file>
    </fixedFiles>
  </bug>
  <bug id="18471" opendate="2020-7-2 00:00:00" fixdate="2020-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-runtime lists "org.uncommons.maths:uncommons-maths:1.2.2a" as a bundled dependency, but it isn&amp;#39;t</summary>
      <description>This is the relevant section in the NOTICE fileThis project bundles the following dependencies under the Apache Software License 2.0. (http://www.apache.org/licenses/LICENSE-2.0.txt)- com.typesafe.akka:akka-remote_2.11:2.5.21- io.netty:netty:3.10.6.Final- org.uncommons.maths:uncommons-maths:1.2.2a.The uncommons-maths dependency is not declared anywhere, nor is it included in the binary file.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18472" opendate="2020-7-2 00:00:00" fixdate="2020-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Local Installation Getting Started Guide</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.try-flink.table.api.zh.md</file>
      <file type="M">docs.try-flink.table.api.md</file>
      <file type="M">docs.try-flink.python.table.api.zh.md</file>
      <file type="M">docs.try-flink.python.table.api.md</file>
      <file type="M">docs.try-flink.flink-operations-playground.zh.md</file>
      <file type="M">docs.try-flink.flink-operations-playground.md</file>
      <file type="M">docs.try-flink.datastream.api.zh.md</file>
      <file type="M">docs.try-flink.datastream.api.md</file>
      <file type="M">docs.index.zh.md</file>
      <file type="M">docs.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="18477" opendate="2020-7-2 00:00:00" fixdate="2020-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ChangelogSocketExample does not work</summary>
      <description>The example fails on a fresh build with: The program finished with the following exception:org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Unable to create a source for reading table 'default_catalog.default_database.UserScores'.Table options are:'byte-delimiter'='10''changelog-csv.column-delimiter'='|''connector'='socket''format'='changelog-csv''hostname'='localhost''port'='9999' at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302) at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198) at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149) at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:699) at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:232) at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:916) at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992) at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30) at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)Caused by: org.apache.flink.table.api.ValidationException: Unable to create a source for reading table 'default_catalog.default_database.UserScores'.Table options are:'byte-delimiter'='10''changelog-csv.column-delimiter'='|''connector'='socket''format'='changelog-csv''hostname'='localhost''port'='9999' at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:125) at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.buildTableScan(CatalogSourceTable.scala:135) at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.scala:78) at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3492) at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2415) at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2102) at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051) at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661) at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642) at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345) at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151) at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:773) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:745) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:238) at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78) at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:658) at org.apache.flink.table.examples.java.connectors.ChangelogSocketExample.main(ChangelogSocketExample.java:89) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288) ... 8 moreCaused by: org.apache.flink.table.api.TableException: Could not load service provider for factories. at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:346) at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:221) at org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:326) at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:118) ... 31 moreCaused by: java.util.ServiceConfigurationError: org.apache.flink.table.factories.Factory: Provider org.apache.flink.table.examples.java.connectors.SocketDynamicTableFactory not found at java.util.ServiceLoader.fail(ServiceLoader.java:239) at java.util.ServiceLoader.access$300(ServiceLoader.java:185) at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:372) at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) at java.util.ServiceLoader$1.next(ServiceLoader.java:480) at java.util.Iterator.forEachRemaining(Iterator.java:116) at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:342) ... 34 more</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18485" opendate="2020-7-4 00:00:00" fixdate="2020-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kerberized YARN per-job on Docker test failed during unzip jce_policy-8.zip</summary>
      <description>Instance on 1.11 branch: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4230&amp;view=logs&amp;j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&amp;t=94459a52-42b6-5bfc-5d74-690b5d3c6de8+ curl -LOH Cookie: oraclelicense=accept-securebackup-cookie http://download.oracle.com/otn-pub/java/jce/8/jce_policy-8.zip % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0100 429 100 429 0 0 1616 0 --:--:-- --:--:-- --:--:-- 1616 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0100 7073 100 7073 0 0 20139 0 --:--:-- --:--:-- --:--:-- 20139+ unzip jce_policy-8.zipArchive: jce_policy-8.zip End-of-central-directory signature not found. Either this file is not a zipfile, or it constitutes one disk of a multi-part archive. In the latter case the central directory and zipfile comment will be found on the last disk(s) of this archive.unzip: cannot find zipfile directory in one of jce_policy-8.zip or jce_policy-8.zip.zip, and cannot find jce_policy-8.zip.ZIP, period.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.Dockerfile</file>
    </fixedFiles>
  </bug>
  <bug id="18486" opendate="2020-7-5 00:00:00" fixdate="2020-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for the &amp;#39;%&amp;#39; modulus function</summary>
      <description>This is a follow-up issue of FLINK-18240 to add documentation for the new system operator %.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.functions.systemFunctions.zh.md</file>
      <file type="M">docs.dev.table.functions.systemFunctions.md</file>
    </fixedFiles>
  </bug>
  <bug id="18487" opendate="2020-7-5 00:00:00" fixdate="2020-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>New table source factory omits unrecognized properties silently</summary>
      <description>For the following DDL, we just omits the unrecognized property 'records-per-second'.CREATE TABLE MyDataGen ( int_field int, double_field double, string_field varchar) WITH ( 'connector' = 'datagen', 'records-per-second' = '1' -- should be rows-per-second)IMO, we should throw Exceptions to tell users that they used a wrong property. CC jark twalthr</description>
      <version>1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FactoryUtil.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.DataGenTableSourceFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.BlackHoleSinkFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.DataGenTableSourceFactory.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.BlackHoleTableSinkFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="18507" opendate="2020-7-7 00:00:00" fixdate="2020-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move get_config implementation to TableEnvironment to eliminate the duplication</summary>
      <description>Currently, TableEnvironment.get_config is abstract and the implementations in the child classes BatchTableEnvironment/StreamTableEnvironment are duplicate. The implementation could be moved to TableEnvironment to eliminate the duplication.</description>
      <version>1.9.0,1.10.0,1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="18519" opendate="2020-7-7 00:00:00" fixdate="2020-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Propagate exception to client when execution fails for REST submission</summary>
      <description>Currently when a user submits an application using the REST api and the execution fails, the exception is logged, but not sent back to the client. This issue aims at propagating the reason back to the client so that it is easier for the user to debug his/her application.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18520" opendate="2020-7-7 00:00:00" fixdate="2020-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>New Table Function type inference fails</summary>
      <description>For a simple UDTF like public class Split extends TableFunction&lt;String&gt; { public Split(){} public void eval(String str, String ch) { if (str == null || str.isEmpty()) { return; } else { String[] ss = str.split(ch); for (String s : ss) { collect(s); } } } }register it using new function type inference tableEnv.createFunction("my_split", Split.class); and using it in a simple query will fail with following exception:Exception in thread "main" org.apache.flink.table.api.ValidationException: SQL validation failed. From line 1, column 93 to line 1, column 115: No match found for function signature my_split(&lt;CHARACTER&gt;, &lt;CHARACTER&gt;) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:527) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:204) at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78) at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:716) at com.bytedance.demo.SqlTest.main(SqlTest.java:64)Caused by: org.apache.calcite.runtime.CalciteContextException: From line 1, column 93 to line 1, column 115: No match found for function signature my_split(&lt;CHARACTER&gt;, &lt;CHARACTER&gt;) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457) at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:839) at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824) at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5089) at org.apache.calcite.sql.validate.SqlValidatorImpl.handleUnresolvedFunction(SqlValidatorImpl.java:1882) at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:305) at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:218) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5858) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5845) at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1800) at org.apache.calcite.sql.validate.ProcedureNamespace.validateImpl(ProcedureNamespace.java:57) at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3303) at org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:86) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3247) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510) at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084) at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059) at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141) ... 7 moreCaused by: org.apache.calcite.sql.validate.SqlValidatorException: No match found for function signature my_split(&lt;CHARACTER&gt;, &lt;CHARACTER&gt;) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457) at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:550) ... 35 moreit's reported from user-zh: http://apache-flink.147419.n8.nabble.com/Flink-1-11-SQL-UDTF-No-match-found-for-function-signature-td4553.htmlCC twalthr</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="18524" opendate="2020-7-8 00:00:00" fixdate="2020-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala varargs cause exception for new inference</summary>
      <description>Scala varargs are supported but cause an exception currently. Because there are two signatures (a valid and invalid one) in the class file.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.extraction.FunctionMappingExtractor.java</file>
      <file type="M">flink-table.flink-table-api-scala.src.test.scala.org.apache.flink.table.types.extraction.TypeInferenceExtractorScalaTest.scala</file>
    </fixedFiles>
  </bug>
  <bug id="18526" opendate="2020-7-8 00:00:00" fixdate="2020-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the configuration of Python UDF using Managed Memory in the doc of Pyflink</summary>
      <description>Add the configuration of Python UDF using Managed Memory in the doc of Pyflink</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.python.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.table.python.vectorized.python.udfs.md</file>
      <file type="M">docs.dev.table.python.python.udfs.zh.md</file>
      <file type="M">docs.dev.table.python.python.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="18528" opendate="2020-7-8 00:00:00" fixdate="2020-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update UNNEST to new type system</summary>
      <description>Updates the built-in UNNEST function to the new type system.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.LogicalUnnestRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.ExplodeFunctionUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalUnnestRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.bridging.BridgingSqlFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.utils.DataTypeUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.RowType.java</file>
    </fixedFiles>
  </bug>
  <bug id="18529" opendate="2020-7-8 00:00:00" fixdate="2020-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query Hive table and filter by timestamp partition can fail</summary>
      <description>The following examplecreate table foo (x int) partitioned by (ts timestamp);select x from foo where timestamp '2020-07-08 13:08:14' = ts;fails withCatalogException: HiveCatalog currently only supports timestamp of precision 9</description>
      <version>None</version>
      <fixedVersion>1.11.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="18545" opendate="2020-7-9 00:00:00" fixdate="2020-11-9 01:00:00" resolution="Done">
    <buginformation>
      <summary>Sql api cannot specify flink job name</summary>
      <description>In Flink 1.11.0, StreamTableEnvironment.executeSql(sql) will explan and execute job Immediately, The job name will special as "insert-into_sink-table-name".  But we have Multiple sql job will insert into a same sink table, this is not very friendly. </description>
      <version>1.11.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.ExecutionEnvironmentTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.StreamExecutionEnvironmentTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.PipelineOptions.java</file>
      <file type="M">docs..includes.generated.pipeline.configuration.html</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.util.PythonConfigUtilTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonConfigUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="1855" opendate="2015-4-9 00:00:00" fixdate="2015-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SocketTextStreamWordCount example cannot be run from the webclient</summary>
      <description>When trying to parameterize(with for instance "localhost 9999") the SocketTextStreamWordCount streaming example from the webclient, I get the following error:HTTP ERROR: 500Problem accessing /runJob. Reason: org/apache/flink/streaming/examples/wordcount/WordCount</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18579" opendate="2020-7-13 00:00:00" fixdate="2020-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove deprecated classes in flink-connector-jdbc</summary>
      <description>We have refactored the class structure of flink-connector-jdbc module and kept the old API classes in org.apache.flink.api.java.io.jdbc compatible purpose. Now, it's safe to remove these classes.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCInputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.split.ParameterValuesProvider.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.split.NumericBetweenParametersProvider.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.split.GenericParameterValuesProvider.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCOutputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="18583" opendate="2020-7-13 00:00:00" fixdate="2020-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The _id field is incorrectly set to index in Elasticsearch6 DynamicTableSink</summary>
      <description>The _id field is incorrectly set to index in Elasticsearch6 DynamicTableSink.It is caused by this line</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="18585" opendate="2020-7-13 00:00:00" fixdate="2020-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dynamic index can not work in new DynamicTableSink</summary>
      <description>because the IndexGenerator.open() was not inited properly.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.RowElasticsearchSinkFunction.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.IndexGeneratorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="18593" opendate="2020-7-14 00:00:00" fixdate="2020-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive bundle jar URLs are broken</summary>
      <description>we should use https://repo.maven.apache.org/maven2/ instead</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="18595" opendate="2020-7-14 00:00:00" fixdate="2020-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deadlock during job shutdown</summary>
      <description>https://travis-ci.org/github/apache/flink/jobs/707843779Found one Java-level deadlock:============================="Canceler for Flat Map -&gt; Sink: Unnamed (9/12) (b87b3f2cae66987d94399f12d7fb4641).": waiting to lock monitor 0x00007f51f655e228 (object 0x00000000812b9180, a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue), which is held by "Flat Map -&gt; Sink: Unnamed (9/12)""Flat Map -&gt; Sink: Unnamed (9/12)": waiting to lock monitor 0x000055fb00bb4b88 (object 0x00000000812b9210, a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue), which is held by "Canceler for Flat Map -&gt; Sink: Unnamed (9/12) (b87b3f2cae66987d94399f12d7fb4641)."Java stack information for the threads listed above:==================================================="Canceler for Flat Map -&gt; Sink: Unnamed (9/12) (b87b3f2cae66987d94399f12d7fb4641).": at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.notifyBufferAvailable(RemoteInputChannel.java:360) - waiting to lock &lt;0x00000000812b9180&gt; (a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue) at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.fireBufferAvailableNotification(LocalBufferPool.java:315) at org.apache.flink.runtime.io.network.b4511: No such processuffer.LocalBufferPool.recycle(LocalBufferPool.java:305) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.deallocate(NetworkBuffer.java:197) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.handleRelease(AbstractReferenceCountedByteBuf.java:110) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:100) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.recycleBuffer(NetworkBuffer.java:171) at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue.releaseAll(RemoteInputChannel.java:665) at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.releaseAllResources(RemoteInputChannel.java:254) - locked &lt;0x00000000812b9210&gt; (a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue) at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.close(SingleInputGate.java:431) - locked &lt;0x0000000080ba2488&gt; (a java.lang.Object) at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.close(InputGateWithMetrics.java:85) at org.apache.flink.runtime.taskmanager.Task.closeNetworkResources(Task.java:901) at org.apache.flink.runtime.taskmanager.Task$$Lambda$434/985222953.run(Unknown Source) at org.apache.flink.runtime.taskmanager.Task$TaskCanceler.run(Task.java:1370) at java.lang.Thread.run(Thread.java:748)"Flat Map -&gt; Sink: Unnamed (9/12)": at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.notifyBufferAvailable(RemoteInputChannel.java:360) - waiting to lock &lt;0x00000000812b9210&gt; (a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue) at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.fireBufferAvailableNotification(LocalBufferPool.java:315) at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.recycle(LocalBufferPool.java:305) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.deallocate(NetworkBuffer.java:197) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.handleRelease(AbstractReferenceCountedByteBuf.java:110) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:100) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.recycleBuffer(NetworkBuffer.java:171) at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue.addExclusiveBuffer(RemoteInputChannel.java:629) at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.recycle(RemoteInputChannel.java:314) - locked &lt;0x00000000812b9180&gt; (a org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel$AvailableBufferQueue) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.deallocate(NetworkBuffer.java:197) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.handleRelease(AbstractReferenceCountedByteBuf.java:110) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:100) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.recycleBuffer(NetworkBuffer.java:171) at org.apache.flink.streaming.runtime.io.CachedBufferStorage.close(CachedBufferStorage.java:113) at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.cleanup(CheckpointedInputGate.java:216) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.close(StreamTaskNetworkInput.java:208) at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.close(StreamOneInputProcessor.java:82) at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanup(StreamTask.java:298) at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:555) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:480) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533) at java.lang.Thread.run(Thread.java:748)Found 1 deadlock.</description>
      <version>1.10.0,1.10.1,1.11.0,1.11.1</version>
      <fixedVersion>1.10.2,1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.BufferManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="18598" opendate="2020-7-15 00:00:00" fixdate="2020-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add instructions for asynchronous execute in PyFlink doc</summary>
      <description>Add instructions for asynchronous execute in PyFlink doc</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.faq.zh.md</file>
      <file type="M">docs.dev.python.faq.md</file>
    </fixedFiles>
  </bug>
  <bug id="18600" opendate="2020-7-15 00:00:00" fixdate="2020-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kerberized YARN per-job on Docker test failed to download JDK 8u251</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4514&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529+ mkdir -p /usr/java/default+ curl -Ls https://download.oracle.com/otn-pub/java/jdk/8u251-b08/3d5a2bb8f8d4428bbe94aed7ec7ae784/jdk-8u251-linux-x64.tar.gz -H Cookie: oraclelicense=accept-securebackup-cookie+ tar --strip-components=1 -xz -C /usr/java/default/gzip: stdin: not in gzip formattar: Child returned status 1</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.Dockerfile</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="18616" opendate="2020-7-16 00:00:00" fixdate="2020-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add SHOW CURRENT DDLs</summary>
      <description>Supports:SHOW CURRENT CATALOG;SHOW CURRENT DATABASE;</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.BatchTableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutorBuilder.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">docs.dev.table.sql.show.zh.md</file>
      <file type="M">docs.dev.table.sql.show.md</file>
      <file type="M">docs.dev.table.hive.hive.dialect.zh.md</file>
      <file type="M">docs.dev.table.hive.hive.dialect.md</file>
    </fixedFiles>
  </bug>
  <bug id="18619" opendate="2020-7-16 00:00:00" fixdate="2020-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update training to use WatermarkStrategy</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.learn-flink.streaming.analytics.zh.md</file>
      <file type="M">docs.learn-flink.streaming.analytics.md</file>
    </fixedFiles>
  </bug>
  <bug id="18643" opendate="2020-7-20 00:00:00" fixdate="2020-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate Jenkins jobs to ci-builds.apache.org</summary>
      <description>Infra is reworking the Jenkins setup, so we have to migrate our jobs that do the snapshot deployments.Alternatively, find other ways to do this (Azure?) to reduce number of used infrastructure services./cc rmetzger</description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.deploy.staging.jars.sh</file>
      <file type="M">tools.deploy.to.maven.sh</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
    </fixedFiles>
  </bug>
  <bug id="18650" opendate="2020-7-21 00:00:00" fixdate="2020-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The description of dispatcher in Flink Architecture document is not accurate</summary>
      <description>The description of dispatcher is like below:The Dispatcher provides a REST interface to submit Flink applications for execution and starts a new JobManager for each submitted job. It also runs the Flink WebUI to provide information about job executions. As I understand it, is it "starts a new JobMaster" rather than JobManager?   </description>
      <version>1.11.0</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.concepts.flink-architecture.zh.md</file>
      <file type="M">docs.concepts.flink-architecture.md</file>
    </fixedFiles>
  </bug>
  <bug id="18672" opendate="2020-7-22 00:00:00" fixdate="2020-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Scala code examples for UDF type inference annotations</summary>
      <description>The Scala code examples for the UDF type inference annotations are not correct.For example: the following FunctionHint annotation @FunctionHint( input = Array(@DataTypeHint("INT"), @DataTypeHint("INT")), output = @DataTypeHint("INT"))needs to be changed to@FunctionHint( input = Array(new DataTypeHint("INT"), new DataTypeHint("INT")), output = new DataTypeHint("INT"))</description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="18676" opendate="2020-7-22 00:00:00" fixdate="2020-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update version of aws to support use of default constructor of "WebIdentityTokenCredentialsProvider"</summary>
      <description>Background:I am using Flink 1.11.0 on kubernetes platform. To give access of aws services to taskmanager/jobmanager, we are using "IAM Roles for Service Accounts" . I have configured below property in flink-conf.yaml to use credential provider.fs.s3a.aws.credentials.provider: com.amazonaws.auth.WebIdentityTokenCredentialsProvider Issue:When taskmanager/jobmanager is starting up, during this it complains that "WebIdentityTokenCredentialsProvider" doesn't have "public constructor" and container doesn't come up. Solution:Currently the above credential's class is being used from "flink-s3-fs-hadoop" which gets "aws-java-sdk-core" dependency from "flink-s3-fs-base". In "flink-s3-fs-base",  version of aws is 1.11.754 . The support of default constructor for "WebIdentityTokenCredentialsProvider" is provided from aws version 1.11.788 and onward.</description>
      <version>1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-base.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="18685" opendate="2020-7-23 00:00:00" fixdate="2020-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JobClient.getAccumulators() blocks until streaming job has finished in local environment</summary>
      <description>Steps to reproduce:JobClient client = env.executeAsync("Test");CompletableFuture&lt;JobStatus&gt; status = client.getJobStatus();LOG.info("status = " + status.get());CompletableFuture&lt;Map&lt;String, Object&gt;&gt; accumulators = client.getAccumulators(StreamingJob.class.getClassLoader());LOG.info("accus = " + accumulators.get(5, TimeUnit.SECONDS));Actual behaviorThe accumulators future will never complete for a streaming job when calling this just in your main() method from the IDE.Expected behaviorReceive the accumulators of the running streaming job.The JavaDocs of the method state the following: "Accumulators can be requested while it is running or after it has finished.". While it is technically true that I can request accumulators, I was expecting as a user that I can access the accumulators of a running job.Also, I can request accumulators if I submit the job to a cluster.</description>
      <version>1.11.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointCompatibilityITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.accumulators.AccumulatorLiveITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterJobClient.java</file>
      <file type="M">docs.content.release-notes.flink-1.13.md</file>
      <file type="M">docs.content.zh.release-notes.flink-1.13.md</file>
      <file type="M">docs.content.zh.release-notes.flink-1.11.md</file>
    </fixedFiles>
  </bug>
  <bug id="18690" opendate="2020-7-23 00:00:00" fixdate="2020-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement LocalInputPreferredSlotSharingStrategy</summary>
      <description>Implement ExecutionSlotSharingGroup, SlotSharingStrategy and default LocalInputPreferredSlotSharingStrategy.The default strategy would be LocalInputPreferredSlotSharingStrategy. It will try to reduce remote data exchanges. Subtasks, which are connected and belong to the same SlotSharingGroup, tend to be put in the same ExecutionSlotSharingGroup.See design doc</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulingTopology.java</file>
    </fixedFiles>
  </bug>
  <bug id="18697" opendate="2020-7-23 00:00:00" fixdate="2020-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding flink-table-api-java-bridge_2.11 to a Flink job kills the IDE logging</summary>
      <description>Steps to reproduce: Set up a Flink project using a Maven archetype Add "flink-table-api-java-bridge_2.11" as a dependency Running Flink won't produce any log outputProbable cause:"flink-table-api-java-bridge_2.11" has a dependency to "org.apache.flink:flink-streaming-java_2.11:test-jar:tests:1.11.0", which contains a "log4j2-test.properties" file.When I disable Log4j2 debugging (with "-Dlog4j2.debug"), I see the following line:DEBUG StatusLogger Reconfiguration complete for context[name=3d4eac69] at URI jar:file:/Users/robert/.m2/repository/org/apache/flink/flink-streaming-java_2.11/1.11.0/flink-streaming-java_2.11-1.11.0-tests.jar!/log4j2-test.properties (org.apache.logging.log4j.core.LoggerContext@568bf312) with optional ClassLoader: null</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18705" opendate="2020-7-24 00:00:00" fixdate="2020-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Debezium-JSON throws NPE when tombstone message is received</summary>
      <description>By default, Debezium will send two messages to Kafka for DELETE operation, one for delete message, the other for tombstone message (message value is null). However, debezium-json will throw NPE when processing such tombstone message. We should just skip such messages. As a workaround, we can diable tombstone on Debezium Connect tombstones.on.delete: false.</description>
      <version>1.11.0,1.11.1</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.debezium.DebeziumJsonDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.debezium.DebeziumJsonDeserializationSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="18716" opendate="2020-7-26 00:00:00" fixdate="2020-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the deprecated "execute" and "insert_into" calls in PyFlink Table API docs</summary>
      <description>Currently the TableEnvironment#execute and the Table#insert_into is deprecated, but the docs of PyFlink Table API still use them. We should replace them with the recommended API.</description>
      <version>1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.python.shell.zh.md</file>
      <file type="M">docs.ops.python.shell.md</file>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.streaming.query.configuration.zh.md</file>
      <file type="M">docs.dev.table.streaming.query.configuration.md</file>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.table.api.connectors.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.table.api.connectors.md</file>
      <file type="M">docs.dev.python.table.api.tutorial.zh.md</file>
      <file type="M">docs.dev.python.table.api.tutorial.md</file>
    </fixedFiles>
  </bug>
  <bug id="18725" opendate="2020-7-27 00:00:00" fixdate="2020-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Run Kubernetes test" failed with "30025: provided port is already allocated"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4901&amp;view=logs&amp;j=08866332-78f7-59e4-4f7e-49a56faa3179&amp;t=3e8647c1-5a28-5917-dd93-bf78594ea994The Service "flink-job-cluster" is invalid: spec.ports[2].nodePort: Invalid value: 30025: provided port is already allocated</description>
      <version>1.11.0,1.11.1</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.container-scripts.job-cluster-service.yaml</file>
    </fixedFiles>
  </bug>
  <bug id="18726" opendate="2020-7-27 00:00:00" fixdate="2020-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support INSERT INTO specific columns</summary>
      <description>Currently Flink only supports insert into a table without specifying columns, but most database systems support insert into specific columns byINSERT INTO table_name(column1, column2, ...) ...The columns not specified will be filled with default values or NULL if no default value is given when creating the table.As Flink currently does not support default values when creating tables, we can fill the unspecified columns with NULL and throw exceptions if there are columns with NOT NULL constraints.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TestData.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.PreValidateReWriter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="18730" opendate="2020-7-27 00:00:00" fixdate="2020-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Beta tag from SQL Client docs</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="18731" opendate="2020-7-27 00:00:00" fixdate="2020-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The monotonicity of UNIX_TIMESTAMP function is not correct</summary>
      <description>Currently, the monotonicity of UNIX_TIMESTAMP function is always INCREASING, actually, when it has empty function arguments (UNIX_TIMESTAMP(), is equivalent to NOW()), its monotonicity is INCREASING. otherwise its monotonicity should be NOT_MONOTONIC. (e.g. UNIX_TIMESTAMP(string))</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCollationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="18831" opendate="2020-8-5 00:00:00" fixdate="2020-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the Python documentation about the operations in Table</summary>
      <description>Currently, there are a few documentation is out of date and should be updated. For example, Python UDTF has been already supported in Python Table API and we could use examples of Python UDTF instead of Java UDTF in the Python doc.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="18838" opendate="2020-8-6 00:00:00" fixdate="2020-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support JdbcCatalog in Python Table API</summary>
      <description>We should provide built-in support for JdbcCatalog in Python Table API.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.catalog.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.table.catalog.py</file>
      <file type="M">docs.dev.table.connectors.jdbc.zh.md</file>
      <file type="M">docs.dev.table.connectors.jdbc.md</file>
      <file type="M">docs.dev.table.catalogs.zh.md</file>
      <file type="M">docs.dev.table.catalogs.md</file>
    </fixedFiles>
  </bug>
  <bug id="18839" opendate="2020-8-6 00:00:00" fixdate="2020-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about how to use catalog in Python Table API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
      <file type="M">docs.dev.table.hive.hive.functions.zh.md</file>
      <file type="M">docs.dev.table.hive.hive.functions.md</file>
      <file type="M">docs.dev.table.hive.hive.dialect.zh.md</file>
      <file type="M">docs.dev.table.hive.hive.dialect.md</file>
      <file type="M">docs.dev.table.connectors.jdbc.zh.md</file>
      <file type="M">docs.dev.table.connectors.jdbc.md</file>
      <file type="M">docs.dev.table.catalogs.zh.md</file>
      <file type="M">docs.dev.table.catalogs.md</file>
    </fixedFiles>
  </bug>
  <bug id="18842" opendate="2020-8-7 00:00:00" fixdate="2020-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>e2e test failed to download "localhost:9999/flink.tgz" in "Wordcount on Docker test"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5260&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=2b7514ee-e706-5046-657b-3430666e7bd92020-08-06T20:51:31.2499580Z [91m+ wget -nv -O flink.tgz localhost:9999/flink.tgz2020-08-06T20:51:31.2501498Z [0m127.0.0.1 - - [06/Aug/2020 20:51:31] "GET /flink.tgz HTTP/1.1" 200 -2020-08-06T20:51:31.2502214Z [91mfailed: Connection refused.2020-08-06T20:51:31.6885068Z [0m [91m2020-08-06 20:51:31 URL:http://localhost:9999/flink.tgz [322693675/322693675] -&gt; "flink.tgz" [1]2020-08-06T20:51:31.6888547Z [0m [91m+ [ false = true ]2020-08-06T20:51:31.6889384Z [0m [91m+ tar -xf flink.tgz --strip-components=12020-08-06T20:51:34.8125585Z [0m [91m+ rm flink.tgz2020-08-06T20:51:34.8699287Z [0m [91m+ chown -R flink:flink .2020-08-07T00:20:42.7919165Z [0m2020-08-07T00:20:43.0365895Z ##[error]The operation was canceled.</description>
      <version>1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.docker.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="18861" opendate="2020-8-9 00:00:00" fixdate="2020-8-9 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support add_source() to get a DataStream for Python StreamExecutionEnvironment</summary>
      <description>Support add_source() to get a DataStream for Python StreamExecutionEnvironment. </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18874" opendate="2020-8-10 00:00:00" fixdate="2020-8-10 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support conversion between Table and DataStream</summary>
      <description>Support Converting a DataStream into a Table, and a Table to append/retract Stream.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="18883" opendate="2020-8-11 00:00:00" fixdate="2020-8-11 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support reduce() operation for Python KeyedStream.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.datastream.runtime.operators.python.DataStreamPythonStatelessFunctionOperator.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="18884" opendate="2020-8-11 00:00:00" fixdate="2020-8-11 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add chaining strategy and slot sharing group interfaces for Python DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="18885" opendate="2020-8-11 00:00:00" fixdate="2020-8-11 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add partitioning interfaces for Python DataStream API.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="18886" opendate="2020-8-11 00:00:00" fixdate="2020-8-11 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Kafka connectors for Python DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.testing.test.case.utils.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.descriptor.py</file>
      <file type="M">flink-python.pyflink.pyflink.gateway.server.py</file>
    </fixedFiles>
  </bug>
  <bug id="18888" opendate="2020-8-11 00:00:00" fixdate="2020-8-11 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support execute_async for StreamExecutionEnvironment.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.util.DataStreamTestCollectSink.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.util.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="18916" opendate="2020-8-13 00:00:00" fixdate="2020-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a "Operations" link(linked to dev/table/tableApi.md) under the "Python API" -&gt; "User Guide" -&gt; "Table API" section</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.user-guide.table.sql.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.sql.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.table.api.connectors.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.table.api.connectors.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.index.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="18917" opendate="2020-8-13 00:00:00" fixdate="2020-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a "Built-in Functions" link (linked to dev/table/functions/systemFunctions.md) under the "Python API" -&gt; "User Guide" -&gt; "Table API" section</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..layouts.base.html</file>
      <file type="M">docs.page.js.flink.js</file>
    </fixedFiles>
  </bug>
  <bug id="1892" opendate="2015-4-16 00:00:00" fixdate="2015-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Local job execution does not exit.</summary>
      <description>When using the LocalTezEnvironment to run a job from the IDE the job fails to exit after producing data. The following thread seems to run and not allow the job to exit:"Thread-31" #46 prio=5 os_prio=31 tid=0x00007fb5d2c43000 nid=0x5507 runnable &amp;#91;0x0000000127319000&amp;#93; java.lang.Thread.State: RUNNABLE at java.lang.Throwable.fillInStackTrace(Native Method) at java.lang.Throwable.fillInStackTrace(Throwable.java:783) locked &lt;0x000000076dfda130&gt; (a java.lang.InterruptedException) at java.lang.Throwable.&lt;init&gt;(Throwable.java:250) at java.lang.Exception.&lt;init&gt;(Exception.java:54) at java.lang.InterruptedException.&lt;init&gt;(InterruptedException.java:57) at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1220) at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:335) at java.util.concurrent.PriorityBlockingQueue.take(PriorityBlockingQueue.java:545) at org.apache.tez.dag.app.rm.LocalTaskSchedulerService$AsyncDelegateRequestHandler.processRequest(LocalTaskSchedulerService.java:322) at org.apache.tez.dag.app.rm.LocalTaskSchedulerService$AsyncDelegateRequestHandler.run(LocalTaskSchedulerService.java:316) at java.lang.Thread.run(Thread.java:745)</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18926" opendate="2020-8-13 00:00:00" fixdate="2020-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a "Environment Variables" document under the "Python API" -&gt; "User Guide" -&gt; "Table API" section</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">docs..includes.generated.python.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="18930" opendate="2020-8-13 00:00:00" fixdate="2020-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate "Hive Dialect" page of "Hive Integration" into Chinese</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.hive.dialect.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="18934" opendate="2020-8-13 00:00:00" fixdate="2020-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Idle stream does not advance watermark in connected stream</summary>
      <description>Per Flink documents, when a stream is idle, it will allow watermarks of downstream operator to advance. However, when I connect an active data stream with an idle data stream, the output watermark of the CoProcessOperator does not increase.Here's a small test that reproduces the problem.https://github.com/kien-truong/flink-idleness-testing</description>
      <version>1.11.0,1.12.0,1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperatorTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamTaskBuilder.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamTask.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OperatorChainTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorWatermarksTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorLatencyMetricsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamSourceContextIdleDetectionTests.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.MockStreamStatusMaintainer.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperatorLifecycleTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTask.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.BoundedStreamTask.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.operators.StateBootstrapWrapperOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractInput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperatorV2.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.CountingOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.Input.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.Output.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.TimestampedCollector.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.TwoInputStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.AbstractDataOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.RecordWriterOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ChainingOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamIterationTail.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractStreamOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractStreamOperatorV2Test.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.CollectorOutput.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockOutput.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.TwoInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.input.FirstInputOfTwoInput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.input.OneInput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.input.SecondInputOfTwoInput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.output.BroadcastingOutput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.output.CopyingSecondInputOfTwoInputStreamOperatorOutput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.output.FirstInputOfTwoInputStreamOperatorOutput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.output.OneInputStreamOperatorOutput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.output.SecondInputOfTwoInputStreamOperatorOutput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.multipleinput.output.BlackHoleOutput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.over.NonBufferOverWindowOperatorTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.SortingBoundedInputITCase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.eventtime.WatermarkOutputMultiplexer.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonTimestampsAndWatermarksOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSourceContexts.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.TimestampsAndWatermarksOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.streamstatus.StreamStatusMaintainer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.streamstatus.StreamStatusProvider.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.CopyingBroadcastingOutputCollector.java</file>
    </fixedFiles>
  </bug>
  <bug id="18957" opendate="2020-8-14 00:00:00" fixdate="2020-9-14 01:00:00" resolution="Done">
    <buginformation>
      <summary>Implement bulk fulfil-ability timeout tracking for shared slots</summary>
      <description>Track fulfil-ability of required physical slots for all SharedSlot(s) (no matter whether they are created at this bulk or not) with timeout. This ensures we will not wait indefinitely if the required slots for this bulk cannot be fully fulfilled at the same time. Create a LogicalSlotRequestBulk to track all physical requests and logical slot requests (logical slot requests only which belong to the bulk) Mark physical slot request fulfilled in LogicalSlotRequestBulk, once its future is done If any physical slot request fails then clear the LogicalSlotRequestBulk to stop the fulfil-ability check Schedule a fulfil-ability check in LogicalSlotRequestBulkChecker for the LogicalSlotRequestBulk In case of timeout: cancel/fail the logical slot futures of the bulk in SharedSlot(s) remove</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkCheckerImpl.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.OneSlotPerExecutionSlotAllocatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkCheckerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.BulkSlotProviderImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkChecker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulk.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.BulkSlotProviderImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.BulkSlotProvider.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.MergingSharedSlotProfileRetrieverTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SharedSlotProfileRetriever.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.MergingSharedSlotProfileRetrieverFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocatorFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator.java</file>
    </fixedFiles>
  </bug>
  <bug id="18978" opendate="2020-8-17 00:00:00" fixdate="2020-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support full table scan of key and namespace from statebackend</summary>
      <description>Support full table scan of keys and namespaces from the state backend. All operations assume the calling code already knows what namespace they are interested in interacting with.This is a prerequisite to support reading window operators with the state processor api because window panes are stored as additional namespace components.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBRocksStateKeysIteratorTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.iterator.RocksStateKeysIterator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.StateTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="18987" opendate="2020-8-18 00:00:00" fixdate="2020-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sort global job parameters in WebUI job configuration view</summary>
      <description>The configuration page for a job has various entries for set GlobalJobParameters.It would be nice if these were sorted alphabetically.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.configuration.job-configuration.component.ts</file>
    </fixedFiles>
  </bug>
  <bug id="18995" opendate="2020-8-19 00:00:00" fixdate="2020-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some Hive functions fail because they need to access SessionState</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.module.hive.HiveModule.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveGenericUDF.java</file>
    </fixedFiles>
  </bug>
  <bug id="18996" opendate="2020-8-19 00:00:00" fixdate="2020-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid disorder for time interval join</summary>
      <description>Currently, the time interval join will produce data with rowtime later than watermark. If we use the rowtime again in downstream, e.t. window aggregation, we'll lose some data. reported from user-zh: http://apache-flink.147419.n8.nabble.com/Re-flink-interval-join-tc4458.html#none</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.interval.RowTimeIntervalJoinTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.interval.ProcTimeIntervalJoinTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.join.interval.TimeIntervalJoin.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.join.interval.RowTimeIntervalJoin.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.join.interval.ProcTimeIntervalJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecIntervalJoin.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.ExecutionConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="18997" opendate="2020-8-19 00:00:00" fixdate="2020-8-19 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Rename type_info to result_type to make it more clear</summary>
      <description>Currently, the name of result type parameter for Python DataStream.map() and flat_map() is type_info. def map(self, func: Union[Callable, MapFunction], type_info: TypeInformation = None) \ -&gt; 'DataStream':I think rename it to result_type would be more clear for users. def map(self, func: Union[Callable, MapFunction], result_type: TypeInformation = None) \ -&gt; 'DataStream':</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="1907" opendate="2015-4-17 00:00:00" fixdate="2015-6-17 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Scala Interactive Shell</summary>
      <description>Build an interactive Shell for the Scala api.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils.src.main.scala.org.apache.flink.test.util.FlinkTestBase.scala</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.TestEnvironment.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.MultipleProgramsTestBase.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.AbstractTestBase.java</file>
      <file type="M">flink-staging.pom.xml</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.StreamingMultipleProgramsTestBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.RemoteEnvironment.java</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19070" opendate="2020-8-28 00:00:00" fixdate="2020-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive connector should throw a meaningful exception if user reads/writes ACID tables</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="19077" opendate="2020-8-28 00:00:00" fixdate="2020-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Import process time temporal join operator</summary>
      <description>import TemporalProcessTimeJoinOperator for Processing-Time temporal join.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalTimeJoinOperatorTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalRowTimeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalProcessTimeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalProcessTimeJoinOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.TemporalTableJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.LegacyTemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.TemporalTableJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.LegacyTemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.LegacyTemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.TemporalJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.LegacyTemporalJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecTemporalJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecLegacyTemporalJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.TemporalJoinRewriteWithUniqueKeyRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableFunctionRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTemporalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTemporalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">docs.dev.table.hive.hive.streaming.zh.md</file>
      <file type="M">docs.dev.table.hive.hive.streaming.md</file>
    </fixedFiles>
  </bug>
  <bug id="19078" opendate="2020-8-28 00:00:00" fixdate="2020-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Import rowtime join temporal operator</summary>
      <description>import TemporalRowTimeJoinOperator for EventTime-Time temporal join.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.TemporalTimeJoinOperatorTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.TemporalRowTimeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.TemporalProcessTimeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalRowTimeJoinOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTemporalJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19079" opendate="2020-8-28 00:00:00" fixdate="2020-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support row time deduplicate operator</summary>
      <description>To convert a insert-only table to versioned table, the recommended way is use deduplicate query as following, the converted versioned_view owns primary key and event time and thus can be a versioned table.CREATE VIEW versioned_rates ASSELECT currency, rate, currency_timeFROM (      SELECT *,      ROW_NUMBER() OVER (PARTITION BY currency -- inferred primary key ORDER BY currency_time  -- the event time  DESC) AS rowNum FROM rates)WHERE rowNum = 1;But currently deduplicate operator only support on process time, this issue aims to support deduplicate on Event time. </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepLastRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepFirstRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepLastRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepFirstRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateFunctionTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepLastRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepFirstRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepLastRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepFirstRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateFunctionHelper.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.DeduplicateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.RankTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeysTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicityTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnUniquenessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecDeduplicateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDeduplicate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecChangelogNormalize.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeys.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19092" opendate="2020-8-29 00:00:00" fixdate="2020-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parse comment on computed column failed</summary>
      <description>Parser Exception will be thrown when add comment on computed column,{{}}CREATE TABLE test ( `id` BIGINT, `age` INT COMMENT 'test comment', //PASS `nominal_age` as age + 1 COMMENT 'test comment' // FAIL) WITH ( 'connector' = 'values', 'data-id' = '$dataId') org.apache.flink.table.api.SqlParserException: SQL parse failed. Encountered "COMMENT" at line 6, column 28.org.apache.flink.table.api.SqlParserException: SQL parse failed. Encountered "COMMENT" at line 6, column 28.Was expecting one of:    ")" ...    "," ...    "." ...    "NOT" ...    "IN" ...    "&lt;" ...    "&lt;=" ...    "&gt;" ...    "&gt;=" ...    "=" ...    "&lt;&gt;" ...    "!=" ...    "BETWEEN" ...    "LIKE" ...    "SIMILAR" ...    "+" ...    "-" ...    "*" ...    "/" ...    "%" ...    "||" ...    "AND" ...    "OR" ...    "IS" ...    "MEMBER" ...    "SUBMULTISET" ...    "CONTAINS" ...    "OVERLAPS" ...    "EQUALS" ...    "PRECEDES" ...    "SUCCEEDS" ...    "IMMEDIATELY" ...    "MULTISET" ...    "[" ...    "FORMAT" ...     at org.apache.flink.table.planner.calcite.CalciteParser.parse(CalciteParser.java:56) at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:76) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:659) at org.apache.flink.table.planner.runtime.stream.sql.LookupJoinITCase.createLookupTable(LookupJoinITCase.scala:96) at org.apache.flink.table.planner.runtime.stream.sql.LookupJoinITCase.before(LookupJoinITCase.scala:72) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)Caused by: org.apache.calcite.sql.parser.SqlParseException: Encountered "COMMENT" at line 6, column 28.Was expecting one of:we should support it as the Flink doc&amp;#91;1&amp;#93; described:&lt;computed_column_definition&gt;: column_name AS computed_column_expression &amp;#91;COMMENT column_comment&amp;#93;  &amp;#91;1&amp;#93;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/create.html</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.MergeTableLikeUtilTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.MergeTableLikeUtil.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlTableColumn.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
    </fixedFiles>
  </bug>
  <bug id="19097" opendate="2020-8-31 00:00:00" fixdate="2020-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support add_jars() for Python StreamExecutionEnvironment</summary>
      <description>Add add_jars() interface in Python StreamExecutionEnvironment to enable users to specify jar dependencies in their Python DataStream Job.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="19098" opendate="2020-8-31 00:00:00" fixdate="2020-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make RowData CSV and JSON converters public</summary>
      <description>Some of the Rowdata converters(SeDer between Rowdata and format objects like GenericRecord/JsonNode) are private or package-private (like Json), this is not easy for other third-party connector projects to utilize to implement its own format factory in Table API.For example, Pravega connector is now developing a schema-registry-based format factory. The Pravega schema registry is a rest service similar with confluent registry , but it can help to serialize/deserialize json/avro/protobuf/custom format data. It will help a lot if these converters are public.Noticed in FLINK-16048, we have already moved the avro converters out and made them public. Similarly, it should be safe to make at least json and csv format converters public.</description>
      <version>1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDataSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDataDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowDataSerializationSchema.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFileSystemFormatFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="19109" opendate="2020-9-1 00:00:00" fixdate="2020-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split Reader eats chained periodic watermarks</summary>
      <description>Attempting to generate watermarks chained to the Split Reader / ContinuousFileReaderOperator, as inSingleOutputStreamOperator&lt;Event&gt; results = env .readTextFile(...) .map(...) .assignTimestampsAndWatermarks(bounded) .keyBy(...) .process(...);leads to the Watermarks failing to be produced. Breaking the chain, via disableOperatorChaining() or a rebalance, works around the bug. Using punctuated watermarks also avoids the issue.Looking at this in the debugger reveals that timer service is being prematurely quiesced.In many respects this is FLINK-7666 brought back to life.The problem is not present in 1.9.3.There's a minimal reproducible example in https://github.com/alpinegizmo/flink-question-001/tree/bug.</description>
      <version>1.10.0,1.10.1,1.10.2,1.11.0,1.11.1</version>
      <fixedVersion>1.10.3,1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImplTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="19110" opendate="2020-9-1 00:00:00" fixdate="2020-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flatten current PyFlink documentation structure</summary>
      <description>The navigation for this entire section is overly complex. I would much rather see something flatter, like this: Python API Installation Table API Tutorial Table API User's Guide DataStream API User's Guide FAQ</description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.try-flink.python.table.api.zh.md</file>
      <file type="M">docs.try-flink.python.table.api.md</file>
      <file type="M">docs.ops.python.shell.zh.md</file>
      <file type="M">docs.ops.python.shell.md</file>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.sql.alter.zh.md</file>
      <file type="M">docs.dev.table.sql.alter.md</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.python.udfs.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.index.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.types.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.types.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.config.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.config.md</file>
      <file type="M">docs.dev.python.user-guide.table.metrics.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.metrics.md</file>
      <file type="M">docs.dev.python.user-guide.table.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.index.md</file>
      <file type="M">docs.dev.python.user-guide.table.dependency.management.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.dependency.management.md</file>
      <file type="M">docs.dev.python.user-guide.table.conversion.of.pandas.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.conversion.of.pandas.md</file>
      <file type="M">docs.dev.python.user-guide.table.built.in.functions.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.built.in.functions.md</file>
      <file type="M">docs.dev.python.user-guide.table.10.minutes.to.table.api.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.10.minutes.to.table.api.md</file>
      <file type="M">docs.dev.python.user-guide.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.index.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.index.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.data.types.zh.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.data.types.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.table.api.tutorial.zh.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.table.api.tutorial.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.index.zh.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.index.md</file>
      <file type="M">docs.dev.python.getting-started.installation.zh.md</file>
      <file type="M">docs.dev.python.getting-started.installation.md</file>
      <file type="M">docs.dev.python.getting-started.index.zh.md</file>
      <file type="M">docs.dev.python.getting-started.index.md</file>
      <file type="M">docs.dev.python.faq.zh.md</file>
      <file type="M">docs.dev.python.faq.md</file>
    </fixedFiles>
  </bug>
  <bug id="19119" opendate="2020-9-2 00:00:00" fixdate="2020-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the documentation to use Expression instead of strings in the Python Table API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.expression.py</file>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.common.zh.md</file>
      <file type="M">docs.dev.table.common.md</file>
      <file type="M">docs.dev.python.table.api.tutorial.zh.md</file>
      <file type="M">docs.dev.python.table.api.tutorial.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.intro.to.table.api.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.intro.to.table.api.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.conversion.of.pandas.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.conversion.of.pandas.md</file>
    </fixedFiles>
  </bug>
  <bug id="19138" opendate="2020-9-4 00:00:00" fixdate="2020-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python UDF supports directly specifying input_types as DataTypes.ROW</summary>
      <description>Python UDF supports input_types=DataTypes.ROW</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udtf.py</file>
    </fixedFiles>
  </bug>
  <bug id="19201" opendate="2020-9-11 00:00:00" fixdate="2020-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink e2e tests is instable and failed with "Connection broken: OSError"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6452&amp;view=logs&amp;j=ff2e2ea5-07e3-5521-7b04-a4fc3ad765e9&amp;t=6945d9e3-ebef-5993-0c44-838d8ad079c02020-09-10T21:37:42.9988117Z install conda ... [SUCCESS]2020-09-10T21:37:43.0018449Z install miniconda... [SUCCESS]2020-09-10T21:37:43.0082244Z installing python environment...2020-09-10T21:37:43.0100408Z installing python3.5...2020-09-10T21:37:58.7214400Z install python3.5... [SUCCESS]2020-09-10T21:37:58.7253792Z installing python3.6...2020-09-10T21:38:06.5855143Z install python3.6... [SUCCESS]2020-09-10T21:38:06.5903358Z installing python3.7...2020-09-10T21:38:11.5444706Z 2020-09-10T21:38:11.5484852Z ('Connection broken: OSError("(104, \'ECONNRESET\')")', OSError("(104, 'ECONNRESET')"))2020-09-10T21:38:11.5513130Z 2020-09-10T21:38:11.8044086Z conda install 3.7 failed. You can retry to exec the script.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19284" opendate="2020-9-18 00:00:00" fixdate="2020-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about how to use Python UDF in the Java Table API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.table.environment.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.table.environment.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.dependency.management.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.dependency.management.md</file>
      <file type="M">docs.dev.python.faq.zh.md</file>
      <file type="M">docs.dev.python.faq.md</file>
      <file type="M">docs.dev.python.datastream-api-users-guide.dependency.management.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="19358" opendate="2020-9-22 00:00:00" fixdate="2020-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HA should not always use jobid 0000000000</summary>
      <description>when submit a flink job on application mode with HA ,the flink job id will be 00000000000000000000000000000000, when I have many jobs ,they have the same job id , it will be lead to a checkpoint error</description>
      <version>1.11.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.ha.per.job.cluster.datastream.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapITCase.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.java</file>
    </fixedFiles>
  </bug>
  <bug id="19364" opendate="2020-9-23 00:00:00" fixdate="2020-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Batch Physical Pandas Group Window Aggregate Rule and RelNode</summary>
      <description>Add Batch Physical Pandas Group Window Aggregate Rule and RelNode to support Pandas Batch Group Window Aggregation</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupWindowAggregateFunctionOperator.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udaf.py</file>
    </fixedFiles>
  </bug>
  <bug id="19365" opendate="2020-9-23 00:00:00" fixdate="2020-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate Hive source to FLIP-27 source interface for batch</summary>
      <description>I'll limit the scope here to batch reading, to make the PR easier to review.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveTableInputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveParallelismInference.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19436" opendate="2020-9-28 00:00:00" fixdate="2020-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TPC-DS end-to-end test (Blink planner) failed during shutdown</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7009&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=2b7514ee-e706-5046-657b-3430666e7bd92020-09-27T22:37:53.2236467Z Stopping taskexecutor daemon (pid: 2992) on host fv-az655.2020-09-27T22:37:53.4450715Z Stopping standalonesession daemon (pid: 2699) on host fv-az655.2020-09-27T22:37:53.8014537Z Skipping taskexecutor daemon (pid: 11173), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8019740Z Skipping taskexecutor daemon (pid: 11561), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8022857Z Skipping taskexecutor daemon (pid: 11849), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8023616Z Skipping taskexecutor daemon (pid: 12180), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8024327Z Skipping taskexecutor daemon (pid: 12950), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8025027Z Skipping taskexecutor daemon (pid: 13472), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8025727Z Skipping taskexecutor daemon (pid: 16577), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8026417Z Skipping taskexecutor daemon (pid: 16959), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8027086Z Skipping taskexecutor daemon (pid: 17250), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8027770Z Skipping taskexecutor daemon (pid: 17601), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8028400Z Stopping taskexecutor daemon (pid: 18438) on host fv-az655.2020-09-27T22:37:53.8029314Z /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/bin/taskmanager.sh: line 99: 18438 Terminated "${FLINK_BIN_DIR}"/flink-daemon.sh $STARTSTOP $ENTRYPOINT "${ARGS[@]}"2020-09-27T22:37:53.8029895Z [FAIL] Test script contains errors.2020-09-27T22:37:53.8032092Z Checking for errors...2020-09-27T22:37:55.3713368Z No errors in log files.2020-09-27T22:37:55.3713935Z Checking for exceptions...2020-09-27T22:37:56.9046391Z No exceptions in log files.2020-09-27T22:37:56.9047333Z Checking for non-empty .out files...2020-09-27T22:37:56.9064402Z No non-empty .out files.2020-09-27T22:37:56.9064859Z 2020-09-27T22:37:56.9065588Z [FAIL] 'TPC-DS end-to-end test (Blink planner)' failed after 16 minutes and 54 seconds! Test exited with exit code 1</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.4,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.sql.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.batch.sql.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="1948" opendate="2015-4-27 00:00:00" fixdate="2015-4-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add manual task slot handling for streaming jobs</summary>
      <description>Currently, all stream operators are automatically assigned to the same task sharing group, and the user has no control over this setting. We should add a way to manually affect the way operators are allocated to task manager slots.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamCrossOperator.scala</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.operators.co.SelfConnectionTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.CoStreamTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.WindowingOptimizer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.StreamNode.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.JSONGenerator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.temporal.StreamCrossOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19480" opendate="2020-9-30 00:00:00" fixdate="2020-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support RichFunction in Python DataStream API</summary>
      <description>Currently, RichFunction is still not supported in Python DataStream API. It's a prerequisite for several features, such as: Metrics support State support The other functionalities which are available in the Java FunctionContext</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.streaming.api.typeutils.PythonTypeUtilsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.utils.PythonOperatorUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.AbstractPythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.AbstractPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonOverWindowAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.AbstractArrowPythonAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.PythonTableFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.AbstractPythonScalarFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.typeutils.PythonTypeUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamDataStreamStatelessPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.StatelessTwoInputPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.StatelessOneInputPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonReduceOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonPartitionCustomOperator.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pyx</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="19542" opendate="2020-10-9 00:00:00" fixdate="2020-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement LeaderElectionService and LeaderRetrievalService based on Kubernetes API</summary>
      <description>LeaderElectionService Contends for the leadership of a service in JobManager. There are four components in a JobManager instance that use LeaderElectionService: ResourceManager, Dispatcher, JobManager, RestEndpoint(aka WebMonitor).  Once the leader election is finished, the active leader addresses will be stored in the ConfigMap so that other components could retrieve successfully.  LeaderRetrievalService Used by Client to get the RestEndpoint address for the job submission. Used by JobManager to get the ResourceManager address for registration. Used by TaskManagers to retrieve addresses of the corresponding LeaderElectionService(e.g. JobManager address, ResourceManager address) for registration and offering slots</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.FlinkMatchers.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesClientTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClientTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">docs..includes.generated.kubernetes.config.configuration.html</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.TestingFlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.java</file>
      <file type="M">flink-annotations.src.main.java.org.apache.flink.annotation.docs.Documentation.java</file>
      <file type="M">docs.ops.config.zh.md</file>
      <file type="M">docs.ops.config.md</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.kubernetes.itcases.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19543" opendate="2020-10-9 00:00:00" fixdate="2020-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement RunningJobsRegistry, JobGraphStore based on Kubernetes API</summary>
      <description>RunningJobsRegistry Registry for the running jobs. All the jobs in the registry will be recovered when JobManager failover.  JobGraphStore JobGraph instances for running JobManagers. Note that only the meta information(aka location reference, DFS path) will be stored in the Zookeeper/ConfigMap. The real data is stored on the DFS.Each component(Dispatcher, ResourceManager, JobManager, RestEndpoint) will have a dedicated ConfigMap. All the HA information relevant for a specific component will be stored in a single ConfigMap. So the Dispatcher's ConfigMap would then contain the current leader, the running jobs and the pointers to the persisted JobGraphs.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.TestingFlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityTestBase.java</file>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.kubernetes.itcases.sh</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServices.java</file>
    </fixedFiles>
  </bug>
  <bug id="19544" opendate="2020-10-9 00:00:00" fixdate="2020-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement CheckpointRecoveryFactory based on Kubernetes API</summary>
      <description>CheckpointRecoveryFactory Stores meta information to Zookeeper/ConfigMap for checkpoint recovery. Stores the latest checkpoint counter.Each component(Dispatcher, ResourceManager, JobManager, RestEndpoint) will have a dedicated ConfigMap. All the HA information relevant for a specific component will be stored in a single ConfigMap. The JobManager's ConfigMap would then contain the current leader, the pointers to the checkpoints and the checkpoint ID counter. Since “Get(check the leader)-and-Update(write back to the ConfigMap)” is a transactional operation, we will completely solved the concurrent modification issues and not using the "lock-and-release" in Zookeeper.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServices.java</file>
    </fixedFiles>
  </bug>
  <bug id="19545" opendate="2020-10-9 00:00:00" fixdate="2020-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add e2e test for native Kubernetes HA</summary>
      <description>We could use minikube for the E2E tests. Start a Flink session/application cluster on K8s, kill one TaskManager pod or JobManager Pod and wait for the job recovered from the latest checkpoint successfully.kubectl exec -it {pod_name} -- /bin/sh -c "kill 1"</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.kubernetes.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19546" opendate="2020-10-9 00:00:00" fixdate="2020-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for native Kubernetes HA</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.jobmanager.high.availability.zh.md</file>
      <file type="M">docs.ops.jobmanager.high.availability.md</file>
    </fixedFiles>
  </bug>
  <bug id="19643" opendate="2020-10-14 00:00:00" fixdate="2020-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Pandas UDAF Doc</summary>
      <description>Add Pandas UDAF Doc</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="19644" opendate="2020-10-14 00:00:00" fixdate="2020-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support read specific partition of Hive table in temporal join</summary>
      <description>It's a common case to use hive partitioned table as dimension table.Currently Hive table only supports load all data, It will be helpful if we can support  read user specific partition in temporal table.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemOptions.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemLookupFunction.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.read.DirectoryMonitorDiscoveryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveLookupJoinITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.util.HivePartitionUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.PartitionDiscovery.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveTableInputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveContinuousMonitoringFunction.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.DirectoryMonitorDiscovery.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.ConsumeOrder.java</file>
    </fixedFiles>
  </bug>
  <bug id="19655" opendate="2020-10-15 00:00:00" fixdate="2020-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE when using blink planner and TemporalTableFunction after setting IdleStateRetentionTime</summary>
      <description>My Code here:EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();final StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, bsSettings);tableEnv.getConfig().setIdleStateRetentionTime(Time.seconds(60), Time.seconds(600));final Table table = tableEnv.from("tableName");final TableFunction&lt;?&gt; function = table.createTemporalTableFunction( temporalTableEntry.getTimeAttribute(), String.join(",", temporalTableEntry.getPrimaryKeyFields()));tableEnv.registerFunction(temporalTableEntry.getName(), function);And NPE throwed when I executed my program.java.lang.NullPointerException at org.apache.flink.table.runtime.operators.join.temporal.BaseTwoInputStreamOperatorWithStateRetention.registerProcessingCleanupTimer(BaseTwoInputStreamOperatorWithStateRetention.java:109) at org.apache.flink.table.runtime.operators.join.temporal.TemporalProcessTimeJoinOperator.processElement2(TemporalProcessTimeJoinOperator.java:98) at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processRecord2(StreamTwoInputProcessor.java:145) at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.lambda$new$1(StreamTwoInputProcessor.java:107) at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor$StreamTaskNetworkOutput.emitRecord(StreamTwoInputProcessor.java:362) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:151) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:128) at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processInput(StreamTwoInputProcessor.java:185) at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:487) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532) at java.lang.Thread.run(Thread.java:748)And When I changed to useOldPlanner, it worked fine.And when I debuged the code ,I found BaseTwoInputStreamOperatorWithStateRetention#open did not be executed.Here is BaseTwoInputStreamOperatorWithStateRetention#open code.public void open() throws Exception { initializeTimerService(); if (stateCleaningEnabled) { ValueStateDescriptor&lt;Long&gt; cleanupStateDescriptor = new ValueStateDescriptor&lt;&gt;(CLEANUP_TIMESTAMP, Types.LONG); latestRegisteredCleanupTimer = getRuntimeContext().getState(cleanupStateDescriptor); } }Here is TemporalProcessTimeJoinOperator#open code.public void open() throws Exception { this.joinCondition = generatedJoinCondition.newInstance(getRuntimeContext().getUserCodeClassLoader()); FunctionUtils.setFunctionRuntimeContext(joinCondition, getRuntimeContext()); FunctionUtils.openFunction(joinCondition, new Configuration()); ValueStateDescriptor&lt;BaseRow&gt; rightStateDesc = new ValueStateDescriptor&lt;&gt;("right", rightType); this.rightState = getRuntimeContext().getState(rightStateDesc); this.collector = new TimestampedCollector&lt;&gt;(output); this.outRow = new JoinedRow(); // consider watermark from left stream only. super.processWatermark2(Watermark.MAX_WATERMARK); }I compared the code with oldplaner(TemporalProcessTimeJoin#open).May be TemporalProcessTimeJoinOperator#open should add super.open()?Here is TemporalProcessTimeJoin#open code.override def open(): Unit = { LOG.debug(s"Compiling FlatJoinFunction: $genJoinFuncName \n\n Code:\n$genJoinFuncCode") val clazz = compile( getRuntimeContext.getUserCodeClassLoader, genJoinFuncName, genJoinFuncCode) LOG.debug("Instantiating FlatJoinFunction.") joinFunction = clazz.newInstance() FunctionUtils.setFunctionRuntimeContext(joinFunction, getRuntimeContext) FunctionUtils.openFunction(joinFunction, new Configuration()) val rightStateDescriptor = new ValueStateDescriptor[Row]("right", rightType) rightState = getRuntimeContext.getState(rightStateDescriptor) collector = new TimestampedCollector[CRow](output) cRowWrapper = new CRowWrappingCollector() cRowWrapper.out = collector super.open() }</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalRowTimeJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.join.temporal.LegacyTemporalProcessTimeJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19656" opendate="2020-10-15 00:00:00" fixdate="2020-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Automatically replace delimiter in metric name components</summary>
      <description>The metric name consists of various components (like job ID, task ID), that are then joined by a delimiter(commonly .).The delimiter isn't just for convention, but also carries semantics for many metric backends, as they organize metrics based on the delimiter.This can behave in unfortunate ways if the delimiter is contained with a given component, as it will now be split up by the backend.We should automatically filter such occurrences to prevent this from happening.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.metrics.SystemResourcesMetricsITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.scope.ScopeFormat.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.FrontMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.AbstractMetricGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.CharacterFilter.java</file>
    </fixedFiles>
  </bug>
  <bug id="19667" opendate="2020-10-15 00:00:00" fixdate="2020-3-15 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add integration with AWS Glue</summary>
      <description>AWS Glue is releasing new features for the AWS Glue Data Catalog. This request is to add a new format to launch an integration for Apache Flink with AWS Glue Data Catalog</description>
      <version>1.11.0,1.11.1,1.11.2,1.11.3</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug id="19675" opendate="2020-10-16 00:00:00" fixdate="2020-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The plan of is incorrect when Calc contains WHERE clause, composite fields access and Python UDF at the same time</summary>
      <description>For the following job:SELECT a, pyFunc1(b, d._1) FROM MyTable WHERE a + 1 &gt; 0The plan is as following:FlinkLogicalCalc(select=[a, pyFunc1(b, f0) AS EXPR$1])+- FlinkLogicalCalc(select=[a, b, d._1 AS f0]) +- FlinkLogicalLegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c, d)]]], fields=[a, b, c, d])It's incorrect as the where condition is missing.</description>
      <version>1.10.1,1.11.0</version>
      <fixedVersion>1.10.3,1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.PythonCalcSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.PythonCalcSplitRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="19749" opendate="2020-10-21 00:00:00" fixdate="2020-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve documentation in Table API page</summary>
      <description>in this link there's a "description":Performs a field add operation. Existing fields will be replaced if add columns name is the same as the existing column name. Moreover, if the added fields have duplicate field name, then the last one is used.Two point:①" add columns name "-&gt;" added column's name"②"if the added fields have duplicate field name",what's the example that has two duplicate field name?Could you modify them?They're a little misleading/unclear.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="19750" opendate="2020-10-21 00:00:00" fixdate="2020-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deserializer is not opened in Kafka consumer when restoring from state</summary>
      <description>When a job using Kafka consumer is recovered from a checkpoint or savepoint, the open method of the record deserializer is not called. This is possibly because this.deserializer.open is put into the else clause by mistake, which will only be called if the job has a clean start. </description>
      <version>1.11.0,1.11.1,1.11.2</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="19755" opendate="2020-10-22 00:00:00" fixdate="2020-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix CEP documentation error of the example in &amp;#39;After Match Strategy&amp;#39; section</summary>
      <description>symbol tax price rowtime======== ===== ======= ===================== XYZ 1 7 2018-09-17 10:00:01 XYZ 2 9 2018-09-17 10:00:02 XYZ 1 10 2018-09-17 10:00:03 XYZ 2 5 2018-09-17 10:00:04 XYZ 2 17 2018-09-17 10:00:05 XYZ 2 14 2018-09-17 10:00:06SELECT *FROM Ticker MATCH_RECOGNIZE( PARTITION BY symbol ORDER BY rowtime MEASURES SUM(A.price) AS sumPrice, FIRST(rowtime) AS startTime, LAST(rowtime) AS endTime ONE ROW PER MATCH [AFTER MATCH STRATEGY] PATTERN (A+ C) DEFINE A AS SUM(A.price) &lt; 30 ) AFTER MATCH SKIP TO LAST A symbol sumPrice startTime endTime======== ========== ===================== ===================== XYZ 26 2018-09-17 10:00:01 2018-09-17 10:00:04 XYZ 15 2018-09-17 10:00:03 2018-09-17 10:00:05 XYZ 22 2018-09-17 10:00:04 2018-09-17 10:00:06 XYZ 17 2018-09-17 10:00:05 2018-09-17 10:00:06Again, the first result matched against the rows #1, #2, #3, #4.Compared to the previous strategy, the next match includes only row #3 (mapped to A) again for the next matching.Therefore, the second result matched against the rows #3, #4, #5.The third result matched against the rows #4, #5, #6.The last result matched against the rows #5, #6.i think it will exist looping match when coming to 17, 14 using "AFTER MATCH SKIP TO LAST A "</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.streaming.match.recognize.zh.md</file>
      <file type="M">docs.dev.table.streaming.match.recognize.md</file>
    </fixedFiles>
  </bug>
  <bug id="19756" opendate="2020-10-22 00:00:00" fixdate="2020-11-22 01:00:00" resolution="Incomplete">
    <buginformation>
      <summary>Use multi-input optimization by default</summary>
      <description>After the multiple input operator is introduced we should use this optimization by default. This will affect a large amount of plan tests so we will do this in an independent subtask.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.MultipleInputITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.MultipleInputCreationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TemporalTableJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.hint.OptionsHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveShuffleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.LegacyLimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SortMergeSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SingleRowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.ShuffledHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.ShuffledHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.JoinReorderTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DeadlockBreakupTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateReduceGroupingTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.executor.BatchExecutorTest.xml</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.OptimizerConfigOptions.java</file>
      <file type="M">docs..includes.generated.optimizer.config.configuration.html</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.batch.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.TableOperatorWrapperGenerator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.batch.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.MultipleInputCreationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.explain.testExplainMultipleInput.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.processors.MultipleInputNodeCreationProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="19894" opendate="2020-10-30 00:00:00" fixdate="2020-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use iloc for positional slicing instead of direct slicing in from_pandas</summary>
      <description>When you use floats are index of pandas, it produces a wrong results: &gt;&gt;&gt; import pandas as pd&gt;&gt;&gt; t_env.from_pandas(pd.DataFrame({'a': [1, 2, 3]}, index=[2., 3., 4.])).to_pandas() a0 11 2 This is because direct slicing uses the value as index when the index contains floats: &gt;&gt;&gt; pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.])[2:] a2.0 13.0 24.0 3&gt;&gt;&gt; pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.]).iloc[2:] a4.0 3&gt;&gt;&gt; pd.DataFrame({'a': [1,2,3]}, index=[2, 3, 4])[2:] a4 3 </description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.conversion.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="19896" opendate="2020-10-30 00:00:00" fixdate="2020-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve first-n-row fetching in the rank operator</summary>
      <description>Currently Deduplicate operator only supports first-row deduplication (ordered by proc-time). In scenario of first-n-rows deduplication, the planner has to resort to Rank operator. However, Rank operator is less efficient than Deduplicate due to larger state and more state access.This issue proposes to extend DeduplicateKeepFirstRowFunction to support first-n-rows deduplication. And the original first-row deduplication would be a special case of first-n-rows deduplication.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.RankJsonPlanITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecRank.java</file>
    </fixedFiles>
  </bug>
  <bug id="19897" opendate="2020-10-30 00:00:00" fixdate="2020-11-30 01:00:00" resolution="Done">
    <buginformation>
      <summary>Improve UI related to FLIP-102</summary>
      <description>This ticket collects issues that came up after merging FLIP-102 related changes into master. The following issues should be fixed. Add Tooltip to Heap metrics cell pointing out that the max metrics might differ from the configured maximum value. This tooltip could be made optional and only appears if heap max is different from the configured value. Here's a proposal for the tooltip text: The maximum heap displayed might differ from the configured values depending on the used GC algorithm for this process. Rename "Network Memory Segments" into "Netty Shuffle Buffers" Rename "Network Garbage Collection" into "Garbage Collection"</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.styles.rewrite.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="19899" opendate="2020-10-30 00:00:00" fixdate="2020-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Kinesis][EFO] Optimise error handling to use a separate exception delivery mechanism</summary>
      <description>BackgroundThere is a queue used to pass events between the network client and consumer application. When an error is thrown in the network thread, the queue is cleared to make space for the error event. This means that records will be thrown away to make space for errors (the records would be subsequently reloaded from the shard).ScopeAdd a new mechanism to pass exceptions between threads, meaning data does not need to be discarded. When an error is thrown, the error event will be processed by the consumer once all of the records have been processed.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.FakeKinesisFanOutBehavioursFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyV2Test.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.java</file>
    </fixedFiles>
  </bug>
  <bug id="19915" opendate="2020-11-2 00:00:00" fixdate="2020-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>wrong comments of cep test</summary>
      <description>// code placeholder@Testpublic void testNFACompilerPatternEndsWithNotFollowedBy() { // adjust the rule expectedException.expect(MalformedPatternException.class); expectedException.expectMessage("NotFollowedBy is not supported as a last part of a Pattern!"); Pattern&lt;Event, ?&gt; invalidPattern = Pattern.&lt;Event&gt;begin("start").where(new TestFilter()) .followedBy("middle").where(new TestFilter()) .notFollowedBy("end").where(new TestFilter()); // here we must have an exception because of the two "start" patterns with the same name. compile(invalidPattern, false);}  // here we must have an exception because of the two "start" patterns with the same name.It is not right</description>
      <version>1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.compiler.NFACompilerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="20077" opendate="2020-11-10 00:00:00" fixdate="2020-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot register a view with MATCH_RECOGNIZE clause</summary>
      <description>TableEnvironment env = TableEnvironment.create(EnvironmentSettings.newInstance() .useBlinkPlanner() .build()); env.executeSql("" + "CREATE TEMPORARY TABLE data (\n" + " id INT,\n" + " ts AS PROCTIME()\n" + ") WITH (\n" + " 'connector' = 'datagen',\n" + " 'rows-per-second' = '3',\n" + " 'fields.id.kind' = 'sequence',\n" + " 'fields.id.start' = '1000000',\n" + " 'fields.id.end' = '2000000'\n" + ")"); env.executeSql("" + "CREATE TEMPORARY VIEW events AS \n" + "SELECT 1 AS key, id, MOD(id, 10) AS measurement, ts \n" + "FROM data"); env.executeSql("" + "CREATE TEMPORARY VIEW foo AS \n" + "SELECT * \n" + "FROM events MATCH_RECOGNIZE (\n" + " PARTITION BY key \n" + " ORDER BY ts ASC \n" + " MEASURES \n" + " this_step.id as startId,\n" + " next_step.id as nextId,\n" + " this_step.ts AS ts1,\n" + " next_step.ts AS ts2,\n" + " next_step.measurement - this_step.measurement AS diff \n" + " AFTER MATCH SKIP TO NEXT ROW \n" + " PATTERN (this_step next_step)\n" + " DEFINE this_step AS TRUE,\n" + " next_step AS TRUE\n" + ")"); env.executeSql("SELECT * FROM foo");fails with: java.lang.AssertionError at org.apache.calcite.sql.SqlMatchRecognize$SqlMatchRecognizeOperator.createCall(SqlMatchRecognize.java:274) at org.apache.calcite.sql.util.SqlShuttle$CallCopyingArgHandler.result(SqlShuttle.java:117) at org.apache.calcite.sql.util.SqlShuttle$CallCopyingArgHandler.result(SqlShuttle.java:101) at org.apache.calcite.sql.util.SqlShuttle.visit(SqlShuttle.java:67) at org.apache.flink.table.planner.utils.Expander$Expanded$1.visit(Expander.java:153) at org.apache.flink.table.planner.utils.Expander$Expanded$1.visit(Expander.java:130) at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) at org.apache.calcite.sql.util.SqlShuttle$CallCopyingArgHandler.visitChild(SqlShuttle.java:134) at org.apache.calcite.sql.util.SqlShuttle$CallCopyingArgHandler.visitChild(SqlShuttle.java:101) at org.apache.calcite.sql.SqlOperator.acceptCall(SqlOperator.java:879) at org.apache.calcite.sql.SqlSelectOperator.acceptCall(SqlSelectOperator.java:133) at org.apache.calcite.sql.util.SqlShuttle.visit(SqlShuttle.java:66) at org.apache.flink.table.planner.utils.Expander$Expanded$1.visit(Expander.java:153) at org.apache.flink.table.planner.utils.Expander$Expanded$1.visit(Expander.java:130) at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) at org.apache.flink.table.planner.utils.Expander$Expanded.substitute(Expander.java:168) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertViewQuery(SqlToOperationConverter.java:728) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertCreateView(SqlToOperationConverter.java:699) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:226) at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:658) at org.apache.flink.table.planner.runtime.stream.table.FunctionITCase.test(FunctionITCase.java:165) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:220) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:53)</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="20243" opendate="2020-11-19 00:00:00" fixdate="2020-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove useless words in documents</summary>
      <description></description>
      <version>1.11.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="2025" opendate="2015-5-15 00:00:00" fixdate="2015-5-15 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support booleans in CSV reader</summary>
      <description>It would be great if Flink allowed to read booleans from CSV files, e.g. 1 for true and 0 for false.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.parser.StringParser.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.parser.FieldParser.java</file>
      <file type="M">docs.apis.programming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="20284" opendate="2020-11-23 00:00:00" fixdate="2020-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error happens in TaskExecutor when closing JobMaster connection if there was a python UDF</summary>
      <description>When a TaskExecutor successfully finished running a python UDF task and disconnecting from JobMaster, errors below will happen. This error, however, seems not affect job execution at the moment.2020-11-20 17:05:21,932 INFO org.apache.beam.runners.fnexecution.logging.GrpcLoggingService [] - 1 Beam Fn Logging clients still connected during shutdown.2020-11-20 17:05:21,938 WARN org.apache.beam.sdk.fn.data.BeamFnDataGrpcMultiplexer [] - Hanged up for unknown endpoint.2020-11-20 17:05:22,126 INFO org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -&gt; select: (f0) -&gt; select: (add_one(f0) AS a) -&gt; to: Tuple2 -&gt; Sink: Streaming select table sink (1/1)#0 (b0c2104dd8f87bb1caf0c83586c22a51) switched from RUNNING to FINISHED.2020-11-20 17:05:22,126 INFO org.apache.flink.runtime.taskmanager.Task [] - Freeing task resources for Source: Custom Source -&gt; select: (f0) -&gt; select: (add_one(f0) AS a) -&gt; to: Tuple2 -&gt; Sink: Streaming select table sink (1/1)#0 (b0c2104dd8f87bb1caf0c83586c22a51).2020-11-20 17:05:22,128 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Un-registering task and sending final execution state FINISHED to JobManager for task Source: Custom Source -&gt; select: (f0) -&gt; select: (add_one(f0) AS a) -&gt; to: Tuple2 -&gt; Sink: Streaming select table sink (1/1)#0 b0c2104dd8f87bb1caf0c83586c22a51.2020-11-20 17:05:22,156 INFO org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ACTIVE, resource profile: ResourceProfile{cpuCores=1.0000000000000000, taskHeapMemory=384.000mb (402653174 bytes), taskOffHeapMemory=0 bytes, managedMemory=512.000mb (536870920 bytes), networkMemory=128.000mb (134217730 bytes)}, allocationId: b67c3307dcf93757adfb4f0f9f7b8c7b, jobId: d05f32162f38ec3ec813c4621bc106d9).2020-11-20 17:05:22,157 INFO org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job d05f32162f38ec3ec813c4621bc106d9 from job leader monitoring.2020-11-20 17:05:22,157 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Close JobManager connection for job d05f32162f38ec3ec813c4621bc106d9.2020-11-20 17:05:23,064 ERROR org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.rejectedExecution [] - Failed to submit a listener notification task. Event loop shut down?java.lang.NoClassDefFoundError: org/apache/beam/vendor/grpc/v1p26p0/io/netty/util/concurrent/GlobalEventExecutor$2 at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.GlobalEventExecutor.startThread(GlobalEventExecutor.java:227) ~[blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT] at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.GlobalEventExecutor.execute(GlobalEventExecutor.java:215) ~[blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT] at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.safeExecute(DefaultPromise.java:841) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT] at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:498) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT] at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT] at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT] at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.setSuccess(DefaultPromise.java:96) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT] at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.SingleThreadEventExecutor$6.run(SingleThreadEventExecutor.java:1089) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT] at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT] at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_261]Caused by: java.lang.ClassNotFoundException: org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.GlobalEventExecutor$2 at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_261] at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_261] at org.apache.flink.util.FlinkUserCodeClassLoader.loadClassWithoutExceptionHandling(FlinkUserCodeClassLoader.java:63) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.util.ChildFirstClassLoader.loadClassWithoutExceptionHandling(ChildFirstClassLoader.java:72) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at org.apache.flink.util.FlinkUserCodeClassLoader.loadClass(FlinkUserCodeClassLoader.java:49) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT] at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_261] ... 11 more</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.SharedResourceHolder.java</file>
    </fixedFiles>
  </bug>
  <bug id="20456" opendate="2020-12-3 00:00:00" fixdate="2020-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make streaming SQL concepts more approachable</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.streaming.dynamic.tables.md</file>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.streaming.index.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
      <file type="M">docs.dev.table.sql.gettingStarted.zh.md</file>
      <file type="M">docs.dev.table.streaming.index.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.zh.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.md</file>
      <file type="M">docs.dev.table.streaming.joins.zh.md</file>
      <file type="M">docs.dev.table.streaming.joins.md</file>
      <file type="M">docs.dev.table.streaming.time.attributes.md</file>
    </fixedFiles>
  </bug>
  <bug id="20562" opendate="2020-12-10 00:00:00" fixdate="2020-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support ExplainDetails for EXPLAIN sytnax</summary>
      <description>Currently, EXPLAIN syntax only supports to print the default AST, logical plan, and physical plan. However, it doesn't support to print detailed information such as CHANGELOG_MODE, ESTIMATED_COST, JSON_EXECUTION_PLAN which are defined in ExplainDetail.Allow users to specify the ExplainDetails in statement.EXPLAIN [ExplainDetail[, ExplainDetail]*] &lt;statement&gt;ExplainDetail: { ESTIMATED_COST, CHANGELOG_MODE, JSON_EXECUTION_PLAN}Print the plan for the statement with specified ExplainDetails. ESTIMATED_COSTgenerates cost information on physical node estimated by optimizer, e.g. TableSourceScan(..., cumulative cost = {1.0E8 rows, 1.0E8 cpu, 2.4E9 io, 0.0 network, 0.0 memory})CHANGELOG_MODEgenerates changelog mode for every physical rel node. e.g. GroupAggregate(..., changelogMode=&amp;#91;I,UA,D&amp;#93;)JSON_EXECUTION_PLANgenerates the execution plan in json format of the program.Flink SQL&gt; EXPLAIN ESTIMATED_COST, CHANGELOG SELECT * FROM MyTable;...</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ExplainOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.resources.org.apache.flink.sql.parser.utils.ParserResource.properties</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.utils.ParserResource.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.dql.SqlRichExplain.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.table.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="20563" opendate="2020-12-10 00:00:00" fixdate="2020-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support built-in functions for Hive versions prior to 1.2.0</summary>
      <description>Currently Hive built-in functions are supported only for Hive-1.2.0 and later. We should investigate how to lift this limitation.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV120.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV100.java</file>
    </fixedFiles>
  </bug>
  <bug id="20894" opendate="2021-1-8 00:00:00" fixdate="2021-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce SupportsAggregatePushDown interface</summary>
      <description>Will introduce the SupportsAggregatePushDown interface for local agg pushdown</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.ScanTableSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="20895" opendate="2021-1-8 00:00:00" fixdate="2021-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support LocalAggregatePushDown in Blink planner</summary>
      <description>Will add related rule to support LocalAggregatePushDown in Blink planner</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.utils.TestData.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.SourceAbilitySpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.SourceAbilityContext.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.AggregateExpression.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.abilities.SupportsAggregatePushDown.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.OptimizerConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.optimizer.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="21073" opendate="2021-1-21 00:00:00" fixdate="2021-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mention that RocksDB ignores equals/hashCode because it works on binary data</summary>
      <description>See https://lists.apache.org/thread.html/ra43e2b5d388831290c293b9daf0eee0b0a5d9712543b62c83234a829%40%3Cuser.flink.apache.org%3E</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.state.backends.md</file>
      <file type="M">docs.content.zh.docs.ops.state.state.backends.md</file>
    </fixedFiles>
  </bug>
  <bug id="21123" opendate="2021-1-25 00:00:00" fixdate="2021-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Beanutils 1.9.x to 1.9.4</summary>
      <description>CVE-2019-10086</description>
      <version>None</version>
      <fixedVersion>1.11.4,1.12.2,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21185" opendate="2021-1-28 00:00:00" fixdate="2021-2-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce a new interface for catalog to listen on temporary object operations</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.Catalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="22666" opendate="2021-5-14 00:00:00" fixdate="2021-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make structured type&amp;#39;s fields more lenient during casting</summary>
      <description>While writing documentation in FLINK-22537, I found some issues when using the Scala DataStream API. We should add more tests to identify those.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastAvoidanceTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeCasts.java</file>
    </fixedFiles>
  </bug>
  <bug id="22673" opendate="2021-5-17 00:00:00" fixdate="2021-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document about add jar related commands</summary>
      <description>Including ADD JAR, SHOW JAR, REMOVE JAR.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.show.md</file>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.show.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="23208" opendate="2021-7-1 00:00:00" fixdate="2021-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Late processing timers need to wait 1ms at least to be fired</summary>
      <description>The problem is from the codes below:public static long getProcessingTimeDelay(long processingTimestamp, long currentTimestamp) { // delay the firing of the timer by 1 ms to align the semantics with watermark. A watermark // T says we won't see elements in the future with a timestamp smaller or equal to T. // With processing time, we therefore need to delay firing the timer by one ms. return Math.max(processingTimestamp - currentTimestamp, 0) + 1;}Assuming a Flink job creates 1 timer per millionseconds, and is able to consume 1 timer/ms. Here is what will happen: Timestmap1(1st ms): timer1 is registered and will be triggered on Timestamp2. Timestamp2(2nd ms): timer2 is registered and timer1 is triggered Timestamp3(3rd ms): timer3 is registered and timer1 is consumed, after this, InternalTimerServiceImpl registers next timer, which is timer2, and timer2 will be triggered on Timestamp4(wait 1ms at least) Timestamp4(4th ms): timer4 is registered and timer2 is triggered Timestamp5(5th ms): timer5 is registered and timer2 is consumed, after this, InternalTimerServiceImpl registers next timer, which is timer3, and timer3 will be triggered on Timestamp6(wait 1ms at least)As we can see here, the ability of the Flink job is consuming 1 timer/ms, but it's actually able to consume 0.5 timer/ms. And another problem is that we cannot observe the delay from the lag metrics of the source(Kafka). Instead, what we can tell is that the moment of output is much later than expected. I've added a metrics in our inner version, we can see the lag of the timer triggering keeps increasing: In another word, we should never let the late processing timer wait 1ms, I think a simple change would be as below:return Math.max(processingTimestamp - currentTimestamp, -1) + 1;</description>
      <version>1.11.0,1.11.3,1.13.0,1.14.0,1.12.4</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ProcessingTimeServiceUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="23936" opendate="2021-8-24 00:00:00" fixdate="2021-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python UDFs instances are reinitialized if there is no input for more than 1 minute</summary>
      <description>We receive this feedback from some PyFlink users. After some investigation, we find out that the root case is that there is a mechanism in Beam that it will released the BundleProcessors which are inactive for more than 1 minute: https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/worker/sdk_worker.py#L90</description>
      <version>1.10.0,1.11.0,1.12.0,1.13.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.sdk.worker.main.py</file>
    </fixedFiles>
  </bug>
  <bug id="23995" opendate="2021-8-26 00:00:00" fixdate="2021-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive dialect: `insert overwrite table partition if not exists` will throw exception when tablename is like &amp;#39;database.table&amp;#39;</summary>
      <description>when run such hive sql insert overwrite table default.dest2 partition (p1=1,p2='static') if not exists select x from src it will throw exceptionCaused by: org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found default</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
