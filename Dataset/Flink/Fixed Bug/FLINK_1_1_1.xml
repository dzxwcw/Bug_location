<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="4341" opendate="2016-8-9 00:00:00" fixdate="2016-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kinesis connector does not emit maximum watermark properly</summary>
      <description>*Prevously reported as "Checkpoint state size grows unbounded when task parallelism not uniform"*This issue was first encountered with Flink release 1.1.0 (commit 45f7825). I was previously using a 1.1.0 snapshot (commit 18995c8) which performed as expected. This issue was introduced somewhere between those commits.I've got a Flink application that uses the Kinesis Stream Consumer to read from a Kinesis stream with 2 shards. I've got 2 task managers with 2 slots each, providing a total of 4 slots. When running the application with a parallelism of 4, the Kinesis consumer uses 2 slots (one per Kinesis shard) and 4 slots for subsequent tasks that process the Kinesis stream data. I use an in-memory store for checkpoint data.Yesterday I upgraded to Flink 1.1.0 (45f7825) and noticed that checkpoint states were growing unbounded when running with a parallelism of 4, checkpoint interval of 10 seconds:ID State Size1 11.3 MB2 20.9 MB3 30.6 MB4 41.4 MB5 52.6 MB6 62.5 MB7 71.5 MB8 83.3 MB9 93.5 MBThe first 4 checkpoints generally succeed, but then fail with an exception like the following:java.lang.RuntimeException: Error triggering a checkpoint as the result of receiving checkpoint barrier at org.apache.flink.streaming.runtime.tasks.StreamTask$2.onEvent(StreamTask.java:768) at org.apache.flink.streaming.runtime.tasks.StreamTask$2.onEvent(StreamTask.java:758) at org.apache.flink.streaming.runtime.io.BarrierBuffer.processBarrier(BarrierBuffer.java:203) at org.apache.flink.streaming.runtime.io.BarrierBuffer.getNextNonBlocked(BarrierBuffer.java:129) at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:183) at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:66) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:266) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:584) at java.lang.Thread.run(Thread.java:745)Caused by: java.io.IOException: Size of the state is larger than the maximum permitted memory-backed state. Size=12105407 , maxSize=5242880 . Consider using a different state backend, like the File System State backend. at org.apache.flink.runtime.state.memory.MemoryStateBackend.checkSize(MemoryStateBackend.java:146) at org.apache.flink.runtime.state.memory.MemoryStateBackend$MemoryCheckpointOutputStream.closeAndGetBytes(MemoryStateBackend.java:200) at org.apache.flink.runtime.state.memory.MemoryStateBackend$MemoryCheckpointOutputStream.closeAndGetHandle(MemoryStateBackend.java:190) at org.apache.flink.runtime.state.AbstractStateBackend$CheckpointStateOutputView.closeAndGetHandle(AbstractStateBackend.java:447) at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.snapshotOperatorState(WindowOperator.java:879) at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:598) at org.apache.flink.streaming.runtime.tasks.StreamTask$2.onEvent(StreamTask.java:762) ... 8 moreOr:2016-08-09 17:44:43,626 INFO org.apache.flink.streaming.runtime.tasks.StreamTask - Restoring checkpointed state to task Fold: property_id, player -&gt; 10-minute Sliding-Window Percentile Aggregation -&gt; Sink: InfluxDB (2/4)2016-08-09 17:44:51,236 ERROR akka.remote.EndpointWriter - Transient association error (association remains live) akka.remote.OversizedPayloadException: Discarding oversized payload sent to Actor[akka.tcp://flink@10.55.2.212:6123/user/jobmanager#510517238]: max allowed size 10485760 bytes, actual size of encoded class org.apache.flink.runtime.messages.checkpoint.AcknowledgeCheckpoint was 10891825 bytes.This can be fixed by simply submitting the job with a parallelism of 2. I suspect there was a regression introduced relating to assumptions about the number of sub-tasks associated with a job stage (e.g. assuming 4 instead of a value ranging from 1-4). This is currently preventing me from using all available Task Manager slots.</description>
      <version>1.1.0,1.1.1</version>
      <fixedVersion>1.1.2,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer.java</file>
      <file type="M">docs.dev.connectors.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug id="4403" opendate="2016-8-16 00:00:00" fixdate="2016-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RPC proxy classloading should use Flink class&amp;#39; classloader</summary>
      <description>The RPC service's proxies use the ClassLoader.getSystemClassLoader() for all reflective classloading.In settings where Flink runs embedded, the Flink framework classes may not be in the System classloader, but for example in the classloader of an OSGI bundle. It is hence better to use the classloader of a Flink Framework class. In most cases, that will be the system classloader, in other cases it will be the classloader for the Flink code bundle: RpcService.class.getClassLoader().</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcService.java</file>
    </fixedFiles>
  </bug>
  <bug id="4406" opendate="2016-8-17 00:00:00" fixdate="2016-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement job master registration at resource manager</summary>
      <description>Job Master needs to register to Resource Manager when starting and then watches leadership changes of RM, and trigger re-registration.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobManagerRunnerMockTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterToResourceManagerConnection.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterRegistrationSuccess.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobManagerRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="4408" opendate="2016-8-17 00:00:00" fixdate="2016-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Submit Job and setup ExecutionGraph</summary>
      <description>Once granted the leadership, JM will start to execute the job.Most code remains the same except that (1) In old implementation where JM manages the execution of multiple jobs, JM has to load all submitted JobGraphs from SubmittedJobGraphStore and recover them. Now that the components creating JM will be responsible for the recovery of JobGraphs, JM will be created with submitted/recovered JobGraph, without the need to load the JobGraph.(2) JM should not rely on Akka to listen on the updates of JobStatus and Execution.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.RpcConnectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.TestingHighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.scheduler.Scheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.NonHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServices.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.JobSubmissionResult.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.JobExecutionResult.java</file>
    </fixedFiles>
  </bug>
  <bug id="4412" opendate="2016-8-17 00:00:00" fixdate="2016-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Chaining does not properly handle broadcast variables</summary>
      <description></description>
      <version>1.1.1</version>
      <fixedVersion>1.1.2,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Environment.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.DataSet.py</file>
    </fixedFiles>
  </bug>
  <bug id="4434" opendate="2016-8-19 00:00:00" fixdate="2016-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a testing RPC service</summary>
      <description>I suggest to add a simple testing RPC service that allows to register mock gateways under certain names.That way, if a component connects to another component via the RPC service, it will call the mock gateway, rather than the proper proxy gateway that will try to send actor messages.Tests on mocks are very easy and powerful with tools like Mockito.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.RpcCompletenessTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="4441" opendate="2016-8-21 00:00:00" fixdate="2016-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RocksDB statebackend makes all operators appear stateful</summary>
      <description>When the state is empty the rocks db state backend returns an empty hashmap instead of a null in the snapshotPartitionedState method.This means that these operators always appear stateful to the flink runtime which makes it impossible for instance to remove a stateless operator using the savepoints.</description>
      <version>1.1.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="4445" opendate="2016-8-22 00:00:00" fixdate="2016-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ignore unmatched state when restoring from savepoint</summary>
      <description>When currently submitting a job with a savepoint, we require that all state is matched to the new job. Many users have noted that this is overly strict. I would like to loosen this and allow savepoints to be restored without matching all state.The following options come to mind:(1) Keep the current behaviour, but add a flag to allow ignoring state when restoring, e.g. bin/flink -s &lt;savepoint&gt; --ignoreUnmatchedState. This would be non-API breaking.(2) Ignore unmatched state and continue. Additionally add a flag to be strict about checking the state, e.g. bin/flink -s &lt;savepoint&gt; --strict. This would be API-breaking as the default behaviour would change. Users might be confused by this because there is no straight forward way to notice that nothing has been restored.I'm not sure what's the best thing here. gyfora, aljoscha What do you think?</description>
      <version>1.1.1</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RescalingITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointLoaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.SavepointRestoreSettings.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointLoader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamContextEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobSubmitTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.JobSnapshottingSettings.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobGraph.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.CliFrontendRunTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgram.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.DetachedEnvironment.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ContextEnvironmentFactory.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ContextEnvironment.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
      <file type="M">docs.setup.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="4446" opendate="2016-8-22 00:00:00" fixdate="2016-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move Flume Sink from Flink to Bahir</summary>
      <description>As per &amp;#91;1&amp;#93; the Flink community decided to move the Flume connector from Flink to Bahir.&amp;#91;1&amp;#93; http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Move-Redis-and-Flume-connectors-to-Apache-Bahir-and-redirect-contributions-there-td13102.html</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-flume.src.main.java.org.apache.flink.streaming.connectors.flume.FlumeSink.java</file>
      <file type="M">flink-connectors.flink-connector-flume.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4488" opendate="2016-8-25 00:00:00" fixdate="2016-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent cluster shutdown after job execution for non-detached jobs</summary>
      <description>In per-job mode, the Yarn cluster currently shuts down after the first interactively executed job. Users may want to execute multiple jobs in one Jar. I would suggest to use this mechanism only for jobs which run detached. For interactive jobs, shutdown of the cluster is additionally handled by the CLI which should be sufficient to ensure cluster shutdown. Cluster shutdown could only become a problem in case of a network partition to the cluster or outage of the CLI.</description>
      <version>1.1.1,1.2.0</version>
      <fixedVersion>1.1.2,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterClient.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.FlinkYarnSessionCliTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="4514" opendate="2016-8-26 00:00:00" fixdate="2016-8-26 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>ExpiredIteratorException in Kinesis Consumer on long catch-ups to head of stream</summary>
      <description>Original mailing thread for the reported issue:http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Kinesis-connector-Iterator-expired-exception-td8711.htmlNormally, the exception is thrown when the consumer uses the same shard iterator after 5 minutes since it was retrieved. I've still yet to clarify &amp; reproduce the root cause of the ExpiredIteratorException, because from the code this seems to be impossible. I'm leaning towards suspecting this is a Kinesis-side issue (from the description in the ML, the behaviour also seems indeterminate).Either way, the exception can be fairly easily handled so that the consumer doesn't just fail. When caught, we request a new shard iterator from Kinesis with the last sequence number.</description>
      <version>1.1.0,1.1.1</version>
      <fixedVersion>1.1.3,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.FakeKinesisBehavioursFactory.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtil.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.ConsumerConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="4743" opendate="2016-10-5 00:00:00" fixdate="2016-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The sqrt/power function not accept the real data types.</summary>
      <description>At time calculate the sequences sql aggregate functions for real type column, got exception: No applicable constructor/method found for actual parameters "float, java.math.BigDecimal"And for column of integer type the problem does not occur.Code reproduce:@Test def test():Unit={ val env = ExecutionEnvironment.getExecutionEnvironment val tEnv = TableEnvironment.getTableEnvironment(env, config) val ds = env.fromElements( (1.0f, 1), (2.0f, 2)).toTable(tEnv) tEnv.registerTable("MyTable", ds) val sqlQuery = "SELECT " + "SQRT((SUM(a*a) - SUM(a)*SUM(a)/COUNT(a))/COUNT(a)) "+ "from (select _1 as a from MyTable)" tEnv.sql(sqlQuery).toDataSet[Row].collect().foreach(x=&gt;print(s"$x ")) }got exception:org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.api.table.runtime.FunctionCompiler$class.compile(FunctionCompiler.scala:37) at org.apache.flink.api.table.runtime.FlatMapRunner.compile(FlatMapRunner.scala:28) at org.apache.flink.api.table.runtime.FlatMapRunner.open(FlatMapRunner.scala:42) at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:38) at org.apache.flink.api.common.operators.base.FlatMapOperatorBase.executeOnCollections(FlatMapOperatorBase.java:62) at org.apache.flink.api.common.operators.CollectionExecutor.executeUnaryOperator(CollectionExecutor.java:249) at org.apache.flink.api.common.operators.CollectionExecutor.execute(CollectionExecutor.java:147) at org.apache.flink.api.common.operators.CollectionExecutor.execute(CollectionExecutor.java:129) at org.apache.flink.api.common.operators.CollectionExecutor.executeDataSink(CollectionExecutor.java:180) at org.apache.flink.api.common.operators.CollectionExecutor.execute(CollectionExecutor.java:156) at org.apache.flink.api.common.operators.CollectionExecutor.execute(CollectionExecutor.java:129) at org.apache.flink.api.common.operators.CollectionExecutor.execute(CollectionExecutor.java:113) at org.apache.flink.api.java.CollectionEnvironment.execute(CollectionEnvironment.java:35) at org.apache.flink.test.util.CollectionTestEnvironment.execute(CollectionTestEnvironment.java:47) at org.apache.flink.test.util.CollectionTestEnvironment.execute(CollectionTestEnvironment.java:42) at org.apache.flink.api.scala.ExecutionEnvironment.execute(ExecutionEnvironment.scala:637) at org.apache.flink.api.scala.DataSet.collect(DataSet.scala:547) at org.apache.flink.api.scala.batch.sql.AggregationsITCase.test(AggregationsITCase.scala:307) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229) at org.junit.runners.ParentRunner.run(ParentRunner.java:309) at org.junit.runners.Suite.runChild(Suite.java:127) at org.junit.runners.Suite.runChild(Suite.java:26) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner.run(ParentRunner.java:309) at org.junit.runner.JUnitCore.run(JUnitCore.java:160) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69) at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:234) at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:74) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)Caused by: org.codehaus.commons.compiler.CompileException: Line 118, Column 57: No applicable constructor/method found for actual parameters "float, java.math.BigDecimal"; candidates are: "public static double org.apache.calcite.runtime.SqlFunctions.power(double, double)", "public static double org.apache.calcite.runtime.SqlFunctions.power(long, java.math.BigDecimal)", "public static double org.apache.calcite.runtime.SqlFunctions.power(long, long)" at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10062) at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7476) at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7346) at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7250) at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3862) at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:182) at org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3263) at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3971) at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290) at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4356) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2669) at org.codehaus.janino.UnitCompiler.access$4500(UnitCompiler.java:182) at org.codehaus.janino.UnitCompiler$7.visitAssignment(UnitCompiler.java:2619) at org.codehaus.janino.Java$Assignment.accept(Java.java:3402) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2654) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1635) at org.codehaus.janino.UnitCompiler.access$1100(UnitCompiler.java:182) at org.codehaus.janino.UnitCompiler$4.visitExpressionStatement(UnitCompiler.java:936) at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2094) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:999) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:985) at org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:182) at org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:935) at org.codehaus.janino.Java$Block.accept(Java.java:2009) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1737) at org.codehaus.janino.UnitCompiler.access$1200(UnitCompiler.java:182) at org.codehaus.janino.UnitCompiler$4.visitIfStatement(UnitCompiler.java:937) at org.codehaus.janino.Java$IfStatement.accept(Java.java:2154) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:999) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2284) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:504) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:390) at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:182) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:344) at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1136) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:351) at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:319) at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:358) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:201) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:77) at org.apache.flink.api.table.runtime.FunctionCompiler$class.compile(FunctionCompiler.scala:34) ... 55 more</description>
      <version>1.1.1</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.InputTypeSpec.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.BuiltInMethods.scala</file>
    </fixedFiles>
  </bug>
  <bug id="4745" opendate="2016-10-5 00:00:00" fixdate="2016-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Convert KafkaTableSource test to unit tests</summary>
      <description>Kafka tests are extremely heavy and Table Sources and Sinks are only thin wrappers on top of the Kafka Sources / Sinks. That should not need to bring up Kafka clusters.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSinkTestBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSinkTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09ITCase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSinkTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08ITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="4881" opendate="2016-10-21 00:00:00" fixdate="2016-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Docker: Remove dependency on shared volumes</summary>
      <description>Our Dockerfile assumes a shared volume configuration to access the config. Instead, we can configure the Docker container to directly write the hostname into /etc/hosts and use "jobmanager" as the default hostname.This has been proposed here: https://github.com/apache/flink/pull/2667</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.docker-flink.README.md</file>
      <file type="M">flink-contrib.docker-flink.docker-entrypoint.sh</file>
      <file type="M">flink-contrib.docker-flink.docker-compose.yml</file>
    </fixedFiles>
  </bug>
  <bug id="4913" opendate="2016-10-25 00:00:00" fixdate="2016-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Per-job Yarn clusters: include user jar in system class loader</summary>
      <description>Including the jar directly in the system classloader avoids loading it for every instantiation of the ExecutionGraph and every Task execution. Note, this is only possible for per-job clusters (i.e. Yarn/Mesos).</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterClient.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.CliFrontendYarnAddressConfigurationTest.java</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.StandaloneClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgram.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.DefaultCLI.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CustomCommandLine.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug id="4918" opendate="2016-10-25 00:00:00" fixdate="2016-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add SSL support to Mesos artifact server</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.SecurityContext.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosArtifactServer.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerRunner.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="4921" opendate="2016-10-26 00:00:00" fixdate="2016-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Mesos 1.0.1</summary>
      <description>Upgrade the client library to 1.0.1.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4924" opendate="2016-10-26 00:00:00" fixdate="2016-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Simplify Operator Test Harness Constructors</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.WindowingTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.TwoInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.KeyedTwoInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.KeyedOneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamMockEnvironment.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowingTestHarnessTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AggregatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.TimestampsAndPeriodicWatermarksOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamOperatorSnapshotRestoreTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSinkTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.ContinuousFileMonitoringTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="5109" opendate="2016-11-21 00:00:00" fixdate="2016-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalid Content-Encoding Header in REST API responses</summary>
      <description>On REST API calls the Flink runtime responds with the header Content-Encoding, containing the value "utf-8". According to the HTTP/1.1 standard this header is invalid. ( https://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.5 ) Possible acceptable values are: gzip, compress, deflate. Or it should be omitted.The invalid header may cause malfunction in projects building against Flink.The invalid header may be present in earlier versions aswell.Proposed solution: Remove lines from the project, where CONTENT_ENCODING header is set to "utf-8". (I could do this in a PR.)Possible solution but may need further knowledge and skills than mine: Introduce content-encoding. Doing so may need some configuration beacuse then Flink would have to encode the responses properly (even paying attention to the request's Accept-Encoding headers).</description>
      <version>1.1.0,1.1.1,1.1.2,1.1.3,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.RuntimeMonitorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.PipelineErrorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.HttpRequestHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.HandlerRedirectUtils.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.ConstantTextHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="5143" opendate="2016-11-23 00:00:00" fixdate="2016-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add EXISTS to list of supported operators</summary>
      <description>EXISTS is supported in certain cases. We should add it so that e.g. TPC-H query 4 runs properly.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="5144" opendate="2016-11-23 00:00:00" fixdate="2016-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error while applying rule AggregateJoinTransposeRule</summary>
      <description>AggregateJoinTransposeRule seems to cause errors. We have to investigate if this is a Flink or Calcite error. Here a simplified example:select sum(l_extendedprice)from lineitem, partwhere p_partkey = l_partkey and l_quantity &lt; ( select avg(l_quantity) from lineitem where l_partkey = p_partkey )Exception:Exception in thread "main" java.lang.AssertionError: Internal error: Error occurred while applying rule AggregateJoinTransposeRule at org.apache.calcite.util.Util.newInternal(Util.java:792) at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:148) at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:225) at org.apache.calcite.rel.rules.AggregateJoinTransposeRule.onMatch(AggregateJoinTransposeRule.java:342) at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:213) at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:819) at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:334) at org.apache.flink.api.table.BatchTableEnvironment.optimize(BatchTableEnvironment.scala:251) at org.apache.flink.api.table.BatchTableEnvironment.translate(BatchTableEnvironment.scala:286) at org.apache.flink.api.scala.table.BatchTableEnvironment.toDataSet(BatchTableEnvironment.scala:139) at org.apache.flink.api.scala.table.package$.table2RowDataSet(package.scala:77) at org.apache.flink.api.scala.sql.tpch.TPCHQueries$.runQ17(TPCHQueries.scala:826) at org.apache.flink.api.scala.sql.tpch.TPCHQueries$.main(TPCHQueries.scala:57) at org.apache.flink.api.scala.sql.tpch.TPCHQueries.main(TPCHQueries.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)Caused by: java.lang.AssertionError: Type mismatch:rowtype of new rel:RecordType(BIGINT l_partkey, BIGINT p_partkey) NOT NULLrowtype of set:RecordType(BIGINT p_partkey) NOT NULL at org.apache.calcite.util.Litmus$1.fail(Litmus.java:31) at org.apache.calcite.plan.RelOptUtil.equal(RelOptUtil.java:1838) at org.apache.calcite.plan.volcano.RelSubset.add(RelSubset.java:273) at org.apache.calcite.plan.volcano.RelSet.add(RelSet.java:148) at org.apache.calcite.plan.volcano.VolcanoPlanner.addRelToSet(VolcanoPlanner.java:1820) at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1766) at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:1032) at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:1052) at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:1942) at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:136) ... 17 more</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5247" opendate="2016-12-3 00:00:00" fixdate="2016-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix incorrect check in allowedLateness() method. Make it a no-op for non-event time windows.</summary>
      <description>Related to FLINK-3714 and FLINK-4239</description>
      <version>1.1.0,1.1.1,1.1.2,1.1.3</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.WindowedStream.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.AllWindowedStream.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.WindowedStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.AllWindowedStream.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
