<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="4417" opendate="2016-8-17 00:00:00" fixdate="2016-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkpoints should be subsumed by CheckpointID not, by timestamp</summary>
      <description>Since the system clocks cannot be expected to be stable/monotonous, the subsumption logic in the checkpoint coordinator should not decide which checkpoint is "newer" based on the system time.The checkpoint ID is guaranteed to be strictly monotonously increasing. It is a better measure to decide which checkpoint is newer.</description>
      <version>1.1.2</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4546" opendate="2016-9-1 00:00:00" fixdate="2016-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove STREAM keyword in Stream SQL</summary>
      <description>It is about to unify Batch SQL and Stream SQL grammar, esp. removing STREAM keyword in Stream SQL. detailed discuss mailing list: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Some-thoughts-about-unify-Stream-SQL-and-Batch-SQL-grammer-td13060.html</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.stream.sql.SqlITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.schema.TransStreamTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.schema.StreamableTableSourceTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.schema.DataStreamTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.datastream.StreamTableSourceScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.datastream.RemoveDeltaRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.datastream.StreamTableSourceScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.StreamTableEnvironment.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="4549" opendate="2016-9-1 00:00:00" fixdate="2016-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test and document implicitly supported SQL functions</summary>
      <description>Calcite supports many SQL functions by translating them into RexNode s. However, SQL functions like NULLIF, OVERLAPS are neither tested nor document although supported.These functions should be tested and added to the documentation. We could adopt parts from the Calcite documentation.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.expression.TemporalTypesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="4554" opendate="2016-9-1 00:00:00" fixdate="2016-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for array types</summary>
      <description>Support creating arrays:ARRAY[1, 2, 3]Access array values:myArray[3]And operations like:UNNEST, UNNEST WITH ORDINALITY, CARDINALITY</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.TypeCheckUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.ProjectionTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.FlinkTypeFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.comparison.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.ScalarOperators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="4608" opendate="2016-9-10 00:00:00" fixdate="2016-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use short-circuit AND in Max/Min AggregationFunction</summary>
      <description>Max/Min AggregationFunction use &amp; instead of &amp;&amp;. Usually we use short-circuit logic in if operators in java</description>
      <version>1.1.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.aggregation.MinAggregationFunction.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.aggregation.MaxAggregationFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="4611" opendate="2016-9-11 00:00:00" fixdate="2016-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make "AUTO" credential provider as default for Kinesis Connector</summary>
      <description>Right now, the Kinesis Consumer / Producer by default directly expects the access key id and secret access key to be given in the config properties.This isn't a good practice for accessing AWS services, and usually Kinesis users would most likely be running their Flink application in AWS instances that have embedded credentials that can be access via the default credential provider chain. Therefore, it makes sense to change the default AWS_CREDENTIALS_PROVIDER to AUTO instead of BASIC.To avoid breaking user code, we only use directly supplied AWS credentials if both access key and secret key is given through AWS_ACCESS_KEY and AWS_SECRET_KEY. Otherwise, the default credential provider chain is used.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="4612" opendate="2016-9-12 00:00:00" fixdate="2016-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Close FileWriter using try with resources</summary>
      <description>FileWriter is not closed properly in many places in the project modules</description>
      <version>1.1.2</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.aggregators.AggregatorsITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.clients.examples.LocalExecutorITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.WriteFormatAsText.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.WriteFormatAsCsv.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.DataSourceTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.FileChannelStreamsTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.PrimitiveInputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CsvInputFormatTest.java</file>
      <file type="M">flink-examples.flink-examples-batch.src.main.java.org.apache.flink.examples.java.relational.util.WebLogDataGenerator.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.testutils.TestFileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="4618" opendate="2016-9-14 00:00:00" fixdate="2016-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FlinkKafkaConsumer09 should start from the next record on startup from offsets in Kafka</summary>
      <description>*Original reported ticket title: Last kafka message gets consumed twice when restarting job*There seem to be an issue with the offset management in Flink. When a job is stopped and startet again, a message from the previous offset is read again.I enabled checkpoints (EXACTLY_ONCE) and FsStateBackend. I started with a new consumer group and emitted one record.You can cleary see, that the consumer waits for a new record at offset 4848911, which is correct. After restarting, it consumes a record at 4848910, causing the record to be consumed more than once.I checked the offset with the Kafka CMD tools, the commited offset in zookeeper is 4848910.Here is my log output:10:29:24,225 DEBUG org.apache.kafka.clients.NetworkClient - Initiating connection to node 2147482646 at hdp1:6667.10:29:24,225 DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - Fetching committed offsets for partitions: [myTopic-0]10:29:24,228 DEBUG org.apache.kafka.clients.NetworkClient - Completed connection to node 214748264610:29:24,234 DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - No committed offset for partition myTopic-010:29:24,238 DEBUG org.apache.kafka.clients.consumer.internals.Fetcher - Resetting offset for partition myTopic-0 to latest offset.10:29:24,244 DEBUG org.apache.kafka.clients.consumer.internals.Fetcher - Fetched offset 4848910 for partition myTopic-010:29:24,245 TRACE org.apache.kafka.clients.consumer.internals.Fetcher - Added fetch request for partition myTopic-0 at offset 484891010:29:24,773 TRACE org.apache.kafka.clients.consumer.internals.Fetcher - Added fetch request for partition myTopic-0 at offset 484891010:29:25,276 TRACE org.apache.kafka.clients.consumer.internals.Fetcher - Added fetch request for partition myTopic-0 at offset 4848910-- Inserting a new event here10:30:22,447 TRACE org.apache.kafka.clients.consumer.internals.Fetcher - Adding fetched record for partition myTopic-0 with offset 4848910 to buffered record list10:30:22,448 TRACE org.apache.kafka.clients.consumer.internals.Fetcher - Returning fetched records at offset 4848910 for assigned partition myTopic-0 and update position to 484891110:30:22,451 TRACE org.apache.kafka.clients.consumer.internals.Fetcher - Added fetch request for partition myTopic-0 at offset 484891110:30:22,953 TRACE org.apache.kafka.clients.consumer.internals.Fetcher - Added fetch request for partition myTopic-0 at offset 484891110:30:23,456 TRACE org.apache.kafka.clients.consumer.internals.Fetcher - Added fetch request for partition myTopic-0 at offset 484891110:30:23,887 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Triggering checkpoint 6 @ 147384182388710:30:23,957 TRACE org.apache.kafka.clients.consumer.internals.Fetcher - Added fetch request for partition myTopic-0 at offset 484891110:30:23,996 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Completed checkpoint 6 (in 96 ms)10:30:24,196 TRACE org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - Sending offset-commit request with {myTopic-0=OffsetAndMetadata{offset=4848910, metadata=''}} to Node(2147482646, hdp1, 6667)10:30:24,204 DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - Committed offset 4848910 for partition myTopic-010:30:24,460 TRACE org.apache.kafka.clients.consumer.internals.Fetcher - Added fetch request for partition myTopic-0 at offset 484891110:30:24,963 TRACE org.apache.kafka.clients.consumer.internals.Fetcher - Added fetch request for partition myTopic-0 at offset 484891110:30:48,057 INFO org.apache.flink.runtime.blob.BlobServer - Stopped BLOB server at 0.0.0.0:2946-- Restarting job10:32:01,672 DEBUG org.apache.kafka.clients.NetworkClient - Initiating connection to node 2147482646 at hdp1:6667.10:32:01,673 DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - Fetching committed offsets for partitions: [myTopic-0]10:32:01,677 DEBUG org.apache.kafka.clients.NetworkClient - Completed connection to node 2147482646// See below! Shouldn't the offset be 4848911?10:32:01,682 DEBUG org.apache.kafka.clients.consumer.internals.Fetcher - Resetting offset for partition myTopic-0 to the committed offset 484891010:32:01,683 TRACE org.apache.kafka.clients.consumer.internals.Fetcher - Added fetch request for partition myTopic-0 at offset 484891010:32:01,685 DEBUG org.apache.kafka.clients.NetworkClient - Initiating connection to node 1001 at hdp1:6667.10:32:01,687 DEBUG org.apache.kafka.clients.NetworkClient - Completed connection to node 1001// Here record 4848910 gets consumed again!10:32:01,707 TRACE org.apache.kafka.clients.consumer.internals.Fetcher - Adding fetched record for partition myTopic-0 with offset 4848910 to buffered record list10:32:01,708 TRACE org.apache.kafka.clients.consumer.internals.Fetcher - Returning fetched records at offset 4848910 for assigned partition myTopic-0 and update position to 484891110:32:03,721 TRACE org.apache.kafka.clients.consumer.internals.Fetcher - Added fetch request for partition myTopic-0 at offset 484891110:32:04,224 TRACE org.apache.kafka.clients.consumer.internals.Fetcher - Added fetch request for partition myTopic-0 at offset 484891110:32:04,726 TRACE org.apache.kafka.clients.consumer.internals.Fetcher - Added fetch request for partition myTopic-0 at offset 484891110:32:04,894 INFO org.apache.flink.runtime.blob.BlobCache - Shutting down BlobCache10:32:04,903 INFO org.apache.flink.runtime.blob.BlobServer - Stopped BLOB server at 0.0.0.0:3079</description>
      <version>1.1.2</version>
      <fixedVersion>1.1.3,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09FetcherTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="4622" opendate="2016-9-15 00:00:00" fixdate="2016-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CLI help message should include &amp;#39;savepoint&amp;#39; action</summary>
      <description>The Flink CLI help message should include the 'savepoint' action in the list of available actions. It currently looks like:bash-4.3# flink foo"foo" is not a valid action.Valid actions are "run", "list", "info", "stop", or "cancel".Specify the version option (-v or --version) to print Flink version.Specify the help option (-h or --help) to get help on the command.</description>
      <version>1.1.2</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug id="4625" opendate="2016-9-15 00:00:00" fixdate="2016-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Guard Flink processes against blocking shutdown hooks</summary>
      <description>Resource managers like YARN send the JVM the SIGTERM signal to kill the process, if it wants to terminate a process.With SIGTERM, the JVM shutdown hooks run, and may cause the process to freeze up on shutdown. Especially since all dependencies (like Hadoop) may install shutdown hooks (and do so), it is not in Flink's control to make sure all Shutdown hooks are well behaved.I propose to add a guard that forcibly terminates the JVM if clean shutdown does not succeed within a certain time (say five seconds).</description>
      <version>1.1.2</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnTaskManagerRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnApplicationMasterRunner.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.TestingApplicationMaster.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.CommonTestUtils.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.TestJvmProcess.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.CommonTestUtils.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerRunner.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="4639" opendate="2016-9-20 00:00:00" fixdate="2016-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Calcite features more pluggable</summary>
      <description>Some users might want to extend the feature set of the Table API by adding or replacing Calcite optimizer rules, modifying the parser etc. It would be good to have means to hook into the Table API and change Calcite behavior. We should implement something like a CalciteConfigBuilder.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.TableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.TableEnvironmentITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableConfig.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.BatchTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug id="4645" opendate="2016-9-20 00:00:00" fixdate="2016-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hard to register Kryo Serializers due to generics</summary>
      <description>It currently does not work to do this:env.registerTypeWithKryoSerializer(TreeMultimap.class, JavaSerializer.class);instead on needs to do that:env.registerTypeWithKryoSerializer(TreeMultimap.class, (Class&lt;? extends Serializer&lt;?&gt;&gt;) JavaSerializer.class);The fix would be to change the signature of the environment method frompublic void registerTypeWithKryoSerializer(Class&lt;?&gt; type, Class&lt;? extends Serializer&lt;?&gt;&gt; serializerClass)topublic void registerTypeWithKryoSerializer(Class&lt;?&gt; type, Class&lt;? extends Serializer&gt; serializerClass)</description>
      <version>1.1.2</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="4646" opendate="2016-9-21 00:00:00" fixdate="2016-12-21 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add BipartiteGraph class</summary>
      <description>Implement a class to represent a bipartite graph in Flink Gelly. Design discussions can be found in the parent task.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.generator.TestUtils.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.Edge.java</file>
    </fixedFiles>
  </bug>
  <bug id="4685" opendate="2016-9-26 00:00:00" fixdate="2016-9-26 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Gather operator checkpoint durations data sizes from the runtime</summary>
      <description></description>
      <version>1.1.2</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamMockEnvironment.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierTrackerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierBufferTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamInputProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BufferSpiller.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BarrierTracker.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.BarrierBuffer.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskAsyncCallTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DummyEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerHARecoveryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.RuntimeEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.CheckpointResponder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayCheckpointResponder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.checkpoint.AcknowledgeCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.StatefulTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.Environment.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBAsyncSnapshotTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="4702" opendate="2016-9-27 00:00:00" fixdate="2016-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka consumer must commit offsets asynchronously</summary>
      <description>The offset commit calls to Kafka may occasionally take very long.In that case, the notifyCheckpointComplete() method blocks for long and the KafkaConsumer cannot make progress and cannot perform checkpoints.Kafka 0.9+ have methods to commit asynchronously.We should use those and make sure no more than one commit is concurrently in progress, to that commit requests do not pile up.</description>
      <version>1.1.2</version>
      <fixedVersion>1.1.3,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="4710" opendate="2016-9-29 00:00:00" fixdate="2016-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove transitive Guice dependency from Hadoop</summary>
      <description>This transitive dependency is not relevant for the parts of the Hadoop code invoked by Flink (Yarn client, Hdfs). Removing it clears dependency conflicts for users.</description>
      <version>1.1.2</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4727" opendate="2016-10-3 00:00:00" fixdate="2016-11-3 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Kafka 0.9 Consumer should also checkpoint auto retrieved offsets even when no data is read</summary>
      <description>This is basically the 0.9 version counterpart for FLINK-3440.When the 0.9 consumer fetches initial offsets from Kafka on startup, but does not have any data to read, it should also checkpoint &amp; commit these initial offsets.</description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09ITCase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010ITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="4739" opendate="2016-10-4 00:00:00" fixdate="2016-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding packaging details for the Elasticsearch connector</summary>
      <description>When an uber-jar containing an Elasticsearch sink is executed, an IllegalArgumentException may occur, which is caused by conflicting files of Elasticsearch and it's dependencies in META-INF/services.As agreed in http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/NoClassDefFoundError-with-ElasticsearchSink-on-Yarn-tt8822.html#none, the documentation should point out how to build a sound uber-jar containing an Elasticsearch sink.</description>
      <version>1.1.2</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.elasticsearch2.md</file>
    </fixedFiles>
  </bug>
  <bug id="4764" opendate="2016-10-7 00:00:00" fixdate="2016-10-7 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce config options</summary>
      <description>It is a bit unorthodox to start a discussion via a pull request, but this suggestion is best motivated via some code.I suggest to move away from the current model with `ConfigConstants` and move to a model where an `Option` object describes a configuration option completely, with default value, fallback keys.Advantages Much simpler / easier access to values that have deprecated keys Not possible to accidentally overlook deprecated keys Key and default values are grouped together in the definition Clearly states the expected type value for each config key (string, int, etc). We can improve this even further to include the description and auto-generate the config docsExampleSimple option:Option&lt;String&gt; TASK_MANAGER_TMP_DIRS = new Option&lt;&gt;( "taskmanager.tmp.dirs", // config key System.getProperty("java.io.tmpdir")); // default valueOption with multiple deprecated keys:Option&lt;String&gt; HA_CLUSTER_ID = new Option&lt;&gt;( "high-availability.cluster-id", // config key null, // no default value "high-availability.zookeeper.path.namespace", // latest deprecated key "recovery.zookeeper.path.namespace"); // even earlier deprecated keyGet a config value, this automatically checks deprecated keys and default values:final String zkQuorum = configuration.getValue(ConfigOptions.HA_ZOOKEEPER_QUORUM);final long connTimeout = configuration.getInteger(ConfigOptions.HA_ZOOKEEPER_CONN_TIMEOUT);Multiple Options classesTo avoid having one huge class (like `ConfigConstants`), we can easily split this up into `TaskManagerOptions`, `JobManagerOptions`, `ZooKeeperOptions`, etc.</description>
      <version>1.1.2</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.UnmodifiableConfigurationTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.DelegatingConfigurationTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.ConfigurationTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.DelegatingConfiguration.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.Configuration.java</file>
    </fixedFiles>
  </bug>
  <bug id="4768" opendate="2016-10-7 00:00:00" fixdate="2016-10-7 01:00:00" resolution="Done">
    <buginformation>
      <summary>Migrate High Availability configuration options</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnApplicationMasterRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNHighAvailabilityITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.CliFrontendYarnAddressConfigurationTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAProcessFailureBatchRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ChaosMonkeyITCase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.SecureTestEnvironment.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.RollingSinkSecuredITCase.java</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingUtils.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperTestEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ZooKeeperUtilTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.ZooKeeperTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderRetrievalTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerHARecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.HighAvailabilityModeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.FlinkZooKeeperQuorumPeer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.SecurityContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.HighAvailabilityMode.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.FileSystemBlobStore.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitorITCase.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.DefaultCLI.java</file>
    </fixedFiles>
  </bug>
  <bug id="4794" opendate="2016-10-11 00:00:00" fixdate="2016-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>partition_by_hash() crashes if no parameter is provided</summary>
      <description>partition_by_hash() crashes if no parameter is provided.Looks like a line of code was missed, check distinct()def distinct(self, *fields): f = None if len(fields) == 0: f = lambda x: (x,) fields = (0,)</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.DataSet.py</file>
    </fixedFiles>
  </bug>
  <bug id="4795" opendate="2016-10-11 00:00:00" fixdate="2016-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CsvStringify crashes in case of tuple in tuple, t.e. ("a", True, (1,5))</summary>
      <description>CsvStringify crashes in case of tuple in tuple, t.e. ("a", True, (1,5))Looks like, mistyping in CsvStringify._map()def _map(self, value): if isinstance(value, (tuple, list)): return "(" + b", ".join([self.map(x) for x in value]) + ")" else: return str(value) self._map() should be calledBut this will affect write_csv() and read_csv().write_csv() will work automaticallyand read_csv() should be implemented to be able to read Tuple type.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.main.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.DataSet.py</file>
    </fixedFiles>
  </bug>
  <bug id="4809" opendate="2016-10-12 00:00:00" fixdate="2016-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Operators should tolerate checkpoint failures</summary>
      <description>Operators should try/catch exceptions in the synchronous and asynchronous part of the checkpoint and send a DeclineCheckpoint message as a result.The decline message should have the failure cause attached to it.The checkpoint barrier should be sent anyways as a first step before attempting to make a state checkpoint, to make sure that downstream operators do not block in alignment.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.BlockingCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.CheckpointConfig.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DummyEnvironment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
      <file type="M">docs.dev.stream.state.checkpointing.md</file>
    </fixedFiles>
  </bug>
  <bug id="4827" opendate="2016-10-14 00:00:00" fixdate="2016-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The scala example of SQL on Streaming Tables with wrong variable name in flink document</summary>
      <description>val env = StreamExecutionEnvironment.getExecutionEnvironmentval tEnv = TableEnvironment.getTableEnvironment(env)// read a DataStream from an external sourceval ds: DataStream&amp;#91;(Long, String, Integer)&amp;#93; = env.addSource(...)// register the DataStream under the name "Orders"tableEnv.registerDataStream("Orders", ds, 'user, 'product, 'amount)// run a SQL query on the Table and retrieve the result as a new Tableval result = tableEnv.sql( "SELECT product, amount FROM Orders WHERE product LIKE '%Rubber%'")There is no variable named tableEnv had defined ,Only tEnv defined here</description>
      <version>1.1.0,1.1.2</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="4832" opendate="2016-10-14 00:00:00" fixdate="2016-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Count/Sum 0 elements</summary>
      <description>Currently, the Table API is unable to count or sum up 0 elements. We should improve DataSet aggregations for this. Maybe by union the original DataSet with a dummy record or by using a MapPartition function. Coming up with a good design for this is also part of this issue.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.DataSetAggregateRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="4833" opendate="2016-10-15 00:00:00" fixdate="2016-10-15 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Unstable test OperatorStatsAccumulatorTest.testAccumulatorHeavyHitterCountMinSketch</summary>
      <description>Some instances:view-source:https://s3.amazonaws.com/archive.travis-ci.org/jobs/167801187/log.txtview-source:https://s3.amazonaws.com/archive.travis-ci.org/jobs/167801191/log.txtview-source:https://s3.amazonaws.com/archive.travis-ci.org/jobs/167801193/log.txtview-source:https://s3.amazonaws.com/archive.travis-ci.org/jobs/167801195/log.txt</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-operator-stats.src.main.java.org.apache.flink.contrib.operatorstatistics.heavyhitters.HeavyHitterMergeException.java</file>
      <file type="M">flink-contrib.flink-operator-stats.src.main.java.org.apache.flink.contrib.operatorstatistics.heavyhitters.CountMinHeavyHitter.java</file>
    </fixedFiles>
  </bug>
  <bug id="4839" opendate="2016-10-17 00:00:00" fixdate="2016-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JobManager handle TaskManager&amp;#39;s slot offering</summary>
      <description>JobManager receives the TaskManager's slot offers, and decide which slots to accept.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.slot.TaskSlotTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.slot.TaskSlot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.SlotPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="4842" opendate="2016-10-17 00:00:00" fixdate="2016-10-17 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce test to enforce order of operator / udf lifecycles</summary>
      <description>We should introduce a test that enforces a certain order in which life cycle methods of operators and udfs are called, so that they are not easily changed by accident.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperatorTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="4843" opendate="2016-10-17 00:00:00" fixdate="2016-10-17 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce Test for FsCheckpointStateOutputStream::getPos</summary>
      <description>Introduce a test for FsCheckpointStateOutputStream::getPos, which is currently not included in the tests.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStateOutputStreamTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="4844" opendate="2016-10-17 00:00:00" fixdate="2016-10-17 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Partitionable Raw Keyed/Operator State</summary>
      <description>Partitionable operator and keyed state are currently only available by using backends. However, the serialization code for many operators is build around reading/writing their state to a stream for checkpointing. We want to provide partitionable states also through streams, so that migrating existing operators becomes more easy.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerHARecoveryTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.StateBackendITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RescalingITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.WindowingTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.TwoInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.KeyedOneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamMockEnvironment.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.InterruptSensitiveRestoreTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.WriteAheadSinkTestBase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AggregatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.GenericWriteAheadSinkTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierTrackerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.BarrierBufferTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContextTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.UserFacingListState.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContext.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.checkpoint.CheckpointedFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.checkpoint.Checkpointed.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.AtLeastOnceProducerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSinkTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskAsyncCallTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.KeyGroupRangeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.KeyGroupRangeOffsetTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DummyEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.messages.CheckpointMessagesTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBAsyncSnapshotTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.OperatorStateStore.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.local.LocalDataInputStream.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointMetaData.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.RoundRobinOperatorStateRepartitioner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointV1Serializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.SubtaskState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.TaskState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.Environment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.fs.hdfs.HadoopDataInputStream.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.StatefulTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.checkpoint.AcknowledgeCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.query.KvStateMessage.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ChainedStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.CheckpointStateHandles.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ClosableRegistry.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.DefaultOperatorStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyGroupRange.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyGroupRangeOffsets.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.MemCheckpointStreamFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.MemoryStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.OperatorStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.OperatorStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.PartitionableCheckpointStateOutputStream.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.SnapshotProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.ActorGatewayCheckpointResponder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.CheckpointResponder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.RuntimeEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.IntArrayList.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.LongArrayList.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointV1SerializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointV1Test.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.stats.SimpleCheckpointStatsTrackerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="4872" opendate="2016-10-20 00:00:00" fixdate="2016-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Type erasure problem exclusively on cluster execution</summary>
      <description>The following codes runs fine on local and collection execution environment but fails when executed on a cluster.Problem.javaimport org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.api.java.tuple.Tuple1;import java.lang.reflect.Array;public class Problem { public static class Pojo { } public static class Foo&lt;T&gt; extends Tuple1&lt;T&gt; { } public static class Bar&lt;T&gt; extends Tuple1&lt;T[]&gt; { } public static class UDF&lt;T&gt; implements MapFunction&lt;Foo&lt;T&gt;, Bar&lt;T&gt;&gt; { private final Class&lt;T&gt; clazz; public UDF(Class&lt;T&gt; clazz) { this.clazz = clazz; } @Override public Bar&lt;T&gt; map(Foo&lt;T&gt; value) throws Exception { Bar&lt;T&gt; bar = new Bar&lt;&gt;(); //noinspection unchecked bar.f0 = (T[]) Array.newInstance(clazz, 10); return bar; } } public static void main(String[] args) throws Exception { // runs in local, collection and cluster execution withLong(); // runs in local and collection execution, fails on cluster execution withPojo(); } public static void withLong() throws Exception { ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); Foo&lt;Long&gt; foo = new Foo&lt;&gt;(); foo.f0 = 42L; DataSet&lt;Foo&lt;Long&gt;&gt; barDataSource = env.fromElements(foo); DataSet&lt;Bar&lt;Long&gt;&gt; map = barDataSource.map(new UDF&lt;&gt;(Long.class)); map.print(); } public static void withPojo() throws Exception { ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); Foo&lt;Pojo&gt; foo = new Foo&lt;&gt;(); foo.f0 = new Pojo(); DataSet&lt;Foo&lt;Pojo&gt;&gt; barDataSource = env.fromElements(foo); DataSet&lt;Bar&lt;Pojo&gt;&gt; map = barDataSource.map(new UDF&lt;&gt;(Pojo.class)); map.print(); }}ProblemTest.javaimport org.apache.flink.test.util.MultipleProgramsTestBase;import org.junit.Test;import org.junit.runner.RunWith;import org.junit.runners.Parameterized;@RunWith(Parameterized.class)public class ProblemTest extends MultipleProgramsTestBase { public ProblemTest(TestExecutionMode mode) { super(mode); } @Test public void testWithLong() throws Exception { Problem.withLong(); } @Test public void testWithPOJO() throws Exception { Problem.withPojo(); }}Exception:The return type of function 'withPojo(Problem.java:58)' could not be determined automatically, due to type erasure. You can give type information hints by using the returns(...) method on the result of the transformation call, or by letting your function implement the 'ResultTypeQueryable' interface. org.apache.flink.api.java.DataSet.getType(DataSet.java:178) org.apache.flink.api.java.DataSet.collect(DataSet.java:407) org.apache.flink.api.java.DataSet.print(DataSet.java:1605) Problem.withPojo(Problem.java:60) Problem.main(Problem.java:38)</description>
      <version>1.1.2</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
    </fixedFiles>
  </bug>
  <bug id="4875" opendate="2016-10-21 00:00:00" fixdate="2016-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>operator name not correctly inferred</summary>
      <description></description>
      <version>1.1.2</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="4876" opendate="2016-10-21 00:00:00" fixdate="2016-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow web interface to be bound to a specific ip/interface/inetHost</summary>
      <description>Currently the web interface automatically binds to all interfaces on 0.0.0.0. IMHO there are some use cases to only bind to a specific ipadress, (e.g. access through an authenticated proxy, not binding on the management or backup interface)</description>
      <version>1.1.2,1.1.3,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorConfig.java</file>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="4877" opendate="2016-10-21 00:00:00" fixdate="2016-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactorings around FLINK-3674 (User Function Timers)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.TwoInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.streamrecord.StreamRecord.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockContext.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.DefaultTimeServiceProviderTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.TestTimeProviderTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TimeServiceProvider.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TestTimeServiceProvider.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.DefaultTimeServiceProvider.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.EvictingWindowOperator.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer010.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010FetcherTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.Kafka08Fetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09FetcherTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.AtLeastOnceProducerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcherTimestampsTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.DataGenerators.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.MockRuntimeContext.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.ContinuousFileMonitoringTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSinkTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.TimestampsAndPeriodicWatermarksOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AggregatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.CollectingOutput.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.NoOpTimerService.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.KeyedOneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.WindowingTestHarness.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.src.main.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.HeapInternalTimerService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSourceContexts.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.ExtractTimestampsOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.TimestampsAndPeriodicWatermarksOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AbstractAlignedProcessingTimeWindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ProcessingTimeCallback.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TestProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamTaskTimerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.TestProcessingTimeServiceTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeServiceTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.StreamTaskTimerITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.Triggerable.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="5109" opendate="2016-11-21 00:00:00" fixdate="2016-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalid Content-Encoding Header in REST API responses</summary>
      <description>On REST API calls the Flink runtime responds with the header Content-Encoding, containing the value "utf-8". According to the HTTP/1.1 standard this header is invalid. ( https://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.5 ) Possible acceptable values are: gzip, compress, deflate. Or it should be omitted.The invalid header may cause malfunction in projects building against Flink.The invalid header may be present in earlier versions aswell.Proposed solution: Remove lines from the project, where CONTENT_ENCODING header is set to "utf-8". (I could do this in a PR.)Possible solution but may need further knowledge and skills than mine: Introduce content-encoding. Doing so may need some configuration beacuse then Flink would have to encode the responses properly (even paying attention to the request's Accept-Encoding headers).</description>
      <version>1.1.0,1.1.1,1.1.2,1.1.3,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.RuntimeMonitorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.PipelineErrorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.HttpRequestHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.HandlerRedirectUtils.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.ConstantTextHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="5143" opendate="2016-11-23 00:00:00" fixdate="2016-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add EXISTS to list of supported operators</summary>
      <description>EXISTS is supported in certain cases. We should add it so that e.g. TPC-H query 4 runs properly.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="5144" opendate="2016-11-23 00:00:00" fixdate="2016-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error while applying rule AggregateJoinTransposeRule</summary>
      <description>AggregateJoinTransposeRule seems to cause errors. We have to investigate if this is a Flink or Calcite error. Here a simplified example:select sum(l_extendedprice)from lineitem, partwhere p_partkey = l_partkey and l_quantity &lt; ( select avg(l_quantity) from lineitem where l_partkey = p_partkey )Exception:Exception in thread "main" java.lang.AssertionError: Internal error: Error occurred while applying rule AggregateJoinTransposeRule at org.apache.calcite.util.Util.newInternal(Util.java:792) at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:148) at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:225) at org.apache.calcite.rel.rules.AggregateJoinTransposeRule.onMatch(AggregateJoinTransposeRule.java:342) at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:213) at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:819) at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:334) at org.apache.flink.api.table.BatchTableEnvironment.optimize(BatchTableEnvironment.scala:251) at org.apache.flink.api.table.BatchTableEnvironment.translate(BatchTableEnvironment.scala:286) at org.apache.flink.api.scala.table.BatchTableEnvironment.toDataSet(BatchTableEnvironment.scala:139) at org.apache.flink.api.scala.table.package$.table2RowDataSet(package.scala:77) at org.apache.flink.api.scala.sql.tpch.TPCHQueries$.runQ17(TPCHQueries.scala:826) at org.apache.flink.api.scala.sql.tpch.TPCHQueries$.main(TPCHQueries.scala:57) at org.apache.flink.api.scala.sql.tpch.TPCHQueries.main(TPCHQueries.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)Caused by: java.lang.AssertionError: Type mismatch:rowtype of new rel:RecordType(BIGINT l_partkey, BIGINT p_partkey) NOT NULLrowtype of set:RecordType(BIGINT p_partkey) NOT NULL at org.apache.calcite.util.Litmus$1.fail(Litmus.java:31) at org.apache.calcite.plan.RelOptUtil.equal(RelOptUtil.java:1838) at org.apache.calcite.plan.volcano.RelSubset.add(RelSubset.java:273) at org.apache.calcite.plan.volcano.RelSet.add(RelSet.java:148) at org.apache.calcite.plan.volcano.VolcanoPlanner.addRelToSet(VolcanoPlanner.java:1820) at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1766) at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:1032) at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:1052) at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:1942) at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:136) ... 17 more</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5247" opendate="2016-12-3 00:00:00" fixdate="2016-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix incorrect check in allowedLateness() method. Make it a no-op for non-event time windows.</summary>
      <description>Related to FLINK-3714 and FLINK-4239</description>
      <version>1.1.0,1.1.1,1.1.2,1.1.3</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.WindowedStream.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.AllWindowedStream.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.WindowedStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.AllWindowedStream.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
