<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="21160" opendate="2021-1-27 00:00:00" fixdate="2021-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ValueDeserializerWrapper throws NullPointerException when getProducedType is invoked</summary>
      <description>The variable deserializer in class ValueDeserializerWrapper won't be instantiated until method deserialize() is invoked in runtime, so in the job compiling stage when invoking getProducedType(), NPE will be thrown because of referencing the uninstantiated variable deserializer.A user reported that the new KafkaSource fails with a NullPointerException:Exception in thread "main" java.lang.NullPointerExceptionat org.apache.flink.connector.kafka.source.reader.deserializer.ValueDeserializerWrapper.getProducedType(ValueDeserializerWrapper.java:79)at org.apache.flink.connector.kafka.source.KafkaSource.getProducedType(KafkaSource.java:171)at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getTypeInfo(StreamExecutionEnvironment.java:2282)at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.fromSource(StreamExecutionEnvironment.java:1744)at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.fromSource(StreamExecutionEnvironment.java:1715)when setting it up like this:val kafkaSource = buildKafkaSource(params)val datastream = env.fromSource(kafkaSource, WatermarkStrategy.noWatermarks(), "kafka")private fun buildKafkaSource(params: ParameterTool): KafkaSource&lt;String&gt; { val builder = KafkaSource.builder&lt;String&gt;() .setBootstrapServers(params.get("bootstrapServers")) .setGroupId(params.get("groupId")) .setStartingOffsets(OffsetsInitializer.earliest()) .setTopics("topic") .setDeserializer(KafkaRecordDeserializer.valueOnly(StringDeserializer::class.java)) if (params.getBoolean("boundedSource", false)) { builder.setBounded(OffsetsInitializer.latest()) } return builder.build()}The problem seems to be that the ValueDeserializerWrapper does not set the deserializer the deserialize method is called, but getProducedType is actually called first resulting in the NullPointerException.https://lists.apache.org/x/thread.html/r8734f9a18c25fd5996fc2edf9889277c185ee9a0b79280938b1cb792@%3Cuser.flink.apache.org%3E</description>
      <version>1.12.3</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.KafkaSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.reader.deserializer.KafkaValueOnlyDeserializerWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="21185" opendate="2021-1-28 00:00:00" fixdate="2021-2-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce a new interface for catalog to listen on temporary object operations</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.Catalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="22373" opendate="2021-4-20 00:00:00" fixdate="2021-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Flink 1.13 release notes</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content..index.md</file>
    </fixedFiles>
  </bug>
  <bug id="2248" opendate="2015-6-19 00:00:00" fixdate="2015-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow disabling of sdtout logging output</summary>
      <description>Currently when a job is submitted through the CLI we get in stdout all the log output about each stage in the job.It would useful to have an easy way to disable this output when submitting the job, as most of the time we are only interested in the log output if something goes wrong.</description>
      <version>None</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.testjar.WordCount.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
      <file type="M">docs.apis.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="22556" opendate="2021-5-4 00:00:00" fixdate="2021-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend license checker to scan for traces of (L)GPL licensed code</summary>
      <description>This is a follow up to FLINK-22555.The goal is to catch this and similar cases automatically.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.java-ci-tools.src.test.java.org.apache.flink.tools.ci.licensecheck.JarFileCheckerTest.java</file>
      <file type="M">tools.ci.java-ci-tools.src.main.java.org.apache.flink.tools.ci.licensecheck.JarFileChecker.java</file>
    </fixedFiles>
  </bug>
  <bug id="22573" opendate="2021-5-5 00:00:00" fixdate="2021-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AsyncIO can timeout elements after completion</summary>
      <description>AsyncIO emits completed elements over the mailbox at which any timer is also canceled. However, if the mailbox cannot process (heavy backpressure), it may be that the timer still triggers on a completed element.</description>
      <version>1.11.3,1.13.0,1.14.0,1.12.3</version>
      <fixedVersion>1.14.0,1.13.1,1.12.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskMailboxTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperatorTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="22606" opendate="2021-5-8 00:00:00" fixdate="2021-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Simplify the usage of SessionState</summary>
      <description>Hive SessionState is used by hive dialect. Starting a SessionState involves some heavy operations like creating scratch folders and instantiating an HMS client. We should investigate how to reduce these operations.It's of course better to completely get rid of SessionState. But that's difficult to achieve because some hive functions rely on it.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserQB.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserContext.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserBaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="22636" opendate="2021-5-11 00:00:00" fixdate="2021-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Group job specific ZooKeeper HA services under common jobs/&lt;JobID&gt; zNode</summary>
      <description>In order to better clean up Zookeeper HA services, I suggest grouping job-specific services under a common jobs/&lt;JobID&gt; zNode. That way, it becomes trivial to clean up the job-specific Zookeeper data (simply deleting the jobs/&lt;JobID&gt; node.Currently, our Zookeeper structure is not really structured well. The current layout looks like this:clusterID -&gt; jobgraphs -&gt; &lt;job-id&gt; -&gt; checkpoints -&gt; &lt;job-id&gt; -&gt; checkpoint-1 -&gt; checkpoint-counter -&gt; &lt;job-id&gt; -&gt; counter -&gt; leaderlatch -&gt; dispatcher_lock -&gt; resourc_emanager_lock -&gt; &lt;job-id&gt; -&gt; leader -&gt; dispatcher_lock -&gt; resource_manager_lock -&gt; &lt;job-id&gt;The new layout could look like this:clusterID -&gt; jobgraphs -&gt; &lt;job-id&gt; -&gt; jobs -&gt; &lt;job-id&gt; -&gt; checkpoints -&gt; checkpoint-1 -&gt; checkpoint_id_counter -&gt; counter -&gt; leader -&gt; latch -&gt; connection_info -&gt; leader -&gt; dispatcher -&gt; latch -&gt; connection_info -&gt; resource_manager -&gt; latch -&gt; connection_info</description>
      <version>1.13.0,1.14.0,1.12.3</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.AbstractHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointIDCounterITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZKCheckpointIDCounterMultiServersTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriverFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperClientHAServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.AbstractHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointRecoveryFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointIDCounter.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServicesTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServices.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HighAvailabilityOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.layouts.shortcodes.generated.high.availability.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.high.availability.zk.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="22759" opendate="2021-5-24 00:00:00" fixdate="2021-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct the applicability of RocksDB related options as per operator</summary>
      <description>Hotfix of https://github.com/apache/flink/commit/f93d350e14ce3e56790672740cba91c06a77b940 tries to clarify RocksDB thread options applicability per operator/TM. However, some descriptions are not correct, and this ticket targets to resolve them.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBConfigurableOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.configurable.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="2276" opendate="2015-6-25 00:00:00" fixdate="2015-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Travis build error</summary>
      <description>testExecutionFailsAfterTaskMarkedFailed on travis. Here is the log output: https://s3.amazonaws.com/archive.travis-ci.org/jobs/68288986/log.txt</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="23233" opendate="2021-7-5 00:00:00" fixdate="2021-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>OperatorEventSendingCheckpointITCase.testOperatorEventLostWithReaderFailure fails on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19857&amp;view=logs&amp;j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&amp;t=c2734c79-73b6-521c-e85a-67c7ecae9107&amp;l=9382Jul 03 01:37:31 [ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 21.415 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.runtime.operators.coordination.OperatorEventSendingCheckpointITCaseJul 03 01:37:31 [ERROR] testOperatorEventLostWithReaderFailure(org.apache.flink.runtime.operators.coordination.OperatorEventSendingCheckpointITCase) Time elapsed: 3.623 s &lt;&lt;&lt; FAILURE!Jul 03 01:37:31 java.lang.AssertionError: expected:&lt;[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]&gt; but was:&lt;[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]&gt;Jul 03 01:37:31 at org.junit.Assert.fail(Assert.java:88)Jul 03 01:37:31 at org.junit.Assert.failNotEquals(Assert.java:834)Jul 03 01:37:31 at org.junit.Assert.assertEquals(Assert.java:118)Jul 03 01:37:31 at org.junit.Assert.assertEquals(Assert.java:144)Jul 03 01:37:31 at org.apache.flink.runtime.operators.coordination.OperatorEventSendingCheckpointITCase.runTest(OperatorEventSendingCheckpointITCase.java:254)Jul 03 01:37:31 at org.apache.flink.runtime.operators.coordination.OperatorEventSendingCheckpointITCase.testOperatorEventLostWithReaderFailure(OperatorEventSendingCheckpointITCase.java:143)Jul 03 01:37:31 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)Jul 03 01:37:31 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)Jul 03 01:37:31 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)Jul 03 01:37:31 at java.lang.reflect.Method.invoke(Method.java:498)Jul 03 01:37:31 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)Jul 03 01:37:31 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)Jul 03 01:37:31 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)Jul 03 01:37:31 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)Jul 03 01:37:31 at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)Jul 03 01:37:31 at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)Jul 03 01:37:31 at org.junit.rules.RunRules.evaluate(RunRules.java:20)Jul 03 01:37:31 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)Jul 03 01:37:31 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)Jul 03 01:37:31 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)</description>
      <version>1.14.0,1.12.3,1.13.1</version>
      <fixedVersion>1.14.0,1.12.5,1.13.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.EventReceivingTasks.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.Runnables.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.SubtaskGatewayImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.java</file>
    </fixedFiles>
  </bug>
  <bug id="24023" opendate="2021-8-27 00:00:00" fixdate="2021-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>the names of metrics are incomplete</summary>
      <description>we cannot see the total name of metics on the page  because the selected box is too narrow . </description>
      <version>1.12.3</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.chart.job-overview-drawer-chart.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="24413" opendate="2021-9-30 00:00:00" fixdate="2021-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Casting to a CHAR() and VARCHAR() doesn&amp;#39;t trim the string to the specified precision</summary>
      <description>CAST('abcdfe' AS CHAR(3)) should trim the string to 3 chars but currently returns the whole string 'abcdfe'. PostgreSQL and Oracle for example behave as such:postgres=# select '123456afas'::char(4); bpchar -------- 1234 (1 row)postgres=# select '123456afas'::varchar(5); varchar --------- 12345 (1 row)</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRuleProviderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.IfCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.TimeToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.TimestampToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.RowToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.RawToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.NumericToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.MapAndMultisetToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.IntervalToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.DateToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRuleProvider.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRulePredicate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.BooleanToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.BinaryToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.ArrayToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.VarCharType.java</file>
    </fixedFiles>
  </bug>
  <bug id="24414" opendate="2021-9-30 00:00:00" fixdate="2021-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Casting a decimal to a String doesn&amp;#39;t do left zero padding</summary>
      <description>CAST(a AS STRING) where a is a DECIMAL(5.3) with value 9.87 results in9.870 so we only get right 0 padding to fill in the fractional digits. Postgres and Oracle also don't do left zero padding.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="24418" opendate="2021-9-30 00:00:00" fixdate="2021-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support cast of RAW() to BINARY/VARBINARY/BYTES</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionMiscITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeCasts.java</file>
    </fixedFiles>
  </bug>
  <bug id="24419" opendate="2021-9-30 00:00:00" fixdate="2021-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Casting to a bounded BINARY/VARBINARY doesn&amp;#39;t trim to the specified precision</summary>
      <description>Currently:CastTestSpecBuilder.testCastTo(BINARY(2)).from(STRING(), "Apache").resultsIn(new byte[] {65, 112, 97, 99, 104, 101})</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionMiscITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.StringToBinaryCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.RawToBinaryCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRuleUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRuleProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="24546" opendate="2021-10-14 00:00:00" fixdate="2021-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Acknowledged description miss on Monitoring Checkpointing page</summary>
      <description>Acknowledged description miss on Monitoring Checkpointing page</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.monitoring.checkpoint.monitoring.md</file>
      <file type="M">docs.content.zh.docs.ops.monitoring.checkpoint.monitoring.md</file>
    </fixedFiles>
  </bug>
  <bug id="24665" opendate="2021-10-27 00:00:00" fixdate="2021-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Presto Hadoop dependency to latest version</summary>
      <description>The Flink Presto Hadoop dependency com.facebook.presto.hadoop:hadoop-apache2 that Flink uses is rather outdated (released in March 2017). We should upgrade to the latest released version for Hadoop2 (which was released in March 2021)</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27649" opendate="2022-5-16 00:00:00" fixdate="2022-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce the number of outputted log lines by Elasticsearch6SinkE2ECase</summary>
      <description>The current ElasticSearch tests create a large number of log lines, see https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35694&amp;view=logs&amp;j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&amp;t=160c9ae5-96fd-516e-1c91-deb81f59292a&amp;l=14702 as an example.We should disable the logging by default.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-elasticsearch6.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2765" opendate="2015-9-25 00:00:00" fixdate="2015-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade hbase version for hadoop-2 to 1.2 release</summary>
      <description>Currently 0.98.11 is used: &lt;hbase.hadoop2.version&gt;0.98.11-hadoop2&lt;/hbase.hadoop2.version&gt;Stable release for hadoop-2 is 1.2.x lineWe should upgrade to 1.2.1</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-batch-connectors.flink-hbase.pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>
