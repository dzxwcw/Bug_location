<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="3838" opendate="2016-4-27 00:00:00" fixdate="2016-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CLI parameter parser is munging application params</summary>
      <description>If parameters for an application use a single '-' (e.g. -maxtasks) then the CLI argument parser will munge these, and the app gets passed either just the parameter name (e.g. 'maxtask') if the start of the parameter doesn't match a Flink parameter, or you get two values, with the first value being the part that matched (e.g. '-m') and the second value being the rest (e.g. 'axtasks').The parser should ignore everything after the jar path parameter.Note that using --&lt;parameter name&gt; does seem to work.</description>
      <version>1.0.2,1.0.3</version>
      <fixedVersion>1.0.4,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.CliFrontendRunTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="3854" opendate="2016-4-29 00:00:00" fixdate="2016-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Avro key-value rolling sink writer</summary>
      <description>Support rolling sink writer in avro key value format.preferably without additional classpath dependenciespreferable in same format as M/R jobs for backward compatibility</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.RollingSinkITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="3855" opendate="2016-4-30 00:00:00" fixdate="2016-5-30 01:00:00" resolution="Done">
    <buginformation>
      <summary>Upgrade Jackson version</summary>
      <description>Jackson version in use (2.4.2) is rather old (and not even the latest patch from minor version), so it'd be make sense to upgrade to bit newer. Latest would be 2.7.4, but at first I propose going to 2.5.5.All tests pass, but if there are issues I'd be happy to help; I'm author of Jackson project.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-elasticsearch2.pom.xml</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3856" opendate="2016-5-2 00:00:00" fixdate="2016-5-2 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create types for java.sql.Date/Time/Timestamp</summary>
      <description>At the moment there is only the Date type which is not sufficient for most use cases about time.The Table API would also benefit from having different types as output result.I would propose to add the three java.sql. types either as BasicTypes or in an additional class TimeTypes.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.TypeExtractorTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.DateSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.DateComparator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.GroupReduceITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="3974" opendate="2016-5-25 00:00:00" fixdate="2016-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>enableObjectReuse fails when an operator chains to multiple downstream operators</summary>
      <description>Given a topology that looks like this:DataStream&lt;A&gt; input = ...input .map(MapFunction&lt;A,B&gt;...) .addSink(...);input .map(MapFunction&lt;A,C&gt;...) â€‹.addSink(...);enableObjectReuse() will cause an exception in the form of "java.lang.ClassCastException: B cannot be cast to A" to be thrown.It looks like the input operator calls Output&lt;StreamRecord&lt;A&gt;&gt;.collect which attempts to loop over the downstream operators and process them.However, the first map operation will call StreamRecord&lt;&gt;.replace which mutates the value stored in the StreamRecord&lt;&gt;. As a result, when the Output&lt;StreamRecord&lt;A&gt;&gt;.collect call passes the StreamRecord&lt;A&gt; to the second map operation it is actually a StreamRecord&lt;B&gt; and behaves as if the two map operations were serial instead of parallel.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.collector.selector.DirectedOutput.java</file>
    </fixedFiles>
  </bug>
  <bug id="3993" opendate="2016-5-31 00:00:00" fixdate="2016-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Add generateSequence() support to Python API</summary>
      <description>Right now, I believe that there is only from_elements() support in order to create a sequence of numbers. It is interesting to be able to create a list of numbers from the Python API also, apart from the Java API. It would not be complicated, since we already have generateSequence(). I am already working on this, and will create a pull request shortly in Github.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.main.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.OperationInfo.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Environment.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Constants.py</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonPlanBinder.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonOperationInfo.java</file>
      <file type="M">docs.apis.batch.python.md</file>
    </fixedFiles>
  </bug>
  <bug id="4002" opendate="2016-6-1 00:00:00" fixdate="2016-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Improve testing infraestructure</summary>
      <description>The Verify() test function errors out when array elements are missing:env.generate_sequence(1, 5)\ .map(Id()).map_partition(Verify([1,2,3,4], "Sequence")).output()IndexError: list index out of rangeThere should also be more documentation in test functions.I am already working on a pull request to fix this.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.main2.py</file>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.main.py</file>
      <file type="M">flink-libraries.flink-python.src.test.java.org.apache.flink.python.api.PythonPlanBinderTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="4017" opendate="2016-6-3 00:00:00" fixdate="2016-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Add Aggregation support to Python API</summary>
      <description>Aggregations are not currently supported in the Python API.I was getting started with setting up and working with Flink and figured this would be a relatively simple task for me to get started with. Currently working on this at https://github.com/geofbot/flink</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.main2.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.DataSet.py</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonPlanBinder.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonOperationInfo.java</file>
      <file type="M">docs.apis.batch.python.md</file>
      <file type="M">docs.apis.batch.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  <bug id="4035" opendate="2016-6-8 00:00:00" fixdate="2016-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Apache Kafka 0.10 connector</summary>
      <description>Kafka 0.10.0.0 introduced protocol changes related to the producer. Published messages now include timestamps and compressed messages now include relative offsets. As it is now, brokers must decompress publisher compressed messages, assign offset to them, and recompress them, which is wasteful and makes it less likely that compression will be used at all.</description>
      <version>1.0.3</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.JobManagerCommunicationUtils.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.DataGenerators.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaShortRetentionTestBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcherTimestampsTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionStateWithPunctuatedWatermarks.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionStateWithPeriodicWatermarks.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer09.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.SimpleConsumerThread.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010ITCase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010TableSource.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010JsonTableSource.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer010.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer010.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.pom.xml</file>
      <file type="M">docs.page.js.flink.js</file>
      <file type="M">docs.dev.connectors.kafka.md</file>
      <file type="M">flink-streaming-connectors.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4067" opendate="2016-6-13 00:00:00" fixdate="2016-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add version header to savepoints</summary>
      <description>Adding a header with version information to savepoints ensures that we can migrate savepoints between Flink versions in the future (for example when changing internal serialization formats between versions).After talking with Till, we propose to add the following meta data: Magic number (int): identify data as savepoint Version (int): savepoint version (independent of Flink version) Data Offset (int): specifies at which point the actual savepoint data starts. With this, we can allow future Flink versions to add fields to the header without breaking stuff, e.g. Flink 1.1 could read savepoints of Flink 2.0.For Flink 1.0 savepoint support, we have to try reading the savepoints without a header before failing if we don't find the magic number.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.YarnJobManager.scala</file>
      <file type="M">flink-yarn-tests.src.test.scala.org.apache.flink.yarn.TestingYarnJobManager.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.classloading.ClassLoaderITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.JobManagerITCase.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.JobManagerLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerHARecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.SavepointStoreFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.SavepointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.HeapStateStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.FileSystemStateStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ExecutionGraphCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.AbstractStateStoreTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.testingUtils.TestingJobManagerMessages.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.testingUtils.TestingJobManagerLike.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.testingUtils.TestingJobManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobVertexID.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointRecoveryFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.SubtaskState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StandaloneCheckpointRecoveryFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.SavepointStoreFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.SavepointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.SavepointCoordinatorDeActivator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.SavepointCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.HeapStateStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.FileSystemStateStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointRecoveryFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.SerializedValue.java</file>
    </fixedFiles>
  </bug>
  <bug id="4070" opendate="2016-6-15 00:00:00" fixdate="2016-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support literals on left side of binary expressions</summary>
      <description>The Table API does not support binary expressions like 12 &lt; 'f0 in Scala DSL where the left side is a literal. Maybe this can be solved by implicits or at least by a 12.toExpr method.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.ExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
    </fixedFiles>
  </bug>
  <bug id="4078" opendate="2016-6-15 00:00:00" fixdate="2016-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use ClosureCleaner for CoGroup where</summary>
      <description>When specifying a key selector in the where clause of a CoGroup, the closure cleaner is not used..coGroup(filteredIds) .where(new KeySelector&lt;T, String&gt;() { @Override public String getKey(T t) throws Exception { String s = (String) t.get(fieldName); return s != null ? s : UUID.randomUUID().toString(); } })The problem is that the KeySelector is an anonymous inner class and as such as a reference to the outer object. Normally, this would be rectified by the closure cleaner but the cleaner is not used in CoGroup.where().</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.CoGroupITCase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.CoGroupOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4079" opendate="2016-6-15 00:00:00" fixdate="2016-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>YARN properties file used for per-job cluster</summary>
      <description>YARN per job clusters (flink run -m yarn-cluster) rely on the hidden YARN properties file, which defines the container configuration. This can lead to unexpected behaviour, because the per-job-cluster configuration is merged with the YARN properties file (or used as only configuration source).A user ran into this as follows: Create a long-lived YARN session with HA (creates a hidden YARN properties file) Submits standalone batch jobs with a per job cluster (flink run -m yarn-cluster). The batch jobs get submitted to the long lived HA cluster, because of the properties file.mxm Am I correct in assuming that this is only relevant for the 1.0 branch and will be fixed with the client refactoring you are working on?</description>
      <version>1.0.3</version>
      <fixedVersion>1.0.4,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
    </fixedFiles>
  </bug>
  <bug id="4080" opendate="2016-6-15 00:00:00" fixdate="2016-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kinesis consumer not exactly-once if stopped in the middle of processing aggregated records</summary>
      <description>I've occasionally experienced unsuccessful ManualExactlyOnceTest after several tries.Kinesis records of the same aggregated batch will have the same sequence number, and different sub-sequence numbers (http://docs.aws.amazon.com/streams/latest/dev/kinesis-kpl-consumer-deaggregation.html). The current code of the consumer is committing state every time it finishes processing a record, even de-aggregated ones. This is a bug since this will incorrectly mark all remaining records of the de-aggregated batch as processed in the state.Proposed fix:1. Use the extended `UserRecord` class in KCL to represent all records (either non- or de-aggregated) instead of the basic `Record` class. This gives access to whether or not the record was originally aggregated.2. The sequence number state we are checkpointing needs to be able to indicate that the last seen sequence number of a shard may be a de-aggregated shard, i.e., {"shard0" -&gt; "5:8", "shard1" -&gt; "2"} meaning the 8th sub-record of the 5th record was last seen for shard 0. On restore, we start again from record 5 for shard 0 and skip the first 7 sub-records; however, for shard 1 we start from record 3 since record 2 is non-aggregated and already fully processed.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumerThreadTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcherTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.model.SentinelSequenceNumber.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumerThread.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer.java</file>
    </fixedFiles>
  </bug>
  <bug id="4082" opendate="2016-6-16 00:00:00" fixdate="2016-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Setting for LargeRecordHandler</summary>
      <description>Now, this is always enabled but there are known problems when users specify a custom TypeInformation. We should introduce a setting for this.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.manual.MassiveCaseClassSortingITCase.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.manual.MassiveStringValueSorting.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.manual.MassiveStringSorting.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.HashVsSortMiniBenchmark.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.UnaryOperatorTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DriverTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.BinaryOperatorTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.ExternalSortLargeRecordsITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.ExternalSortITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.CombiningUnilateralSortMergerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.ReduceTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.ReduceTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.util.TaskConfig.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.UnilateralSortMerger.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.CombiningUnilateralSortMerger.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DataSinkTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.BatchTask.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plantranslate.JobGraphGenerator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="4083" opendate="2016-6-16 00:00:00" fixdate="2016-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use ClosureCleaner for Join where and equalTo</summary>
      <description>When specifying a key selector in the where or equalTo clause of a Join, the closure cleaner is not used. Same problem as FLINK-4078..join(ds) .where(new KeySelector&lt;CustomType, Integer&gt;() { @Override public Integer getKey(CustomType value) { return value.myInt; } }) .equalTo(new KeySelector&lt;CustomType, Integer&gt;(){ @Override public Integer getKey(CustomType value) throws Exception { return value.myInt; } });The problem is that the KeySelector is an anonymous inner class and as such as a reference to the outer object. Normally, this would be rectified by the closure cleaner but the cleaner is not used in Join.where() and Join.equalTo().</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.JoinITCase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.JoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4134" opendate="2016-6-30 00:00:00" fixdate="2016-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>EventTimeSessionWindows trigger for empty windows when dropping late events</summary>
      <description>It seems like EventTimeSessionWindows sometimes trigger for empty windows. The behavior is observed in connection with dropping late events:stream .keyBy("sessionKey") .window(EventTimeSessionWindows.withGap(Time.milliseconds(100))) .allowedLateness(Time.milliseconds(0)) .apply(new ValidatingWindowFunction()) .print();I wrote a generator that generates events for several parallel sessions and that allows to reproduce the error. For now, I can share this generator privately for debugging purposes, but my plan is to use the generator as basis for an integration test.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.EvictingWindowOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4139" opendate="2016-7-1 00:00:00" fixdate="2016-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn: Adjust parallelism and task slots correctly</summary>
      <description>The Yarn CLI should handle the following situations correctly: The user specifies no parallelism -&gt; parallelism is adjusted to #taskSlots * #nodes. The user specifies parallelism but no #taskSlots or too few slots -&gt; #taskSlots are set such that they meet the parallelismThese functionality has been present in Flink 1.0.x but there were some glitches in the implementation.</description>
      <version>1.0.3,1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.FlinkYarnSessionCliTest.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.CliFrontendYarnAddressConfigurationTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.CliFrontendAddressConfigurationTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="4141" opendate="2016-7-1 00:00:00" fixdate="2016-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TaskManager failures not always recover when killed during an ApplicationMaster failure in HA mode on Yarn</summary>
      <description>High availability on Yarn often fails to recover in the following test scenario:1. Kill application master process.2. Then, while application master is recovering, randomly kill several task managers (with some delay).After the application master recovered, not all the killed task manager are brought back and no further attempts are made the restart them.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManagerCallbackHandler.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnFlinkResourceManager.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.messages.ContainersComplete.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.messages.ContainersAllocated.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.standalone.StandaloneResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.FlinkResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="4142" opendate="2016-7-1 00:00:00" fixdate="2016-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Recovery problem in HA on Hadoop Yarn 2.4.1</summary>
      <description>On Hadoop Yarn 2.4.1, recovery in HA fails in the following scenario:1) Kill application master, let it recover normally.2) After that, kill a task manager.Now, Yarn tries to restart the killed task manager in an endless loop.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.jobmanager.high.availability.md</file>
    </fixedFiles>
  </bug>
  <bug id="4144" opendate="2016-7-1 00:00:00" fixdate="2016-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn properties file: replace hostname/port with Yarn application id</summary>
      <description>We should use the application id instead of the host/port. The hostname and port of the JobManager can change (HA). Also, it is not unique depending on the network configuration.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.CliFrontendYarnAddressConfigurationTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="4145" opendate="2016-7-3 00:00:00" fixdate="2016-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JmxReporterTest fails due to port conflicts</summary>
      <description>I saw multiple failures of the JmxReporterTest most likely due to a port conflicts. The test relies on the default JMX reporter port range, which spans 5 ports. Running on Travis with multiple concurrent builds and bad timings, this can lead to port conflicts.Some example failed runs:https://s3.amazonaws.com/archive.travis-ci.org/jobs/141999066/log.txt (one out of 5 jobs failed)https://travis-ci.org/uce/flink/builds/141917901 (all 5 jobs failed)I propose to take the fork number into account (like the forkable Flink testing cluster) and configure a larger port range.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerMetricTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.reporter.JMXReporterTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="4150" opendate="2016-7-4 00:00:00" fixdate="2016-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Problem with Blobstore in Yarn HA setting on recovery after cluster shutdown</summary>
      <description>Submitting a job in Yarn with HA can lead to the following exception:org.apache.flink.streaming.runtime.tasks.StreamTaskException: Cannot load user class: org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09ClassLoader info: URL ClassLoader: file: '/tmp/blobStore-ccec0f4a-3e07-455f-945b-4fcd08f5bac1/cache/blob_7fafffe9595cd06aff213b81b5da7b1682e1d6b0' (invalid JAR: zip file is empty)Class not resolvable through given classloader. at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperator(StreamConfig.java:207) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:222) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:588) at java.lang.Thread.run(Thread.java:745)Some job information, including the Blob ids, are stored in Zookeeper. The actual Blobs are stored in a dedicated BlobStore, if the recovery mode is set to Zookeeper. This BlobStore is typically located in a FS like HDFS. When the cluster is shut down, the path for the BlobStore is deleted. When the cluster is then restarted, recovering jobs cannot restore because it's Blob ids stored in Zookeeper now point to deleted files.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingLeaderElectionService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.FileSystemBlobStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobServerConnection.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="4151" opendate="2016-7-5 00:00:00" fixdate="2016-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Address Travis CI build time: We are exceeding the 2 hours limit</summary>
      <description>We've recently started hitting the two hours limit for Travis CI.I'll look into some approaches to get our build stable again.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="4155" opendate="2016-7-5 00:00:00" fixdate="2016-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Get Kafka producer partition info in open method instead of constructor</summary>
      <description>Currently the Flink Kafka producer does not really do any error handling if something is wrong with the partition metadata as it is serialized with the user function.This means that in some cases the job can go into an error loop when using the checkpoints. Getting the partition info in the open method would solve this problem (like restarting from a savepoint which re-runs the constructor).</description>
      <version>1.0.3,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSinkTestBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.AtLeastOnceProducerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSinkTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSinkTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="4166" opendate="2016-7-7 00:00:00" fixdate="2016-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate automatic different namespaces in Zookeeper for Flink applications</summary>
      <description>We should automatically generate different namespaces per Flink application in Zookeeper to avoid interference between different applications that refer to the same Zookeeper entries.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnConfigKeys.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnApplicationMasterRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.FlinkYarnSessionCliTest.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.CliFrontendYarnAddressConfigurationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.DefaultCLI.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">docs.setup.yarn.setup.md</file>
      <file type="M">docs.setup.jobmanager.high.availability.md</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="4169" opendate="2016-7-7 00:00:00" fixdate="2016-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CEP Does Not Work with RocksDB StateBackend</summary>
      <description>A job will never match any patterns because ValueState.update() is not called in the keyed CEP operators for updating the NFA state and the priority queue state.The reason why it works for other state backends is that they are very lax in their handling of state: if the object returned from ValueState.value()) is mutable changes to this will be reflected in checkpoints even if ValueState.update() is not called. RocksDB, on the other hand, does always deserialize/serialize state values when accessing/updating them, so changes to the returned object will not be reflected in the state unless update() is called.We should fix this and also add a test for it. This might be tricky because we have to pull together RocksDB and CEP.</description>
      <version>1.0.0,1.0.1,1.0.2,1.0.3,1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractCEPBasePatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4185" opendate="2016-7-8 00:00:00" fixdate="2016-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reflecting rename from Tachyon to Alluxio</summary>
      <description>The Tachyon project has been renamed to Alluxio earlier this year. The goal of this issue is to reflect this in the Flink documentation.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.batch.connectors.md</file>
    </fixedFiles>
  </bug>
  <bug id="4196" opendate="2016-7-11 00:00:00" fixdate="2016-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove "recoveryTimestamp"</summary>
      <description>I think we should remove the recoveryTimestamp that is attached on state restore calls.Given that this is a wall clock timestamp from a master node, which may change when clocks are adjusted, and between different master nodes during leader change, this is an unsafe concept.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskAsyncCheckpointTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.WriteAheadSinkTestBase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AggregatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AbstractAlignedProcessingTimeWindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.GenericWriteAheadSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskAsyncCallTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.SavepointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.AbstractMemStateSnapshot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KvStateSnapshot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.GenericReducingState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.GenericListState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.GenericFoldingState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.AbstractFsStateSnapshot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AsynchronousKvStateSnapshot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.StatefulTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.SavepointCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractCEPPatternOperator.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="4197" opendate="2016-7-12 00:00:00" fixdate="2016-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow Kinesis Endpoint to be Overridden via Config</summary>
      <description>I perform local testing of my application stack with Flink configured as a consumer on a Kinesis stream provided by Kinesalite, an implementation of Kinesis built on LevelDB. This requires me to override the AWS endpoint to refer to my local Kinesalite server rather than reference the real AWS endpoint. I'd like to add a configuration property to the Kinesis streaming connector that allows the AWS endpoint to be specified explicitly.This should be a fairly small change and provide a lot of flexibility to people looking to integrate Flink with Kinesis in a non-production setup.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.KinesisConfigConstants.java</file>
      <file type="M">docs.apis.streaming.connectors.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug id="4216" opendate="2016-7-14 00:00:00" fixdate="2016-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WordWithCount example with Java has wrong generics type</summary>
      <description>The Java example of the POJOs results in the:Exception in thread "main" java.lang.Error: Unresolved compilation problem: due to the wrong type of the generics of the DataStream.Currently it is DataStream&lt;Tuple2&lt;String,Integer&gt;&gt;but should be DataSource&lt;WordWithCount&gt;.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.common.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="4226" opendate="2016-7-17 00:00:00" fixdate="2016-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo: Define Keys using Field Expressions example should use window and not reduce</summary>
      <description>...val words: DataStream[WC] = // [...]val wordCounts = words.keyBy("word").window(/*window specification*/)// or, as a case class, which is less typingcase class WC(word: String, count: Int)val words: DataStream[WC] = // [...]val wordCounts = words.keyBy("word").reduce(/*window specification*/)Should be: val wordCounts = words.keyBy("word").reduce window(/window specification/)</description>
      <version>1.0.3</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.common.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="4228" opendate="2016-7-18 00:00:00" fixdate="2016-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>YARN artifact upload does not work with S3AFileSystem</summary>
      <description>The issue now is exclusive to running on YARN with s3a:// as your configured FileSystem. If so, the Flink session will fail on staging itself because it tries to copy the flink/lib directory to S3 and the S3aFileSystem does not support recursive copy.Old IssueUsing the RocksDBStateBackend with semi-async snapshots (current default) leads to an Exception when uploading the snapshot to S3 when using the S3AFileSystem.AsynchronousException{com.amazonaws.AmazonClientException: Unable to calculate MD5 hash: /var/folders/_c/5tc5q5q55qjcjtqwlwvwd1m00000gn/T/flink-io-5640e9f1-3ea4-4a0f-b4d9-3ce9fbd98d8a/7c6e745df2dddc6eb70def1240779e44/StreamFlatMap_3_0/dummy_state/47daaf2a-150c-4208-aa4b-409927e9e5b7/local-chk-2886 (Is a directory)} at org.apache.flink.streaming.runtime.tasks.StreamTask$AsyncCheckpointThread.run(StreamTask.java:870)Caused by: com.amazonaws.AmazonClientException: Unable to calculate MD5 hash: /var/folders/_c/5tc5q5q55qjcjtqwlwvwd1m00000gn/T/flink-io-5640e9f1-3ea4-4a0f-b4d9-3ce9fbd98d8a/7c6e745df2dddc6eb70def1240779e44/StreamFlatMap_3_0/dummy_state/47daaf2a-150c-4208-aa4b-409927e9e5b7/local-chk-2886 (Is a directory) at com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1298) at com.amazonaws.services.s3.transfer.internal.UploadCallable.uploadInOneChunk(UploadCallable.java:108) at com.amazonaws.services.s3.transfer.internal.UploadCallable.call(UploadCallable.java:100) at com.amazonaws.services.s3.transfer.internal.UploadMonitor.upload(UploadMonitor.java:192) at com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:150) at com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:50) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.io.FileNotFoundException: /var/folders/_c/5tc5q5q55qjcjtqwlwvwd1m00000gn/T/flink-io-5640e9f1-3ea4-4a0f-b4d9-3ce9fbd98d8a/7c6e745df2dddc6eb70def1240779e44/StreamFlatMap_3_0/dummy_state/47daaf2a-150c-4208-aa4b-409927e9e5b7/local-chk-2886 (Is a directory) at java.io.FileInputStream.open0(Native Method) at java.io.FileInputStream.open(FileInputStream.java:195) at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:138) at com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1294) ... 9 moreRunning with S3NFileSystem, the error does not occur. The problem might be due to HDFSCopyToLocal assuming that sub-folders are going to be created automatically. We might need to manually create folders and copy only actual files for S3AFileSystem. More investigation is required.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="4229" opendate="2016-7-18 00:00:00" fixdate="2016-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not start Metrics Reporter by default</summary>
      <description>By default, we start a JMX reported that binds to a port and comes with extra threads. We should not start any reported by default to keep the overhead to a minimum.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerMetricTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.reporter.JMXReporterTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.reporter.JMXReporter.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.MetricRegistry.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.apis.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="4230" opendate="2016-7-18 00:00:00" fixdate="2016-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Session Windowing IT Case</summary>
      <description>An ITCase for Session Windows is missing that tests correct behavior under several parallel sessions, with timely events, late events within and after the lateness interval.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.TestEventPayload.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.StreamEventFactory.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.StreamConfiguration.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.SessionWindowITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.SessionStreamConfiguration.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.SessionEventGeneratorImpl.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.SessionEvent.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.SessionConfiguration.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.ParallelSessionsEventGenerator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.LongRandomGenerator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.EventGeneratorFactory.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.EventGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4288" opendate="2016-8-1 00:00:00" fixdate="2016-1-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make it possible to unregister tables</summary>
      <description>Table names can not be changed yet. After registration you can not modify the table behind a table name. Maybe this behavior is too restrictive.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.TableEnvironmentITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="4292" opendate="2016-8-1 00:00:00" fixdate="2016-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HCatalog project incorrectly set up</summary>
      <description>The HCatalog project is erroneous in IntelliJ, because it misses the Scala SDK dependency.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.1,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-batch-connectors.flink-hcatalog.src.main.scala.org.apache.flink.hcatalog.scala.HCatInputFormat.scala</file>
      <file type="M">flink-batch-connectors.flink-hcatalog.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4293" opendate="2016-8-1 00:00:00" fixdate="2016-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Malformatted Apache Headers</summary>
      <description>Several files contain this header:/** * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file * distributed with this work for additional information * regarding copyright ownership. The ASF licenses this file * to you under the Apache License, Version 2.0 (the * "License"); you may not use this file except in compliance * with the License. You may obtain a copy of the License at * &lt;p/&gt; * http://www.apache.org/licenses/LICENSE-2.0 * &lt;p/&gt; * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */The correct header format should be:/* * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file * distributed with this work for additional information * regarding copyright ownership. The ASF licenses this file * to you under the Apache License, Version 2.0 (the * "License"); you may not use this file except in compliance * with the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */</description>
      <version>1.0.3</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.WriteAheadSinkTestBase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.GenericWriteAheadSinkTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.util.HDFSCopyToLocal.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.util.HDFSCopyFromLocal.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.GenericWriteAheadSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.CheckpointCommitter.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.example.CassandraTupleWriteAheadSinkExample.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.streaming.connectors.cassandra.ClusterBuilder.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.streaming.connectors.cassandra.CassandraTupleWriteAheadSink.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.streaming.connectors.cassandra.CassandraSink.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.streaming.connectors.cassandra.CassandraCommitter.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.BloomFilterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.GenericReducingState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.GenericListState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.GenericFoldingState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ArrayListSerializer.java</file>
      <file type="M">flink-batch-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCTestBase.java</file>
      <file type="M">flink-batch-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.split.ParameterValuesProvider.java</file>
      <file type="M">flink-batch-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.split.NumericBetweenParametersProvider.java</file>
      <file type="M">flink-batch-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.split.GenericParameterValuesProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="4294" opendate="2016-8-1 00:00:00" fixdate="2016-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow access of composite type fields</summary>
      <description>Currently all Flink CompositeTypes are treated as GenericRelDataTypes. It would be better to access individual fields of composite types, too. e.g.SELECT composite.name FROM compositesSELECT tuple.f0 FROM tuples'f0.getField(0)</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.TableWithSQLITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.sql.SqlITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.ProjectionTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.FlinkRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.logical.LogicalNode.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.FlinkTypeFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.FlinkPlannerImpl.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.fieldExpression.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="4298" opendate="2016-8-1 00:00:00" fixdate="2016-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up Storm Compatibility Dependencies</summary>
      <description>The flink-storm project contains unnecessary (transitive) dependencies Google Guava Ring Closure Servlet BindingsParticularly the last one is frequently troublesome in Maven builds (unstable downloads) and is not required, because the Storm Compatibility layer does not start a web UI.</description>
      <version>1.0.3,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.WrapperSetupHelperTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.SpoutWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.MergedInputsBoltWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.FlinkTopologyContext.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.BoltWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkClient.java</file>
      <file type="M">flink-contrib.flink-storm.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4299" opendate="2016-8-1 00:00:00" fixdate="2016-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show loss of job manager in Client</summary>
      <description>If the client looses the connection to a job manager and the job recovers from this, the client will only print the job status as RUNNING again. It is hard to actually notice that something went wrong and a job manager was lost....08/01/2016 14:35:43 Flat Map -&gt; Sink: Unnamed(8/8) switched to RUNNING08/01/2016 14:35:43 Source: Custom Source(6/8) switched to RUNNING&lt;------ EVERYTHING'S RUNNING ------&gt;08/01/2016 14:40:40 Job execution switched to status RUNNING &lt;--- JOB MANAGER FAIL OVER08/01/2016 14:40:40 Source: Custom Source(1/8) switched to SCHEDULED08/01/2016 14:40:40 Source: Custom Source(1/8) switched to DEPLOYING08/01/2016 14:40:40 Source: Custom Source(2/8) switched to SCHEDULED...After 14:35:43 everything is running and the client does not print any execution state updates. When the job manager fails, the job will be recovered and enter the running state again eventually (at 14:40:40), but the user might never notice this.I would like to improve on this by printing some messages about the state of the job manager connection. For example, between 14:35:43 and 14:40:40 it might say that the job manager connection was lost, a new one established, etc.</description>
      <version>None</version>
      <fixedVersion>1.1.0,1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.client.JobClientActor.java</file>
    </fixedFiles>
  </bug>
  <bug id="4306" opendate="2016-8-2 00:00:00" fixdate="2016-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Flink and Storm dependencies in flink-storm and flink-storm-examples</summary>
      <description>Flink dependencies should be in scope provided, like in the other libraries. flink-storm-examples should not draw storm-core directly, but only via flink-storm, so it gets the proper transitive dependency exclusions flink-storm-examples should have the clojure jar repository as an additional maven repository</description>
      <version>1.0.3,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm.pom.xml</file>
      <file type="M">flink-contrib.flink-storm-examples.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4307" opendate="2016-8-2 00:00:00" fixdate="2016-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Broken user-facing API for ListState</summary>
      <description>The user-facing ListState is supposed to return an empty list when no element is contained in the state.A previous change altered that behavior to make it in the runtime classes accessible whether a ListState is empty.To not break the user-facing API, we need to restore the behavior for ListState exposed to the users via the RuntimeContext.</description>
      <version>None</version>
      <fixedVersion>1.1.0,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContextTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="6776" opendate="2017-5-30 00:00:00" fixdate="2017-6-30 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Use skip instead of seek for small forward repositioning in DFS streams</summary>
      <description>Reading checkpoint meta data and finding key-groups in restores sometimes require to seek in input streams. Currently, we always use a seek, even for small position changes. As small true seeks are far more expensive than small reads/skips, we should just skip over small gaps instead of performing the seek.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.fs.hdfs.HadoopDataInputStream.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
