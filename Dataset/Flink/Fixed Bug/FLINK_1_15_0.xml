<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="1696" opendate="2015-3-13 00:00:00" fixdate="2015-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add multiple linear regression to ML library</summary>
      <description>Add multiple linear regression to ML library.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.regression.MultipleLinearRegression.scala</file>
      <file type="M">docs..layouts.default.html</file>
      <file type="M">docs..includes.sidenav.html</file>
      <file type="M">docs..includes.navbar.html</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Vector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Matrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseMatrix.scala</file>
      <file type="M">flink-staging.flink-ml.pom.xml</file>
      <file type="M">flink-examples.flink-scala-examples.src.main.scala.org.apache.flink.examples.scala.ml.LinearRegression.scala</file>
    </fixedFiles>
  </bug>
  <bug id="18082" opendate="2020-6-3 00:00:00" fixdate="2020-1-3 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>UnsignedTypeConversionITCase stalls in ch.vorburger.mariadb4j.DB.stop</summary>
      <description>CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2582&amp;view=logs&amp;j=d44f43ce-542c-597d-bf94-b0718c71e5e8&amp;t=03dca39c-73e8-5aaf-601d-328ae5c35f202020-06-02T19:01:31.8486456Z ==============================================================================2020-06-02T19:01:31.8487052Z Printing stack trace of Java process 86532020-06-02T19:01:31.8487424Z ==============================================================================2020-06-02T19:01:31.8541169Z Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError2020-06-02T19:01:32.1665740Z 2020-06-02 19:01:322020-06-02T19:01:32.1666470Z Full thread dump OpenJDK 64-Bit Server VM (25.242-b08 mixed mode):2020-06-02T19:01:32.1666735Z 2020-06-02T19:01:32.1667614Z "Attach Listener" #537 daemon prio=9 os_prio=0 tid=0x00007f61f8001000 nid=0x3b9f waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1668130Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1668311Z 2020-06-02T19:01:32.1668958Z "flink-akka.actor.default-dispatcher-193" #535 prio=5 os_prio=0 tid=0x00007f6034001000 nid=0x3af7 waiting on condition [0x00007f61a25b8000]2020-06-02T19:01:32.1669418Z java.lang.Thread.State: TIMED_WAITING (parking)2020-06-02T19:01:32.1669730Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1670301Z - parking to wait for &lt;0x0000000080c51528&gt; (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)2020-06-02T19:01:32.1670791Z at akka.dispatch.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:2135)2020-06-02T19:01:32.1671329Z at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2067)2020-06-02T19:01:32.1671763Z at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)2020-06-02T19:01:32.1672211Z at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)2020-06-02T19:01:32.1672491Z 2020-06-02T19:01:32.1673104Z "flink-akka.actor.default-dispatcher-191" #533 prio=5 os_prio=0 tid=0x00007f619801e000 nid=0x3ae1 waiting on condition [0x00007f60770f1000]2020-06-02T19:01:32.1673564Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1673839Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1674422Z - parking to wait for &lt;0x0000000080c51528&gt; (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)2020-06-02T19:01:32.1674865Z at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)2020-06-02T19:01:32.1675305Z at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)2020-06-02T19:01:32.1675751Z at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)2020-06-02T19:01:32.1676046Z 2020-06-02T19:01:32.1676669Z "jobmanager-future-thread-2" #466 daemon prio=5 os_prio=0 tid=0x00007f6124001000 nid=0x3795 waiting on condition [0x00007f61a23b6000]2020-06-02T19:01:32.1677316Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1677617Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1678220Z - parking to wait for &lt;0x00000000816a4c90&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1678702Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1679209Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1679822Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)2020-06-02T19:01:32.1680422Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1680962Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1681424Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1682062Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1682445Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1682656Z 2020-06-02T19:01:32.1683271Z "Flink-DispatcherRestEndpoint-thread-4" #349 daemon prio=5 os_prio=0 tid=0x00007f618c00a000 nid=0x29a4 waiting on condition [0x00007f61a029f000]2020-06-02T19:01:32.1683750Z java.lang.Thread.State: TIMED_WAITING (parking)2020-06-02T19:01:32.1684057Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1684648Z - parking to wait for &lt;0x0000000081731ff8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1685145Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-06-02T19:01:32.1685673Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)2020-06-02T19:01:32.1686400Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)2020-06-02T19:01:32.1687200Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1687724Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1688211Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1688679Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1689076Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1689266Z 2020-06-02T19:01:32.1689923Z "FlinkCompletableFutureDelayScheduler-thread-1" #123 daemon prio=5 os_prio=0 tid=0x00007f60e801d000 nid=0x277b waiting on condition [0x00007f61104dd000]2020-06-02T19:01:32.1690403Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1690698Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1691291Z - parking to wait for &lt;0x00000000879019c0&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1691779Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1692314Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1692905Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)2020-06-02T19:01:32.1693506Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1694040Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1694500Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1694988Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1695372Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1695583Z 2020-06-02T19:01:32.1696176Z "Flink-DispatcherRestEndpoint-thread-3" #84 daemon prio=5 os_prio=0 tid=0x00007f614c003800 nid=0x26ca waiting on condition [0x00007f6113bfa000]2020-06-02T19:01:32.1696685Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1697117Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1697737Z - parking to wait for &lt;0x0000000081731ff8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1698205Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1698749Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1699339Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)2020-06-02T19:01:32.1699942Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1700582Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1701042Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1701528Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1701926Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1702114Z 2020-06-02T19:01:32.1702714Z "mysql-cj-abandoned-connection-cleanup" #83 daemon prio=5 os_prio=0 tid=0x00007f61625c9000 nid=0x26c7 in Object.wait() [0x00007f6113dfc000]2020-06-02T19:01:32.1703201Z java.lang.Thread.State: TIMED_WAITING (on object monitor)2020-06-02T19:01:32.1703530Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1703857Z at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)2020-06-02T19:01:32.1704416Z - locked &lt;0x00000000814a4b00&gt; (a java.lang.ref.ReferenceQueue$Lock)2020-06-02T19:01:32.1704939Z at com.mysql.cj.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:85)2020-06-02T19:01:32.1705464Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2020-06-02T19:01:32.1705950Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1706329Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1706566Z 2020-06-02T19:01:32.1707083Z "Exec Stream Pumper" #76 daemon prio=5 os_prio=0 tid=0x00007f6118001000 nid=0x269c runnable [0x00007f6113ffe000]2020-06-02T19:01:32.1707485Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1707773Z at java.io.FileInputStream.readBytes(Native Method)2020-06-02T19:01:32.1708117Z at java.io.FileInputStream.read(FileInputStream.java:255)2020-06-02T19:01:32.1708522Z at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)2020-06-02T19:01:32.1708944Z at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)2020-06-02T19:01:32.1709357Z at java.io.BufferedInputStream.read(BufferedInputStream.java:345)2020-06-02T19:01:32.1709977Z - locked &lt;0x0000000081555688&gt; (a java.lang.UNIXProcess$ProcessPipeInputStream)2020-06-02T19:01:32.1710386Z at java.io.FilterInputStream.read(FilterInputStream.java:107)2020-06-02T19:01:32.1710779Z at org.apache.commons.exec.StreamPumper.run(StreamPumper.java:107)2020-06-02T19:01:32.1711147Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1711643Z 2020-06-02T19:01:32.1711987Z "Exec Default Executor" #75 prio=5 os_prio=0 tid=0x00007f63009ee800 nid=0x269a in Object.wait() [0x00007f61a019e000]2020-06-02T19:01:32.1712409Z java.lang.Thread.State: WAITING (on object monitor)2020-06-02T19:01:32.1712729Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1713189Z - waiting on &lt;0x00000000816a4ab0&gt; (a java.lang.UNIXProcess)2020-06-02T19:01:32.1713521Z at java.lang.Object.wait(Object.java:502)2020-06-02T19:01:32.1713849Z at java.lang.UNIXProcess.waitFor(UNIXProcess.java:395)2020-06-02T19:01:32.1714370Z - locked &lt;0x00000000816a4ab0&gt; (a java.lang.UNIXProcess)2020-06-02T19:01:32.1714778Z at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:364)2020-06-02T19:01:32.1715244Z at org.apache.commons.exec.DefaultExecutor.access$200(DefaultExecutor.java:48)2020-06-02T19:01:32.1715710Z at org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:200)2020-06-02T19:01:32.1716078Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1716292Z 2020-06-02T19:01:32.1716642Z "process reaper" #71 daemon prio=10 os_prio=0 tid=0x00007f61a805f000 nid=0x2654 runnable [0x00007f61a3441000]2020-06-02T19:01:32.1717218Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1717519Z at java.lang.UNIXProcess.waitForProcessExit(Native Method)2020-06-02T19:01:32.1717906Z at java.lang.UNIXProcess.lambda$initStreams$3(UNIXProcess.java:289)2020-06-02T19:01:32.1718285Z at java.lang.UNIXProcess$$Lambda$7/861659238.run(Unknown Source)2020-06-02T19:01:32.1718721Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2020-06-02T19:01:32.1719318Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1719700Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1719908Z 2020-06-02T19:01:32.1720537Z "Flink-DispatcherRestEndpoint-thread-2" #66 daemon prio=5 os_prio=0 tid=0x00007f614c002800 nid=0x2506 waiting on condition [0x00007f61a2bbc000]2020-06-02T19:01:32.1721013Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1721288Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1721899Z - parking to wait for &lt;0x0000000081731ff8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1722366Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1722891Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1723582Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)2020-06-02T19:01:32.1724166Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1724705Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1725166Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1725653Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1726050Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1726238Z 2020-06-02T19:01:32.1727051Z "mini-cluster-io-thread-10" #64 daemon prio=5 os_prio=0 tid=0x00007f613c00b000 nid=0x236e waiting on condition [0x00007f61a06a1000]2020-06-02T19:01:32.1727500Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1727795Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1728406Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1728888Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1729397Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1729941Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1730413Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1730874Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1731360Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1731740Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1731947Z 2020-06-02T19:01:32.1732513Z "mini-cluster-io-thread-9" #63 daemon prio=5 os_prio=0 tid=0x00007f613c006800 nid=0x236d waiting on condition [0x00007f61a07a2000]2020-06-02T19:01:32.1732963Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1733237Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1733844Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1734329Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1734836Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1735379Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1735833Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1736314Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1737055Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1737598Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1737787Z 2020-06-02T19:01:32.1738405Z "jobmanager-future-thread-1" #62 daemon prio=5 os_prio=0 tid=0x00007f613c01c000 nid=0x236c waiting on condition [0x00007f61a08a3000]2020-06-02T19:01:32.1738863Z java.lang.Thread.State: TIMED_WAITING (parking)2020-06-02T19:01:32.1739148Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1739755Z - parking to wait for &lt;0x00000000816a4c90&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1740227Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-06-02T19:01:32.1740771Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)2020-06-02T19:01:32.1741390Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)2020-06-02T19:01:32.1742053Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1742594Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1743076Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1743544Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1743942Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1744129Z 2020-06-02T19:01:32.1744722Z "mini-cluster-io-thread-8" #59 daemon prio=5 os_prio=0 tid=0x00007f6190005000 nid=0x2369 waiting on condition [0x00007f61a0ba6000]2020-06-02T19:01:32.1745147Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1745440Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1746027Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1746553Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1747227Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1747753Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1748230Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1748691Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1749186Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1749585Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1749771Z 2020-06-02T19:01:32.1750347Z "mini-cluster-io-thread-7" #58 daemon prio=5 os_prio=0 tid=0x00007f6190003000 nid=0x2368 waiting on condition [0x00007f61a0ca7000]2020-06-02T19:01:32.1750793Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1751092Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1751686Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1752170Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1752679Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1753226Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1753703Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1754167Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1754652Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1755031Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1755240Z 2020-06-02T19:01:32.1755806Z "mini-cluster-io-thread-6" #57 daemon prio=5 os_prio=0 tid=0x00007f6190001800 nid=0x2367 waiting on condition [0x00007f61a0da8000]2020-06-02T19:01:32.1756340Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1756674Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1757414Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1757914Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1758437Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1758963Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1759434Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1759895Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1760468Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1760864Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1761052Z 2020-06-02T19:01:32.1761621Z "mini-cluster-io-thread-5" #56 daemon prio=5 os_prio=0 tid=0x00007f6150008000 nid=0x2366 waiting on condition [0x00007f61a0ea9000]2020-06-02T19:01:32.1762063Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1762352Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1762938Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1763417Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1763926Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1764464Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1764944Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1765409Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1765886Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1766284Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1766511Z 2020-06-02T19:01:32.1767253Z "mini-cluster-io-thread-4" #55 daemon prio=5 os_prio=0 tid=0x00007f6300311000 nid=0x2365 waiting on condition [0x00007f61a0faa000]2020-06-02T19:01:32.1767692Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1767965Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1768575Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1769051Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1769558Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1770111Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1770563Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1771042Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1771526Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1771906Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1772094Z 2020-06-02T19:01:32.1772676Z "mini-cluster-io-thread-3" #54 daemon prio=5 os_prio=0 tid=0x00007f6198013800 nid=0x2364 waiting on condition [0x00007f61a10ab000]2020-06-02T19:01:32.1773113Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1773526Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1774138Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1774697Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1775231Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1775770Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1776225Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1776747Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1777357Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1777752Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1777943Z 2020-06-02T19:01:32.1778554Z "mini-cluster-io-thread-2" #53 daemon prio=5 os_prio=0 tid=0x00007f630030c800 nid=0x2363 waiting on condition [0x00007f61a11ac000]2020-06-02T19:01:32.1779065Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1779372Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1779968Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1780447Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1780980Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1781503Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1781977Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1782451Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1782920Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1783316Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1783510Z 2020-06-02T19:01:32.1784119Z "Flink-DispatcherRestEndpoint-thread-1" #52 daemon prio=5 os_prio=0 tid=0x00007f6161fad000 nid=0x2362 waiting on condition [0x00007f61a12ad000]2020-06-02T19:01:32.1784565Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1784856Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1785445Z - parking to wait for &lt;0x0000000081731ff8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1785925Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1786475Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1787228Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)2020-06-02T19:01:32.1787826Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1788354Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1788830Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1789300Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1789691Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1789880Z 2020-06-02T19:01:32.1790488Z "mini-cluster-io-thread-1" #51 daemon prio=5 os_prio=0 tid=0x00007f6161faa800 nid=0x2361 waiting on condition [0x00007f61a13ae000]2020-06-02T19:01:32.1790911Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1791202Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1791786Z - parking to wait for &lt;0x0000000081f00080&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1792265Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1792794Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1793408Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1793880Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1794359Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1794829Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1795224Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1795411Z 2020-06-02T19:01:32.1796007Z "flink-rest-server-netty-boss-thread-1" #50 daemon prio=5 os_prio=0 tid=0x00007f6161fa5000 nid=0x2360 runnable [0x00007f61a14af000]2020-06-02T19:01:32.1796466Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1796778Z at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)2020-06-02T19:01:32.1797363Z at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)2020-06-02T19:01:32.1797790Z at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)2020-06-02T19:01:32.1798206Z at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)2020-06-02T19:01:32.1798866Z - locked &lt;0x0000000081f004e8&gt; (a org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySet)2020-06-02T19:01:32.1799476Z - locked &lt;0x0000000081f00500&gt; (a java.util.Collections$UnmodifiableSet)2020-06-02T19:01:32.1799991Z - locked &lt;0x0000000081f004a0&gt; (a sun.nio.ch.EPollSelectorImpl)2020-06-02T19:01:32.1800359Z at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)2020-06-02T19:01:32.1800875Z at org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)2020-06-02T19:01:32.1801498Z at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:806)2020-06-02T19:01:32.1802036Z at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:454)2020-06-02T19:01:32.1802624Z at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)2020-06-02T19:01:32.1803260Z at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)2020-06-02T19:01:32.1803690Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1803895Z 2020-06-02T19:01:32.1804241Z "IOManager reader thread #1" #45 daemon prio=5 os_prio=0 tid=0x00007f63014fa000 nid=0x235d waiting on condition [0x00007f61a1db0000]2020-06-02T19:01:32.1804685Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1804981Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1805571Z - parking to wait for &lt;0x0000000081901368&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1806053Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1806604Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1807373Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1807892Z at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$ReaderThread.run(IOManagerAsync.java:354)2020-06-02T19:01:32.1808204Z 2020-06-02T19:01:32.1808550Z "IOManager writer thread #1" #44 daemon prio=5 os_prio=0 tid=0x00007f63014f9000 nid=0x235c waiting on condition [0x00007f61a1eb1000]2020-06-02T19:01:32.1808996Z java.lang.Thread.State: WAITING (parking)2020-06-02T19:01:32.1809285Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1809897Z - parking to wait for &lt;0x0000000081977a50&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1810382Z at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2020-06-02T19:01:32.1810893Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)2020-06-02T19:01:32.1811530Z at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)2020-06-02T19:01:32.1812047Z at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$WriterThread.run(IOManagerAsync.java:460)2020-06-02T19:01:32.1812361Z 2020-06-02T19:01:32.1812888Z "Timer-2" #42 daemon prio=5 os_prio=0 tid=0x00007f63014c0000 nid=0x235b in Object.wait() [0x00007f61a1fb2000]2020-06-02T19:01:32.1813326Z java.lang.Thread.State: TIMED_WAITING (on object monitor)2020-06-02T19:01:32.1813641Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1814113Z - waiting on &lt;0x0000000081901c00&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1814450Z at java.util.TimerThread.mainLoop(Timer.java:552)2020-06-02T19:01:32.1814940Z - locked &lt;0x0000000081901c00&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1815270Z at java.util.TimerThread.run(Timer.java:505)2020-06-02T19:01:32.1815469Z 2020-06-02T19:01:32.1816092Z "Timer-1" #40 daemon prio=5 os_prio=0 tid=0x00007f63014be000 nid=0x235a in Object.wait() [0x00007f61a20b3000]2020-06-02T19:01:32.1816569Z java.lang.Thread.State: TIMED_WAITING (on object monitor)2020-06-02T19:01:32.1817028Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1817513Z - waiting on &lt;0x0000000081901570&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1817846Z at java.util.TimerThread.mainLoop(Timer.java:552)2020-06-02T19:01:32.1818339Z - locked &lt;0x0000000081901570&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1818650Z at java.util.TimerThread.run(Timer.java:505)2020-06-02T19:01:32.1818862Z 2020-06-02T19:01:32.1819196Z "BLOB Server listener at 39424" #36 daemon prio=5 os_prio=0 tid=0x00007f63014bb000 nid=0x2359 runnable [0x00007f61a21b4000]2020-06-02T19:01:32.1819610Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1819903Z at java.net.PlainSocketImpl.socketAccept(Native Method)2020-06-02T19:01:32.1820304Z at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)2020-06-02T19:01:32.1820726Z at java.net.ServerSocket.implAccept(ServerSocket.java:560)2020-06-02T19:01:32.1821107Z at java.net.ServerSocket.accept(ServerSocket.java:528)2020-06-02T19:01:32.1821511Z at org.apache.flink.runtime.blob.BlobServer.run(BlobServer.java:262)2020-06-02T19:01:32.1821756Z 2020-06-02T19:01:32.1822269Z "Timer-0" #37 daemon prio=5 os_prio=0 tid=0x00007f63014a5800 nid=0x2358 in Object.wait() [0x00007f61a22b5000]2020-06-02T19:01:32.1822709Z java.lang.Thread.State: TIMED_WAITING (on object monitor)2020-06-02T19:01:32.1823030Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1823478Z - waiting on &lt;0x0000000081901740&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1823816Z at java.util.TimerThread.mainLoop(Timer.java:552)2020-06-02T19:01:32.1824304Z - locked &lt;0x0000000081901740&gt; (a java.util.TaskQueue)2020-06-02T19:01:32.1824629Z at java.util.TimerThread.run(Timer.java:505)2020-06-02T19:01:32.1824823Z 2020-06-02T19:01:32.1825397Z "flink-metrics-scheduler-1" #32 prio=5 os_prio=0 tid=0x00007f6301468000 nid=0x2354 waiting on condition [0x00007f61a26b9000]2020-06-02T19:01:32.1825831Z java.lang.Thread.State: TIMED_WAITING (sleeping)2020-06-02T19:01:32.1826134Z at java.lang.Thread.sleep(Native Method)2020-06-02T19:01:32.1826559Z at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)2020-06-02T19:01:32.1827253Z at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)2020-06-02T19:01:32.1827792Z at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)2020-06-02T19:01:32.1828191Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1828392Z 2020-06-02T19:01:32.1829014Z "flink-scheduler-1" #27 prio=5 os_prio=0 tid=0x00007f6300ec1800 nid=0x22f5 waiting on condition [0x00007f61a2fbe000]2020-06-02T19:01:32.1829625Z java.lang.Thread.State: TIMED_WAITING (sleeping)2020-06-02T19:01:32.1830040Z at java.lang.Thread.sleep(Native Method)2020-06-02T19:01:32.1830784Z at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)2020-06-02T19:01:32.1831508Z at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)2020-06-02T19:01:32.1832253Z at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)2020-06-02T19:01:32.1832842Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1833195Z 2020-06-02T19:01:32.1833740Z "process reaper" #24 daemon prio=10 os_prio=0 tid=0x00007f61a8048000 nid=0x2222 waiting on condition [0x00007f61a3a81000]2020-06-02T19:01:32.1834332Z java.lang.Thread.State: TIMED_WAITING (parking)2020-06-02T19:01:32.1834709Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1835502Z - parking to wait for &lt;0x0000000080ba0518&gt; (a java.util.concurrent.SynchronousQueue$TransferStack)2020-06-02T19:01:32.1836129Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-06-02T19:01:32.1837204Z at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)2020-06-02T19:01:32.1837935Z at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)2020-06-02T19:01:32.1838607Z at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)2020-06-02T19:01:32.1839184Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)2020-06-02T19:01:32.1839801Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1840433Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1840957Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1841215Z 2020-06-02T19:01:32.1842084Z "surefire-forkedjvm-ping-30s" #23 daemon prio=5 os_prio=0 tid=0x00007f63003c3000 nid=0x221f waiting on condition [0x00007f61b07c6000]2020-06-02T19:01:32.1842680Z java.lang.Thread.State: TIMED_WAITING (parking)2020-06-02T19:01:32.1843116Z at sun.misc.Unsafe.park(Native Method)2020-06-02T19:01:32.1843930Z - parking to wait for &lt;0x0000000080b92410&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)2020-06-02T19:01:32.1844585Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2020-06-02T19:01:32.1845348Z at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)2020-06-02T19:01:32.1846221Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)2020-06-02T19:01:32.1847278Z at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)2020-06-02T19:01:32.1848018Z at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)2020-06-02T19:01:32.1848653Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)2020-06-02T19:01:32.1849341Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2020-06-02T19:01:32.1849850Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1850130Z 2020-06-02T19:01:32.1850983Z "surefire-forkedjvm-command-thread" #22 daemon prio=5 os_prio=0 tid=0x00007f63003ac000 nid=0x221c runnable [0x00007f61b0ad1000]2020-06-02T19:01:32.1851572Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1851989Z at java.io.FileInputStream.readBytes(Native Method)2020-06-02T19:01:32.1852454Z at java.io.FileInputStream.read(FileInputStream.java:255)2020-06-02T19:01:32.1852980Z at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)2020-06-02T19:01:32.1853504Z at java.io.BufferedInputStream.read(BufferedInputStream.java:265)2020-06-02T19:01:32.1854228Z - locked &lt;0x0000000080c53790&gt; (a java.io.BufferedInputStream)2020-06-02T19:01:32.1854701Z at java.io.DataInputStream.readInt(DataInputStream.java:387)2020-06-02T19:01:32.1855323Z at org.apache.maven.surefire.booter.MasterProcessCommand.decode(MasterProcessCommand.java:115)2020-06-02T19:01:32.1856252Z at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:391)2020-06-02T19:01:32.1857036Z at java.lang.Thread.run(Thread.java:748)2020-06-02T19:01:32.1857304Z 2020-06-02T19:01:32.1857755Z "Service Thread" #21 daemon prio=9 os_prio=0 tid=0x00007f63002d5000 nid=0x221a runnable [0x0000000000000000]2020-06-02T19:01:32.1858280Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1858532Z 2020-06-02T19:01:32.1858991Z "C1 CompilerThread14" #20 daemon prio=9 os_prio=0 tid=0x00007f63002c8000 nid=0x2219 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1859548Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1859786Z 2020-06-02T19:01:32.1860247Z "C1 CompilerThread13" #19 daemon prio=9 os_prio=0 tid=0x00007f63002c6000 nid=0x2218 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1860802Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1861042Z 2020-06-02T19:01:32.1861653Z "C1 CompilerThread12" #18 daemon prio=9 os_prio=0 tid=0x00007f63002c4000 nid=0x2217 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1862227Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1862465Z 2020-06-02T19:01:32.1862931Z "C1 CompilerThread11" #17 daemon prio=9 os_prio=0 tid=0x00007f63002c2000 nid=0x2216 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1863458Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1863711Z 2020-06-02T19:01:32.1864153Z "C1 CompilerThread10" #16 daemon prio=9 os_prio=0 tid=0x00007f63002c0000 nid=0x2215 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1864696Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1864932Z 2020-06-02T19:01:32.1865389Z "C2 CompilerThread9" #15 daemon prio=9 os_prio=0 tid=0x00007f63002bd000 nid=0x2214 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1865921Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1866175Z 2020-06-02T19:01:32.1866632Z "C2 CompilerThread8" #14 daemon prio=9 os_prio=0 tid=0x00007f63002bb800 nid=0x2213 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1867418Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1867660Z 2020-06-02T19:01:32.1868123Z "C2 CompilerThread7" #13 daemon prio=9 os_prio=0 tid=0x00007f63002b9000 nid=0x2212 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1868652Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1868908Z 2020-06-02T19:01:32.1869440Z "C2 CompilerThread6" #12 daemon prio=9 os_prio=0 tid=0x00007f63002b7800 nid=0x2211 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1869969Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1870199Z 2020-06-02T19:01:32.1870646Z "C2 CompilerThread5" #11 daemon prio=9 os_prio=0 tid=0x00007f63002b5000 nid=0x2210 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1871166Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1871396Z 2020-06-02T19:01:32.1871860Z "C2 CompilerThread4" #10 daemon prio=9 os_prio=0 tid=0x00007f63002b3000 nid=0x220f waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1872387Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1872635Z 2020-06-02T19:01:32.1873060Z "C2 CompilerThread3" #9 daemon prio=9 os_prio=0 tid=0x00007f63002a9000 nid=0x220e waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1873590Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1873815Z 2020-06-02T19:01:32.1874267Z "C2 CompilerThread2" #8 daemon prio=9 os_prio=0 tid=0x00007f63002a6800 nid=0x220d waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1874805Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1875035Z 2020-06-02T19:01:32.1875466Z "C2 CompilerThread1" #7 daemon prio=9 os_prio=0 tid=0x00007f63002a4800 nid=0x220c waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1875992Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1876218Z 2020-06-02T19:01:32.1876676Z "C2 CompilerThread0" #6 daemon prio=9 os_prio=0 tid=0x00007f63002a2800 nid=0x220b waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1877580Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1877815Z 2020-06-02T19:01:32.1878222Z "Signal Dispatcher" #5 daemon prio=9 os_prio=0 tid=0x00007f63002a0800 nid=0x220a runnable [0x0000000000000000]2020-06-02T19:01:32.1878748Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1878977Z 2020-06-02T19:01:32.1879472Z "Surrogate Locker Thread (Concurrent GC)" #4 daemon prio=9 os_prio=0 tid=0x00007f630029f000 nid=0x2209 waiting on condition [0x0000000000000000]2020-06-02T19:01:32.1880026Z java.lang.Thread.State: RUNNABLE2020-06-02T19:01:32.1880278Z 2020-06-02T19:01:32.1880699Z "Finalizer" #3 daemon prio=8 os_prio=0 tid=0x00007f630026e800 nid=0x2208 in Object.wait() [0x00007f620958e000]2020-06-02T19:01:32.1881124Z java.lang.Thread.State: WAITING (on object monitor)2020-06-02T19:01:32.1881408Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1882065Z - waiting on &lt;0x0000000080c53a60&gt; (a java.lang.ref.ReferenceQueue$Lock)2020-06-02T19:01:32.1882609Z at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)2020-06-02T19:01:32.1883144Z - locked &lt;0x0000000080c53a60&gt; (a java.lang.ref.ReferenceQueue$Lock)2020-06-02T19:01:32.1883522Z at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)2020-06-02T19:01:32.1883906Z at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)2020-06-02T19:01:32.1884151Z 2020-06-02T19:01:32.1884675Z "Reference Handler" #2 daemon prio=10 os_prio=0 tid=0x00007f630026a000 nid=0x2207 in Object.wait() [0x00007f620968f000]2020-06-02T19:01:32.1885086Z java.lang.Thread.State: WAITING (on object monitor)2020-06-02T19:01:32.1885390Z at java.lang.Object.wait(Native Method)2020-06-02T19:01:32.1885872Z - waiting on &lt;0x0000000080c53a50&gt; (a java.lang.ref.Reference$Lock)2020-06-02T19:01:32.1886186Z at java.lang.Object.wait(Object.java:502)2020-06-02T19:01:32.1886587Z at java.lang.ref.Reference.tryHandlePending(Reference.java:191)2020-06-02T19:01:32.1887443Z - locked &lt;0x0000000080c53a50&gt; (a java.lang.ref.Reference$Lock)2020-06-02T19:01:32.1887838Z at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)2020-06-02T19:01:32.1888077Z 2020-06-02T19:01:32.1888380Z "main" #1 prio=5 os_prio=0 tid=0x00007f630000b800 nid=0x21cf waiting on condition [0x00007f6306d43000]2020-06-02T19:01:32.1888757Z java.lang.Thread.State: TIMED_WAITING (sleeping)2020-06-02T19:01:32.1889050Z at java.lang.Thread.sleep(Native Method)2020-06-02T19:01:32.1889437Z at org.apache.commons.exec.DefaultExecuteResultHandler.waitFor(DefaultExecuteResultHandler.java:121)2020-06-02T19:01:32.1889917Z at ch.vorburger.exec.ManagedProcess.destroy(ManagedProcess.java:344)2020-06-02T19:01:32.1890289Z at ch.vorburger.mariadb4j.DB.stop(DB.java:327)2020-06-02T19:01:32.1890764Z - locked &lt;0x00000000816a5170&gt; (a ch.vorburger.mariadb4j.DB)2020-06-02T19:01:32.1891151Z at ch.vorburger.mariadb4j.junit.MariaDB4jRule.after(MariaDB4jRule.java:64)2020-06-02T19:01:32.1891571Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:50)2020-06-02T19:01:32.1892001Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-06-02T19:01:32.1892425Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-06-02T19:01:32.1892802Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-06-02T19:01:32.1893180Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2020-06-02T19:01:32.1893594Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)2020-06-02T19:01:32.1894085Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)2020-06-02T19:01:32.1894575Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2020-06-02T19:01:32.1895059Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)2020-06-02T19:01:32.1895567Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2020-06-02T19:01:32.1896211Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2020-06-02T19:01:32.1896744Z at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2020-06-02T19:01:32.1897423Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)</description>
      <version>1.12.0,1.15.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20188" opendate="2020-11-17 00:00:00" fixdate="2020-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Documentation for new File Source</summary>
      <description></description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.execution.mode.md</file>
      <file type="M">docs.content.docs.deployment.filesystems.s3.md</file>
      <file type="M">docs.content.docs.connectors.table.filesystem.md</file>
      <file type="M">docs.content.docs.connectors.datastream.streamfile.sink.md</file>
      <file type="M">docs.content.docs.connectors.datastream.overview.md</file>
      <file type="M">docs.content.docs.connectors.datastream.formats.text.files.md</file>
      <file type="M">docs.content.docs.connectors.datastream.formats.parquet.md</file>
      <file type="M">docs.content.docs.connectors.datastream.formats.azure.table.storage.md</file>
      <file type="M">docs.content.docs.connectors.datastream.file.sink.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.execution.mode.md</file>
      <file type="M">docs.content.zh.docs.deployment.filesystems.s3.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.filesystem.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.streamfile.sink.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.file.sink.md</file>
    </fixedFiles>
  </bug>
  <bug id="22070" opendate="2021-3-31 00:00:00" fixdate="2021-3-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support FileSink in PyFlink DataStream API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.connectors.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.py</file>
      <file type="M">flink-python.pyflink.common.serialization.py</file>
    </fixedFiles>
  </bug>
  <bug id="23192" opendate="2021-6-30 00:00:00" fixdate="2021-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move connector/format option classes into a common package</summary>
      <description>For built-in connectors, we need to refactor their corresponding *Options classes to … be located in a common package</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.formats.raw.RawFormatSerDeSchemaTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.formats.raw.RawFormatFactoryTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.formats.raw.RawFormatSerializationSchema.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.formats.raw.RawFormatOptions.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.formats.raw.RawFormatFactory.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.formats.raw.RawFormatDeserializationSchema.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.table.PrintConnectorITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.table.BlackHoleConnectorITCase.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.PrintSinkFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.datagen.types.DecimalDataRandomGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.DataGenTableSourceFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.BlackHoleSinkFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.PrintTableSinkFactory.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.types.RowDataGenerator.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.types.DecimalDataRandomGenerator.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.types.DataGeneratorMapper.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.SequenceGeneratorVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.RandomGeneratorVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.DataGenVisitorBase.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.DataGenTableSource.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.datagen.DataGeneratorContainer.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.DataGenTableSourceFactory.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.DataGenConnectorOptionsUtil.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.DataGenConnectorOptions.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.BlackHoleTableSinkFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.maxwell.MaxwellJsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.debezium.DebeziumJsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.canal.CanalJsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.java.org.apache.flink.formats.avro.registry.confluent.RegistryAvroFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.java.org.apache.flink.formats.avro.registry.confluent.debezium.DebeziumAvroFormatFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.main.java.org.apache.flink.connector.hbase.options.HBaseConnectorOptionsUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.main.java.org.apache.flink.connector.hbase.options.HBaseConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.src.main.java.org.apache.flink.connector.hbase2.HBase2DynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.src.main.java.org.apache.flink.connector.hbase1.HBase1DynamicTableFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="23240" opendate="2021-7-5 00:00:00" fixdate="2021-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ResumeCheckpointManuallyITCase.testExternalizedFSCheckpointsWithLocalRecoveryZookeeper fails on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19872&amp;view=logs&amp;j=b0a398c0-685b-599c-eb57-c8c2a771138e&amp;t=d13f554f-d4b9-50f8-30ee-d49c6fb0b3cc&amp;l=10186Jul 04 22:17:29 [ERROR] Tests run: 12, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 91.407 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCaseJul 04 22:17:29 [ERROR] testExternalizedFSCheckpointsWithLocalRecoveryZookeeper(org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase) Time elapsed: 31.356 s &lt;&lt;&lt; ERROR!Jul 04 22:17:29 java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Invocation of public abstract java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.cancelJob(org.apache.flink.api.common.JobID,org.apache.flink.api.common.time.Time) timed out.Jul 04 22:17:29 at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)Jul 04 22:17:29 at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)Jul 04 22:17:29 at org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase.runJobAndGetExternalizedCheckpoint(ResumeCheckpointManuallyITCase.java:303)Jul 04 22:17:29 at org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase.testExternalizedCheckpoints(ResumeCheckpointManuallyITCase.java:275)Jul 04 22:17:29 at org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase.testExternalizedFSCheckpointsWithLocalRecoveryZookeeper(ResumeCheckpointManuallyITCase.java:215)Jul 04 22:17:29 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)Jul 04 22:17:29 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)Jul 04 22:17:29 at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)Jul 04 22:17:29 at java.base/java.lang.reflect.Method.invoke(Method.java:566)Jul 04 22:17:29 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)Jul 04 22:17:29 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)Jul 04 22:17:29 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)Jul 04 22:17:29 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)Jul 04 22:17:29 at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)Jul 04 22:17:29 at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)Jul 04 22:17:29 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)Jul 04 22:17:29 at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)Jul 04 22:17:29 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)Jul 04 22:17:29 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)Jul 04 22:17:29 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)Jul 04 22:17:29 at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)Jul 04 22:17:29 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)Jul 04 22:17:29 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)Jul 04 22:17:29 at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)Jul 04 22:17:29 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)Jul 04 22:17:29 at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)Jul 04 22:17:29 at org.junit.rules.RunRules.evaluate(RunRules.java:20)Jul 04 22:17:29 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)Jul 04 22:17:29 at org.junit.runners.ParentRunner.run(ParentRunner.java:413)Jul 04 22:17:29 at org.junit.runners.Suite.runChild(Suite.java:128)Jul 04 22:17:29 at org.junit.runners.Suite.runChild(Suite.java:27)Jul 04 22:17:29 at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)Jul 04 22:17:29 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)Jul 04 22:17:29 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)Jul 04 22:17:29 at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)Jul 04 22:17:29 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)Jul 04 22:17:29 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)Jul 04 22:17:29 at org.junit.runners.ParentRunner.run(ParentRunner.java:413)Jul 04 22:17:29 at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)Jul 04 22:17:29 at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)Jul 04 22:17:29 at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)Jul 04 22:17:29 at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)Jul 04 22:17:29 at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)Jul 04 22:17:29 at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)Jul 04 22:17:29 at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)Jul 04 22:17:29 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)Jul 04 22:17:29 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)Jul 04 22:17:29 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)Jul 04 22:17:29 Caused by: java.util.concurrent.TimeoutException: Invocation of public abstract java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.cancelJob(org.apache.flink.api.common.JobID,org.apache.flink.api.common.time.Time) timed out.Jul 04 22:17:29 at com.sun.proxy.$Proxy30.cancelJob(Unknown Source)Jul 04 22:17:29 at org.apache.flink.runtime.minicluster.MiniCluster.lambda$cancelJob$7(MiniCluster.java:716)Jul 04 22:17:29 at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:680)Jul 04 22:17:29 at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)Jul 04 22:17:29 at java.base/java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:2094)Jul 04 22:17:29 at org.apache.flink.runtime.minicluster.MiniCluster.runDispatcherCommand(MiniCluster.java:758)Jul 04 22:17:29 at org.apache.flink.runtime.minicluster.MiniCluster.cancelJob(MiniCluster.java:715)Jul 04 22:17:29 at org.apache.flink.client.program.MiniClusterClient.cancel(MiniClusterClient.java:83)Jul 04 22:17:29 ... 46 moreJul 04 22:17:29 Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/dispatcher_2#-1806874751]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.Jul 04 22:17:29 at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)Jul 04 22:17:29 at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)Jul 04 22:17:29 at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)Jul 04 22:17:29 at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)Jul 04 22:17:29 at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)Jul 04 22:17:29 at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)Jul 04 22:17:29 at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)Jul 04 22:17:29 at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)Jul 04 22:17:29 at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)Jul 04 22:17:29 at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)Jul 04 22:17:29 at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)Jul 04 22:17:29 at java.base/java.lang.Thread.run(Thread.java:834)</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.TestingResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderChangeClusterComponentsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="23395" opendate="2021-7-15 00:00:00" fixdate="2021-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Okhttp to 3.14.9</summary>
      <description>We currently use 3 different version of Okhttp, which are partially lagging behind the last 3.X version by quite a bit.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-metrics.flink-metrics-datadog.pom.xml</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-metrics-reporter-prometheus-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23493" opendate="2021-7-26 00:00:00" fixdate="2021-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>python tests hang on Azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20898&amp;view=logs&amp;j=821b528f-1eed-5598-a3b4-7f748b13f261&amp;t=4fad9527-b9a5-5015-1b70-8356e5c91490&amp;l=22829</description>
      <version>1.14.0,1.13.1,1.12.4,1.15.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.sdk.worker.main.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.boot.py</file>
    </fixedFiles>
  </bug>
  <bug id="23798" opendate="2021-8-16 00:00:00" fixdate="2021-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid using reflection to get filter when partition filter is enabled</summary>
      <description>FLINK-20496 introduce partitioned index &amp; filter to Flink. However, RocksDB only support new full format of filter in this feature, and we need to replace previous filter if user enabled. Previous implementation use reflection to get the filter and we could use API to get that after upgrading to newer version.</description>
      <version>None</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainerTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainer.java</file>
    </fixedFiles>
  </bug>
  <bug id="23843" opendate="2021-8-17 00:00:00" fixdate="2021-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exceptions during "SplitEnumeratorContext.runInCoordinatorThread()" should cause Global Failure instead of Process Kill</summary>
      <description>Currently, when a the method "SplitEnumeratorContext.runInCoordinatorThread()" throws an exception, the effect is a process kill of the JobManager process.The chain how the process kill happens is: An exception bubbling up in the executor, killing the executor thread The executor starts a replacement thread, which is forbidden by the thread factory (as a safety net) and causes a process kill.We should prevent such exceptions from bubbling up in the coordinator executor.</description>
      <version>1.13.2,1.14.4,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0,1.13.7</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorContextTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.MockOperatorCoordinatorContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="23902" opendate="2021-8-20 00:00:00" fixdate="2021-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive version 3.1.3</summary>
      <description>Make flink support Hive version 3.1.3 version.</description>
      <version>1.14.0,1.15.0,1.15.1</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV310.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">docs.content.docs.connectors.table.hive.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="24001" opendate="2021-8-26 00:00:00" fixdate="2021-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support evaluating individual window table-valued function in runtime</summary>
      <description>Currently, window table-valued function has to be used with other window operation, such as window aggregate, window topN and window join. In the ticket, we aim to support evaluating individual window table-valued function in runtime, which means, introduce an operator to handle this.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.window.WindowTableFunctionOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.window.WindowTableFunctionOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="24020" opendate="2021-8-27 00:00:00" fixdate="2021-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Aggregate HTTP requests before custom netty handers are getting the data</summary>
      <description>Custom netty handlers can do authentication (amongst other possibilities).This requires that the handlers are getting the whole HttpRequest content and not just partial data.At the moment it's not implemented this way which ends-up in flaky behaviour.Namely sometimes for example History server responds properly (when the request fits into one netty chunk) but sometimes authentication fails (when the request split into multiple netty chunks).</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.utils.WebFrontendBootstrap.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.HttpRequestHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="24036" opendate="2021-8-28 00:00:00" fixdate="2021-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SSL cannot be installed on CI</summary>
      <description># install libssl1.0.0 for netty tcnativewget http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.6_amd64.debsudo apt install ./libssl1.0.0_1.0.2n-1ubuntu5.6_amd64.deb--2021-08-27 20:48:49-- http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.6_amd64.debResolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 91.189.91.38, 2001:67c:1562::15, ...Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.HTTP request sent, awaiting response... 404 Not Found2021-08-27 20:48:49 ERROR 404: Not Found.</description>
      <version>1.14.0,1.12.5,1.13.2,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="24043" opendate="2021-8-30 00:00:00" fixdate="2021-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reuse the code of &amp;#39;check savepoint preconditions&amp;#39;.</summary>
      <description>here</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.stopwithsavepoint.StopWithSavepointTerminationManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.Executing.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.SavepointRestoreSettings.java</file>
    </fixedFiles>
  </bug>
  <bug id="24130" opendate="2021-9-2 00:00:00" fixdate="2021-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RowDataSerializerTest fails on Azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23376&amp;view=logs&amp;j=fc5181b0-e452-5c8f-68de-1097947f6483&amp;t=995c650b-6573-581c-9ce6-7ad4cc038461&amp;l=30176Sep 02 10:12:18 [ERROR] testSerializedCopyAsSequence Time elapsed: 0.009 s &lt;&lt;&lt; FAILURE!Sep 02 10:12:18 java.lang.AssertionError: Exception in test: Row arity of input element does not match serializers.Sep 02 10:12:18 at org.junit.Assert.fail(Assert.java:89)Sep 02 10:12:18 at org.apache.flink.api.common.typeutils.SerializerTestBase.testSerializedCopyAsSequence(SerializerTestBase.java:429)Sep 02 10:12:18 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)Sep 02 10:12:18 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)Sep 02 10:12:18 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)Sep 02 10:12:18 at java.lang.reflect.Method.invoke(Method.java:498)Sep 02 10:12:18 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)Sep 02 10:12:18 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)Sep 02 10:12:18 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)Sep 02 10:12:18 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)Sep 02 10:12:18 at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)Sep 02 10:12:18 at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)Sep 02 10:12:18 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)Sep 02 10:12:18 at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)Sep 02 10:12:18 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)Sep 02 10:12:18 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)Sep 02 10:12:18 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)Sep 02 10:12:18 at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)Sep 02 10:12:18 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)Sep 02 10:12:18 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)Sep 02 10:12:18 at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)Sep 02 10:12:18 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)Sep 02 10:12:18 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)Sep 02 10:12:18 at org.junit.runners.ParentRunner.run(ParentRunner.java:413)Sep 02 10:12:18 at org.junit.runner.JUnitCore.run(JUnitCore.java:137)Sep 02 10:12:18 at org.junit.runner.JUnitCore.run(JUnitCore.java:115)Sep 02 10:12:18 at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)Sep 02 10:12:18 at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)Sep 02 10:12:18 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)Sep 02 10:12:18 at java.util.Iterator.forEachRemaining(Iterator.java:116)Sep 02 10:12:18 at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)Sep 02 10:12:18 at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)Sep 02 10:12:18 at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)Sep 02 10:12:18 at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)Sep 02 10:12:18 at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)Sep 02 10:12:18 at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)Sep 02 10:12:18 at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)Sep 02 10:12:18 at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)Sep 02 10:12:18 at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)Sep 02 10:12:18 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)Sep 02 10:12:18 at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)Sep 02 10:12:18 at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)Sep 02 10:12:18 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)Sep 02 10:12:18 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)Sep 02 10:12:18 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)Sep 02 10:12:18 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:116)Sep 02 10:12:18 at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)Sep 02 10:12:18 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)Sep 02 10:12:18 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)Sep 02 10:12:18 at org.apache.flink.table.runtime.typeutils.RowDataSerializer.toBinaryRow(RowDataSerializer.java:199)Sep 02 10:12:18 at org.apache.flink.table.runtime.typeutils.RowDataSerializer.serialize(RowDataSerializer.java:103)Sep 02 10:12:18 at org.apache.flink.table.runtime.typeutils.RowDataSerializer.serialize(RowDataSerializer.java:48)Sep 02 10:12:18 at org.apache.flink.api.common.typeutils.SerializerTestBase$SerializerRunner.run(SerializerTestBase.java:580)</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.util.StreamRecordUtils.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.sink.SinkUpsertMaterializerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="24155" opendate="2021-9-3 00:00:00" fixdate="2021-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate documentation for how to configure the CheckpointFailureManager</summary>
      <description>Documentation added in FLINK-23916 should be translated to it's Chinese counterpart. Note that this applies to three separate commits:merged to master as cd01d4c0279merged to release-1.14 as 2e769746bf2merged to release-1.13 as e1a71219454</description>
      <version>1.14.0,1.13.2,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.checkpointing.md</file>
    </fixedFiles>
  </bug>
  <bug id="24181" opendate="2021-9-7 00:00:00" fixdate="2021-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update org.jsoup.jsoup to 1.14.2</summary>
      <description>Update org.jsoup.jsoup to at least 1.14.2 to address CVE GHSA-m72m-mhq2-9p6cFlink itself isn't directly affected, but it's still good to update the dependency to avoid any scanners reporting vulnerabilities in Flink</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24208" opendate="2021-9-8 00:00:00" fixdate="2021-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow idempotent savepoint triggering</summary>
      <description>As a user of Flink, I want to be able to trigger a savepoint from an external system in a way that I can detect if I have requested this savepoint already.By passing a custom ID to the savepoint request, I can check (in case of an error of the original request, or the external system) if the request has been made already.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.savepoints.SavepointTriggerRequestBodyTest.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
      <file type="M">docs.content.docs.ops.rest.api.md</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.SavepointHandlerRequestBodyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.savepoints.StopWithSavepointHandlersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlersTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.savepoints.stop.StopWithSavepointRequestBody.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.savepoints.SavepointTriggerRequestBody.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientSavepointTriggerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="24212" opendate="2021-9-8 00:00:00" fixdate="2021-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>kerberos krb5.conf file is mounted as empty directory, not the expected file</summary>
      <description>From FLINK-18971，we can mount kerberos krb5 conf file to pod with path /etc/krb5.conf，however if the krb5 conf file is not named krb5.conf (e.g named mykrb5.conf)，the mount path /etc/krb5.conf in pod will be an empty directory, not a file that we expect.root@mykrb5-conf-test-6dd5c76f87-vfwh5:/# ls /etc/krb5.conf/ -latotal 8drwxrwxrwx 2 root root 4096 Sep 8 10:42 .drwxr-xr-x 1 root root 4096 Sep 8 10:42 ..  The reason is that, the code  in KerberosMountDecrator#decroateFlinkPod, we create the deployment like this: ... volumeMounts: - mountPath: /etc/krb5.conf name: my-krb5conf-volume subPath: krb5.conf ... volumes: - configMap: defaultMode: 420 items: - key: mykrb5.conf path: mykrb5.conf name: my-krb5conf name: my-krb5conf-volumepath value should be set to const value "krb5.conf", not the file name that user provide (path: mykrb5.conf). we can use the yaml description file attachment to reproduce the problem.  mykrb5conf.yaml </description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.KerberosMountDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.KerberosMountDecorator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2422" opendate="2015-7-28 00:00:00" fixdate="2015-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web client is showing a blank page if "Meta refresh" is disabled in browser</summary>
      <description>A user reported via the Flink IRC channel that Firefox was showing only a blank page instead of the web client.We should add a link to that page as well, so that users can click it if the redirect doesn't work.Workaround: browse to launch.html directly.</description>
      <version>None</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.resources.web-docs.index.html</file>
    </fixedFiles>
  </bug>
  <bug id="24244" opendate="2021-9-10 00:00:00" fixdate="2021-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add logging about whether it&amp;#39;s executed in loopback mode</summary>
      <description>Currently, it's unclear whether a job is running in loopback mode or process mode, it would be great to add some logging to make it clear. This would be helpful for debugging. It makes it clear whether a failed test is running in loopback mode or process mode.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.worker.pool.service.py</file>
    </fixedFiles>
  </bug>
  <bug id="24246" opendate="2021-9-10 00:00:00" fixdate="2021-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump PulsarClient to 2.9.1 with better transaction management</summary>
      <description>Pulsar 2.9.1 has been released, the hack for getting TxnID from Pulsar Transaction instance could be removed after bumping flink-connector-pulsar's pulsar-client-all to 2.9.1.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.common.FlinkContainerWithPulsarEnvironment.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-pulsar.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.deserializer.PulsarDeserializationSchemaTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarUnorderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.utils.PulsarTransactionUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24266" opendate="2021-9-13 00:00:00" fixdate="2021-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log improvement for aborting checkpoint due to tasks are finishing</summary>
      <description>When checkpoints are aborted due to tasks are finishing when triggering, 1. There is not log in the JM side, which might cause confusion.2. When the mail is rejected, the exception description for CHECKPOINT_DECLINED_TASK_CLOSING: "Checkpoint was declined (task's operators partially closed)", does not fully match the implementation now.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureReason.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="24281" opendate="2021-9-14 00:00:00" fixdate="2021-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate all existing tests to new Kafka Sink</summary>
      <description>The FlinkKafkaProducer is deprecated since 1.14 but a lot of existing tests are still using.We should replace it with the KafkaSink because it completely subsumes it.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.KafkaSinkBuilder.java</file>
      <file type="M">flink-end-to-end-tests.flink-confluent-schema-registry.src.main.java.org.apache.flink.schema.registry.test.TestAvroConsumerConfluent.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaChangelogTableITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="24292" opendate="2021-9-15 00:00:00" fixdate="2021-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Flink&amp;#39;s Kafka examples to use KafkaSink</summary>
      <description></description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.statemachine.KafkaEventsGeneratorJob.java</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kafka-test.src.main.java.org.apache.flink.streaming.kafka.test.KafkaExample.java</file>
    </fixedFiles>
  </bug>
  <bug id="24300" opendate="2021-9-15 00:00:00" fixdate="2021-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MultipleInputOperator is running much more slowly in TPCDS</summary>
      <description>When we are running TPCDS with release 1.14 we find that the job with MultipleInputOperator is running much more slowly than before. With a binary search among the commits, we find that the issue may be introduced by FLINK-23408. At the commit 64570e4c56955713ca599fd1d7ae7be891a314c6, the job in TPCDS runs normally, as the image below illustrates:At the commit e3010c16947ed8da2ecb7d89a3aa08dacecc524a, the job q2.sql gets stuck for a pretty long time (longer than half an hour), as the image below illustrates:The detail of the job is illustrated below:The job uses a MultipleInputOperator with one normal input and two chained FileSource. It has finished reading the normal input and start to read the chained source. Each chained source has one source data fetcher.We capture the jstack of the stuck tasks and attach the file below. From the jstack.txt we can see the main thread is blocked on waiting for the lock, and the lock is held by a source data fetcher. The source data fetcher is still running but the stack keeps on CompletableFuture.cleanStack.This issue happens in a batch job. However, from where it get blocked, it seems also affects the streaming jobs.For the reference, the code of TPCDS we are running is located at https://github.com/ververica/flink-sql-benchmark/tree/dev.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.SourceOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="24305" opendate="2021-9-16 00:00:00" fixdate="2021-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BatchPandasUDAFITTests.test_over_window_aggregate_function fails on azure</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24170&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=c67e71ed-6451-5d26-8920-5a8cf9651901&amp;l=23011Sep 15 20:40:43 cls = &lt;class 'pyflink.table.tests.test_pandas_udaf.BatchPandasUDAFITTests'&gt;Sep 15 20:40:43 actual = JavaObject id=o8666Sep 15 20:40:43 expected = ['+I[1, 4.3333335, 13, 5.5, 3.0, 3.0, 4.3333335, 8.0, 5.0, 5.0]', '+I[1, 4.3333335, 5, 4.3333335, 3.0, 3.0, 2.5, 4.333....0, 4.0, 2.0]', '+I[2, 2.0, 9, 2.0, 4.0, 4.0, 2.0, 2.0, 4.0, 4.0]', '+I[3, 2.0, 3, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0]']Sep 15 20:40:43 Sep 15 20:40:43 @classmethodSep 15 20:40:43 def assert_equals(cls, actual, expected):Sep 15 20:40:43 if isinstance(actual, JavaObject):Sep 15 20:40:43 actual_py_list = cls.to_py_list(actual)Sep 15 20:40:43 else:Sep 15 20:40:43 actual_py_list = actualSep 15 20:40:43 actual_py_list.sort()Sep 15 20:40:43 expected.sort()Sep 15 20:40:43 assert len(actual_py_list) == len(expected)Sep 15 20:40:43 &gt; assert all(x == y for x, y in zip(actual_py_list, expected))Sep 15 20:40:43 E AssertionError: assert FalseSep 15 20:40:43 E + where False = all(&lt;generator object PyFlinkTestCase.assert_equals.&lt;locals&gt;.&lt;genexpr&gt; at 0x7f792d98b900&gt;)</description>
      <version>1.14.0,1.12.5,1.13.2,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  <bug id="24310" opendate="2021-9-16 00:00:00" fixdate="2021-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>A bug in the BufferingSink example in the doc</summary>
      <description>The following line in the BufferingSink on this page has a bug:if (bufferedElements.size() == threshold) {It should be &gt;= instead of == , because when restoring from a checkpoint during downscaling, the task may get more elements than the threshold. </description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.state.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.state.md</file>
    </fixedFiles>
  </bug>
  <bug id="24317" opendate="2021-9-17 00:00:00" fixdate="2021-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize the test implementation in test_flat_aggregate</summary>
      <description></description>
      <version>1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.14.0,1.13.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.row.based.operation.py</file>
    </fixedFiles>
  </bug>
  <bug id="24318" opendate="2021-9-17 00:00:00" fixdate="2021-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Casting a number to boolean has different results between &amp;#39;select&amp;#39; fields and &amp;#39;where&amp;#39; condition</summary>
      <description>The same cast in the following two sql:// SQL 1SELECT cast(0.1 as boolean)// SQL 2SELECT * from test2 where cast(0.1 as boolean)has different results.The cast result in SQL 1 is true and the cast in SQL 2 is false.</description>
      <version>None</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyJoinConditionRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SimplifyJoinConditionRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyJoinConditionRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinDependentConditionDerivationRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinConditionTypeCoerceRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinConditionEqualityTransferRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkCalcMergeRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterInCalcIntoTableSourceScanRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="24324" opendate="2021-9-17 00:00:00" fixdate="2021-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create ElasticSearch 7 Sink</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
    </fixedFiles>
  </bug>
  <bug id="24325" opendate="2021-9-17 00:00:00" fixdate="2021-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create ElasticSearch 6.8 Sink</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.PreElasticsearch6BulkProcessorIndexer.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.KeyExtractorTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.KeyExtractor.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch7SinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch7SinkBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch7SinkBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch6SinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch6SinkBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch6SinkBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBuilderBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBaseITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchSinkBuilderSupplier.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBuilderBase.java</file>
      <file type="M">tools.ci.compile.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch5-test.src.main.java.org.apache.flink.streaming.tests.Elasticsearch5SinkExample.java</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch5-test.pom.xml</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.EmbeddedElasticsearchNodeEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.test.java.org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.licenses.LICENSE.webbit</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.licenses.LICENSE.jzlib</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.licenses.LICENSE.jsr166y</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.licenses.LICENSE.joptsimple</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.licenses.LICENSE.hdrhistogram</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.licenses.LICENSE.base64</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.Elasticsearch5ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.IndexGeneratorFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.index.IndexGeneratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.util.ElasticsearchUtils.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.IndexGeneratorFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.IndexGenerator.java</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch7-test.src.main.java.org.apache.flink.streaming.tests.Elasticsearch7SinkExample.java</file>
      <file type="M">flink-end-to-end-tests.flink-quickstart-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-quickstart-test.src.main.java.org.apache.flink.quickstarts.test.Elasticsearch5SinkExample.java</file>
      <file type="M">flink-end-to-end-tests.flink-quickstart-test.src.main.scala.org.apache.flink.quickstarts.test.Elasticsearch5SinkExample.scala</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.elasticsearch-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.quickstarts.sh</file>
      <file type="M">tools.ci.java-ci-tools.src.main.resources.modules-skipping-deployment.modulelist</file>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.ElasticsearchConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.ElasticsearchConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.ElasticsearchValidationUtils.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.RequestFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.RowElasticsearchSinkFunction.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6Configuration.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7Configuration.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7ConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.RowElasticsearchEmitter.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.BulkProcessorConfig.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchEmitter.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriter.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.FlushBackoffType.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.NetworkClientConfig.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.RequestIndexer.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.connector.elasticsearch.sink.TestClient.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.connector.elasticsearch.sink.TestEmitter.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.index.AbstractTimeIndexGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.index.IndexGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.index.IndexGeneratorBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.index.IndexGeneratorFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.index.StaticIndexGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="24326" opendate="2021-9-17 00:00:00" fixdate="2021-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Docs with new Sinks</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.elasticsearch.md</file>
      <file type="M">docs.content.docs.connectors.datastream.elasticsearch.md</file>
    </fixedFiles>
  </bug>
  <bug id="24327" opendate="2021-9-17 00:00:00" fixdate="2021-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate unified Elasticsearch 7 sink with Table API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7Configuration.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriter.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.KeyExtractor.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.IndexGeneratorFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.ElasticsearchValidationUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="24331" opendate="2021-9-18 00:00:00" fixdate="2021-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PartiallyFinishedSourcesITCase fails with "No downstream received 0 from xxx;"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24287&amp;view=logs&amp;j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&amp;t=7b25afdf-cc6c-566f-5459-359dc2585798&amp;l=10945Sep 18 02:21:08 [ERROR] Tests run: 12, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 224.44 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCaseSep 18 02:21:08 [ERROR] test[complex graph SINGLE_SUBTASK, failover: true, strategy: region] Time elapsed: 28.807 s &lt;&lt;&lt; FAILURE!Sep 18 02:21:08 java.lang.AssertionError: No downstream received 0 from 00000000000000000000000000000003[0]; received: {0=OperatorFinished 00000000000000000000000000000007/0, 1=OperatorFinished 00000000000000000000000000000007/1, 2=OperatorFinished 00000000000000000000000000000007/2, 3=OperatorFinished 00000000000000000000000000000007/3}Sep 18 02:21:08 at org.junit.Assert.fail(Assert.java:89)Sep 18 02:21:08 at org.junit.Assert.assertTrue(Assert.java:42)Sep 18 02:21:08 at org.apache.flink.runtime.operators.lifecycle.validation.TestJobDataFlowValidator.lambda$checkDataFlow$1(TestJobDataFlowValidator.java:96)Sep 18 02:21:08 at java.util.HashMap.forEach(HashMap.java:1289)Sep 18 02:21:08 at org.apache.flink.runtime.operators.lifecycle.validation.TestJobDataFlowValidator.checkDataFlow(TestJobDataFlowValidator.java:94)Sep 18 02:21:08 at org.apache.flink.runtime.operators.lifecycle.validation.TestJobDataFlowValidator.checkDataFlow(TestJobDataFlowValidator.java:62)Sep 18 02:21:08 at org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase.test(PartiallyFinishedSourcesITCase.java:139)</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.graph.TestEventSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="24339" opendate="2021-9-20 00:00:00" fixdate="2021-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove guard against CRLF split across chunks</summary>
      <description>Revert FLINK-24197 once Netty was upgraded to a version that includes a fix for https://github.com/netty/netty/issues/11668.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.FileUploadHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="2434" opendate="2015-7-30 00:00:00" fixdate="2015-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>org.apache.hadoop:hadoop-yarn-common:jar with value &amp;#39;jersey-test-framework-grizzly2+&amp;#39; does not match a valid id pattern</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-shaded-hadoop.flink-shaded-include-yarn.pom.xml</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop1.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24342" opendate="2021-9-21 00:00:00" fixdate="2021-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filesystem sink does not escape right bracket in partition name</summary>
      <description>How to reproduce the problemIn the following code snippet filesystem sink creates a partition named "{date}" and writes value "1" to file.create table sink ( val int, part string) partitioned by (part) with ( 'connector' = 'filesystem', 'path' = '/tmp/sink', 'format' = 'csv');insert into sink values (1, '{date}');Expected behaviorEscaped "{" and "}" in partition name$ ls /tmp/sink/part=%7Bdate%7DActual behaviorEscaped only "{" in partition name$ ls /tmp/sink/part=%7Bdate}</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.PartitionPathUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.PartitionPathUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="24358" opendate="2021-9-23 00:00:00" fixdate="2021-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-avro-glue-schema-registry fails compiling with scala 2.12 due to dependency convergence</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24405&amp;view=logs&amp;j=ed6509f5-1153-558c-557a-5ee0afbcdf24&amp;t=241b1e5e-1a8e-5e6a-469a-a9b8cad87065&amp;l=13506[WARNING] Dependency convergence error for io.netty:netty-handler:4.1.63.Final paths to dependency are:+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-common:1.1.2 +-software.amazon.awssdk:glue:2.16.92 +-software.amazon.awssdk:netty-nio-client:2.16.92 +-io.netty:netty-codec-http:4.1.63.Final +-io.netty:netty-handler:4.1.63.Finaland+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-common:1.1.2 +-software.amazon.awssdk:glue:2.16.92 +-software.amazon.awssdk:netty-nio-client:2.16.92 +-io.netty:netty-codec-http2:4.1.63.Final +-io.netty:netty-handler:4.1.63.Finaland+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-common:1.1.2 +-software.amazon.awssdk:glue:2.16.92 +-software.amazon.awssdk:netty-nio-client:2.16.92 +-io.netty:netty-handler:4.1.63.Finaland+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-common:1.1.2 +-software.amazon.awssdk:glue:2.16.92 +-software.amazon.awssdk:netty-nio-client:2.16.92 +-com.typesafe.netty:netty-reactive-streams-http:2.0.5 +-com.typesafe.netty:netty-reactive-streams:2.0.5 +-io.netty:netty-handler:4.1.52.Final[WARNING] Dependency convergence error for org.jetbrains.kotlin:kotlin-stdlib:1.3.50 paths to dependency are:+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-serde:1.1.2 +-com.kjetland:mbknor-jackson-jsonschema_2.12:1.0.39 +-org.jetbrains.kotlin:kotlin-scripting-compiler-embeddable:1.3.50 +-org.jetbrains.kotlin:kotlin-scripting-compiler-impl-embeddable:1.3.50 +-org.jetbrains.kotlin:kotlin-scripting-common:1.3.50 +-org.jetbrains.kotlin:kotlin-reflect:1.3.50 +-org.jetbrains.kotlin:kotlin-stdlib:1.3.50and+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-serde:1.1.2 +-com.kjetland:mbknor-jackson-jsonschema_2.12:1.0.39 +-org.jetbrains.kotlin:kotlin-scripting-compiler-embeddable:1.3.50 +-org.jetbrains.kotlin:kotlin-scripting-compiler-impl-embeddable:1.3.50 +-org.jetbrains.kotlin:kotlin-scripting-common:1.3.50 +-org.jetbrains.kotlin:kotlin-stdlib:1.3.50and+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-serde:1.1.2 +-com.kjetland:mbknor-jackson-jsonschema_2.12:1.0.39 +-org.jetbrains.kotlin:kotlin-scripting-compiler-embeddable:1.3.50 +-org.jetbrains.kotlin:kotlin-scripting-compiler-impl-embeddable:1.3.50 +-org.jetbrains.kotlin:kotlin-scripting-jvm:1.3.50 +-org.jetbrains.kotlin:kotlin-stdlib:1.3.50and+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-serde:1.1.2 +-com.kjetland:mbknor-jackson-jsonschema_2.12:1.0.39 +-org.jetbrains.kotlin:kotlin-scripting-compiler-embeddable:1.3.50 +-org.jetbrains.kotlin:kotlin-scripting-compiler-impl-embeddable:1.3.50 +-org.jetbrains.kotlin:kotlin-stdlib:1.3.50and+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-serde:1.1.2 +-com.kjetland:mbknor-jackson-jsonschema_2.12:1.0.39 +-org.jetbrains.kotlin:kotlin-scripting-compiler-embeddable:1.3.50 +-org.jetbrains.kotlin:kotlin-scripting-compiler-impl-embeddable:1.3.50 +-org.jetbrains.kotlinx:kotlinx-coroutines-core:1.1.1 +-org.jetbrains.kotlin:kotlin-stdlib:1.3.20and+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-serde:1.1.2 +-com.kjetland:mbknor-jackson-jsonschema_2.12:1.0.39 +-org.jetbrains.kotlin:kotlin-scripting-compiler-embeddable:1.3.50 +-org.jetbrains.kotlin:kotlin-stdlib:1.3.50[WARNING] Dependency convergence error for io.netty:netty-codec-http:4.1.63.Final paths to dependency are:+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-common:1.1.2 +-software.amazon.awssdk:glue:2.16.92 +-software.amazon.awssdk:netty-nio-client:2.16.92 +-io.netty:netty-codec-http:4.1.63.Finaland+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-common:1.1.2 +-software.amazon.awssdk:glue:2.16.92 +-software.amazon.awssdk:netty-nio-client:2.16.92 +-io.netty:netty-codec-http2:4.1.63.Final +-io.netty:netty-codec-http:4.1.63.Finaland+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT +-software.amazon.glue:schema-registry-common:1.1.2 +-software.amazon.awssdk:glue:2.16.92 +-software.amazon.awssdk:netty-nio-client:2.16.92 +-com.typesafe.netty:netty-reactive-streams-http:2.0.5 +-io.netty:netty-codec-http:4.1.52.Final[WARNING] Rule 0: org.apache.maven.plugins.enforcer.DependencyConvergence failed with message:Failed while enforcing releasability. See above detailed error message.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.pom.xml</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24360" opendate="2021-9-23 00:00:00" fixdate="2021-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop Scala Shell</summary>
      <description>The community has discussed and voted to drop the Scala Shell from FlinkDiscussion thread: https://lists.apache.org/thread.html/rf7a7f935c43d3e98f94193be81b69f1c0d6e60b6fa09570531c3fa67%40%3Cdev.flink.apache.org%3EVote thread: https://lists.apache.org/thread.html/r5f605db6be4788af58ffa6bcd366028e0cd9bf9df1dfeaf054cec86b%40%3Cdev.flink.apache.org%3E</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-scala-shell.start-script.start-scala-shell.sh</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellLocalStartupITCase.scala</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.jar.TestingData.scala</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.jar.package.scala</file>
      <file type="M">flink-scala-shell.src.test.resources.flink-conf.yaml</file>
      <file type="M">flink-scala-shell.src.test.java.org.apache.flink.api.java.FlinkILoopTest.java</file>
      <file type="M">flink-scala-shell.src.test.assembly.test-scalashell-customjar-assembly.xml</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkILoop.scala</file>
      <file type="M">flink-scala-shell.src.main.java.org.apache.flink.api.java.ScalaShellStreamEnvironment.java</file>
      <file type="M">flink-scala-shell.src.main.java.org.apache.flink.api.java.ScalaShellEnvironment.java</file>
      <file type="M">flink-scala-shell.src.main.java.org.apache.flink.api.java.JarHelper.java</file>
      <file type="M">flink-scala-shell.pom.xml</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.RemoteEnvironmentConfigUtils.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.LocalEnvironment.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.licenses.LICENSE.scopt</file>
      <file type="M">flink-dist.src.main.assemblies.bin-scala2.11.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">docs.content.docs.deployment.repls.scala.shell.md</file>
      <file type="M">docs.content.docs.deployment.overview.md</file>
      <file type="M">docs.content.docs.connectors.table.hive.hive.catalog.md</file>
      <file type="M">docs.content.zh.docs.deployment.repls.scala.shell.md</file>
      <file type="M">docs.content.zh.docs.deployment.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.catalog.md</file>
    </fixedFiles>
  </bug>
  <bug id="24372" opendate="2021-9-24 00:00:00" fixdate="2021-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecate Elasticsearch Sinkfunctions</summary>
      <description>Once all other tickets of FLINK-24323 are resolved we can mark the Elasticsearch sinks implementing sinkfunction as deprecated.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch7.RestClientFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch6.RestClientFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.util.RetryRejectedExecutionFailureHandler.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkFunction.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ActionRequestFailureHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="24373" opendate="2021-9-24 00:00:00" fixdate="2021-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove BETA Tag from FLIP-27 Source Docs</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.14.0,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.sources.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.sources.md</file>
    </fixedFiles>
  </bug>
  <bug id="24382" opendate="2021-9-27 00:00:00" fixdate="2021-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RecordsOut metric for sinks is inaccurate</summary>
      <description>Currently, the metric is computed on the operator level and it is assumed that every record flowing into the sink also generates one outgoing record.This is often not reasonable because the sinks can transform incoming records into multiple outgoing records, thus the metric should be implemented by the sink implementors and not be reasoned by the framework.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.SinkMetricsITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.SinkOperator.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.KafkaWriter.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.writer.FileWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.sink.writer.FileWriter.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.sink.FileSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="24397" opendate="2021-9-28 00:00:00" fixdate="2021-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce legacy in Table API connectors</summary>
      <description>The TableSchema class is now deprecated for a while but a lot of connectors still depend on it use (DynamicTableSources and DynamicTableSinks). We should reduce the usage of the TableSchema to provide a better example for external contributions and allow us eventually to drop the TableSchema.It also drops the legacy JDBC connector.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaConnectorOptionsUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSource.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.test.java.org.apache.flink.connector.hbase.util.HBaseSerdeTest.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.main.java.org.apache.flink.connector.hbase.util.HBaseTableSchema.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.main.java.org.apache.flink.connector.hbase.table.HBaseConnectorOptionsUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.main.java.org.apache.flink.connector.hbase.source.AbstractHBaseDynamicTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.src.test.java.org.apache.flink.connector.hbase2.source.HBaseRowDataAsyncLookupFunctionTest.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.src.main.java.org.apache.flink.connector.hbase2.source.HBaseDynamicTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.src.main.java.org.apache.flink.connector.hbase2.HBase2DynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.src.main.java.org.apache.flink.connector.hbase1.HBase1DynamicTableFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.TableSchemaUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.TableSchemaUtils.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.utils.JdbcTypeUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcUpsertTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcTableSourceSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcLookupTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcLookupFunctionTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcDataTypeTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalogTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalogTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.table.descriptors.JdbcValidator.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.utils.JdbcUtils.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.utils.JdbcTypeUtil.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcUpsertTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcTableSourceSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcLookupFunction.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.executor.InsertOrUpdateJdbcExecutor.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.converter.AbstractJdbcRowConverter.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="2441" opendate="2015-7-30 00:00:00" fixdate="2015-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Introduce an OpInfo object on the python side</summary>
      <description>All information required to construct on operation are currently saved in a plain dictionary on the python side, whose fields are generally accessed using a variety of string constants.so right now you find lines like: op_info[_Fields.KEYS] = keysThe following shortcomings exist in the current system: There is no central place to define default values. This is done all over the place, to some extent in a redundant way, It produces fairly long code, is surprisingly cumbersome to write.Instead i would like to add a separate OperationInfo object. This code be a special dictionary with preset values for each field, but due to points 2 and 3, Id prefer having an attribute for every field. the resulting code would look like this:op_info.keys = keysisn't that lovely.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.type.deduction.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Environment.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.DataSet.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Constants.py</file>
    </fixedFiles>
  </bug>
  <bug id="24410" opendate="2021-9-30 00:00:00" fixdate="2021-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Confluent Platform OSS version in end-to-end tests</summary>
      <description>Flink uses Confluent Platform OSS/community edition 5.0.0, which doesn't exist in a Scala 2.12 version. We should bump the used version to at least 5.2.x.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.confluent.schema.registry.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="24437" opendate="2021-10-1 00:00:00" fixdate="2021-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unhandled exception handler from CuratorFramework before closing it</summary>
      <description>With FLINK-24021 we add an unhandled exception handler to the started CuratorFramework. In order to avoid that shutting down the CuratorFramework causes triggering of this handler, we should unregister it before closing the CuratorFramework instance.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperTestEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ZooKeeperUtilsTreeCacheTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.ZooKeeperJobGraphStoreWatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperRegistryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZKCheckpointIDCounterMultiServersTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperClientHAServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="24492" opendate="2021-10-9 00:00:00" fixdate="2021-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>incorrect implicit type conversion between numeric and (var)char</summary>
      <description>The result of the sql "select 1 = '1'" is false. This is caused by the CodeGen. CodeGen  incorrectly transform this "=" to "BinaryStringData.equals (int 1)". And "&lt;&gt;" has the same wrong result.In my opinion, "=" should have the same behavior with "&gt;" and "&lt;", which have the correct results. So before calcite solves this bug or flink supports this kind of implicit type conversion, we'd better temporarily forbidding this implicit type conversion in "=" and "&lt;&gt;".</description>
      <version>None</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinConditionTypeCoerceRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="24495" opendate="2021-10-11 00:00:00" fixdate="2021-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python installdeps hangs</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24922&amp;view=logs&amp;j=821b528f-1eed-5598-a3b4-7f748b13f261&amp;t=6bb545dd-772d-5d8c-f258-f5085fba3295&amp;l=23587Oct 10 02:30:01 py38-cython create: /__w/1/s/flink-python/.tox/py38-cythonOct 10 02:30:04 py38-cython installdeps: pytest, apache-beam==2.27.0, cython==0.29.16, grpcio&gt;=1.29.0,&lt;2, grpcio-tools&gt;=1.3.5,&lt;=1.14.2, apache-flink-librariesOct 10 02:45:22 ==============================================================================Oct 10 02:45:22 Process produced no output for 900 seconds.Oct 10 02:45:22 ==============================================================================</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.dependency.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug id="24513" opendate="2021-10-12 00:00:00" fixdate="2021-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AkkaRpcSystemLoader must be an ITCase</summary>
      <description>Since the tests specifically tests the AkkaRpcSystemLoader, which relies on the flink-rpc-akka jar to be available (which runs in the package phase since FLINK-24445), it needs to be an ITCase.</description>
      <version>1.14.1,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka-loader.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcSystemLoaderTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="24538" opendate="2021-10-14 00:00:00" fixdate="2021-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ZooKeeperLeaderElectionTest.testLeaderShouldBeCorrectedWhenOverwritten fails with NPE</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25020&amp;view=logs&amp;j=f2b08047-82c3-520f-51ee-a30fd6254285&amp;t=3810d23d-4df2-586c-103c-ec14ede6af00&amp;l=7573Oct 13 22:26:04 [ERROR] Tests run: 8, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 12.355 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTestOct 13 22:26:04 [ERROR] testLeaderShouldBeCorrectedWhenOverwritten Time elapsed: 1.138 s &lt;&lt;&lt; ERROR!Oct 13 22:26:04 java.lang.NullPointerExceptionOct 13 22:26:04 at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.testLeaderShouldBeCorrectedWhenOverwritten(ZooKeeperLeaderElectionTest.java:434)</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderretrieval.SettableLeaderRetrievalServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingRetrievalBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderRetrievalDriverTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="24559" opendate="2021-10-15 00:00:00" fixdate="2021-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-rpc-akka-loader does not bundle flink-rpc-akka</summary>
      <description>Plugins are executed in the wrong order, causing packaging to occur before the dependency has been copied.</description>
      <version>1.14.1,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-rpc.flink-rpc-akka-loader.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24565" opendate="2021-10-15 00:00:00" fixdate="2021-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port Avro FileSystemFormatFactory to BulkFormat</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroFilesystemITCase.java</file>
      <file type="M">flink-formats.flink-avro.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroFileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroFileFormatFactory.java</file>
      <file type="M">flink-formats.flink-avro.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24585" opendate="2021-10-19 00:00:00" fixdate="2021-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Print the change in the size of the compacted files</summary>
      <description>LOG.info( "Compaction time cost is '{}S', target file is '{}', input files are '{}'", costSeconds, target, paths);only print the file name and time cost in compacting, maybe we need to print the size change.we have a demand in this, and have implemented it, please assign this to me, thanks</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.filesystem.stream.compact.CompactOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="24586" opendate="2021-10-19 00:00:00" fixdate="2021-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL functions should return STRING instead of VARCHAR(2000)</summary>
      <description>There are some SQL functions which currently return VARCHAR(2000). With more strict CAST behavior from FLINK-24413, this could become an issue.The following functions return VARCHAR(2000) and should be changed to return STRING instead: JSON_VALUE JSON_QUERY JSON_OBJECT JSON_ARRAYThere are also some more functions which should be evaluated: CHR REVERSE SPLIT_INDEX PARSE_URL FROM_UNIXTIME DECODE</description>
      <version>None</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTableWithProjectionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTable.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testRowTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testProcessingTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.type.FlinkReturnTypes.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.SqlJsonObjectFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.JsonFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="2460" opendate="2015-8-1 00:00:00" fixdate="2015-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ReduceOnNeighborsWithExceptionITCase failure</summary>
      <description>I noticed a build error due to failure on this case. It was on a branch of my fork, which didn't actually have anything to do with the failed test or the runtime system at all.Here's the error log: https://s3.amazonaws.com/archive.travis-ci.org/jobs/73695554/log.txt</description>
      <version>None</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SubpartitionTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpilledSubpartitionViewSyncIO.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpilledSubpartitionViewAsyncIO.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="24608" opendate="2021-10-21 00:00:00" fixdate="2021-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sinks built with the unified sink framework do not receive timestamps when used in Table API</summary>
      <description>All sinks built with the unified sink framework extract the timestamp from the internal StreamRecord. The Table API does not facilitate the timestamp field in the StreamRecord but extracts the timestamp from the actual data. We either have to use a dedicated operator before all the sinks to simulate the behavior or allow a customizable timestamp extraction during the sink translation.</description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.match.RowtimeProcessFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="24612" opendate="2021-10-21 00:00:00" fixdate="2021-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka test container creates a large amount of logs</summary>
      <description>When we use a testcontainer setup we try to forward all container STDOUT logs to the surrounding test logger. Unfortunately, Kafka loggers are by default writing a large number of logs because some of the internal loggers are defaulting to TRACE logging.A good example is this test job https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25084&amp;view=logs&amp;j=32a18cd8-d404-5807-996d-abcee436b891where one of the test was stuck and the generated artifact is ~25GB. This makes debugging very hard because the file is hard to parse.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaTransactionLogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.FlinkKafkaInternalProducerITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="24662" opendate="2021-10-27 00:00:00" fixdate="2021-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink sphinx check failed with "node class &amp;#39;meta&amp;#39; is already registered, its visitors will be overridden"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25481&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=8d78fe4f-d658-5c70-12f8-4921589024c3==========mypy checks... [SUCCESS]===========Oct 26 22:08:34 rm -rf _build/*Oct 26 22:08:34 /__w/1/s/flink-python/dev/.conda/bin/sphinx-build -b html -d _build/doctrees -a -W . _build/htmlOct 26 22:08:34 Running Sphinx v2.4.4Oct 26 22:08:34 Oct 26 22:08:34 Warning, treated as error:Oct 26 22:08:34 node class 'meta' is already registered, its visitors will be overriddenOct 26 22:08:34 Makefile:76: recipe for target 'html' failed</description>
      <version>1.12.0,1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug id="24676" opendate="2021-10-28 00:00:00" fixdate="2021-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema does not match if explain insert statement with partial column</summary>
      <description>create table MyTable (a int, b int) with ('connector' = 'datagen');create table MySink (c int, d int) with ('connector' = 'print');explain plan for insert into MySink(d) select a from MyTable where a &gt; 10;If execute the above statement, we will get the following exceptionorg.apache.flink.table.api.ValidationException: Column types of query result and sink for registered table 'default_catalog.default_database.MySink' do not match.Cause: Different number of columns.Query schema: &amp;#91;a: BIGINT&amp;#93;Sink schema: &amp;#91;d: BIGINT, e: INT&amp;#93;</description>
      <version>1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.PreValidateReWriter.scala</file>
    </fixedFiles>
  </bug>
  <bug id="24684" opendate="2021-10-28 00:00:00" fixdate="2021-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port to string casting rules to the new CastRule interface</summary>
      <description>For more details, check https://issues.apache.org/jira/browse/FLINK-24462</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.rules.TimestampToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.rules.IdentityCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.rules.CastRuleUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.rules.ArrayToArrayCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.rules.AbstractNullAwareCodeGeneratorCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.rules.AbstractCodeGeneratorCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.rules.AbstractCharacterFamilyTargetRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRuleProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="24685" opendate="2021-10-28 00:00:00" fixdate="2021-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use the new casting rules in TableResult#print</summary>
      <description>For more details, check https://issues.apache.org/jira/browse/FLINK-24462. The planner should now propagate back to the api package the built `CastExecutor` for the output data type</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSinkUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.CollectDynamicSink.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.PrintUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.TimestampStringUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.PrintUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.CollectModifyOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableResultInternal.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableResultImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.StaticResultProvider.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.ResultProvider.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.InsertResultProvider.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.view.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.table.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.module.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.catalog.database.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.utils.TestTableResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliTableauResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.ResultDescriptor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliTableResultView.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliTableauResultView.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliResultView.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliChangelogResultView.java</file>
      <file type="M">flink-examples.flink-examples-table.src.test.java.org.apache.flink.table.examples.utils.ExampleOutputTestBase.java</file>
      <file type="M">flink-examples.flink-examples-table.src.test.java.org.apache.flink.table.examples.java.functions.AdvancedFunctionsExampleITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="24695" opendate="2021-10-29 00:00:00" fixdate="2021-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update how to configure unaligned checkpoints in the documentation</summary>
      <description>It looks like we don't have a code example how to enabled unaligned checkpoints anywhere in the docs. That should be fixed.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.checkpointing.under.backpressure.md</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.checkpointing.md</file>
      <file type="M">docs.content.zh.docs.ops.state.checkpointing.under.backpressure.md</file>
    </fixedFiles>
  </bug>
  <bug id="24697" opendate="2021-10-29 00:00:00" fixdate="2021-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka table source cannot change the auto.offset.reset setting for &amp;#39;group-offsets&amp;#39; startup mode</summary>
      <description>Because Flink 1.13 SQL does not use the new Source API in FLIP-27, the behavior to start from group offsets in flink 1.13 will use the kafka 'auto.offset.reset' default value(latest), when the 'auto.offset.reset' configuration is not set in table options. But in flink 1.13 we could change the behavior by setting 'auto.offset.reset' to other values. See the method setStartFromGroupOffsets in the class FlinkKafkaConsumerBase.Flink 1.14 uses the new Source API, but we have no ways to change the default 'auto.offset.reset' value when use 'group-offsets' startup mode. In DataStream API, we could change it by `kafkaSourceBuilder.setStartingOffsets(OffsetsInitializer.committedOffsets(OffsetResetStrategy))`.So we need the way to change auto offset reset configuration.The design is that when 'auto.offset.reset' is set, the 'group-offsets' startup mode will use the provided auto offset reset strategy, or else 'none' reset strategy in order to be consistent with the DataStream API.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.FlinkAssertions.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="24699" opendate="2021-10-29 00:00:00" fixdate="2021-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move scalastyle execution to validation phase</summary>
      <description>For some reason the scalstyle plugin by default runs in the verify phase.I propose to move it to the validate phase where other source QA plugins are run, which also makes it easier to run it locally.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24703" opendate="2021-10-29 00:00:00" fixdate="2021-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CSV format support for filesystem based on StreamFormat and BulkWriter interfaces.</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.utils.JsonPlanTestBase.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.DataStreamCsvITCase.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.RowDataToCsvConverters.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowDataSerializationSchema.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFileFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvFilesystemBatchITCase.java</file>
      <file type="M">flink-formats.flink-csv.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvToRowDataConverters.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFileSystemFormatFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="24740" opendate="2021-11-2 00:00:00" fixdate="2021-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update testcontainers dependency to v1.16.2</summary>
      <description>We should update our testcontainers dependency to the latest version, which is 1.16.2Main benefits (based on https://github.com/testcontainers/testcontainers-java/releases) Better startup performance for all containers Faster Cassandra startup Host port access for containers (make hosts ports accessible to containers, even after the container has started) New Azure Cosmos DB module</description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24742" opendate="2021-11-2 00:00:00" fixdate="2021-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL client add info about key strokes to docs</summary>
      <description>SQL client supports key strokes from jline.Unfortunately there is no docs about that in jline however there is source from which it could be found &amp;#91;1&amp;#93;here it is a list of most useful key strokes which are already supported by all existing Flink SQL client Key-Stroke Description `alt-b` Backward word `alt-f` Forward word `alt-c` Capitalize word `alt-l` Lowercase word `alt-u` Uppercase word `alt-d` Kill word `alt-n` History search forward `alt-p` History search backward `alt-t` Transpose words `ctrl-a` To the beginning of line `ctrl-e` To the end of line `ctrl-b` Backward char `ctrl-f` Forward char `ctrl-d` Delete char `ctrl-h` Backward delete char `ctrl-t` Transpose chars `ctrl-i` Invoke completion `ctrl-j` Submit a query `ctrl-m` Submit a query `ctrl-k` Kill the line to the right from the cursor `ctrl-w` Kill the line to the left from the cursor `ctrl-u` Kill the whole line `ctrl-l` Clear screen `ctrl-n` Down line from history `ctrl-p` Up line from history `ctrl-r` History incremental search backward `ctrl-s` History incremental search forward &amp;#91;1&amp;#93; https://github.com/jline/jline3/blob/997496e6a6338ca5d82c7dec26f32cf089dd2838/reader/src/main/java/org/jline/reader/impl/LineReaderImpl.java#L5907</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="24746" opendate="2021-11-3 00:00:00" fixdate="2021-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Alibaba maven mirror is unstable</summary>
      <description>Our Maven mirror setup for CI alicloud-mvn-mirror is currently incredibly unstable.</description>
      <version>None</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.maven-utils.sh</file>
      <file type="M">tools.ci.alibaba-mirror-settings.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24749" opendate="2021-11-3 00:00:00" fixdate="2021-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reuse CheckpointStatsTracker across rescaling</summary>
      <description>We can solve the collision of checkpointing metrics by using the same CheckpointStatsTracker instance.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.TestingDefaultExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraphBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="2475" opendate="2015-8-3 00:00:00" fixdate="2015-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename Flink Client log file</summary>
      <description>Currently, JoManager and TaskManager log/out files are names as follows: flink-mjsax-jobmanager-....log flink-mjsax-jobmanager-....out flink-mjsax-taskmanager-....log flink-mjsax-taskmanager-....outHowever, CLI log file is named differently: flink-mjsax-flink-client-....logThis should be "client" only and not "flink-client" for consistency.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink</file>
    </fixedFiles>
  </bug>
  <bug id="24760" opendate="2021-11-4 00:00:00" fixdate="2021-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update user document for batch window tvf</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-tvf.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-topn.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-join.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-agg.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.window-tvf.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.window-topn.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.window-join.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.window-agg.md</file>
    </fixedFiles>
  </bug>
  <bug id="24765" opendate="2021-11-4 00:00:00" fixdate="2021-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Kafka dependency</summary>
      <description>We rely on a very old Kafka version 2.4.1 which was the last version supporting scala 2.11. Since we dropped Scala 2.11 we can now update to a more recent one.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.KafkaSourceE2ECase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.KafkaSourceITCase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-examples.flink-examples-build-helper.flink-examples-streaming-state-machine.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.enumerator.KafkaEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.metrics.KafkaMetricWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.metrics.KafkaMetricMutableWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.FlinkKafkaInternalProducer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.FlinkKafkaInternalProducer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="24772" opendate="2021-11-4 00:00:00" fixdate="2021-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update user document for individual window table-valued function</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.queries.window-tvf.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.window-tvf.md</file>
    </fixedFiles>
  </bug>
  <bug id="24802" opendate="2021-11-5 00:00:00" fixdate="2021-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve cast ROW to STRING</summary>
      <description>When casting ROW to string, we should have a space after the comma to be consistent with ARRAY, MAP, etc.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.RowTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.CompositeAccessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.RowToStringCastRule.java</file>
      <file type="M">flink-examples.flink-examples-table.src.test.java.org.apache.flink.table.examples.java.functions.AdvancedFunctionsExampleITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="24803" opendate="2021-11-5 00:00:00" fixdate="2021-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix cast BINARY/VARBINARY to STRING</summary>
      <description>BINARY/VARBINARY should be printed as regular arrays instead of interpreting them in an arbitrary character set as a string.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionMiscITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.StringToBinaryCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.MapAndMultisetToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRuleUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.BinaryToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.ArrayToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.EncodingUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="24804" opendate="2021-11-5 00:00:00" fixdate="2021-4-5 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Upgrade oshi-core from v3.4.0 to v.6.1.5</summary>
      <description>Flink still uses com.github.oshi:oshi-core:3.4.0 (released Feb 2017) while com.github.oshi:oshi-core:5.8.3 (released Oct 2021) is also available.The license for 3.4.0 is EPL 1.0 which has also changed with the newer version, which is now MIT. Upgrading to the newer version would allow us to remove a "weak copyleft" license per https://www.apache.org/legal/resolved.html</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.metrics.SystemResourcesMetricsITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.utils.SystemResourcesCounterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.util.SystemResourcesMetricsInitializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.util.SystemResourcesCounter.java</file>
      <file type="M">docs.content.docs.ops.metrics.md</file>
      <file type="M">docs.content.zh.docs.ops.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="24822" opendate="2021-11-8 00:00:00" fixdate="2021-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve CatalogTableSpecBase and its subclass method parameter</summary>
      <description>Currently, CatalogTableSpecBase and its subclass related method use PlannerBase as parameter, we can improve it use FlinkContext enough.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DynamicTableSourceSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DynamicTableSinkSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.spec.TemporalTableSourceSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.spec.DynamicTableSourceSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.spec.DynamicTableSinkSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLookupJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="24827" opendate="2021-11-8 00:00:00" fixdate="2021-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump maven-dependency-plugin to 3.2.0</summary>
      <description>Flink currently uses version 3.1.1 of org.apache.maven.plugins:maven-dependency-plugin. We should update to 3.2.0 (currently the latest version)</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.pyflink.pyflink.gateway.server.py</file>
    </fixedFiles>
  </bug>
  <bug id="24828" opendate="2021-11-8 00:00:00" fixdate="2021-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Powermock to v2.0.9</summary>
      <description>We should update Powermock to the latest available version, which is currently v2.0.9</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24831" opendate="2021-11-8 00:00:00" fixdate="2021-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade DataStream Window examples</summary>
      <description>Upgrade DataStream window examples to not rely on any deprecated APIs and work for both batch and streaming workloads. </description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.src.test.scala.org.apache.flink.streaming.scala.examples.StreamingExamplesITCase.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.test.java.org.apache.flink.streaming.test.StreamingExamplesITCase.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.test.java.org.apache.flink.streaming.test.scala.examples.windowing.TopSpeedWindowingExampleITCase.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.test.java.org.apache.flink.streaming.test.examples.windowing.TopSpeedWindowingExampleITCase.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.WindowWordCount.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.TopSpeedWindowing.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.SessionWindowing.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.GroupedProcessingTimeWindowExample.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.windowing.WindowWordCount.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.windowing.TopSpeedWindowing.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.windowing.SessionWindowing.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.windowing.GroupedProcessingTimeWindowExample.java</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24832" opendate="2021-11-8 00:00:00" fixdate="2021-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update JUnit5 to v5.8.1</summary>
      <description>We should update to the latest version of JUnit5, v5.8.1</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24833" opendate="2021-11-8 00:00:00" fixdate="2021-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent use of deprecated APIs in flink-examples</summary>
      <description>We should be able to setup java compiler for examplesto fail on any usage of deprecated APIs.Something along the lines of:&lt;plugin&gt;  &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;  &lt;version&gt;...&lt;/version&gt;  &lt;executions&gt;    &lt;execution&gt;      &lt;id&gt;compile&lt;/id&gt;      &lt;phase&gt;process-sources&lt;/phase&gt;      &lt;goals&gt;        &lt;goal&gt;compile&lt;/goal&gt;      &lt;/goals&gt;      &lt;configuration&gt;        &lt;compilerArgument&gt;-Xlint:deprecation&lt;/compilerArgument&gt;        &lt;failOnWarning&gt;true&lt;/failOnWarning&gt;      &lt;/configuration&gt;    &lt;/execution&gt;  &lt;/executions&gt;&lt;/plugin&gt;</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-examples.flink-examples-batch.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24858" opendate="2021-11-10 00:00:00" fixdate="2021-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TypeSerializer version mismatch during eagerly restore</summary>
      <description>Currently, some of our TypeSerializer snapshots assume information about the binary layout of the actual data rather than only holding information about the TypeSerialzer.Multiple users ran into this problem i.e.https://lists.apache.org/thread/4q5q7wp0br96op6p7f695q2l8lz4wfzxThis manifest itself when state is restored egarly (for example an operator state) but, for example a user doesn't register the state on their intializeState/open,* and then a checkpoint happens.The result is that we will have elements serialized according to an old binary layout, but our serializer snapshot declares a new version which indicates that the elements are written with a new binary layout.The next restore will fail.</description>
      <version>1.14.0,1.13.3,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.typeutils.LinkedListSerializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.TypeSerializerUpgradeTestBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.runtime.RowSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.RowTypeInfo.java</file>
      <file type="M">docs.content.release-notes.flink-1.14.md</file>
    </fixedFiles>
  </bug>
  <bug id="24859" opendate="2021-11-10 00:00:00" fixdate="2021-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document new File formats</summary>
      <description>The project recently introduced new formats: BulkFormat and StreamFormat interfaces. There are already implementations of these formats: hive, parquet, orc and textLine formats that need to be documented.</description>
      <version>None</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.filesystem.LimitableBulkFormatTest.java</file>
      <file type="M">flink-python.pyflink.datastream.connectors.py</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcColumnarRowFileInputFormatTest.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcFileFormatFactory.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcColumnarRowFileInputFormat.java</file>
      <file type="M">flink-formats.flink-orc-nohive.src.main.java.org.apache.flink.orc.nohive.OrcNoHiveColumnarRowInputFormat.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.wordcount.WordCount.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.WindowWordCount.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.TopSpeedWindowing.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.wordcount.WordCount.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.windowing.WindowWordCount.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.windowing.TopSpeedWindowing.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveCompactReaderFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSourceBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.impl.FileSourceReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.FileSourceTextLinesITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.reader.TextLineFormat.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.hybrid.HybridSource.java</file>
      <file type="M">docs.content.docs.connectors.datastream.hybridsource.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.hybridsource.md</file>
    </fixedFiles>
  </bug>
  <bug id="24865" opendate="2021-11-10 00:00:00" fixdate="2021-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support MATCH_RECOGNIZE in Batch mode</summary>
      <description>Currently MATCH_RECOGNIZE only works in Streaming mode. It should also be supported in Batch mode</description>
      <version>1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.match.PatternTranslatorTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.validation.MatchRecognizeValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.operator.BatchOperatorNameTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.operator.BatchOperatorNameTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalMatchRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalMatch.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch.java</file>
    </fixedFiles>
  </bug>
  <bug id="24869" opendate="2021-11-11 00:00:00" fixdate="2021-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-core should be provided in flink-file-sink-common</summary>
      <description>As example flink-connector-files brings flink-core with compile scope via flink-file-sink-common.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-file-sink-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24887" opendate="2021-11-12 00:00:00" fixdate="2021-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retrying savepoints may cause early cluster shutdown</summary>
      <description>If an operation is retried we potentially access the result of a previous attempt to see if it has already failed and eagerly fail the trigger request. If that attempt is already complete then this may lead to an unexpected shutdown of the cluster.Beyond this issue, the eager checking of previous attempts makes error handling more complicated, because you have to cover all cases for both the trigger and status-retrieval operations.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherCachedOperationsHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherCachedOperationsHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.async.CompletedOperationCacheTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.async.CompletedOperationCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="24904" opendate="2021-11-15 00:00:00" fixdate="2021-2-15 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add documentation for KDS Async Sink</summary>
      <description>MotivationFLINK-24227 introduces a new sink for Kinesis Data Streams that supersedes the existing one based on KPL.Scope: Deprecate the current section in the docs for the Kinesis KPL sink and write documentation and usage guide for the new sink.ReferencesMore details to be found https://cwiki.apache.org/confluence/display/FLINK/FLIP-171%3A+Async+Sink</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.metrics.md</file>
      <file type="M">docs.content.docs.connectors.datastream.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug id="24905" opendate="2021-11-15 00:00:00" fixdate="2021-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[FLIP-171] KDS implementation of Async Sink Table API</summary>
      <description>MotivationUser stories:As a Flink user, I’d like to use the Table API for the new Kinesis Data Streams  sink.Scope: Introduce AsyncDynamicTableSink that enables Sinking Tables into Async Implementations. Implement a new KinesisDynamicTableSink that uses KinesisDataStreamSink Async Implementation and implements AsyncDynamicTableSink. The implementation introduces Async Sink configurations as optional options in the table definition, with default values derived from the KinesisDataStream default values. Unit/Integration testing. modify KinesisTableAPI tests for the new implementation, add unit tests for AsyncDynamicTableSink and KinesisDynamicTableSink and KinesisDynamicTableSinkFactory. Java / code-level docs.ReferencesMore details to be found https://cwiki.apache.org/confluence/display/FLINK/FLIP-171%3A+Async+Sink </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.kinesis.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.kinesis.md</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.src.test.resources.filter-large-orders.sql</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.src.test.java.org.apache.flink.streaming.kinesis.test.KinesisTableApiITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.table.KinesisDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.table.KinesisDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.table.KinesisDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.table.KinesisConnectorOptionsUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.table.KinesisConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.test.java.org.apache.flink.connector.kinesis.table.RowDataFieldsKinesisKeyGeneratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.table.RowDataFieldsKinesisKeyGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.table.RandomKinesisKeyGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.table.FixedKinesisKeyGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.table.RowDataFieldsKinesisPartitionerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.table.RowDataFieldsKinesisPartitioner.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.RandomKinesisPartitioner.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.KinesisPartitioner.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.FixedKinesisPartitioner.java</file>
    </fixedFiles>
  </bug>
  <bug id="24909" opendate="2021-11-15 00:00:00" fixdate="2021-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL syntax highlighting in SQL Client</summary>
      <description>What to hightlight: keywords, quoted strings, sql identifier quoted string, line comments, block comments, hints.Property sql-client.color-schema to set current highlighting schema</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.SqlClientOptions.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
      <file type="M">docs.layouts.shortcodes.generated.sql.client.configuration.html</file>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="24912" opendate="2021-11-15 00:00:00" fixdate="2021-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate state processor API to DataStream API</summary>
      <description>Now that DataStream supports bounded execution, we should investigate migrating the State Processor API off DataSet. </description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.WindowReader.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.Savepoint.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.OneInputOperatorTransformation.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.NewSavepoint.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.KeyedOperatorTransformation.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.ExistingSavepoint.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.EvictingWindowReader.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.BootstrapTransformation.java</file>
      <file type="M">docs.content.docs.libs.state.processor.api.md</file>
      <file type="M">docs.content.zh.docs.libs.state.processor.api.md</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.SavepointWriterWindowITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.SavepointWriterITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.WritableSavepoint.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.metadata.SavepointMetadata.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.metadata.OperatorStateSpec.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.BootstrapTransformationWithID.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.TimestampAssignerWrapper.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.partitioner.KeyGroupRangePartitioner.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.partitioner.HashSelector.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.BoundedStreamTask.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.BoundedOneInputStreamTaskRunner.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.OperatorTransformation.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.SavepointWindowReaderITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.SavepointReaderKeyedStateITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.SavepointReaderITTestBase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.SavepointReaderITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.SavepointReaderCustomSerializerITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.SavepointDeepCopyTest.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.RocksDBStateBackendWindowITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.RocksDBStateBackendReaderKeyedStateITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.MemoryStateBackendWindowITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.MemoryStateBackendReaderKeyedStateITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.HashMapStateBackendWindowITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.HashMapStateBackendReaderKeyedStateITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.EmbeddedRocksDBStateBackendWindowITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.EmbeddedRocksDBStateBackendReaderKeyedStateITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.SavepointWriter.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.operator.StateReaderOperator.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.OperatorStateInputFormat.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.KeyedStateInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="24932" opendate="2021-11-17 00:00:00" fixdate="2021-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Frocksdb cannot run on Apple M1</summary>
      <description>After we bump up RocksDB version to 6.20.3, we support to run RocksDB on linux arm cluster. However, according to the feedback from Robert, Apple M1 machines cannot run FRocksDB yet:java.lang.Exception: Exception while creating StreamOperatorStateContext. at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:255) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:109) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:711) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:687) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-runtime-1.14.0.jar:1.14.0] at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-runtime-1.14.0.jar:1.14.0] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-runtime-1.14.0.jar:1.14.0] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-runtime-1.14.0.jar:1.14.0] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_312]Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for StreamFlatMap_c21234bcbf1e8eb4c61f1927190efebd_(1/1) from any of the 1 provided restore options. at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:160) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:346) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:164) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] ... 11 moreCaused by: java.io.IOException: Could not load the native RocksDB library at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.ensureRocksDBIsLoaded(EmbeddedRocksDBStateBackend.java:882) ~[flink-statebackend-rocksdb_2.11-1.14.0.jar:1.14.0] at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:402) ~[flink-statebackend-rocksdb_2.11-1.14.0.jar:1.14.0] at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:345) ~[flink-statebackend-rocksdb_2.11-1.14.0.jar:1.14.0] at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:87) ~[flink-statebackend-rocksdb_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:329) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:346) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:164) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] ... 11 moreCaused by: java.lang.UnsatisfiedLinkError: /private/var/folders/js/yfk_y2450q7559kygttykwk00000gn/T/rocksdb-lib-5783c058ce68d31d371327abc9b51cac/librocksdbjni-osx.jnilib: dlopen(/private/var/folders/js/yfk_y2450q7559kygttykwk00000gn/T/rocksdb-lib-5783c058ce68d31d371327abc9b51cac/librocksdbjni-osx.jnilib, 0x0001): tried: '/private/var/folders/js/yfk_y2450q7559kygttykwk00000gn/T/rocksdb-lib-5783c058ce68d31d371327abc9b51cac/librocksdbjni-osx.jnilib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e')), '/usr/lib/librocksdbjni-osx.jnilib' (no such file) at java.lang.ClassLoader$NativeLibrary.load(Native Method) ~[?:1.8.0_312] at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1950) ~[?:1.8.0_312] at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1832) ~[?:1.8.0_312] at java.lang.Runtime.load0(Runtime.java:811) ~[?:1.8.0_312] at java.lang.System.load(System.java:1088) ~[?:1.8.0_312] at org.rocksdb.NativeLibraryLoader.loadLibraryFromJar(NativeLibraryLoader.java:79) ~[frocksdbjni-6.20.3-ververica-1.0.jar:?] at org.rocksdb.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:57) ~[frocksdbjni-6.20.3-ververica-1.0.jar:?] at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.ensureRocksDBIsLoaded(EmbeddedRocksDBStateBackend.java:856) ~[flink-statebackend-rocksdb_2.11-1.14.0.jar:1.14.0] at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:402) ~[flink-statebackend-rocksdb_2.11-1.14.0.jar:1.14.0] at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:345) ~[flink-statebackend-rocksdb_2.11-1.14.0.jar:1.14.0] at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:87) ~[flink-statebackend-rocksdb_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:329) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:346) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:164) ~[flink-streaming-java_2.11-1.14.0.jar:1.14.0] ... 11 moreThis issue is tracked by RocksDB community: rocksdb/issues/7720</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="24947" opendate="2021-11-18 00:00:00" fixdate="2021-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support host network for native K8s integration</summary>
      <description>For the use of flink on k8s, for performance considerations, it is important to choose a CNI plug-in. Usually we have two environments: Managed and UnManaged.  Managed: Cloud vendors usually provide very efficient CNI plug-ins, we don’t need to care about network performance issues  UnManaged: On self-built K8s clusters, CNI plug-ins are usually optional, similar to Flannel and Calcico, but such software network cards usually lose some performance or require some additional network strategies.For an unmanaged environment, if we also want to achieve the best network performance, should we support the HostNetWork model?Use the host network to achieve the best performance</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriverTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.TestingFlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClientTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriver.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesClusterDescriptor.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesService.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.AbstractKubernetesParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitTaskManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InitJobManagerDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.entrypoint.KubernetesResourceManagerFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.entrypoint.KubernetesEntrypointUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesResourceManagerDriverConfiguration.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.cli.KubernetesSessionCli.java</file>
      <file type="M">docs.layouts.shortcodes.generated.kubernetes.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="24960" opendate="2021-11-19 00:00:00" fixdate="2021-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>YARNSessionCapacitySchedulerITCase.testVCoresAreSetCorrectlyAndJobManagerHostnameAreShownInWebInterfaceAndDynamicPropertiesAndYarnApplicationNameAndTaskManagerSlots hangs on azure</summary>
      <description>Nov 18 22:37:08 ================================================================================Nov 18 22:37:08 Test testVCoresAreSetCorrectlyAndJobManagerHostnameAreShownInWebInterfaceAndDynamicPropertiesAndYarnApplicationNameAndTaskManagerSlots(org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase) is running.Nov 18 22:37:08 --------------------------------------------------------------------------------Nov 18 22:37:25 22:37:25,470 [ main] INFO org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase [] - Extracted hostname:port: 5718b812c7ab:38622Nov 18 22:52:36 ==============================================================================Nov 18 22:52:36 Process produced no output for 900 seconds.Nov 18 22:52:36 ============================================================================== https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26722&amp;view=logs&amp;j=f450c1a5-64b1-5955-e215-49cb1ad5ec88&amp;t=cc452273-9efa-565d-9db8-ef62a38a0c10&amp;l=36395</description>
      <version>1.14.3,1.15.0,1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
    </fixedFiles>
  </bug>
  <bug id="24978" opendate="2021-11-22 00:00:00" fixdate="2021-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade ASM to 9.2</summary>
      <description>As per usual we need to bump flink-shaded-asm.</description>
      <version>None</version>
      <fixedVersion>shaded-15.0,1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.extraction.ExtractionUtils.java</file>
      <file type="M">flink-table.flink-table-common.pom.xml</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ClosureCleaner.scala</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.ClassRelocator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractionUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.ClosureCleaner.java</file>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24979" opendate="2021-11-22 00:00:00" fixdate="2021-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove MaxPermSize configuration in HBase surefire config</summary>
      <description>The MaxPermSize parameter has no effect since JDK 8, and is actively rejected in Java 17. Given that we for years it worked just fine in without it, we can just remove it.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24983" opendate="2021-11-22 00:00:00" fixdate="2021-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade surefire to 3.0.0-M5</summary>
      <description>Surefire 3.0.0-M5 comes with a new TCP/IP communication channel between surefire and JVM forks.This will allow us to resolve "Corrupted STDOUT" issues when the JVM is printing warnings due to unsafe accesses.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24988" opendate="2021-11-22 00:00:00" fixdate="2021-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade lombok to 1.18.22</summary>
      <description>Our current Lombok version fails on Java 17 due to illegal accesses to JDK internals. Newer versions of Lombok do some hacks at runtime to resolve the issue...</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24989" opendate="2021-11-22 00:00:00" fixdate="2021-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade shade-plugin to 3.2.4</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25039" opendate="2021-11-24 00:00:00" fixdate="2021-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable shading of test jars by default</summary>
      <description>The AZP build fails with a license check:21:26:40,233 ERROR org.apache.flink.tools.ci.licensecheck.JarFileChecker [] - Missing META-INF/LICENSE in /tmp/flink-validation-deployment/org/apache/flink/flink-sql-parquet_2.12/1.15-SNAPSHOT/flink-sql-parquet_2.12-1.15-20211123.212027-1-tests.jar21:26:40,738 ERROR org.apache.flink.tools.ci.licensecheck.JarFileChecker [] - The notice file in /tmp/flink-validation-deployment/org/apache/flink/flink-connector-cassandra_2.12/1.15-SNAPSHOT/flink-connector-cassandra_2.12-1.15-20211123.211736-1-tests.jar does not contain the expected entries.21:26:40,739 ERROR org.apache.flink.tools.ci.licensecheck.JarFileChecker [] - Missing META-INF/LICENSE in /tmp/flink-validation-deployment/org/apache/flink/flink-connector-cassandra_2.12/1.15-SNAPSHOT/flink-connector-cassandra_2.12-1.15-20211123.211736-1-tests.jar21:26:41,673 ERROR org.apache.flink.tools.ci.licensecheck.JarFileChecker [] - The notice file in /tmp/flink-validation-deployment/org/apache/flink/flink-kubernetes/1.15-SNAPSHOT/flink-kubernetes-1.15-20211123.212114-1-tests.jar does not contain the expected entries.21:26:41,675 ERROR org.apache.flink.tools.ci.licensecheck.JarFileChecker [] - Missing META-INF/LICENSE in /tmp/flink-validation-deployment/org/apache/flink/flink-kubernetes/1.15-SNAPSHOT/flink-kubernetes-1.15-20211123.212114-1-tests.jar21:28:00,582 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Found a total of 5 severe license issues==============================================================================License Check failed. See previous output for details.==============================================================================##[error]Bash exited with code '1'.https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26967&amp;view=logs&amp;j=946871de-358d-5815-3994-8175615bc253&amp;t=e0240c62-4570-5d1c-51af-dd63d2093da1&amp;l=30668https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26967&amp;view=logs&amp;j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&amp;t=a7382ec4-87d2-5a9d-7c53-a2f93e317458&amp;l=31863https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26967&amp;view=logs&amp;j=6e8542d7-de38-5a33-4aca-458d6c87066d&amp;t=dffc2faa-5b48-5b4e-0797-dec1b1f74872&amp;l=31863</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-code-splitter.pom.xml</file>
      <file type="M">flink-rpc.flink-rpc-akka-loader.pom.xml</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-formats.flink-sql-avro.pom.xml</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.pom.xml</file>
      <file type="M">flink-examples.flink-examples-build-helper.flink-examples-streaming-twitter.pom.xml</file>
      <file type="M">flink-examples.flink-examples-build-helper.flink-examples-streaming-state-machine.pom.xml</file>
      <file type="M">flink-examples.flink-examples-build-helper.flink-examples-streaming-gcp-pubsub.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-rabbitmq.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-files.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25042" opendate="2021-11-24 00:00:00" fixdate="2021-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow calls to @VisibleForTesting from inner classes</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-architecture-tests.violations.e5126cae-f3fe-48aa-b6fb-60ae6cc3fcd5</file>
      <file type="M">flink-architecture-tests.src.test.java.org.apache.flink.architecture.rules.ApiAnnotationRules.java</file>
    </fixedFiles>
  </bug>
  <bug id="25044" opendate="2021-11-24 00:00:00" fixdate="2021-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add More Unit Test For Pulsar Source</summary>
      <description>We should enhance the pulsar source connector tests by adding more unit tests.  SourceReader SplitReader Enumerator SourceBuilder</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.start.MessageIdStartCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntimeOperator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarUnorderedPartitionSplitReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.PulsarSourceBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarUnorderedSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarOrderedSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSourceBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="25045" opendate="2021-11-24 00:00:00" fixdate="2021-1-24 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce AdaptiveBatchScheduler</summary>
      <description>Introduce AdaptiveBatchScheduler</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SsgNetworkMemoryCalculationUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SchedulerTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerComponentsFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.StateTrackingMockExecutionGraph.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.IntermediateResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertexTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocatorFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.ExecutionVertexOperations.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultSchedulerComponents.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.BlockingResultInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.topology.DefaultLogicalTopology.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobEdge.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.IntermediateResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.SchedulingPipelinedRegionComputeUtil.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.PipelinedRegionComputeUtil.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.LogicalPipelinedRegionComputeUtil.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plantranslate.JobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="25046" opendate="2021-11-24 00:00:00" fixdate="2021-2-24 01:00:00" resolution="Done">
    <buginformation>
      <summary>Convert unspecified edge to rescale when using adaptive batch scheduler</summary>
      <description>Currently, unspecified edges(partitioner == null) will be setted to FORWARD when the parallelism of both upstream and downstream is -1. This may cause many job vertices whose parallelism is not calculated based on data volume, but is aligned with their upstream vertices' parallelism because of forward edges, which is contrary to the original intention of our adaptive batch scheduler.For this problem, we will convert the unspecfied edges between OperatorChains to RESCALE edges(instead of FORWARD edges) after OperatorChains' creation.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.partitioner.ForwardForConsecutiveHashPartitionerTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="25053" opendate="2021-11-25 00:00:00" fixdate="2021-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document how to use the usrlib to load code in the user code class loader</summary>
      <description>With FLINK-13993 we introduced the usrlib directory that can be used to load code in the user code class loader. This functionality has not been properly documented so that it is very hard to use. I would suggest to change this so that our users can benefit from this cool feature.</description>
      <version>1.14.0,1.12.5,1.13.3,1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.debugging.debugging.classloading.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.yarn.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.content.zh.docs.ops.debugging.debugging.classloading.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.yarn.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug id="25067" opendate="2021-11-26 00:00:00" fixdate="2021-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct the description of RocksDB&amp;#39;s background threads</summary>
      <description>RocksDB actually has changed the maximum number of concurrent background flush and compaction jobs to 2 for long time, we should fix the related documentation.</description>
      <version>None</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBConfigurableOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.configurable.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="25072" opendate="2021-11-26 00:00:00" fixdate="2021-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce description for operator</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.DataStreamTest.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.DataStreamTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamNode.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.SimpleTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.JSONGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSink.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.dag.Transformation.java</file>
    </fixedFiles>
  </bug>
  <bug id="25073" opendate="2021-11-26 00:00:00" fixdate="2021-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce Tree Mode description for job vertex</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.node.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.detail.job-overview-drawer-detail.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.detail.job-overview-drawer-detail.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.detail.job-overview-drawer-detail.component.html</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.PipelineOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.pipeline.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="25074" opendate="2021-11-26 00:00:00" fixdate="2021-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Simplify name of window operators in DS by moving details to description</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.DataStreamTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorBuilder.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.WindowedStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.AllWindowedStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="25076" opendate="2021-11-26 00:00:00" fixdate="2021-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Simplify name of SQL operators</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.explain.testStreamTableEnvironmentExecutionExplain.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.explain.testStatementSetExecutionExplain.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.explain.testExplainJsonPlan.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.explain.testExecuteSqlWithExplainDetailsSelect.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.explain.testExecuteSqlWithExplainDetailsInsert.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.explain.testExecuteSqlWithExplainDetailsAndUnion.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.explain.filesystem.testFileSystemTableSinkWithParallelismInStreamingSql1.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.explain.filesystem.testFileSystemTableSinkWithParallelismInStreamingSql0.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.explain.filesystem.testFileSystemTableSinkWithParallelismInBatch.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.api.internal.TableEnvironmentInternalTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.connector.file.table.FileSystemTableSinkTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.ScanUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CorrelateCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.ExecNodeUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowRank.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowDeduplicate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecWatermarkAssigner.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecTemporalSort.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecTemporalJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSort.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecRank.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupTableAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMiniBatchAssigner.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLocalWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLocalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLegacyTableSourceScan.java</file>
      <file type="M">docs.layouts.shortcodes.generated.optimizer.config.configuration.html</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.resources.explain.testHiveTableSinkWithParallelismInBatch.out</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.resources.explain.testHiveTableSinkWithParallelismInStreaming.out</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.table.q</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.OptimizerConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.ExternalDynamicSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.ExternalDynamicSource.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecBoundedStreamScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecHashAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecHashJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecHashWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecLegacyTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecLimit.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecMultipleInput.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecNestedLoopJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecPythonOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecRank.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSort.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortLimit.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortMergeJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCorrelate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecExpand.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLegacySink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLookupJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCalc.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCorrelate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecValues.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecWindowTableFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecChangelogNormalize.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecDataStreamScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecDeduplicate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecDropUpdateBefore.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGlobalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGlobalWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGroupTableAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecIncrementalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecIntervalJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="25080" opendate="2021-11-26 00:00:00" fixdate="2021-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move FutureUtils to flink-core</summary>
      <description>The same applies to the RetryStrategy implementation tests.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.FixedRetryStrategyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.ExponentialBackoffRetryStrategyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.ConjunctFutureTest.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnConfigurationITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.AbstractOperatorRestoreTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RescalingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.cancelling.CancelingTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.accumulators.AccumulatorLiveITCase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTerminationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.TestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TestTaskBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskCancelAsyncProducerConsumerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerServicesBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.slot.TaskSlotUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SchedulerTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.benchmark.SchedulerBenchmarkBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestExternalHandlersITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestClientTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestClientMultipartTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingMetricsHandlerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobSubmitHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobConfigHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorMasterHooksTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorRestoringTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTriggeringTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.FailoverStrategyCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.FutureUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.ManuallyTriggeredScheduledExecutor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.FileExecutionGraphInfoStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphRestartTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphSuspendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.heartbeat.HeartbeatManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.DefaultJobLeaderIdServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultSchedulerTest.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.runtime.jobmanager.JMXJobManagerMetricTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.AbstractQueryableStateTestBase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlers.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunnerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.TestingDefaultExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.CancelPartitionRequestTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.ClientTransportErrorHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.ServerTransportErrorHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartialConsumePipelinedResultTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.SlotCountExceedingParallelismTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridgeBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolInteractionsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.TestingJobManagerSharedServicesBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderChangeClusterComponentsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.registration.RetryingRegistrationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManagerBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.DefaultSlotStatusSyncerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerConfigurationBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TaskExecutorManagerBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TaskExecutorManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandlerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="25105" opendate="2021-11-30 00:00:00" fixdate="2021-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enables final checkpoint by default</summary>
      <description>We would fix all the conflict tests and enable the feature by default in this version.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RescalingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.BoundedSourceITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskFinalCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskChainedSourcesCheckpointingTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBaseITCase.java</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.checkpointing.configuration.html</file>
      <file type="M">docs.content.docs.internals.task.lifecycle.md</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.checkpointing.md</file>
    </fixedFiles>
  </bug>
  <bug id="25128" opendate="2021-12-1 00:00:00" fixdate="2021-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reorganize table modules and introduce flink-table-planner-loader</summary>
      <description>For more details, see https://docs.google.com/document/d/12yDUCnvcwU2mODBKTHQ1xhfOq1ujYUrXltiN_rbhT34/edit?usp=sharing</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueAggFunctionWithOrderTest.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.sql.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RewriteMinusAllRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RewriteIntersectAllRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.common.PartialInsertTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.physical.batch.PushLocalAggIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.MinWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWsWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunctionWithOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.TimestampToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.WindowCodeGenerator.scala</file>
      <file type="M">flink-architecture-tests.pom.xml</file>
      <file type="M">flink-architecture-tests.violations.5b9eed8a-5fb6-4373-98ac-3be2a71941b8</file>
      <file type="M">flink-architecture-tests.violations.7602816f-5c01-4b7a-9e3e-235dfedec245</file>
      <file type="M">flink-architecture-tests.violations.e5126cae-f3fe-48aa-b6fb-60ae6cc3fcd5</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-stream-sql-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-tpcds-test.pom.xml</file>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
      <file type="M">flink-python.apache-flink-libraries.setup.py</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-table.flink-table-common.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.JsonSerdeUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.licenses.LICENSE.icu4j</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.licenses.LICENSE.janino</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-runtime.pom.xml</file>
      <file type="M">flink-table.flink-table-runtime.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-uber.pom.xml</file>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">tools.ci.java-ci-tools.src.main.java.org.apache.flink.tools.ci.suffixcheck.ScalaSuffixChecker.java</file>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.CollectAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.JsonArrayAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.JsonObjectAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.LagAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.LastValueWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWsWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.MinWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.tablefunctions.ReplicateRows.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.CommonPythonUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.SetOpRewriteUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueAggFunctionWithOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunctionWithOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.FirstValueWithRetractAggFunctionWithoutOrderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.LagAggFunctionTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="25129" opendate="2021-12-1 00:00:00" fixdate="2021-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update docs to use flink-table-planner-loader instead of flink-table-planner</summary>
      <description>For more details https://docs.google.com/document/d/12yDUCnvcwU2mODBKTHQ1xhfOq1ujYUrXltiN_rbhT34/edit?usp=sharing</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.README.md</file>
      <file type="M">docs.content.docs.dev.configuration.testing.md</file>
      <file type="M">docs.content.docs.dev.configuration.overview.md</file>
      <file type="M">docs.content.docs.dev.configuration.maven.md</file>
      <file type="M">docs.content.docs.dev.configuration.connector.md</file>
      <file type="M">docs.content.docs.dev.configuration.advanced.md</file>
      <file type="M">docs.layouts.shortcodes.artifact.html</file>
      <file type="M">docs.content.docs.libs.gelly.overview.md</file>
      <file type="M">docs.content.docs.libs.cep.md</file>
      <file type="M">docs.content.docs.flinkDev.ide.setup.md</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.match.recognize.md</file>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.docs.dev.table.sourcesSinks.md</file>
      <file type="M">docs.content.docs.dev.table.overview.md</file>
      <file type="M">docs.content.docs.dev.table.data.stream.api.md</file>
      <file type="M">docs.content.docs.dev.datastream..index.md</file>
      <file type="M">docs.content.docs.dev.datastream.testing.md</file>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.queryable.state.md</file>
      <file type="M">docs.content.docs.connectors.table.upsert-kafka.md</file>
      <file type="M">docs.content.docs.connectors.table.overview.md</file>
      <file type="M">docs.content.docs.connectors.table.kinesis.md</file>
      <file type="M">docs.content.docs.connectors.table.kafka.md</file>
      <file type="M">docs.content.docs.connectors.table.jdbc.md</file>
      <file type="M">docs.content.docs.connectors.table.hbase.md</file>
      <file type="M">docs.content.docs.connectors.table.elasticsearch.md</file>
      <file type="M">docs.content.docs.connectors.datastream.twitter.md</file>
      <file type="M">docs.content.docs.connectors.datastream.rabbitmq.md</file>
      <file type="M">docs.content.docs.connectors.datastream.pulsar.md</file>
      <file type="M">docs.content.docs.connectors.datastream.pubsub.md</file>
      <file type="M">docs.content.docs.connectors.datastream.nifi.md</file>
      <file type="M">docs.content.docs.connectors.datastream.kafka.md</file>
      <file type="M">docs.content.docs.connectors.datastream.jdbc.md</file>
      <file type="M">docs.content.docs.connectors.datastream.elasticsearch.md</file>
      <file type="M">docs.content.docs.connectors.datastream.cassandra.md</file>
    </fixedFiles>
  </bug>
  <bug id="25145" opendate="2021-12-2 00:00:00" fixdate="2021-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for Zookeeper 3.6</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>shaded-15.0,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.container.FlinkContainersBuilder.java</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.ha.datastream.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.ha.per.job.cluster.datastream.sh</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcFilters.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.DefaultLastStateConnectionStateListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.LastStateConnectionStateListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointIDCounter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointRecoveryFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.AbstractZooKeeperHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.CuratorFrameworkWithUnhandledErrorListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperMultipleComponentLeaderElectionHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.scheduler.CoLocationGroupImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.ZooKeeperJobGraphStoreWatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriverFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriverFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriverFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.ZooKeeperSharedCount.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.ZooKeeperSharedValue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.ZooKeeperVersionedValue.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZKCheckpointIDCounterMultiServersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointIDCounterITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.ZooKeeperJobGraphsStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.ZooKeeperJobGraphStoreWatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriverTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ZooKeeperUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ZooKeeperUtilsTreeCacheTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperTestEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="25153" opendate="2021-12-3 00:00:00" fixdate="2021-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Inappropriate variable naming</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.ClosingFSDataOutputStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="25155" opendate="2021-12-3 00:00:00" fixdate="2021-12-3 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement claim snapshot mode</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBodyTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBody.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.set.q</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointMetadataLoadingTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.SavepointRestoreSettings.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.SavepointConfigOptions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.Checkpoints.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">docs.layouts.shortcodes.generated.savepoint.config.configuration.html</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendRunTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="25157" opendate="2021-12-3 00:00:00" fixdate="2021-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce NULL type to string cast rule</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.data.binary.BinaryStringDataUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.IdentityCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRuleProvider.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastCodeBlock.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.AbstractNullAwareCodeGeneratorCastRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="25158" opendate="2021-12-3 00:00:00" fixdate="2021-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix formatting for true, false and null to uppercase</summary>
      <description>All the cast rules using the constant strings true, false and null should use TRUE, FALSE and NULL instead.This behavior should be enabled only if legacy behavior is not enabled</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.data.binary.BinaryStringDataUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.validation.ScalarFunctionsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.RowTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.NonDeterministicTests.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.MapTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.LiteralTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.DecimalTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.CompositeAccessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ArrayTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.RowToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.RowDataToStringConverterImpl.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.MapAndMultisetToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRuleUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.BooleanToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.ArrayToStringCastRule.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.set.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.select.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliTableauResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="25159" opendate="2021-12-3 00:00:00" fixdate="2021-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streamline E2E surefire configuration setup</summary>
      <description>Our current surefire setup for the Java E2E tests is exceedingly complicated, because of when it was introduced we split the e2e tests across separate profiles.This is at least currently not necessary.Additionally, it appears that not many people are aware of this setup, which has resulted in various tests for pulsar, pubsub, kinesis and kafka (and maybe others) to not even being run on CI.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.PulsarSourceUnorderedE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.PulsarSourceOrderedE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.KafkaSourceE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.README.md</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.src.test.java.org.apache.flink.streaming.kinesis.test.KinesisTableApiITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-metrics-reporter-prometheus-test.src.test.java.org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-metrics-availability-test.src.test.java.org.apache.flink.metrics.tests.MetricsAvailabilityITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.src.test.java.org.apache.flink.glue.schema.registry.test.json.GlueSchemaRegistryJsonKinesisITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.src.test.java.org.apache.flink.glue.schema.registry.test.GlueSchemaRegistryAvroKinesisITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-scala.src.test.java.org.apache.flink.tests.scala.ScalaFreeITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-hbase.src.test.java.org.apache.flink.tests.util.hbase.SQLClientHBaseITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.categories.TravisGroup6.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.categories.TravisGroup5.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.categories.TravisGroup4.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.categories.TravisGroup3.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.categories.TravisGroup2.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.categories.TravisGroup1.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.StreamingKafkaITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-connector-gcp-pubsub-emulator-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="25180" opendate="2021-12-6 00:00:00" fixdate="2021-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jepsen test fails while setting up libzip4</summary>
      <description>The Jepsen tests fail from time to time while trying to set up libzip4. java.util.concurrent.ExecutionException: clojure.lang.ExceptionInfo: throw+: {:type :jepsen.control/nonzero-exit, :cmd "sudo -S -u root bash -c \"cd /; env DEBIAN_FRONTEND=noninteractive apt-get install -y --force-yes libzip4\"", :exit -1, :out "Reading package lists...Building dependency tree...Reading state information...The following NEW packages will be installed: libzip40 upgraded, 1 newly installed, 0 to remove and 120 not upgraded.Need to get 40.6 kB of archives.After this operation, 103 kB of additional disk space will be used.Get:1 http://cdn-aws.deb.debian.org/debian stretch/main amd64 libzip4 amd64 1.1.2-1.1+b1 [40.6 kB]Fetched 40.6 kB in 0s (0 B/s)Selecting previously unselected package libzip4:amd64. (Reading database ... (Reading database ... 5% (Reading database ... 10% (Reading database ... 15% (Reading database ... 20% (Reading database ... 25% (Reading database ... 30% (Reading database ... 35% (Reading database ... 40% (Reading database ... 45% (Reading database ... 50% (Reading database ... 55% (Reading database ... 60% (Reading database ... 65% (Reading database ... 70% (Reading database ... 75% (Reading database ... 80% (Reading database ... 85% (Reading database ... 90% (Reading database ... 95% (Reading database ... 100% (Reading database ... 49065 files and directories currently installed.) Preparing to unpack .../libzip4_1.1.2-1.1+b1_amd64.deb ... Unpacking libzip4:amd64 (1.1.2-1.1+b1) ... Setting up libzip4:amd64 (1.1.2-1.1+b1) ... ", :err "", :host "172.31.4.8", :action {:cmd "sudo -S -u root bash -c \"cd /; env DEBIAN_FRONTEND=noninteractive apt-get install -y --force-yes libzip4\"", :in "root"}} {:type :jepsen.control/nonzero-exit, :cmd "sudo -S -u root bash -c \"cd /; env DEBIAN_FRONTEND=noninteractive apt-get install -y --force-yes libzip4\"", :exit -1, :out "Reading package lists...Building dependency tree...Reading state information...The following NEW packages will be installed: libzip40 upgraded, 1 newly installed, 0 to remove and 120 not upgraded.Need to get 40.6 kB of archives.After this operation, 103 kB of additional disk space will be used.Get:1 http://cdn-aws.deb.debian.org/debian stretch/main amd64 libzip4 amd64 1.1.2-1.1+b1 [40.6 kB]Fetched 40.6 kB in 0s (0 B/s)Selecting previously unselected package libzip4:amd64. (Reading database ... (Reading database ... 5% (Reading database ... 10% (Reading database ... 15% (Reading database ... 20% (Reading database ... 25% (Reading database ... 30% (Reading database ... 35% (Reading database ... 40% (Reading database ... 45% (Reading database ... 50% (Reading database ... 55% (Reading database ... 60% (Reading database ... 65% (Reading database ... 70% (Reading database ... 75% (Reading database ... 80% (Reading database ... 85% (Reading database ... 90% (Reading database ... 95% (Reading database ... 100% (Reading database ... 49065 files and directories currently installed.) Preparing to unpack .../libzip4_1.1.2-1.1+b1_amd64.deb ... Unpacking libzip4:amd64 (1.1.2-1.1+b1) ... Setting up libzip4:amd64 (1.1.2-1.1+b1) ... ", :err "", :host "172.31.4.8", :action {:cmd "sudo -S -u root bash -c \"cd /; env DEBIAN_FRONTEND=noninteractive apt-get install -y --force-yes libzip4\"", :in "root"}}https://app.travis-ci.com/github/dataArtisans/flink-jepsen-ci/jobs/550915650#L1300</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.db.clj</file>
      <file type="M">flink-jepsen.project.clj</file>
    </fixedFiles>
  </bug>
  <bug id="25186" opendate="2021-12-6 00:00:00" fixdate="2021-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaDynamicTableFactoryTest and UpsertKafkaDynamicTableFactoryTest fails on AZP</summary>
      <description>A lot of KafkaDynamicTableFactoryTest and UpsertKafkaDynamicTableFactoryTest tests fail on AZP.Dec 06 03:00:28 [ERROR] UpsertKafkaDynamicTableFactoryTest.testInvalidSinkBufferFlush Dec 06 03:00:28 Expected: (an instance of org.apache.flink.table.api.ValidationException and Expected failure cause is &lt;org.apache.flink.table.api.ValidationException: 'sink.buffer-flush.max-rows' and 'sink.buffer-flush.interval' must be set to be greater than zero together to enable sink buffer flushing.&gt;)Dec 06 03:00:28 but: Expected failure cause is &lt;org.apache.flink.table.api.ValidationException: 'sink.buffer-flush.max-rows' and 'sink.buffer-flush.interval' must be set to be greater than zero together to enable sink buffer flushing.&gt; The throwable &lt;org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default.default.t1'.Dec 06 03:00:28 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27569&amp;view=logs&amp;j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&amp;t=918e890f-5ed9-5212-a25e-962628fb4bc5&amp;l=10186</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.ServiceLoaderUtil.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FactoryUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="25187" opendate="2021-12-6 00:00:00" fixdate="2021-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Apply padding for BINARY(&lt;precision&gt;)</summary>
      <description>When the resulting byte array that is generated for a CAST(XXX AS BINARY(&lt;precision&gt;) has length &lt; precision, then it should be padded with 0 to the right, to end up with a byte array of precision length, similarly to padding with spaces for CHAR(&lt;precision&gt;).</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionMiscITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.StringToBinaryCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.RawToBinaryCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.BinaryToBinaryCastRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="25220" opendate="2021-12-8 00:00:00" fixdate="2021-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Develop the ArchUnit Infra for test code and write an architectural rule for all IT cases w.r.t. MiniCluster</summary>
      <description>The original idea of this PR is to build architectural rules for ITCase and quickly found that current architecture test submodule only focuses on production code. In order to check the architecture violation of test code, followings should be done: build the architecture test infra for test code isolate the classpaths of production code and test code, i.e. separation of concers. define architectural rules for ITCase create ArchUnit test for some submodulesThe first try was using test jars of submodules and check the architectural violation centrally. There are some cons of this solution. First, it will need each submodule to create standard test jars that have conflict with some submodules who need extra exclusion filter for their test jars. Second, production code and test code mixed up, which makes it very difficult to define the scope of analyse classes for each rule, because some rules should only have effect on production code and others should only be used for test code. As second try, a distributed solution will be used. The architecture-test module will be split into three submodules: base for ArchUnit common extension, production for ArchUnit test of production code, test for defining rules for test code centrally. The real ArchUnit tests will be distributed and developed within submodules where architectural violation check is required.Architectural rules are required to verify that all IT cases should have: for JUnit4a public, static, final member of type MiniClusterWithClientResource annotated with ClassRule.ora public, non-static, final member of type MiniClusterWithClientResource annotated with Rule. for JUnit5a public, static, final member of type MiniClusterExtensionanda public, static, final member of type AllCallbackWrapper annotated with RegisterExtensionThe inheritance hierarchy need to be checked, because the member of MiniCluster could be defined in the super class. </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-architecture-tests.src.test.resources.archunit.properties</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-test.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-architecture-tests.violations.stored.rules</file>
      <file type="M">flink-architecture-tests.violations.e577412e-8d38-496c-a680-b842112e4b94</file>
      <file type="M">flink-architecture-tests.violations.e5126cae-f3fe-48aa-b6fb-60ae6cc3fcd5</file>
      <file type="M">flink-architecture-tests.violations.7602816f-5c01-4b7a-9e3e-235dfedec245</file>
      <file type="M">flink-architecture-tests.violations.5b9eed8a-5fb6-4373-98ac-3be2a71941b8</file>
      <file type="M">flink-architecture-tests.violations.52c5c6a1-204f-462d-9efa-4ffcb100fb4d</file>
      <file type="M">flink-architecture-tests.violations.18509c9e-3250-4c52-91b9-11ccefc85db1</file>
      <file type="M">flink-architecture-tests.src.test.java.org.apache.flink.architecture.rules.TableApiRules.java</file>
      <file type="M">flink-architecture-tests.src.test.java.org.apache.flink.architecture.rules.ApiAnnotationRules.java</file>
      <file type="M">flink-architecture-tests.src.test.java.org.apache.flink.architecture.common.SourcePredicates.java</file>
      <file type="M">flink-architecture-tests.src.test.java.org.apache.flink.architecture.common.Predicates.java</file>
      <file type="M">flink-architecture-tests.src.test.java.org.apache.flink.architecture.common.GivenJavaClasses.java</file>
      <file type="M">flink-architecture-tests.src.test.java.org.apache.flink.architecture.common.Conditions.java</file>
      <file type="M">flink-architecture-tests.src.test.java.org.apache.flink.architecture.ArchitectureTest.java</file>
      <file type="M">flink-architecture-tests.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.pom.xml</file>
      <file type="M">flink-architecture-tests.README.md</file>
      <file type="M">flink-connectors.flink-connector-files.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25222" opendate="2021-12-8 00:00:00" fixdate="2021-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove NetworkFailureProxy used for Kafka connector tests</summary>
      <description>Recently the number of Kafka connector tests either hitting a timeout due to blocked networking or corrupted network responses increased significantly. We think it is caused by our custom network failure implementation since all the tests are for the legacy FlinkKafkaProducer or FlinkKafkaConsumer we can safely remove them because we will not add more features to this connector, to increase the overall stability.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.test.java.org.apache.flink.networking.NetworkFailuresProxyTest.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.networking.NetworkFailuresProxy.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.networking.NetworkFailureHandler.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="25225" opendate="2021-12-9 00:00:00" fixdate="2021-2-9 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add e2e TPCDS tests to run against the AdaptiveBatchScheduler</summary>
      <description>To automatically and continuously verify the AdaptiveBatchScheduler, we should add a new e2e test which runs TPCDS against the AdaptiveBatchScheduler.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.flink-tpcds-test.src.main.java.org.apache.flink.table.tpcds.TpcdsTestProgram.java</file>
    </fixedFiles>
  </bug>
  <bug id="25226" opendate="2021-12-9 00:00:00" fixdate="2021-3-9 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add documentation about the AdaptiveBatchScheduler</summary>
      <description>Documentation is needed to explain to users how to enable the AdaptiveBatchScheduler and properly configuring it.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.elastic.scaling.md</file>
      <file type="M">docs.content.zh.docs.deployment.elastic.scaling.md</file>
    </fixedFiles>
  </bug>
  <bug id="25243" opendate="2021-12-10 00:00:00" fixdate="2021-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fabric8FlinkKubeClientITCase.testCheckAndUpdateConfigMapConcurrently</summary>
      <description>The test Fabric8FlinkKubeClientITCase.testCheckAndUpdateConfigMapConcurrently fails on AZP with2021-12-10T01:11:11.4908955Z Dec 10 01:11:11 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.186 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClientITCase2021-12-10T01:11:11.4920172Z Dec 10 01:11:11 [ERROR] testCheckAndUpdateConfigMapConcurrently(org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClientITCase) Time elapsed: 0.637 s &lt;&lt;&lt; ERROR!2021-12-10T01:11:11.4921592Z Dec 10 01:11:11 java.util.concurrent.ExecutionException: org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Could not complete the operation. Number of retries has been exhausted.2021-12-10T01:11:11.4925669Z Dec 10 01:11:11 at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)2021-12-10T01:11:11.4926769Z Dec 10 01:11:11 at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)2021-12-10T01:11:11.4927867Z Dec 10 01:11:11 at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClientITCase.testCheckAndUpdateConfigMapConcurrently(Fabric8FlinkKubeClientITCase.java:144)2021-12-10T01:11:11.4928904Z Dec 10 01:11:11 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2021-12-10T01:11:11.4929769Z Dec 10 01:11:11 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2021-12-10T01:11:11.4930729Z Dec 10 01:11:11 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2021-12-10T01:11:11.4931607Z Dec 10 01:11:11 at java.lang.reflect.Method.invoke(Method.java:498)2021-12-10T01:11:11.4932485Z Dec 10 01:11:11 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)2021-12-10T01:11:11.4933449Z Dec 10 01:11:11 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2021-12-10T01:11:11.4934395Z Dec 10 01:11:11 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)2021-12-10T01:11:11.4935330Z Dec 10 01:11:11 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2021-12-10T01:11:11.4937971Z Dec 10 01:11:11 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2021-12-10T01:11:11.4939315Z Dec 10 01:11:11 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2021-12-10T01:11:11.4940050Z Dec 10 01:11:11 at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)2021-12-10T01:11:11.4940711Z Dec 10 01:11:11 at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)2021-12-10T01:11:11.4941320Z Dec 10 01:11:11 at org.junit.rules.RunRules.evaluate(RunRules.java:20)2021-12-10T01:11:11.4941928Z Dec 10 01:11:11 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)2021-12-10T01:11:11.4942797Z Dec 10 01:11:11 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)2021-12-10T01:11:11.4943519Z Dec 10 01:11:11 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)2021-12-10T01:11:11.4944180Z Dec 10 01:11:11 at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)2021-12-10T01:11:11.4944815Z Dec 10 01:11:11 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)2021-12-10T01:11:11.4945461Z Dec 10 01:11:11 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)2021-12-10T01:11:11.4946107Z Dec 10 01:11:11 at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)2021-12-10T01:11:11.4946749Z Dec 10 01:11:11 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)2021-12-10T01:11:11.4947402Z Dec 10 01:11:11 at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2021-12-10T01:11:11.4948030Z Dec 10 01:11:11 at org.junit.rules.RunRules.evaluate(RunRules.java:20)2021-12-10T01:11:11.4948625Z Dec 10 01:11:11 at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2021-12-10T01:11:11.4949291Z Dec 10 01:11:11 at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)2021-12-10T01:11:11.4951248Z Dec 10 01:11:11 at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)2021-12-10T01:11:11.4951968Z Dec 10 01:11:11 at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2021-12-10T01:11:11.4952643Z Dec 10 01:11:11 at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)2021-12-10T01:11:11.4953346Z Dec 10 01:11:11 at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2021-12-10T01:11:11.4954066Z Dec 10 01:11:11 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2021-12-10T01:11:11.4954749Z Dec 10 01:11:11 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2021-12-10T01:11:11.4955371Z Dec 10 01:11:11 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)2021-12-10T01:11:11.4956126Z Dec 10 01:11:11 Caused by: org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Could not complete the operation. Number of retries has been exhausted.2021-12-10T01:11:11.4956924Z Dec 10 01:11:11 at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperation$1(FutureUtils.java:183)2021-12-10T01:11:11.4958988Z Dec 10 01:11:11 at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)2021-12-10T01:11:11.4960177Z Dec 10 01:11:11 at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)2021-12-10T01:11:11.4961252Z Dec 10 01:11:11 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)2021-12-10T01:11:11.4961973Z Dec 10 01:11:11 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2021-12-10T01:11:11.4962692Z Dec 10 01:11:11 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2021-12-10T01:11:11.4963293Z Dec 10 01:11:11 at java.lang.Thread.run(Thread.java:748)2021-12-10T01:11:11.4968062Z Dec 10 01:11:11 Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.persistence.PossibleInconsistentStateException: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: PUT at: https://10.1.0.82:8443/api/v1/namespaces/default/configmaps/test-config-map. Message: Operation cannot be fulfilled on configmaps "test-config-map": the object has been modified; please apply your changes to the latest version and try again. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=null, kind=configmaps, name=test-config-map, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=Operation cannot be fulfilled on configmaps "test-config-map": the object has been modified; please apply your changes to the latest version and try again, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=Conflict, status=Failure, additionalProperties={}).2021-12-10T01:11:11.4971865Z Dec 10 01:11:11 at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$null$7(Fabric8FlinkKubeClient.java:316)2021-12-10T01:11:11.4972946Z Dec 10 01:11:11 at java.util.Optional.map(Optional.java:215)2021-12-10T01:11:11.4973764Z Dec 10 01:11:11 at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$null$8(Fabric8FlinkKubeClient.java:290)2021-12-10T01:11:11.4974416Z Dec 10 01:11:11 at java.util.Optional.map(Optional.java:215)2021-12-10T01:11:11.4975062Z Dec 10 01:11:11 at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$null$10(Fabric8FlinkKubeClient.java:287)2021-12-10T01:11:11.4975797Z Dec 10 01:11:11 at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)2021-12-10T01:11:11.4976309Z Dec 10 01:11:11 ... 3 more2021-12-10T01:11:11.4980247Z Dec 10 01:11:11 Caused by: org.apache.flink.runtime.persistence.PossibleInconsistentStateException: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: PUT at: https://10.1.0.82:8443/api/v1/namespaces/default/configmaps/test-config-map. Message: Operation cannot be fulfilled on configmaps "test-config-map": the object has been modified; please apply your changes to the latest version and try again. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=null, kind=configmaps, name=test-config-map, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=Operation cannot be fulfilled on configmaps "test-config-map": the object has been modified; please apply your changes to the latest version and try again, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=Conflict, status=Failure, additionalProperties={}).2021-12-10T01:11:11.4983161Z Dec 10 01:11:11 ... 9 more2021-12-10T01:11:11.4986826Z Dec 10 01:11:11 Caused by: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: PUT at: https://10.1.0.82:8443/api/v1/namespaces/default/configmaps/test-config-map. Message: Operation cannot be fulfilled on configmaps "test-config-map": the object has been modified; please apply your changes to the latest version and try again. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=null, kind=configmaps, name=test-config-map, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=Operation cannot be fulfilled on configmaps "test-config-map": the object has been modified; please apply your changes to the latest version and try again, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=Conflict, status=Failure, additionalProperties={}).2021-12-10T01:11:11.4989960Z Dec 10 01:11:11 at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:568)2021-12-10T01:11:11.4990726Z Dec 10 01:11:11 at io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:507)2021-12-10T01:11:11.4991573Z Dec 10 01:11:11 at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:471)2021-12-10T01:11:11.4992315Z Dec 10 01:11:11 at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:430)2021-12-10T01:11:11.4993041Z Dec 10 01:11:11 at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleReplace(OperationSupport.java:289)2021-12-10T01:11:11.4993777Z Dec 10 01:11:11 at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleReplace(OperationSupport.java:269)2021-12-10T01:11:11.4994494Z Dec 10 01:11:11 at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleReplace(BaseOperation.java:820)2021-12-10T01:11:11.4995314Z Dec 10 01:11:11 at io.fabric8.kubernetes.client.dsl.base.HasMetadataOperation.lambda$replace$1(HasMetadataOperation.java:86)2021-12-10T01:11:11.4996033Z Dec 10 01:11:11 at io.fabric8.kubernetes.api.model.DoneableConfigMap.done(DoneableConfigMap.java:26)2021-12-10T01:11:11.5028417Z Dec 10 01:11:11 at io.fabric8.kubernetes.api.model.DoneableConfigMap.done(DoneableConfigMap.java:5)2021-12-10T01:11:11.5029177Z Dec 10 01:11:11 at io.fabric8.kubernetes.client.dsl.base.HasMetadataOperation.replace(HasMetadataOperation.java:92)2021-12-10T01:11:11.5030107Z Dec 10 01:11:11 at io.fabric8.kubernetes.client.dsl.base.HasMetadataOperation.replace(HasMetadataOperation.java:36)2021-12-10T01:11:11.5030856Z Dec 10 01:11:11 at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$null$7(Fabric8FlinkKubeClient.java:301)2021-12-10T01:11:11.5031427Z Dec 10 01:11:11 ... 8 morehttps://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27917&amp;view=logs&amp;j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&amp;t=3425d8ba-5f03-540a-c64b-51b8481bf7d6&amp;l=3029</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesResource.java</file>
    </fixedFiles>
  </bug>
  <bug id="25244" opendate="2021-12-10 00:00:00" fixdate="2021-1-10 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Deprecate Java 8 support</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25269" opendate="2021-12-13 00:00:00" fixdate="2021-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Return 503 Service Unavailable if endpoint is not ready yet</summary>
      <description>If an endpoint (e.g., the RM) has not been started yet when receiving a request from the REST API, then an exception is thrown that is treated as an unhandled exception. we can handle this in a nicer way by returning 503 Service Unavailable.Dec 10 13:12:17 at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_ae8961cc-571e-483d-b468-da652db90bb9.jar:1.15-SNAPSHOT]Dec 10 13:12:17 at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_ae8961cc-571e-483d-b468-da652db90bb9.jar:1.15-SNAPSHOT]Dec 10 13:12:17 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_ae8961cc-571e-483d-b468-da652db90bb9.jar:1.15-SNAPSHOT]Dec 10 13:12:17 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_ae8961cc-571e-483d-b468-da652db90bb9.jar:1.15-SNAPSHOT]Dec 10 13:12:17 at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_ae8961cc-571e-483d-b468-da652db90bb9.jar:1.15-SNAPSHOT]Dec 10 13:12:17 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_ae8961cc-571e-483d-b468-da652db90bb9.jar:1.15-SNAPSHOT]Dec 10 13:12:17 at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_ae8961cc-571e-483d-b468-da652db90bb9.jar:1.15-SNAPSHOT]Dec 10 13:12:17 at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_ae8961cc-571e-483d-b468-da652db90bb9.jar:1.15-SNAPSHOT]Dec 10 13:12:17 at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]Dec 10 13:12:17 at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]Dec 10 13:12:17 at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]Dec 10 13:12:17 at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]Dec 10 13:12:17 2021-12-10 13:12:07,296 INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Starting the resource manager.Dec 10 13:12:17 2021-12-10 13:12:09,426 INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 10.1.0.79:35275-d26a40 (akka.tcp://flink@10.1.0.79:35275/user/rpc/taskmanager_0) at ResourceManagerDec 10 13:12:17 2021-12-10 13:12:09,496 INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 10.1.0.79:35275-d26a40 (akka.tcp://flink@10.1.0.79:35275/user/rpc/taskmanager_0) at ResourceManagerDec 10 13:12:17 2021-12-10 13:12:13,189 INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Received JobGraph submission 'Elasticsearch7.x end to end sink test example' (effb86be1cfd961e947988200a724de7).Dec 10 13:12:17 2021-12-10 13:12:13,190 INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Submitting job 'Elasticsearch7.x end to end sink test example' (effb86be1cfd961e947988200a724de7).Dec 10 13:12:17 2021-12-10 13:12:13,225 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_2 .Dec 10 13:12:17 2021-12-10 13:12:13,252 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Initializing job 'Elasticsearch7.x end to end sink test example' (effb86be1cfd961e947988200a724de7).Dec 10 13:12:17 2021-12-10 13:12:13,324 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Using restart back off time strategy FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=2147483647, backoffTimeMS=1000) for Elasticsearch7.x end to end sink test example (effb86be1cfd961e947988200a724de7).Dec 10 13:12:17 2021-12-10 13:12:13,428 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Running initialization on master for job Elasticsearch7.x end to end sink test example (effb86be1cfd961e947988200a724de7). https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27953&amp;view=logs&amp;j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&amp;t=160c9ae5-96fd-516e-1c91-deb81f59292a&amp;l=3408</description>
      <version>1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.AbstractHandler.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.AbstractHandlerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="25280" opendate="2021-12-13 00:00:00" fixdate="2021-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaPartitionSplitReaderTest failed on azure due to Offsets out of range with no configured reset policy for partitions</summary>
      <description>2021-12-13T03:30:12.8392593Z Dec 13 03:30:12 [ERROR] Tests run: 6, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 85.344 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReaderTest2021-12-13T03:30:12.8394604Z Dec 13 03:30:12 [ERROR] testNumBytesInCounter Time elapsed: 0.24 s &lt;&lt;&lt; ERROR!2021-12-13T03:30:12.8396218Z Dec 13 03:30:12 org.apache.kafka.clients.consumer.OffsetOutOfRangeException: Offsets out of range with no configured reset policy for partitions: {topic1-0=0}2021-12-13T03:30:12.8397052Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.internals.Fetcher.initializeCompletedFetch(Fetcher.java:1260)2021-12-13T03:30:12.8397697Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.internals.Fetcher.fetchedRecords(Fetcher.java:607)2021-12-13T03:30:12.8398394Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1313)2021-12-13T03:30:12.8399306Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1240)2021-12-13T03:30:12.8399924Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1211)2021-12-13T03:30:12.8400610Z Dec 13 03:30:12 at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.fetch(KafkaPartitionSplitReader.java:113)2021-12-13T03:30:12.8401385Z Dec 13 03:30:12 at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReaderTest.testNumBytesInCounter(KafkaPartitionSplitReaderTest.java:153)2021-12-13T03:30:12.8402174Z Dec 13 03:30:12 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2021-12-13T03:30:12.8402911Z Dec 13 03:30:12 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2021-12-13T03:30:12.8403818Z Dec 13 03:30:12 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2021-12-13T03:30:12.8404452Z Dec 13 03:30:12 at java.lang.reflect.Method.invoke(Method.java:498)2021-12-13T03:30:12.8405028Z Dec 13 03:30:12 at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)2021-12-13T03:30:12.8405740Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)2021-12-13T03:30:12.8406749Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)2021-12-13T03:30:12.8407886Z Dec 13 03:30:12 at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)2021-12-13T03:30:12.8408845Z Dec 13 03:30:12 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)2021-12-13T03:30:12.8409507Z Dec 13 03:30:12 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)2021-12-13T03:30:12.8410219Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)2021-12-13T03:30:12.8411081Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)2021-12-13T03:30:12.8411785Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)2021-12-13T03:30:12.8412740Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)2021-12-13T03:30:12.8413553Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)2021-12-13T03:30:12.8414293Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)2021-12-13T03:30:12.8415078Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)2021-12-13T03:30:12.8415977Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)2021-12-13T03:30:12.8417383Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)2021-12-13T03:30:12.8418339Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8419209Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)2021-12-13T03:30:12.8419942Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)2021-12-13T03:30:12.8420716Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)2021-12-13T03:30:12.8421423Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)2021-12-13T03:30:12.8422192Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8423061Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)2021-12-13T03:30:12.8423740Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2021-12-13T03:30:12.8424514Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)2021-12-13T03:30:12.8425224Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8425912Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)2021-12-13T03:30:12.8426672Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)2021-12-13T03:30:12.8427251Z Dec 13 03:30:12 at java.util.ArrayList.forEach(ArrayList.java:1259)2021-12-13T03:30:12.8427947Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)2021-12-13T03:30:12.8428848Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)2021-12-13T03:30:12.8429554Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8430394Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)2021-12-13T03:30:12.8431046Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2021-12-13T03:30:12.8431704Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)2021-12-13T03:30:12.8432455Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8433310Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)2021-12-13T03:30:12.8433983Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)2021-12-13T03:30:12.8434685Z Dec 13 03:30:12 at java.util.ArrayList.forEach(ArrayList.java:1259)2021-12-13T03:30:12.8435379Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)2021-12-13T03:30:12.8436164Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)2021-12-13T03:30:12.8436853Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8437658Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)2021-12-13T03:30:12.8438321Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2021-12-13T03:30:12.8439127Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)2021-12-13T03:30:12.8439833Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8440559Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)2021-12-13T03:30:12.8441222Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)2021-12-13T03:30:12.8442108Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)2021-12-13T03:30:12.8443108Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)2021-12-13T03:30:12.8443845Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)2021-12-13T03:30:12.8444481Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)2021-12-13T03:30:12.8445164Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)2021-12-13T03:30:12.8445835Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)2021-12-13T03:30:12.8446527Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)2021-12-13T03:30:12.8447150Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)2021-12-13T03:30:12.8447920Z Dec 13 03:30:12 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)2021-12-13T03:30:12.8448754Z Dec 13 03:30:12 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)2021-12-13T03:30:12.8449532Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2021-12-13T03:30:12.8450305Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2021-12-13T03:30:12.8450976Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2021-12-13T03:30:12.8451567Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)2021-12-13T03:30:12.8451989Z Dec 13 03:30:12 2021-12-13T03:30:12.8452504Z Dec 13 03:30:12 [ERROR] testHandleSplitChangesAndFetch Time elapsed: 0.037 s &lt;&lt;&lt; ERROR!2021-12-13T03:30:12.8454253Z Dec 13 03:30:12 org.apache.kafka.clients.consumer.OffsetOutOfRangeException: Offsets out of range with no configured reset policy for partitions: {topic1-2=2}2021-12-13T03:30:12.8455008Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.internals.Fetcher.initializeCompletedFetch(Fetcher.java:1260)2021-12-13T03:30:12.8455818Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.internals.Fetcher.fetchedRecords(Fetcher.java:607)2021-12-13T03:30:12.8456428Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1282)2021-12-13T03:30:12.8457043Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1240)2021-12-13T03:30:12.8457626Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1211)2021-12-13T03:30:12.8458300Z Dec 13 03:30:12 at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.fetch(KafkaPartitionSplitReader.java:113)2021-12-13T03:30:12.8459238Z Dec 13 03:30:12 at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReaderTest.assignSplitsAndFetchUntilFinish(KafkaPartitionSplitReaderTest.java:261)2021-12-13T03:30:12.8460115Z Dec 13 03:30:12 at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReaderTest.testHandleSplitChangesAndFetch(KafkaPartitionSplitReaderTest.java:105)2021-12-13T03:30:12.8460760Z Dec 13 03:30:12 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2021-12-13T03:30:12.8461310Z Dec 13 03:30:12 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2021-12-13T03:30:12.8461937Z Dec 13 03:30:12 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2021-12-13T03:30:12.8462742Z Dec 13 03:30:12 at java.lang.reflect.Method.invoke(Method.java:498)2021-12-13T03:30:12.8463452Z Dec 13 03:30:12 at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)2021-12-13T03:30:12.8464077Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)2021-12-13T03:30:12.8464794Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)2021-12-13T03:30:12.8465636Z Dec 13 03:30:12 at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)2021-12-13T03:30:12.8466298Z Dec 13 03:30:12 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)2021-12-13T03:30:12.8466990Z Dec 13 03:30:12 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)2021-12-13T03:30:12.8467722Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)2021-12-13T03:30:12.8468454Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)2021-12-13T03:30:12.8469192Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)2021-12-13T03:30:12.8469941Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)2021-12-13T03:30:12.8470649Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)2021-12-13T03:30:12.8471493Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)2021-12-13T03:30:12.8472238Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)2021-12-13T03:30:12.8473107Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)2021-12-13T03:30:12.8473827Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)2021-12-13T03:30:12.8474551Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8475262Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)2021-12-13T03:30:12.8476109Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)2021-12-13T03:30:12.8477234Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)2021-12-13T03:30:12.8478320Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)2021-12-13T03:30:12.8479437Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8480706Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)2021-12-13T03:30:12.8481725Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2021-12-13T03:30:12.8482890Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)2021-12-13T03:30:12.8483628Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8484318Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)2021-12-13T03:30:12.8484989Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)2021-12-13T03:30:12.8485557Z Dec 13 03:30:12 at java.util.ArrayList.forEach(ArrayList.java:1259)2021-12-13T03:30:12.8486228Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)2021-12-13T03:30:12.8487030Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)2021-12-13T03:30:12.8487731Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8488431Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)2021-12-13T03:30:12.8489079Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2021-12-13T03:30:12.8489731Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)2021-12-13T03:30:12.8490576Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8491262Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)2021-12-13T03:30:12.8491917Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)2021-12-13T03:30:12.8492550Z Dec 13 03:30:12 at java.util.ArrayList.forEach(ArrayList.java:1259)2021-12-13T03:30:12.8493404Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)2021-12-13T03:30:12.8494341Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)2021-12-13T03:30:12.8495043Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8495729Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)2021-12-13T03:30:12.8496390Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2021-12-13T03:30:12.8497036Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)2021-12-13T03:30:12.8497730Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8498411Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)2021-12-13T03:30:12.8499075Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)2021-12-13T03:30:12.8499820Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)2021-12-13T03:30:12.8500699Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)2021-12-13T03:30:12.8501427Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)2021-12-13T03:30:12.8502155Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)2021-12-13T03:30:12.8502853Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)2021-12-13T03:30:12.8503533Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)2021-12-13T03:30:12.8504175Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)2021-12-13T03:30:12.8504790Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)2021-12-13T03:30:12.8505463Z Dec 13 03:30:12 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)2021-12-13T03:30:12.8506155Z Dec 13 03:30:12 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)2021-12-13T03:30:12.8506837Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2021-12-13T03:30:12.8507495Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2021-12-13T03:30:12.8508098Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2021-12-13T03:30:12.8508684Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)2021-12-13T03:30:12.8509121Z Dec 13 03:30:12 2021-12-13T03:30:12.8509555Z Dec 13 03:30:12 [ERROR] testPendingRecordsGauge{String}[1] Time elapsed: 0.037 s &lt;&lt;&lt; ERROR!2021-12-13T03:30:12.8510837Z Dec 13 03:30:12 org.apache.kafka.clients.consumer.OffsetOutOfRangeException: Offsets out of range with no configured reset policy for partitions: {topic1-0=0}2021-12-13T03:30:12.8511603Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.internals.Fetcher.initializeCompletedFetch(Fetcher.java:1260)2021-12-13T03:30:12.8512314Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.internals.Fetcher.fetchedRecords(Fetcher.java:607)2021-12-13T03:30:12.8513023Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1313)2021-12-13T03:30:12.8513726Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1240)2021-12-13T03:30:12.8514320Z Dec 13 03:30:12 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1211)2021-12-13T03:30:12.8514990Z Dec 13 03:30:12 at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.fetch(KafkaPartitionSplitReader.java:113)2021-12-13T03:30:12.8515785Z Dec 13 03:30:12 at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReaderTest.testPendingRecordsGauge(KafkaPartitionSplitReaderTest.java:195)2021-12-13T03:30:12.8516439Z Dec 13 03:30:12 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2021-12-13T03:30:12.8516982Z Dec 13 03:30:12 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2021-12-13T03:30:12.8517599Z Dec 13 03:30:12 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2021-12-13T03:30:12.8518168Z Dec 13 03:30:12 at java.lang.reflect.Method.invoke(Method.java:498)2021-12-13T03:30:12.8518689Z Dec 13 03:30:12 at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)2021-12-13T03:30:12.8519286Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)2021-12-13T03:30:12.8519965Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)2021-12-13T03:30:12.8520724Z Dec 13 03:30:12 at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)2021-12-13T03:30:12.8521346Z Dec 13 03:30:12 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)2021-12-13T03:30:12.8522067Z Dec 13 03:30:12 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)2021-12-13T03:30:12.8522856Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)2021-12-13T03:30:12.8523569Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)2021-12-13T03:30:12.8524281Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)2021-12-13T03:30:12.8525034Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)2021-12-13T03:30:12.8525759Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)2021-12-13T03:30:12.8526477Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)2021-12-13T03:30:12.8527146Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)2021-12-13T03:30:12.8527788Z Dec 13 03:30:12 at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)2021-12-13T03:30:12.8528479Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)2021-12-13T03:30:12.8529197Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8529907Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)2021-12-13T03:30:12.8530615Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)2021-12-13T03:30:12.8531290Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)2021-12-13T03:30:12.8532087Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)2021-12-13T03:30:12.8532981Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8533690Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)2021-12-13T03:30:12.8534344Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2021-12-13T03:30:12.8535079Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)2021-12-13T03:30:12.8535920Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8536611Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)2021-12-13T03:30:12.8537256Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)2021-12-13T03:30:12.8538004Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)2021-12-13T03:30:12.8538773Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:212)2021-12-13T03:30:12.8539606Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:192)2021-12-13T03:30:12.8540337Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)2021-12-13T03:30:12.8541043Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)2021-12-13T03:30:12.8541698Z Dec 13 03:30:12 at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)2021-12-13T03:30:12.8542374Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2021-12-13T03:30:12.8543038Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)2021-12-13T03:30:12.8543597Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2021-12-13T03:30:12.8544175Z Dec 13 03:30:12 at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)2021-12-13T03:30:12.8544752Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$11$1.accept(ReferencePipeline.java:440)2021-12-13T03:30:12.8545336Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2021-12-13T03:30:12.8545909Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2021-12-13T03:30:12.8546483Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2021-12-13T03:30:12.8547063Z Dec 13 03:30:12 at java.util.stream.Streams$StreamBuilderImpl.forEachRemaining(Streams.java:419)2021-12-13T03:30:12.8547657Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647)2021-12-13T03:30:12.8548221Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)2021-12-13T03:30:12.8548796Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2021-12-13T03:30:12.8549382Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2021-12-13T03:30:12.8549941Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)2021-12-13T03:30:12.8550510Z Dec 13 03:30:12 at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)2021-12-13T03:30:12.8551089Z Dec 13 03:30:12 at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)2021-12-13T03:30:12.8551742Z Dec 13 03:30:12 at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)2021-12-13T03:30:12.8552355Z Dec 13 03:30:12 at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)2021-12-13T03:30:12.8553007Z Dec 13 03:30:12 at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)2021-12-13T03:30:12.8553600Z Dec 13 03:30:12 at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)2021-12-13T03:30:12.8554173Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)2021-12-13T03:30:12.8554746Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)2021-12-13T03:30:12.8555332Z Dec 13 03:30:12 at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)2021-12-13T03:30:12.8555910Z Dec 13 03:30:12 at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)2021-12-13T03:30:12.8556492Z Dec 13 03:30:12 at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)2021-12-13T03:30:12.8557062Z Dec 13 03:30:12 at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)2021-12-13T03:30:12.8557661Z Dec 13 03:30:12 at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)2021-12-13T03:30:12.8558246Z Dec 13 03:30:12 at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)2021-12-13T03:30:12.8558915Z Dec 13 03:30:12 at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)2021-12-13T03:30:12.8559554Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:107)2021-12-13T03:30:12.8560251Z Dec 13 03:30:12 at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:42)2021-12-13T03:30:12.8560959Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)2021-12-13T03:30:12.8561662Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8562420Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)2021-12-13T03:30:12.8563140Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2021-12-13T03:30:12.8563795Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)2021-12-13T03:30:12.8564500Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8565183Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)2021-12-13T03:30:12.8565852Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)2021-12-13T03:30:12.8566632Z Dec 13 03:30:12 at java.util.ArrayList.forEach(ArrayList.java:1259)2021-12-13T03:30:12.8567724Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)2021-12-13T03:30:12.8568691Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)2021-12-13T03:30:12.8569384Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8570088Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)2021-12-13T03:30:12.8570756Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2021-12-13T03:30:12.8571405Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)2021-12-13T03:30:12.8572436Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8573210Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)2021-12-13T03:30:12.8573873Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)2021-12-13T03:30:12.8574445Z Dec 13 03:30:12 at java.util.ArrayList.forEach(ArrayList.java:1259)2021-12-13T03:30:12.8575106Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)2021-12-13T03:30:12.8575902Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)2021-12-13T03:30:12.8576603Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8577308Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)2021-12-13T03:30:12.8577953Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2021-12-13T03:30:12.8578602Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)2021-12-13T03:30:12.8579383Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2021-12-13T03:30:12.8580063Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)2021-12-13T03:30:12.8580724Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)2021-12-13T03:30:12.8581456Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)2021-12-13T03:30:12.8582313Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)2021-12-13T03:30:12.8583105Z Dec 13 03:30:12 at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)2021-12-13T03:30:12.8583737Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)2021-12-13T03:30:12.8584367Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)2021-12-13T03:30:12.8585019Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)2021-12-13T03:30:12.8585660Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)2021-12-13T03:30:12.8586287Z Dec 13 03:30:12 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)2021-12-13T03:30:12.8586947Z Dec 13 03:30:12 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)2021-12-13T03:30:12.8587623Z Dec 13 03:30:12 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)2021-12-13T03:30:12.8588313Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2021-12-13T03:30:12.8588963Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2021-12-13T03:30:12.8589570Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2021-12-13T03:30:12.8590154Z Dec 13 03:30:12 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)2021-12-13T03:30:12.8590666Z Dec 13 03:30:12 testHandleSplitChangesAndFetchtestNumBytesInCountertestPendingRecordsGaugefrom KafkaPartitionSplitReaderTest failed with this issue</description>
      <version>1.13.5,1.14.3,1.15.0</version>
      <fixedVersion>1.13.6,1.14.4,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="25282" opendate="2021-12-13 00:00:00" fixdate="2021-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move runtime dependencies from table-planner to table-runtime</summary>
      <description>There are several runtime dependencies (e.g. functions used in codegen) that are shipped by table-planner and calcite-core. We should move these dependencies to runtime</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.SqlJsonUtils.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.SqlFunctionUtils.java</file>
      <file type="M">flink-table.flink-table-runtime.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.StringCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.LikeCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.JsonValueCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.FloorCeilCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLegacyTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecIntervalJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.WatermarkPushDownSpec.java</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.DateTimeUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.functions.SqlLikeUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="25341" opendate="2021-12-16 00:00:00" fixdate="2021-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve casting STRUCTURED type to STRING</summary>
      <description>Structured types currently use ROW to STRING logic. However, this is not very useful for users as the field order might be determined by Flink. Also, structured types has the nice property of defining a custom toString and attribute names.I would suggest the following:If the structured type has a StructuredType.getImplementationClass convert to external class and call toString.If no implementation class is present or the toString is not possible, use the string representation of maps.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.RowToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.RowToRowCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.MapAndMultisetToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CodeGeneratorCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRuleProvider.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.ArrayToStringCastRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.ArrayToArrayCastRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="25348" opendate="2021-12-16 00:00:00" fixdate="2021-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update release guide to reset japicmp exceptions for every release</summary>
      <description>I propose to clean up the japicmp maven plugin exclusion for every minor release for @Public and the exclusions for @PublicEvolving with every minor release. Currently, we don’t do this and that’s why we have accumulated quite some list of exclusions that a) might shadow other problems and b) nobody really knows why they are still relevant. I would propose to make this part of the release guide. The result should be that we minimize our set of exclusions.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-streaming-java.pom.xml</file>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25351" opendate="2021-12-16 00:00:00" fixdate="2021-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add FlinkVersion</summary>
      <description>In order to check when a method needs to graduate we need a FlinkVersion enum that can represent the different versions. Moreover, we should add it to the release guide that this enum needs to be extended for every Flink version.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.typeutils.LinkedListSerializerUpgradeTest.java</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.migration.StatefulJobWBroadcastStateMigrationITCase.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.migration.StatefulJobSavepointMigrationITCase.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.ChainUnionTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.ChainOrderTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.ChainLengthStatelessDecreaseTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.ChainLengthIncreaseTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.ChainLengthDecreaseTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.ChainBreakTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.AbstractNonKeyedOperatorRestoreTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.keyed.KeyedComplexChainTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.keyed.AbstractKeyedOperatorRestoreTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.migration.TypeSerializerSnapshotMigrationITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.StatefulJobWBroadcastStateMigrationITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.StatefulJobSavepointMigrationITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.LegacyStatefulJobSavepointMigrationITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.xa.JdbcXaSinkMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerMigrationOperatorTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaMigrationTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaSerializerUpgradeTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerMigrationTest.java</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.src.test.java.org.apache.flink.api.java.typeutils.runtime.WritableSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.array.PrimitiveArraySerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.array.PrimitiveArraySerializerUpgradeTestSpecifications.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.BasicTypeSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.BasicTypeSerializerUpgradeTestSpecifications.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.EnumSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.ListSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.MapSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.CompositeTypeSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.TypeSerializerUpgradeTestBase.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.CopyableSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.NullableSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.PojoSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.PojoSerializerUpgradeTestSpecifications.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.RowSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.java.typeutils.runtime.ValueSerializerUpgradeTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.testutils.migration.MigrationVersion.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.typeutils.AvroSerializerUpgradeTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.ContinuousFileProcessingMigrationTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.NFASerializerUpgradeTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.sharedbuffer.LockableTypeSerializerUpgradeTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPMigrationTest.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.test.java.org.apache.flink.graph.drivers.transform.LongValueWithProperHashCodeSerializerUpgradeTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.types.valuearray.ValueArraySerializerUpgradeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ArrayListSerializerUpgradeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.JavaSerializerUpgradeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlSerializerUpgradeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.VoidNamespaceSerializerUpgradeTest.java</file>
      <file type="M">flink-scala.src.test.java.org.apache.flink.api.scala.typeutils.OptionSerializerUpgradeTest.java</file>
      <file type="M">flink-scala.src.test.java.org.apache.flink.api.scala.typeutils.ScalaEitherSerializerUpgradeTest.java</file>
      <file type="M">flink-scala.src.test.java.org.apache.flink.api.scala.typeutils.ScalaTrySerializerUpgradeTest.java</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.typeutils.EnumValueSerializerUpgradeTest.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.typeutils.ScalaCaseClassSerializerUpgradeTest.scala</file>
      <file type="M">flink-scala.src.test.scala.org.apache.flink.api.scala.typeutils.TraversableSerializerUpgradeTest.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.datastream.UnionSerializerUpgradeTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkStateSerializerUpgradeTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.co.BufferEntrySerializerUpgradeTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.TimerSerializerUpgradeTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorMigrationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowSerializerUpgradeTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializerUpgradeTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="25368" opendate="2021-12-18 00:00:00" fixdate="2021-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use AdminClient to get offsets rather than KafkaConsumer</summary>
      <description>`AdminClient.listOffsets` is provided in Kafka 2.7, In the future more `OffsetSpce` types will be added to it, for example, OffsetSpec.MaxTimestampSpec is added in Kafka 3.0. so it's better to substitute `KafkaConsumer` with `AdminClient`.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.enumerator.KafkaEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="25370" opendate="2021-12-18 00:00:00" fixdate="2021-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase SQL Connector Options &amp;#39;table-name&amp;#39; description</summary>
      <description>In document the HBase SQL Connector Options "table-name" description is "The name of HBase table to connect.". All demo of HBase SQL Connector document 'table-name' just give the 'tablename', it does't explain how to use the table in the specified namespace of HBase. So I think we need to add some description for this options</description>
      <version>1.13.5,1.14.2,1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase-base.src.main.java.org.apache.flink.connector.hbase.table.HBaseConnectorOptions.java</file>
      <file type="M">docs.content.docs.connectors.table.hbase.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hbase.md</file>
    </fixedFiles>
  </bug>
  <bug id="25372" opendate="2021-12-18 00:00:00" fixdate="2021-12-18 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add thread dump feature for jobmanager</summary>
      <description>Add thread dump feature for jobmanager in addition to the previous work on TM side: FLINK-14816. It is useful for debugging job submission and scheduling issues especially in OLAP scenarios.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager-routing.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-manager.ts</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingRestfulGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingDispatcherGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.RestfulGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.ThreadDumpInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGatewayBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.taskmanager.ThreadDumpInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.utils.TestingResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGatewayDecoratorBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.ThreadDumpInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerThreadDumpHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="25395" opendate="2021-12-20 00:00:00" fixdate="2021-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FileNotFoundException during recovery caused by Incremental shared state being discarded by TM</summary>
      <description>Extracting from FLINK-25185 discussionOn checkpoint abortion or any failure in AsyncCheckpointRunnable,it discards the state, in particular shared (incremental) state.Since FLINK-24611, this creates a problem because shared state can be re-used for future checkpoints. A similar case is in PeriodicMaterializationManager (uploaded SST files will be deleted on failure without notifying the wrapped RocksDB state backend). Symptom of this failure is a following exception during recovery:Caused by: java.io.FileNotFoundException: /tmp/junit3146957979516280339/junit1602669867129285236/d6a6dbdd-3fd7-4786-9dc1-9ccc161740da (No such file or directory) at java.io.FileInputStream.open0(Native Method) ~[?:1.8.0_292] at java.io.FileInputStream.open(FileInputStream.java:195) ~[?:1.8.0_292] at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:138) ~[?:1.8.0_292] at org.apache.flink.core.fs.local.LocalDataInputStream.&lt;init&gt;(LocalDataInputStream.java:50) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT] at org.apache.flink.core.fs.local.LocalFileSystem.open(LocalFileSystem.java:134) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT] at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.open(SafetyNetWrapperFileSystem.java:87) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT] at org.apache.flink.runtime.state.filesystem.FileStateHandle.openInputStream(FileStateHandle.java:68) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT] at org.apache.flink.changelog.fs.StateChangeFormat.read(StateChangeFormat.java:92) ~[flink-dstl-dfs-1.15-SNAPSHOT.jar:1.15-SNAPSHOT] at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:85) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.TestJobExecutor.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.graph.TestJobBuilders.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.graph.TestEventSource.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.graph.OneInputTestStreamOperatorFactory.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.graph.OneInputTestStreamOperator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.command.TestCommandDispatcherImpl.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.command.TestCommand.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.ChangelogStateHandleStreamImpl.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
    </fixedFiles>
  </bug>
  <bug id="25398" opendate="2021-12-21 00:00:00" fixdate="2021-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show complete stacktrace when requesting thread dump</summary>
      <description>WhyNow the stacktrace is not complete when clicking the task executor's threaddump  in runtime webui. Hence it's hard to the initial calling according to the stacktrace.Now the thread stacktrace is limited to 8, refer to openjdk: https://github.com/openjdk/jdk/blob/master/src/java.management/share/classes/java/lang/management/ThreadInfo.java#L597 SolutionUsing the custom stringify method to return stacktrace instead of using ThreadInfo.toString directly </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.ThreadDumpInfoTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.ThreadDumpInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.cluster.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.cluster.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="25399" opendate="2021-12-21 00:00:00" fixdate="2021-1-21 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>AZP fails with exit code 137 when running checkpointing test cases</summary>
      <description>The AZP build for fine grained resource management failed with exit code 137, when running an extensive list of checkpointing tests:2021-12-21T06:06:08.8728404Z Dec 21 06:06:08 [INFO] Running org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase2021-12-21T06:06:37.6584668Z Dec 21 06:06:37 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[upscale union from 3 to 7, buffersPerChannel = 0].2021-12-21T06:06:37.6585685Z Dec 21 06:06:37 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[upscale union from 3 to 7, buffersPerChannel = 0].2021-12-21T06:06:37.6593448Z Dec 21 06:06:37 Finished org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[upscale union from 3 to 7, buffersPerChannel = 0].2021-12-21T06:06:41.3044200Z Dec 21 06:06:41 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testSlidingTimeWindow[statebackend type =MEM, buffersPerChannel = 0].2021-12-21T06:06:41.3045146Z Dec 21 06:06:41 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testSlidingTimeWindow[statebackend type =MEM, buffersPerChannel = 0].2021-12-21T06:06:49.7482529Z Dec 21 06:06:49 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMinMaxParallelism[statebackend type =MEM, buffersPerChannel = 0].2021-12-21T06:06:49.7483922Z Dec 21 06:06:49 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMinMaxParallelism[statebackend type =MEM, buffersPerChannel = 0].2021-12-21T06:06:56.7462828Z Dec 21 06:06:56 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindow[statebackend type =MEM, buffersPerChannel = 0].2021-12-21T06:06:56.7463831Z Dec 21 06:06:56 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindow[statebackend type =MEM, buffersPerChannel = 0].2021-12-21T06:07:06.7225398Z Dec 21 06:07:06 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMaxMaxParallelism[statebackend type =MEM, buffersPerChannel = 0].2021-12-21T06:07:06.7226580Z Dec 21 06:07:06 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMaxMaxParallelism[statebackend type =MEM, buffersPerChannel = 0].2021-12-21T06:07:12.1987555Z Dec 21 06:07:12 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[upscale union from 3 to 7, buffersPerChannel = 1].2021-12-21T06:07:12.1992168Z Dec 21 06:07:12 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[upscale union from 3 to 7, buffersPerChannel = 1].2021-12-21T06:07:12.1993591Z Dec 21 06:07:12 Finished org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[upscale union from 3 to 7, buffersPerChannel = 1].2021-12-21T06:07:16.3825669Z Dec 21 06:07:15 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testPreAggregatedTumblingTimeWindow[statebackend type =MEM, buffersPerChannel = 0].2021-12-21T06:07:16.3826827Z Dec 21 06:07:15 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testPreAggregatedTumblingTimeWindow[statebackend type =MEM, buffersPerChannel = 0].2021-12-21T06:07:23.4489701Z Dec 21 06:07:23 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testPreAggregatedSlidingTimeWindow[statebackend type =MEM, buffersPerChannel = 0].2021-12-21T06:07:23.4495250Z Dec 21 06:07:23 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testPreAggregatedSlidingTimeWindow[statebackend type =MEM, buffersPerChannel = 0].2021-12-21T06:07:29.8407484Z Dec 21 06:07:29 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testSlidingTimeWindow[statebackend type =MEM, buffersPerChannel = 2].2021-12-21T06:07:29.8417385Z Dec 21 06:07:29 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testSlidingTimeWindow[statebackend type =MEM, buffersPerChannel = 2].2021-12-21T06:07:31.2376012Z Dec 21 06:07:31 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[downscale union from 2 to 1, buffersPerChannel = 0].2021-12-21T06:07:31.2382722Z Dec 21 06:07:31 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[downscale union from 2 to 1, buffersPerChannel = 0].2021-12-21T06:07:31.2387732Z Dec 21 06:07:31 Finished org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[downscale union from 2 to 1, buffersPerChannel = 0].2021-12-21T06:07:35.4068535Z Dec 21 06:07:35 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMinMaxParallelism[statebackend type =MEM, buffersPerChannel = 2].2021-12-21T06:07:35.4104210Z Dec 21 06:07:35 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMinMaxParallelism[statebackend type =MEM, buffersPerChannel = 2].2021-12-21T06:07:41.8835831Z Dec 21 06:07:41 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindow[statebackend type =MEM, buffersPerChannel = 2].2021-12-21T06:07:41.8850176Z Dec 21 06:07:41 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindow[statebackend type =MEM, buffersPerChannel = 2].2021-12-21T06:07:43.4534043Z Dec 21 06:07:43 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[downscale union from 2 to 1, buffersPerChannel = 1].2021-12-21T06:07:43.4566611Z Dec 21 06:07:43 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[downscale union from 2 to 1, buffersPerChannel = 1].2021-12-21T06:07:43.4570536Z Dec 21 06:07:43 Finished org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[downscale union from 2 to 1, buffersPerChannel = 1].2021-12-21T06:07:48.8854031Z Dec 21 06:07:48 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMaxMaxParallelism[statebackend type =MEM, buffersPerChannel = 2].2021-12-21T06:07:48.8858894Z Dec 21 06:07:48 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMaxMaxParallelism[statebackend type =MEM, buffersPerChannel = 2].2021-12-21T06:07:54.2639437Z Dec 21 06:07:54 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testPreAggregatedTumblingTimeWindow[statebackend type =MEM, buffersPerChannel = 2].2021-12-21T06:07:54.2649928Z Dec 21 06:07:54 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testPreAggregatedTumblingTimeWindow[statebackend type =MEM, buffersPerChannel = 2].2021-12-21T06:07:57.0921700Z Dec 21 06:07:57 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[downscale union from 3 to 2, buffersPerChannel = 0].2021-12-21T06:07:57.0930134Z Dec 21 06:07:57 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[downscale union from 3 to 2, buffersPerChannel = 0].2021-12-21T06:07:57.0937347Z Dec 21 06:07:57 Finished org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[downscale union from 3 to 2, buffersPerChannel = 0].2021-12-21T06:08:00.4006969Z Dec 21 06:08:00 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testPreAggregatedSlidingTimeWindow[statebackend type =MEM, buffersPerChannel = 2].2021-12-21T06:08:00.4017900Z Dec 21 06:08:00 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testPreAggregatedSlidingTimeWindow[statebackend type =MEM, buffersPerChannel = 2].2021-12-21T06:08:12.1410498Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.2021-12-21T06:08:12.1415409Z at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)2021-12-21T06:08:12.1416272Z at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)2021-12-21T06:08:12.1417854Z at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)2021-12-21T06:08:12.1428251Z at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)2021-12-21T06:08:12.1434755Z at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)2021-12-21T06:08:12.1435731Z at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)2021-12-21T06:08:12.1597764Z at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:258)2021-12-21T06:08:12.1605280Z at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)2021-12-21T06:08:12.1617993Z at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)2021-12-21T06:08:12.1624766Z at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)2021-12-21T06:08:12.1628608Z at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)2021-12-21T06:08:12.1633487Z at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)2021-12-21T06:08:12.1636953Z at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)2021-12-21T06:08:12.1641835Z at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)2021-12-21T06:08:12.1645859Z at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)2021-12-21T06:08:12.1649336Z at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)2021-12-21T06:08:12.1650090Z at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)2021-12-21T06:08:12.1651243Z at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)2021-12-21T06:08:12.1655861Z at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)2021-12-21T06:08:12.1656895Z at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)2021-12-21T06:08:12.1661986Z at akka.dispatch.OnComplete.internal(Future.scala:300)2021-12-21T06:08:12.1662576Z at akka.dispatch.OnComplete.internal(Future.scala:297)2021-12-21T06:08:12.1663437Z at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)2021-12-21T06:08:12.1664213Z at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)2021-12-21T06:08:12.1664869Z at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)2021-12-21T06:08:12.1670608Z at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)2021-12-21T06:08:12.1671491Z at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)2021-12-21T06:08:12.1672703Z at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)2021-12-21T06:08:12.1673473Z at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)2021-12-21T06:08:12.1678327Z at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)2021-12-21T06:08:12.1679245Z at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)2021-12-21T06:08:12.1679919Z at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)2021-12-21T06:08:12.1680642Z at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)2021-12-21T06:08:12.1681284Z at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)2021-12-21T06:08:12.1686459Z at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)2021-12-21T06:08:12.1687299Z at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)2021-12-21T06:08:12.1688250Z at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)2021-12-21T06:08:12.1689135Z at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)2021-12-21T06:08:12.1690169Z at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)2021-12-21T06:08:12.1695578Z at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)2021-12-21T06:08:12.1696237Z at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)2021-12-21T06:08:12.1697027Z at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)2021-12-21T06:08:12.1697753Z at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)2021-12-21T06:08:12.1702217Z at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)2021-12-21T06:08:12.1703523Z at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)2021-12-21T06:08:12.1704260Z at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)2021-12-21T06:08:12.1705010Z at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)2021-12-21T06:08:12.1705582Z at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)2021-12-21T06:08:12.1710510Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=1, backoffTimeMS=0)2021-12-21T06:08:12.1711429Z at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)2021-12-21T06:08:12.1712468Z at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)2021-12-21T06:08:12.1713557Z at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)2021-12-21T06:08:12.1718442Z at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)2021-12-21T06:08:12.1719496Z at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)2021-12-21T06:08:12.1720212Z at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:696)2021-12-21T06:08:12.1721098Z at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)2021-12-21T06:08:12.1721774Z at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:445)2021-12-21T06:08:12.1726562Z at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)2021-12-21T06:08:12.1727387Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2021-12-21T06:08:12.1728007Z at java.lang.reflect.Method.invoke(Method.java:498)2021-12-21T06:08:12.1728636Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)2021-12-21T06:08:12.1729709Z at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)2021-12-21T06:08:12.1734890Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)2021-12-21T06:08:12.1735620Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)2021-12-21T06:08:12.1736333Z at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)2021-12-21T06:08:12.1737197Z at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)2021-12-21T06:08:12.1742688Z at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)2021-12-21T06:08:12.1743341Z at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)2021-12-21T06:08:12.1744018Z at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)2021-12-21T06:08:12.1744572Z at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)2021-12-21T06:08:12.1745157Z at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)2021-12-21T06:08:12.1750660Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)2021-12-21T06:08:12.1751300Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)2021-12-21T06:08:12.1751892Z at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)2021-12-21T06:08:12.1752445Z at akka.actor.Actor.aroundReceive(Actor.scala:537)2021-12-21T06:08:12.1753074Z at akka.actor.Actor.aroundReceive$(Actor.scala:535)2021-12-21T06:08:12.1758203Z at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)2021-12-21T06:08:12.1758926Z at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)2021-12-21T06:08:12.1759473Z at akka.actor.ActorCell.invoke(ActorCell.scala:548)2021-12-21T06:08:12.1760015Z at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)2021-12-21T06:08:12.1764801Z at akka.dispatch.Mailbox.run(Mailbox.scala:231)2021-12-21T06:08:12.1765570Z at akka.dispatch.Mailbox.exec(Mailbox.scala:243)2021-12-21T06:08:12.1765954Z ... 4 more2021-12-21T06:08:12.1766547Z Caused by: java.lang.Exception: Exception while creating StreamOperatorStateContext.2021-12-21T06:08:12.1767317Z at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:255)2021-12-21T06:08:12.1772588Z at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268)2021-12-21T06:08:12.1773465Z at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:110)2021-12-21T06:08:12.1774335Z at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:697)2021-12-21T06:08:12.1775070Z at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)2021-12-21T06:08:12.1775769Z at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:673)2021-12-21T06:08:12.1780965Z at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:640)2021-12-21T06:08:12.1781675Z at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)2021-12-21T06:08:12.1782463Z at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917)2021-12-21T06:08:12.1783093Z at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)2021-12-21T06:08:12.1787626Z at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)2021-12-21T06:08:12.1788244Z at java.lang.Thread.run(Thread.java:748)2021-12-21T06:08:12.1788922Z Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for WindowOperator_0a448493b4782967b150582570326227_(1/4) from any of the 1 provided restore options.2021-12-21T06:08:12.1789829Z at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:160)2021-12-21T06:08:12.1790696Z at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:346)2021-12-21T06:08:12.1796225Z at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:164)2021-12-21T06:08:12.1796880Z ... 11 more2021-12-21T06:08:12.1929572Z Caused by: java.lang.RuntimeException: java.io.FileNotFoundException: /tmp/junit1602319139396542693/junit2304571881994301673/dd79b29a-682f-4a25-bca7-9e43cef5e1d1 (No such file or directory)2021-12-21T06:08:12.1930651Z at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:319)2021-12-21T06:08:12.1935588Z at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:87)2021-12-21T06:08:12.1938897Z at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.hasNext(StateChangelogHandleStreamHandleReader.java:69)2021-12-21T06:08:12.1944207Z at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.readBackendHandle(ChangelogBackendRestoreOperation.java:92)2021-12-21T06:08:12.1945156Z at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.restore(ChangelogBackendRestoreOperation.java:74)2021-12-21T06:08:12.1950132Z at org.apache.flink.state.changelog.ChangelogStateBackend.restore(ChangelogStateBackend.java:221)2021-12-21T06:08:12.1951349Z at org.apache.flink.state.changelog.ChangelogStateBackend.createKeyedStateBackend(ChangelogStateBackend.java:145)2021-12-21T06:08:12.1952172Z at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:329)2021-12-21T06:08:12.1953295Z at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168)2021-12-21T06:08:12.1954441Z at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)2021-12-21T06:08:12.1960137Z ... 13 more2021-12-21T06:08:12.1961315Z Caused by: java.io.FileNotFoundException: /tmp/junit1602319139396542693/junit2304571881994301673/dd79b29a-682f-4a25-bca7-9e43cef5e1d1 (No such file or directory)2021-12-21T06:08:12.1962007Z at java.io.FileInputStream.open0(Native Method)2021-12-21T06:08:12.1962665Z at java.io.FileInputStream.open(FileInputStream.java:195)2021-12-21T06:08:12.1967137Z at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:138)2021-12-21T06:08:12.1967995Z at org.apache.flink.core.fs.local.LocalDataInputStream.&lt;init&gt;(LocalDataInputStream.java:50)2021-12-21T06:08:12.1968661Z at org.apache.flink.core.fs.local.LocalFileSystem.open(LocalFileSystem.java:134)2021-12-21T06:08:12.1969318Z at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.open(SafetyNetWrapperFileSystem.java:87)2021-12-21T06:08:12.1970019Z at org.apache.flink.runtime.state.filesystem.FileStateHandle.openInputStream(FileStateHandle.java:68)2021-12-21T06:08:12.1976459Z Dec 21 06:08:12 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testSlidingTimeWindow[statebackend type =FILE, buffersPerChannel = 0].2021-12-21T06:08:12.1977524Z Dec 21 06:08:12 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testSlidingTimeWindow[statebackend type =FILE, buffersPerChannel = 0].2021-12-21T06:08:12.1978346Z at org.apache.flink.changelog.fs.StateChangeFormat.read(StateChangeFormat.java:92)2021-12-21T06:08:12.1979239Z at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:85)2021-12-21T06:08:12.1982680Z ... 21 more2021-12-21T06:08:15.8131284Z Dec 21 06:08:15 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[downscale union from 3 to 2, buffersPerChannel = 1].2021-12-21T06:08:15.8132921Z Dec 21 06:08:15 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[downscale union from 3 to 2, buffersPerChannel = 1].2021-12-21T06:08:15.8134329Z Dec 21 06:08:15 Finished org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[downscale union from 3 to 2, buffersPerChannel = 1].2021-12-21T06:08:18.5902129Z Dec 21 06:08:18 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMinMaxParallelism[statebackend type =FILE, buffersPerChannel = 0].2021-12-21T06:08:18.5926512Z Dec 21 06:08:18 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMinMaxParallelism[statebackend type =FILE, buffersPerChannel = 0].2021-12-21T06:08:27.4704772Z Dec 21 06:08:27 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindow[statebackend type =FILE, buffersPerChannel = 0].2021-12-21T06:08:27.4705814Z Dec 21 06:08:27 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindow[statebackend type =FILE, buffersPerChannel = 0].2021-12-21T06:08:34.4616437Z Dec 21 06:08:34 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMaxMaxParallelism[statebackend type =FILE, buffersPerChannel = 0].2021-12-21T06:08:34.4631814Z Dec 21 06:08:34 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMaxMaxParallelism[statebackend type =FILE, buffersPerChannel = 0].2021-12-21T06:08:37.4353476Z Dec 21 06:08:37 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[downscale union from 7 to 3, buffersPerChannel = 0].2021-12-21T06:08:37.4358241Z Dec 21 06:08:37 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[downscale union from 7 to 3, buffersPerChannel = 0].2021-12-21T06:08:37.4362799Z Dec 21 06:08:37 Finished org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[downscale union from 7 to 3, buffersPerChannel = 0].2021-12-21T06:08:41.4421350Z Dec 21 06:08:41 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testPreAggregatedTumblingTimeWindow[statebackend type =FILE, buffersPerChannel = 0].2021-12-21T06:08:41.4422769Z Dec 21 06:08:41 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testPreAggregatedTumblingTimeWindow[statebackend type =FILE, buffersPerChannel = 0].2021-12-21T06:08:49.8087106Z Dec 21 06:08:49 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testPreAggregatedSlidingTimeWindow[statebackend type =FILE, buffersPerChannel = 0].2021-12-21T06:08:49.8091055Z Dec 21 06:08:49 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testPreAggregatedSlidingTimeWindow[statebackend type =FILE, buffersPerChannel = 0].2021-12-21T06:08:56.3765029Z Dec 21 06:08:56 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testSlidingTimeWindow[statebackend type =FILE, buffersPerChannel = 2].2021-12-21T06:08:56.3766170Z Dec 21 06:08:56 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testSlidingTimeWindow[statebackend type =FILE, buffersPerChannel = 2].2021-12-21T06:08:58.9072907Z Dec 21 06:08:58 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[downscale union from 7 to 3, buffersPerChannel = 1].2021-12-21T06:08:58.9079161Z Dec 21 06:08:58 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[downscale union from 7 to 3, buffersPerChannel = 1].2021-12-21T06:08:58.9085274Z Dec 21 06:08:58 Finished org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[downscale union from 7 to 3, buffersPerChannel = 1].2021-12-21T06:09:02.5717900Z Dec 21 06:09:02 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMinMaxParallelism[statebackend type =FILE, buffersPerChannel = 2].2021-12-21T06:09:02.5724454Z Dec 21 06:09:02 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMinMaxParallelism[statebackend type =FILE, buffersPerChannel = 2].2021-12-21T06:09:08.3095467Z Dec 21 06:09:08 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindow[statebackend type =FILE, buffersPerChannel = 2].2021-12-21T06:09:08.3096598Z Dec 21 06:09:08 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindow[statebackend type =FILE, buffersPerChannel = 2].2021-12-21T06:09:10.4300059Z Dec 21 06:09:10 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[no scale union from 1 to 1, buffersPerChannel = 0].2021-12-21T06:09:10.4309488Z Dec 21 06:09:10 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[no scale union from 1 to 1, buffersPerChannel = 0].2021-12-21T06:09:10.4315846Z Dec 21 06:09:10 Finished org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[no scale union from 1 to 1, buffersPerChannel = 0].2021-12-21T06:09:14.4353046Z Dec 21 06:09:14 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMaxMaxParallelism[statebackend type =FILE, buffersPerChannel = 2].2021-12-21T06:09:14.4358399Z Dec 21 06:09:14 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMaxMaxParallelism[statebackend type =FILE, buffersPerChannel = 2].2021-12-21T06:09:19.8779097Z Dec 21 06:09:19 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testPreAggregatedTumblingTimeWindow[statebackend type =FILE, buffersPerChannel = 2].2021-12-21T06:09:19.8785621Z Dec 21 06:09:19 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testPreAggregatedTumblingTimeWindow[statebackend type =FILE, buffersPerChannel = 2].2021-12-21T06:09:21.4836766Z Dec 21 06:09:21 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[no scale union from 1 to 1, buffersPerChannel = 1].2021-12-21T06:09:21.4840441Z Dec 21 06:09:21 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[no scale union from 1 to 1, buffersPerChannel = 1].2021-12-21T06:09:21.4844309Z Dec 21 06:09:21 Finished org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[no scale union from 1 to 1, buffersPerChannel = 1].2021-12-21T06:09:26.1985322Z Dec 21 06:09:26 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testPreAggregatedSlidingTimeWindow[statebackend type =FILE, buffersPerChannel = 2].2021-12-21T06:09:26.1998219Z Dec 21 06:09:26 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testPreAggregatedSlidingTimeWindow[statebackend type =FILE, buffersPerChannel = 2].2021-12-21T06:09:50.4088289Z Dec 21 06:09:50 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testSlidingTimeWindow[statebackend type =ROCKSDB_FULL, buffersPerChannel = 0].2021-12-21T06:09:50.4096473Z Dec 21 06:09:50 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testSlidingTimeWindow[statebackend type =ROCKSDB_FULL, buffersPerChannel = 0].2021-12-21T06:09:58.0775962Z Dec 21 06:09:58 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMinMaxParallelism[statebackend type =ROCKSDB_FULL, buffersPerChannel = 0].2021-12-21T06:09:58.0777522Z Dec 21 06:09:58 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMinMaxParallelism[statebackend type =ROCKSDB_FULL, buffersPerChannel = 0].2021-12-21T06:10:05.5467071Z Dec 21 06:10:05 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindow[statebackend type =ROCKSDB_FULL, buffersPerChannel = 0].2021-12-21T06:10:05.5474709Z Dec 21 06:10:05 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindow[statebackend type =ROCKSDB_FULL, buffersPerChannel = 0].2021-12-21T06:10:17.4955728Z Dec 21 06:10:17 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMaxMaxParallelism[statebackend type =ROCKSDB_FULL, buffersPerChannel = 0].2021-12-21T06:10:17.4969255Z Dec 21 06:10:17 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMaxMaxParallelism[statebackend type =ROCKSDB_FULL, buffersPerChannel = 0].2021-12-21T06:10:24.4758454Z Dec 21 06:10:24 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[no scale union from 7 to 7, buffersPerChannel = 0].2021-12-21T06:10:24.4772548Z Dec 21 06:10:24 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[no scale union from 7 to 7, buffersPerChannel = 0].2021-12-21T06:10:24.4773746Z Dec 21 06:10:24 Finished org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[no scale union from 7 to 7, buffersPerChannel = 0].2021-12-21T06:10:28.5247207Z Dec 21 06:10:28 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testPreAggregatedTumblingTimeWindow[statebackend type =ROCKSDB_FULL, buffersPerChannel = 0].2021-12-21T06:10:28.5255460Z Dec 21 06:10:28 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testPreAggregatedTumblingTimeWindow[statebackend type =ROCKSDB_FULL, buffersPerChannel = 0].2021-12-21T06:10:59.3126158Z Dec 21 06:10:59 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testPreAggregatedSlidingTimeWindow[statebackend type =ROCKSDB_FULL, buffersPerChannel = 0].2021-12-21T06:10:59.3131182Z Dec 21 06:10:59 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testPreAggregatedSlidingTimeWindow[statebackend type =ROCKSDB_FULL, buffersPerChannel = 0].2021-12-21T06:11:07.5597066Z Dec 21 06:11:07 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[no scale union from 7 to 7, buffersPerChannel = 1].2021-12-21T06:11:07.5603856Z Dec 21 06:11:07 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[no scale union from 7 to 7, buffersPerChannel = 1].2021-12-21T06:11:07.5609023Z Dec 21 06:11:07 Finished org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[no scale union from 7 to 7, buffersPerChannel = 1].2021-12-21T06:11:27.2695728Z Dec 21 06:11:27 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testSlidingTimeWindow[statebackend type =ROCKSDB_FULL, buffersPerChannel = 2].2021-12-21T06:11:27.2700158Z Dec 21 06:11:27 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testSlidingTimeWindow[statebackend type =ROCKSDB_FULL, buffersPerChannel = 2].2021-12-21T06:11:27.7464683Z Dec 21 06:11:27 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[upscale multi_input from 1 to 2, buffersPerChannel = 0].2021-12-21T06:11:27.7467983Z Dec 21 06:11:27 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[upscale multi_input from 1 to 2, buffersPerChannel = 0].2021-12-21T06:11:27.7468950Z Dec 21 06:11:27 Finished org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[upscale multi_input from 1 to 2, buffersPerChannel = 0].2021-12-21T06:11:34.8076350Z Dec 21 06:11:34 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMinMaxParallelism[statebackend type =ROCKSDB_FULL, buffersPerChannel = 2].2021-12-21T06:11:34.8077569Z Dec 21 06:11:34 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMinMaxParallelism[statebackend type =ROCKSDB_FULL, buffersPerChannel = 2].2021-12-21T06:11:40.5262452Z Dec 21 06:11:40 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[upscale multi_input from 1 to 2, buffersPerChannel = 1].2021-12-21T06:11:40.5366961Z Dec 21 06:11:40 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[upscale multi_input from 1 to 2, buffersPerChannel = 1].2021-12-21T06:11:40.5370724Z Dec 21 06:11:40 Finished org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[upscale multi_input from 1 to 2, buffersPerChannel = 1].2021-12-21T06:11:57.4279555Z Dec 21 06:11:57 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindow[statebackend type =ROCKSDB_FULL, buffersPerChannel = 2].2021-12-21T06:11:57.4290116Z Dec 21 06:11:57 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindow[statebackend type =ROCKSDB_FULL, buffersPerChannel = 2].2021-12-21T06:12:01.7009109Z Dec 21 06:12:01 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[upscale multi_input from 2 to 3, buffersPerChannel = 0].2021-12-21T06:12:01.7010678Z Dec 21 06:12:01 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[upscale multi_input from 2 to 3, buffersPerChannel = 0].2021-12-21T06:12:01.7021826Z Dec 21 06:12:01 Finished org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[upscale multi_input from 2 to 3, buffersPerChannel = 0].2021-12-21T06:12:10.9736526Z Dec 21 06:12:10 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMaxMaxParallelism[statebackend type =ROCKSDB_FULL, buffersPerChannel = 2].2021-12-21T06:12:10.9740685Z Dec 21 06:12:10 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindowWithKVStateMaxMaxParallelism[statebackend type =ROCKSDB_FULL, buffersPerChannel = 2].2021-12-21T06:12:17.0251830Z Dec 21 06:12:16 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[upscale multi_input from 2 to 3, buffersPerChannel = 1].2021-12-21T06:12:17.0265114Z Dec 21 06:12:16 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[upscale multi_input from 2 to 3, buffersPerChannel = 1].2021-12-21T06:12:17.0271157Z Dec 21 06:12:16 Finished org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[upscale multi_input from 2 to 3, buffersPerChannel = 1].2021-12-21T06:12:33.8727663Z Dec 21 06:12:33 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testPreAggregatedTumblingTimeWindow[statebackend type =ROCKSDB_FULL, buffersPerChannel = 2].2021-12-21T06:12:33.8747903Z Dec 21 06:12:33 Finished org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testPreAggregatedTumblingTimeWindow[statebackend type =ROCKSDB_FULL, buffersPerChannel = 2].2021-12-21T06:13:10.5181732Z ##[error]Exit code 137 returned from process: file name '/usr/bin/docker', arguments 'exec -i -u 1001 -w /home/vsts_azpcontainer 7796bde44510825a871e84646b6e60d94c2c0ceb70380c294c1cb043c858368c /__a/externals/node/bin/node /__w/_temp/containerHandlerInvoker.js'.2021-12-21T06:13:10.6200841Z ##[section]Finishing: Test - finegrained_resource_managementhttps://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28403&amp;view=logs&amp;j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&amp;t=7b25afdf-cc6c-566f-5459-359dc2585798</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2540" opendate="2015-8-18 00:00:00" fixdate="2015-8-18 01:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>LocalBufferPool.requestBuffer gets into infinite loop</summary>
      <description>I'm trying to run a complicated computation that looks like this: &amp;#91;1&amp;#93;.One of the DataSource-&gt;Filter-&gt;Map chains finishes fine, but the other one freezes. Debugging shows that it is spinning in the while loop in LocalBufferPool.requestBuffer.askToRecycle is false. Both numberOfRequestedMemorySegments and currentPoolSize is 128, so it never goes into that if either.This is a stack trace: &amp;#91;2&amp;#93;And here is the code, if you would like to run it: &amp;#91;3&amp;#93;. Unfortunately, I can't make it more minimal, becuase if I remove some operators, the problem disappears. The class to start is malom.Solver. (On first run, it calculates some lookuptables for a few minutes, and puts them into /tmp/movegen)&amp;#91;1&amp;#93; http://compalg.inf.elte.hu/~ggevay/flink/plan.txt&amp;#91;2&amp;#93; http://compalg.inf.elte.hu/~ggevay/flink/stacktrace.txt&amp;#91;3&amp;#93; https://github.com/ggevay/flink/tree/deadlock-malom</description>
      <version>None</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plan.Channel.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plantranslate.JobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="25402" opendate="2021-12-21 00:00:00" fixdate="2021-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FLIP-198: Working directory for Flink processes</summary>
      <description>This issue implements FLIP-198. The idea is to introduce a working directory that can be used by Flink processes to store data that can be recovered if the process is restarted with access to the same working directory (e.g. via persistent volumes or restarting the process on the same node).The working directory will be default created in the temporary directory of the executing node. Moreover, the path will include the resource id of the Flink process. This will ensure that by default (random resource id), a newly started Flink process will see an empty working directory.This is an optional feature that might not be usable for all Flink deployments.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">docs.layouts.shortcodes.generated.cluster.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.cluster.section.html</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.application.ApplicationClusterEntryPoint.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigurationUtils.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.ConfigurationUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypointUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.entrypoint.ClusterEntrypointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunnerTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest.java</file>
      <file type="M">docs.layouts.shortcodes.generated.checkpointing.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.common.state.backends.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.rocksdb.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.configuration.html</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CheckpointingOptions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.WorkingDirectory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServicesConfiguration.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunnerStartupTest.java</file>
      <file type="M">flink-architecture-tests.violations.e5126cae-f3fe-48aa-b6fb-60ae6cc3fcd5</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.testutils.TestFileUtils.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointEnvironment.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointTaskManagerRuntimeInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.TaskManagerRuntimeInfo.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorExecutionDeploymentReconciliationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorSlotLifetimeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.JvmExitOnFatalErrorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.TestingTaskManagerRuntimeInfo.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">docs.layouts.shortcodes.generated.all.jobmanager.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.job.manager.configuration.html</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerProcessContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.FileExecutionGraphInfoStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.TestingMiniCluster.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.TestingResourceManagerService.java</file>
    </fixedFiles>
  </bug>
  <bug id="25404" opendate="2021-12-21 00:00:00" fixdate="2021-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Store blobs in &lt;WORKING_DIR&gt;/blobs</summary>
      <description>With FLINK-25402, we now have a working directory at our disposal. I suggest to store the blobs in this directory to make the recoverable in case of a process restart with the same working directory.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.BlobServerResource.java</file>
      <file type="M">docs.layouts.shortcodes.generated.blob.server.configuration.html</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.BlobServerOptions.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.HDFSTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.AbstractBlobCache.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobCacheService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobServer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.PermanentBlobCache.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.TransientBlobCache.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.WorkingDirectory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobCacheCleanupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobCacheCorruptionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobCacheDeleteTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobCacheGetTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobCachePutTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobCacheRecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobCacheRetriesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobCacheSuccessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobClientSslTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobClientTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobServerCleanupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobServerCorruptionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobServerDeleteTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobServerGetTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobServerPutTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobServerRangeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobServerRecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobServerSSLTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobServerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobUtilsNonWritableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.PermanentBlobCacheSizeLimitTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.TestingFailingBlobServer.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.client.ClientUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.AbstractDispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherResourceCleanupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.MiniDispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraphDeploymentWithBlobCacheTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraphDeploymentWithBlobServerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraphDeploymentWithSmallBlobCacheSizeLimitTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobManagerSharedServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobSubmitHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.BlobServerExtension.java</file>
    </fixedFiles>
  </bug>
  <bug id="25415" opendate="2021-12-22 00:00:00" fixdate="2021-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>implement retrial on connections to Cassandra container</summary>
      <description>NoHostAvailableException is raised by Cassandra client under load while connecting to a cluster. It happens no matter the Cassandra backend we use: I saw that using Achilles test backend, cassandra daemon and testContainers. So we should implement a retrial on the connection.</description>
      <version>1.13.5,1.14.2,1.15.0</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="25429" opendate="2021-12-23 00:00:00" fixdate="2021-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid to close output streams twice during uploading changelogs</summary>
      <description>Current uploader implementation would close stream and fsStream one by one, which lead to fsStream closed twice. try (FSDataOutputStream fsStream = fileSystem.create(path, NO_OVERWRITE)) { fsStream.write(compression ? 1 : 0); try (OutputStreamWithPos stream = wrap(fsStream); ) { final Map&lt;UploadTask, Map&lt;StateChangeSet, Long&gt;&gt; tasksOffsets = new HashMap&lt;&gt;(); for (UploadTask task : tasks) { tasksOffsets.put(task, format.write(stream, task.changeSets)); } FileStateHandle handle = new FileStateHandle(path, stream.getPos()); // WARN: streams have to be closed before returning the results // otherwise JM may receive invalid handles return new LocalResult(tasksOffsets, handle); } }Not all file system supports to close same stream twice.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.StateChangeFsUploader.java</file>
    </fixedFiles>
  </bug>
  <bug id="25433" opendate="2021-12-23 00:00:00" fixdate="2021-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate retry strategy for cleanup stage</summary>
      <description>The ResourceCleaner should be able to cleanup not only once but retry infinitely.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.retriever.impl.RpcGatewayRetrieverTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.TestingDispatcher.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherFailoverITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.cleanup.DispatcherResourceCleanerFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.cleanup.DefaultResourceCleanerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.cleanup.DispatcherResourceCleanerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.cleanup.DefaultResourceCleaner.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.job.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.jobmanager.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="25460" opendate="2021-12-27 00:00:00" fixdate="2021-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update slf4j-api dependency to 1.7.32</summary>
      <description>Flink is using slf4j-api version 1.7.15 (from February 2016), while version 1.7.32 (Jul 2021) has been released. We should upgrade to the latest dependency.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.project-configuration.md</file>
    </fixedFiles>
  </bug>
  <bug id="25461" opendate="2021-12-27 00:00:00" fixdate="2021-1-27 01:00:00" resolution="Done">
    <buginformation>
      <summary>Update net.sf.py4j:py4j dependency to 0.10.8.1</summary>
      <description>Flink uses net.sf.py4j:py4j version 0.10.8.1, while version 0.10.9.3 with multiple improvements has been released. We should upgrade this dependency.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">NOTICE</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.README.md</file>
      <file type="M">flink-python.lib.py4j-0.10.8.1-src.zip</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
    </fixedFiles>
  </bug>
  <bug id="25472" opendate="2021-12-29 00:00:00" fixdate="2021-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update to Log4j 2.17.1</summary>
      <description>We should update from Log4j 2.17.0 to 2.17.1 to address CVE-2021-44832: Apache Log4j2 vulnerable to RCE via JDBC Appender when attacker controls configuration.</description>
      <version>1.12.8,1.13.6,1.14.3,1.15.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.NOTICE-binary.PREAMBLE.txt</file>
      <file type="M">pom.xml</file>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.project-configuration.md</file>
    </fixedFiles>
  </bug>
  <bug id="25474" opendate="2021-12-29 00:00:00" fixdate="2021-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Idea Scala plugin can not compile RexExplainUtil</summary>
      <description>Idea version: 2021.2.3Scala version: 2.11.12There are some errors in RexExplainUtil and many classes which use the methods in RexExplainUtil. NOTES: those class can be compiled and executed successfully</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
    </fixedFiles>
  </bug>
  <bug id="25477" opendate="2021-12-29 00:00:00" fixdate="2021-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The directory structure of the State Backends document is not standardized</summary>
      <description>The State Backends document uses multiple first-level headings. It may cause the directory structure displayed incorrectly.Just as the picture shows, the two titles are not in the table of contents on the right. </description>
      <version>None</version>
      <fixedVersion>1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.state.backends.md</file>
    </fixedFiles>
  </bug>
  <bug id="25479" opendate="2021-12-29 00:00:00" fixdate="2021-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Changlog materialization with incremental checkpoint cannot work well in local tests</summary>
      <description>Currently, changelog materialization would call RocksDB state backend's snapshot method to generate IncrementalRemoteKeyedStateHandle as ChangelogStateBackendHandleImpl's materialized artifacts. And before next materialization, it will always report the same IncrementalRemoteKeyedStateHandle as before.For local tests, TM would report the IncrementalRemoteKeyedStateHandle to JM via local LocalRpcInvocation. However, as LocalRpcInvocation would not de/serialize message, which leads once we register the IncrementalRemoteKeyedStateHandle on JM side, it will also add a sharedStateRegistry to the one located on TM side. For the 2nd checkpoint, TM would reported same IncrementalRemoteKeyedStateHandle with sharedStateRegistry to JM. And it will then throw exception as it already contains a sharedStateRegistry:IncrementalRemoteKeyedStateHandlepublic void registerSharedStates(SharedStateRegistry stateRegistry, long checkpointID) { Preconditions.checkState( sharedStateRegistry != stateRegistry, "The state handle has already registered its shared states to the given registry.");}This bug would go in distribution environment as IncrementalRemoteKeyedStateHandle would be serialized and sharedStateRegistry is tagged as transient.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointFailureHandlingITCase.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogKeyedStateBackendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackendBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.TestingJobMasterGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.JobMasterTester.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.rpc.RpcCheckpointResponder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.StateChange.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.SequenceNumber.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.TaskStateSnapshot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorGateway.java</file>
    </fixedFiles>
  </bug>
  <bug id="25484" opendate="2021-12-30 00:00:00" fixdate="2021-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TableRollingPolicy do not support inactivityInterval config which is supported in datastream api</summary>
      <description>TableRollingPolicy do not support inactivityInterval configpublic static class TableRollingPolicy extends CheckpointRollingPolicy&lt;RowData, String&gt; {private final boolean rollOnCheckpoint;private final long rollingFileSize;private final long rollingTimeInterval;public TableRollingPolicy(boolean rollOnCheckpoint, long rollingFileSize, long rollingTimeInterval) {this.rollOnCheckpoint = rollOnCheckpoint;Preconditions.checkArgument(rollingFileSize &gt; 0L);Preconditions.checkArgument(rollingTimeInterval &gt; 0L);this.rollingFileSize = rollingFileSize;this.rollingTimeInterval = rollingTimeInterval;} </description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.stream.StreamingFileWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.writer.FileWriterBucketTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemConnectorOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="25490" opendate="2021-12-30 00:00:00" fixdate="2021-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the Chinese document related to final checkpoint</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.checkpointing.md</file>
      <file type="M">docs.content.zh.docs.internals.task.lifecycle.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.checkpointing.md</file>
    </fixedFiles>
  </bug>
  <bug id="25504" opendate="2022-1-3 00:00:00" fixdate="2022-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update and synchronise used versions of Kafka Client and Confluent Platform</summary>
      <description>The Flink codebases uses Kafka Client and Confluent Platform in multiple places: AVRO Confluent Schema Registry Flink end-to-end tests (Bash e2e tests) Flink end-to-end tests common (Java e2e tests) SQL AVRO Confluent Schema Registry Flink Test Utils Flink TestsThe used versions are currently not in sync, which could result in unexpected results. For context, these are the currently used versions:Kafka Client: 2.2.0, 2.2.2, 2.4.1, 2.6.0Confluent Platform: 5.2.6, 5.5.2, 6.0.4, 6.2.1Given https://docs.confluent.io/platform/current/installation/versions-interoperability.html it probably makes sense to update to Kafka Client 2.8.1 and Confluent Platform 6.2.2.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.confluent.schema.registry.sh</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25511" opendate="2022-1-4 00:00:00" fixdate="2022-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pre-emptively uploaded changelog not discarded up if materialized before checkpoint</summary>
      <description>This is related to Garbage Collection. The problem is more severe than FLINK-25512 because there is leftover after materialization truncation each time (if pre-emptive uploads are enabled).However, the probability can still be not high because of grouping of backends into a single file (so only if ALL the backends on TM materialize semi-simultaneously then it happen).</description>
      <version>1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.StateChangeLoggerTestBase.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogStateBackendTestUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.StateChangelogWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogWriter.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.FsStateChangelogWriterTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.FsStateChangelogWriterSqnTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.FsStateChangelogStorageTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.ChangelogStorageMetricsTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.UploadResult.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.StateChangeUploadScheduler.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.StateChangeUploader.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.StateChangeSet.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.StateChangeFsUploader.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.FsStateChangelogWriter.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.FsStateChangelogStorage.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.FsStateChangelogOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.fs.state.changelog.configuration.html</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.InterruptSensitiveRestoreTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateDownloaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.testutils.EmptyStreamStateHandle.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.SharedStateRegistryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ChangelogTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.messages.CheckpointMessagesTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StreamStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.RetrievableStreamStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.PlaceholderStreamStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.OperatorStreamStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.ByteStreamStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyGroupsStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FileStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.ChangelogStateBackendHandle.java</file>
    </fixedFiles>
  </bug>
  <bug id="25524" opendate="2022-1-5 00:00:00" fixdate="2022-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>If enabled changelog, RocksDB incremental checkpoint would always be full</summary>
      <description>Once changelog is enabled, RocksDB incremental checkpoint would only be executed during materialization. During this phase, it will leverage the materization id as the checkpoint id for RocksDB state backend's snapshot method.However, current incremental checkpoint mechanism heavily depends on the checkpoint id. And SortedMap&lt;Long, Set&lt;StateHandleID&gt;&gt; uploadedStateIDs with checkpoint id as the key within RocksIncrementalSnapshotStrategy is the kernel for incremental checkpoint. Once we notify checkpoint complete of previous checkpoint, it will then remove the uploaded stateIds of that checkpoint, leading to we cannot get proper checkpoint information on the next RocksDBKeyedStateBackend#snapshot. That is to say, we will always upload all RocksDB artifacts.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogStateBackendTestUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.PeriodicMaterializationManager.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.IncrementalKeyedStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.ChangelogStateBackendHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="25525" opendate="2022-1-5 00:00:00" fixdate="2022-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-examples-table is not runnable in the IDE</summary>
      <description>The following exception is thrown: Exception in thread "main" org.apache.flink.table.api.ValidationException: Could not find any factories that implement 'org.apache.flink.table.delegation.ExecutorFactory' in the classpath.    at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:453)    at org.apache.flink.table.api.internal.TableEnvironmentImpl.create(TableEnvironmentImpl.java:295)    at org.apache.flink.table.api.internal.TableEnvironmentImpl.create(TableEnvironmentImpl.java:266)    at org.apache.flink.table.api.TableEnvironment.create(TableEnvironment.java:95)    at org.apache.flink.table.examples.scala.basics.GettingStartedExample$.main(GettingStartedExample.scala:55)    at org.apache.flink.table.examples.scala.basics.GettingStartedExample.main(GettingStartedExample.scala)</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2553" opendate="2015-8-20 00:00:00" fixdate="2015-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Example Jars not build correctly</summary>
      <description>Due to new examples that introduced some package restructuring, example jar files are not build correctly. Furthermore, package/class structure got messed up to some extend. Additionally, documentation need to be extended to point out the correct example jar files.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.wordcount.WordCountTopology.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.wordcount.StormWordCountRemoteBySubmitter.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.wordcount.StormWordCountRemoteByClient.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.wordcount.SpoutSourceWordCount.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.util.StormWordCountInMemorySpout.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.util.StormWordCountFileSpout.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.util.StormInMemorySpout.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.util.StormFileSpout.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.util.FiniteStormInMemorySpout.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.util.FiniteStormFileSpout.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.main.java.org.apache.flink.stormcompatibility.singlejoin.stormoperators.GenderSpout.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.src.assembly.word-count-storm.xml</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.README.md</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-examples.pom.xml</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-core.src.test.java.org.apache.flink.stormcompatibility.wrappers.StormOutputFieldsDeclarerTest.java</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-core.src.main.java.org.apache.flink.stormcompatibility.api.FlinkSubmitter.java</file>
      <file type="M">docs.apis.storm.compatibility.md</file>
    </fixedFiles>
  </bug>
  <bug id="25532" opendate="2022-1-5 00:00:00" fixdate="2022-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide Flink SQL CLI as Docker image</summary>
      <description>Flink is currently available via as Docker images. However, the Flink SQL CLI isn't available as a Docker image. We should also provide this.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.docker.md</file>
    </fixedFiles>
  </bug>
  <bug id="25553" opendate="2022-1-6 00:00:00" fixdate="2022-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove MapR filesystem</summary>
      <description>Pending a positive outcome in the Dev mailing list https://lists.apache.org/thread/od2137fk5j1gq034sopj5n2th2w719w4 we can remove the MapR filesystem</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">tools.ci.shade.sh</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-filesystems.pom.xml</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.test.resources.log4j2-test.properties</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.test.java.org.apache.flink.runtime.fs.maprfs.MapRNotInClassPathTest.java</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.test.java.org.apache.flink.runtime.fs.maprfs.MapRFsFactoryTest.java</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.test.java.com.mapr.fs.MapRFileSystem.java</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.main.resources.META-INF.services.org.apache.flink.core.fs.FileSystemFactory</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.main.java.org.apache.flink.runtime.fs.maprfs.MapRFsFactory.java</file>
      <file type="M">flink-filesystems.flink-mapr-fs.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.FileSystem.java</file>
      <file type="M">docs.content.docs.internals.filesystems.md</file>
      <file type="M">docs.content.docs.deployment.filesystems.plugins.md</file>
      <file type="M">docs.content.docs.deployment.filesystems.overview.md</file>
      <file type="M">docs.content.zh.docs.internals.filesystems.md</file>
      <file type="M">docs.content.zh.docs.deployment.filesystems.plugins.md</file>
      <file type="M">docs.content.zh.docs.deployment.filesystems.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="25557" opendate="2022-1-6 00:00:00" fixdate="2022-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce incremental/full checkpoint size stats</summary>
      <description>Currently, the "checkpointed data size" would be incremental checkpoint size if incremental checkpoint is enabled, otherwise full checkpoint size. This is not friendly for users to know the exact incremental/full checkpoint size.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.IncrementalLocalKeyedStateHandle.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.OperatorSnapshotFuturesTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.OperatorSnapshotFutures.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategyTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBRestoreResult.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateUtilTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.IncrementalRemoteKeyedStateHandleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SchedulerUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.checkpoints.TaskCheckpointStatisticsWithSubtaskDetailsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.checkpoints.TaskCheckpointStatisticsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointingStatisticsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.TaskStateStatsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.SubtaskStateStatsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StateHandleDummyUtil.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointStatsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.FailedCheckpointStatsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStatsTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateUtil.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.PlaceholderStreamStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyGroupsStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.IncrementalRemoteKeyedStateHandle.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.FsStateChangelogWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.ChangelogStateBackendHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.ChangelogStateHandleStreamImpl.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogStateBackend.java</file>
      <file type="M">docs.content.zh.docs.ops.monitoring.checkpoint.monitoring.md</file>
      <file type="M">docs.content.docs.ops.monitoring.checkpoint.monitoring.md</file>
      <file type="M">docs.static.fig.checkpoint.monitoring-details.png</file>
      <file type="M">docs.static.fig.checkpoint.monitoring-details.subtasks.png</file>
      <file type="M">docs.static.fig.checkpoint.monitoring-details.summary.png</file>
      <file type="M">docs.static.fig.checkpoint.monitoring-history-subtasks.png</file>
      <file type="M">docs.static.fig.checkpoint.monitoring-history.png</file>
      <file type="M">docs.static.fig.checkpoint.monitoring-summary.png</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-checkpoint.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.detail.job-checkpoints-detail.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.subtask.job-checkpoints-subtask.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.subtask.job-checkpoints-subtask.component.ts</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.AbstractCheckpointStats.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointMetrics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointMetricsBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointStatsTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStats.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStatsSummary.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStatsSummarySnapshot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.FailedCheckpointStats.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorSubtaskState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpointStats.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateObjectCollection.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.SubtaskState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.SubtaskStateStats.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.TaskState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.TaskStateSnapshot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.TaskStateStats.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointingStatistics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointStatistics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.SubtaskCheckpointStatistics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.TaskCheckpointStatistics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.TaskCheckpointStatisticsWithSubtaskDetails.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.inmemory.InMemoryChangelogStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.CompositeStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.DirectoryKeyedStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.IncrementalKeyedStateHandle.java</file>
    </fixedFiles>
  </bug>
  <bug id="25569" opendate="2022-1-7 00:00:00" fixdate="2022-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce decomposed Sink V2 interfaces</summary>
      <description>This task introduces the interfaces described in &amp;#91;1&amp;#93; without the datastream extension hooks. &amp;#91;1&amp;#93; https://cwiki.apache.org/confluence/display/FLINK/FLIP-191%3A+Extend+unified+Sink+interface+to+support+small+file+compaction</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.StreamTaskTimerITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.graph.TwoInputTestStreamOperator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.graph.OneInputTestStreamOperator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.lifecycle.graph.MultiInputTestOperator.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperator.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.wmassigners.ProcTimeMiniBatchAssignerOperator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeServiceTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapperTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamTaskTimerTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ProcessingTimeCallback.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.TimestampsAndWatermarksOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSourceContexts.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.LatencyMarkerEmitter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSinkHelper.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.NeverFireProcessingTimeService.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.UserCodeClassLoader.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.io.SimpleVersionedSerializer.java</file>
      <file type="M">flink-architecture-tests.violations.5b9eed8a-5fb6-4373-98ac-3be2a71941b8</file>
    </fixedFiles>
  </bug>
  <bug id="25570" opendate="2022-1-7 00:00:00" fixdate="2022-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce Sink V2 extension interfaces</summary>
      <description>This task introduces the interfaces needed to implement the custom operations before/after the writer and committer.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.function.SerializableSupplier.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.completeness.TypeSerializerTestCoverageTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="25571" opendate="2022-1-7 00:00:00" fixdate="2022-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Elasticsearch Sink to use decomposed interfaces</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-quickstart-test.src.main.scala.org.apache.flink.quickstarts.test.Elasticsearch7SinkExample.scala</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchDynamicSinkFactoryBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchDynamicSinkBaseITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.TestEmitter.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.RowElasticsearchEmitter.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.RequestIndexer.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriter.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchEmitter.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.5b9eed8a-5fb6-4373-98ac-3be2a71941b8</file>
    </fixedFiles>
  </bug>
  <bug id="25572" opendate="2022-1-7 00:00:00" fixdate="2022-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update File Sink to use decomposed interfaces</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestFileFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaCommitterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.writer.FileWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.writer.FileWriterBucketStateSerializerMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.committer.FileCommitterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.sink.writer.FileWriterBucket.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.sink.writer.FileWriter.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.sink.FileSink.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.sink.committer.FileCommitter.java</file>
    </fixedFiles>
  </bug>
  <bug id="25573" opendate="2022-1-7 00:00:00" fixdate="2022-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Kafka Sink to use decomposed interfaces</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.sink.SinkV2Provider.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SinkV1Adapter.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.sink.InitContextInitializationContextAdapter.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.ReducingUpsertWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaCommitterTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.ReducingUpsertWriter.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.ReducingUpsertSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.KafkaWriter.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.KafkaSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.KafkaCommitter.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.7602816f-5c01-4b7a-9e3e-235dfedec245</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.5b9eed8a-5fb6-4373-98ac-3be2a71941b8</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSinkITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="25574" opendate="2022-1-7 00:00:00" fixdate="2022-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Async Sink to use decomposed interfaces</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.sink.writer.TestSinkInitContext.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.sink.writer.AsyncSinkWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.sink.ArrayListAsyncSink.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.sink.writer.ElementConverter.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.sink.writer.AsyncSinkWriter.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.sink.AsyncSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.table.KinesisFirehoseDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.table.KinesisFirehoseDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkWriter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkElementConverter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSink.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.test.java.org.apache.flink.connector.kinesis.table.KinesisDynamicTableSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.table.KinesisDynamicSink.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.sink.KinesisDataStreamsSinkWriter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.sink.KinesisDataStreamsSinkElementConverter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.sink.KinesisDataStreamsSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="25575" opendate="2022-1-7 00:00:00" fixdate="2022-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement StreamGraph translation for Sink V2 interfaces</summary>
      <description>This task covers the translation from the user-defined interfaces and the extension topologies to the actual operators.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.PartitionTransformationTranslator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.SinkITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.utils.DummyStreamExecutionEnvironment.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.CompactManagedTableITCase.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.StreamingScalaAPICompletenessTest.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.connector.sink2.IntegerSerializer.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.7602816f-5c01-4b7a-9e3e-235dfedec245</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.TransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectStreamSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SinkTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.AbstractCommitterHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.AbstractStreamingCommitterHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.BatchCommitterHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.CommitRetrier.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.committables.CommitRequestImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.CommitterHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.CommitterOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.CommitterOperatorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.ForwardCommittingHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.GlobalBatchCommitterHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.GlobalStreamingCommitterHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.NoopCommitterHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.SinkOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.SinkOperatorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.SinkWriterStateHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.StatefulSinkWriterStateHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.StatelessSinkWriterStateHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.StreamingCommitterHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.StreamingCommitterState.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.StreamingCommitterStateSerializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.SinkTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.connector.sink2.CommittableSummaryAssert.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.connector.sink2.SinkV2Assertions.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.datastream.DataStreamSinkTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.SinkTransformationTranslatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.transformations.SinkTransformationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.sink.BatchCommitterHandlerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.sink.CommitRetrierTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.sink.committables.CheckpointCommittableManagerImplTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.sink.GlobalBatchCommitterHandlerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.sink.GlobalStreamingCommitterHandlerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.sink.SinkTestUtil.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.sink.StreamingCommitterHandlerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.sink.StreamingCommitterStateSerializerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.sink.StreamingCommitterStateTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.sink.TestSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="25576" opendate="2022-1-7 00:00:00" fixdate="2022-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update com.h2database:h2 to 2.0.206</summary>
      <description>Flink uses com.h2database:h2 version 1.4.200, we should update this to 2.0.206</description>
      <version>1.13.5,1.14.2,1.15.0</version>
      <fixedVersion>1.13.6,1.14.4,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2559" opendate="2015-8-21 00:00:00" fixdate="2015-10-21 01:00:00" resolution="Done">
    <buginformation>
      <summary>Fix Javadoc (e.g. Code Examples)</summary>
      <description>Many multiline Javadoc code examples are not correctly rendered. One of the problems is that an @ inside a code block breaks the rendering.This is an example that works: * &lt;pre&gt;{@code * private static class MyIndexRequestBuilder implements IndexRequestBuilder&lt;String&gt; { * * public IndexRequest createIndexRequest(String element, RuntimeContext ctx) { * Map&lt;String, Object&gt; json = new HashMap&lt;&gt;(); * json.put("data", element); * * return Requests.indexRequest() * .index("my-index") * .type("my-type") * .source(json); * } * } * }&lt;/pre&gt;</description>
      <version>None</version>
      <fixedVersion>0.10.0,1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.StringValue.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobs.wordcount.WordCount.java</file>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.MultipleProgramsTestBase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.time.Time.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.StreamTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.SourceFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.RichSourceFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.wordcount.WordCount.java</file>
      <file type="M">flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.wordcount.PojoExample.java</file>
      <file type="M">flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.twitter.TwitterStream.java</file>
      <file type="M">flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.socket.SocketTextStreamWordCount.java</file>
      <file type="M">flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.ml.IncrementalLearningSkeleton.java</file>
      <file type="M">flink-streaming-examples.src.main.java.org.apache.flink.streaming.examples.iteration.IterateExample.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.partitioner.FixedPartitioner.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer.java</file>
      <file type="M">flink-staging.flink-table.src.main.java.org.apache.flink.api.java.table.package-info.java</file>
      <file type="M">flink-staging.flink-scala-shell.src.main.java.org.apache.flink.api.java.JarHelper.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.HadoopReduceFunction.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.HadoopReduceCombineFunction.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.HadoopMapFunction.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.StateStorageHelper.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.yarn.AbstractFlinkYarnCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.util.BloomFilter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.AbstractMergeInnerJoinIterator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.scheduler.Scheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.task.IterationIntermediateTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.InputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.BlockChannelWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.BlockChannelReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.SlotSharingGroupAssignment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.filecache.FileCache.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.ExecutionState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobService.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.postpass.SparseKeySchema.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plan.Channel.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.operators.OperatorDescriptorSingle.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dag.OptimizerNode.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.spargel.VertexUpdateFunction.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.spargel.VertexCentricIteration.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.spargel.MessagingFunction.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.PageRank.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.GSAPageRank.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.GSAConnectedComponents.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.ConnectedComponents.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.gsa.SumFunction.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.gsa.Neighbor.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.gsa.GatherFunction.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.gsa.ApplyFunction.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.Graph.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.example.MusicProfiles.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.example.IncrementalSSSP.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.example.GraphMetrics.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.example.EuclideanGraphWeighing.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.utils.DataSetUtils.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.Utils.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.kryo.Serializers.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.AvroTypeInfo.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.record.io.ExternalProcessInputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.record.functions.FunctionAnnotation.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.UnsortedGrouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.TwoInputUdfOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.TupleWrappingCollector.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.Tuple3WrappingCollector.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SortedGrouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SingleInputUdfOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.join.JoinOperatorSetsBase.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.JoinOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.Grouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.DataSink.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.CrossOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.CoGroupOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.SplitDataProperties.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.hadoop.mapred.HadoopOutputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.hadoop.mapred.HadoopInputFormat.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.functions.FunctionAnnotation.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.DataSet.java</file>
      <file type="M">flink-java8.src.main.java.org.apache.flink.examples.java8.relational.TPCHQuery10.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.wordcount.WordCountMeta.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.wordcount.WordCount.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.wordcount.PojoExample.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.relational.WebLogAnalysis.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.relational.TPCHQuery3.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.relational.TPCHQuery10.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.ml.LinearRegression.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.graph.PageRankBasic.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.distcp.DistCp.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.clustering.util.KMeansDataGenerator.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.clustering.KMeans.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.Visitable.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.excamation.ExclamationLocal.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.excamation.ExclamationTopology.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.excamation.ExclamationWithBolt.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.excamation.ExclamationWithSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.split.SpoutSplitExample.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.BoltTokenizerWordCount.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.BoltTokenizerWordCountPojo.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.BoltTokenizerWordCountWithNames.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.SpoutSourceWordCount.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountLocal.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountLocalByName.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountRemoteByClient.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountRemoteBySubmitter.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountTopology.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkClient.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkTopologyBuilder.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.util.SplitStreamType.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.BoltWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.SpoutWrapper.java</file>
      <file type="M">flink-contrib.flink-tweet-inputformat.src.main.java.org.apache.flink.contrib.tweetinputformat.model.tweet.entities.Entities.java</file>
      <file type="M">flink-contrib.flink-tweet-inputformat.src.main.java.org.apache.flink.contrib.tweetinputformat.model.tweet.entities.HashTags.java</file>
      <file type="M">flink-contrib.flink-tweet-inputformat.src.main.java.org.apache.flink.contrib.tweetinputformat.model.tweet.entities.Media.java</file>
      <file type="M">flink-contrib.flink-tweet-inputformat.src.main.java.org.apache.flink.contrib.tweetinputformat.model.tweet.entities.Symbol.java</file>
      <file type="M">flink-contrib.flink-tweet-inputformat.src.main.java.org.apache.flink.contrib.tweetinputformat.model.tweet.entities.URL.java</file>
      <file type="M">flink-contrib.flink-tweet-inputformat.src.main.java.org.apache.flink.contrib.tweetinputformat.model.tweet.entities.UserMention.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.accumulators.Histogram.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.aggregators.Aggregator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.distributions.DataDistribution.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionMode.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.CoGroupFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.CrossFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.FilterFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.FlatJoinFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.FlatMapFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.FoldFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.GroupReduceFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.JoinFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.MapFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.MapPartitionFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.ReduceFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.RichGroupReduceFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.FileOutputFormat.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.AbstractUdfOperator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.TypeComparator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.FileSystem.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.MemorySegment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.Record.java</file>
    </fixedFiles>
  </bug>
  <bug id="25590" opendate="2022-1-10 00:00:00" fixdate="2022-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Logging warning of insufficient memory for all configured buffers</summary>
      <description>Right now, if exclusive buffers for the input channel and one buffer for each subpartition would be allocated on start successfully but there would be not enough memory for the rest of the buffers(floating buffers, rest buffers for subpartitions), then we see nothing in the log about that(as I understand).So first of all, we need to check what logs we have right now about situations when flink doesn't have enough memory for all configured buffers. And if we have nothing (or not enough) we should add such a log.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.metrics.NettyShuffleMetricFactory.java</file>
      <file type="M">docs.content.docs.ops.metrics.md</file>
      <file type="M">docs.content.zh.docs.ops.metrics.md</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.NetworkBufferPoolTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.NettyShuffleUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.LocalBufferPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="25613" opendate="2022-1-11 00:00:00" fixdate="2022-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove excessive surefire-plugin versions</summary>
      <description>Various modules overwrite the default surefire version. We should remove that unless there is a good reason to do so.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25638" opendate="2022-1-13 00:00:00" fixdate="2022-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase the default write buffer size of sort-shuffle to 16M</summary>
      <description>As discussed in https://lists.apache.org/thread/pt2b1f17x2l5rlvggwxs6m265lo4ly7p, this ticket aims to increase the default write buffer size of sort-shuffle to 16M.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.BlockingShuffleITCase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.NettyShuffleEnvironmentOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.netty.shuffle.environment.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.taskmanager.network.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="25639" opendate="2022-1-13 00:00:00" fixdate="2022-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase the default read buffer size of sort-shuffle to 64M</summary>
      <description>As discussed in https://lists.apache.org/thread/pt2b1f17x2l5rlvggwxs6m265lo4ly7p, this ticket aims to increase the default read buffer size of sort-shuffle to 64M.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.task.manager.memory.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.common.memory.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="25645" opendate="2022-1-13 00:00:00" fixdate="2022-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UnsupportedOperationException would thrown out when hash shuffle by a field with array type</summary>
      <description>Currently array type is not supported as hash shuffle key because CodeGen does not support it yet. An unsupportedOperationException would thrown out when hash shuffle by a field with array type,</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.HashFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.codegen.HashCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.HashCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug id="25668" opendate="2022-1-17 00:00:00" fixdate="2022-1-17 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support calcuate network memory for dynamic graph.</summary>
      <description>Currently, when fine-grained resource management is enabled, the scheduler will calculate network memory for all SlotSharingGroups during its initialization.For dynamic graphs, the network memory of SlotSharingGroup should also be calculated lazily. Once a vertex is initialized, find its corresponding slot sharing group and check whether all vertices in this slot sharing group have been initialized. If all vertices have been initialized, the scheduler will calculate the network resources for this slot sharing group.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SsgNetworkMemoryCalculationUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.IntermediateResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertexTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SsgNetworkMemoryCalculationUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.IntermediateResult.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptorFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.SubpartitionIndexRange.java</file>
    </fixedFiles>
  </bug>
  <bug id="25669" opendate="2022-1-17 00:00:00" fixdate="2022-1-17 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support register operator coordinators for newly initialized ExecutionJobVertex.</summary>
      <description>For dynamic graphs, the operator coordinators of execution job vertices should also be registered and started lazily.  We need to support register operator coordinators for newly initialized ExecutionJobVertex.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.TestingOperatorCoordinatorHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.TestingOperatorCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.OperatorCoordinatorHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="25701" opendate="2022-1-19 00:00:00" fixdate="2022-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add API annotation to some Kafka connector core classes and interface</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.5b9eed8a-5fb6-4373-98ac-3be2a71941b8</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.split.KafkaPartitionSplitState.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.split.KafkaPartitionSplitSerializer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.split.KafkaPartitionSplit.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.reader.KafkaSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.reader.fetcher.KafkaSourceFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.reader.deserializer.KafkaRecordDeserializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.metrics.KafkaSourceReaderMetrics.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.KafkaSourceOptions.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.KafkaSourceBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.KafkaSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.enumerator.subscriber.KafkaSubscriber.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumStateSerializer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumState.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.KafkaRecordSerializationSchemaBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="25709" opendate="2022-1-19 00:00:00" fixdate="2022-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>GlueSchemaRegistryAvroKinesisITCase fails on AZP</summary>
      <description>The GlueSchemaRegistryAvroKinesisITCase fails on AZP with Jan 19 18:04:11 java.lang.IllegalStateException: Check failed: Docker environment should have more than 2GB free disk spaceJan 19 18:04:11 at org.testcontainers.DockerClientFactory.check(DockerClientFactory.java:312)Jan 19 18:04:11 at org.testcontainers.DockerClientFactory.checkDiskSpace(DockerClientFactory.java:301)Jan 19 18:04:11 at org.testcontainers.DockerClientFactory.client(DockerClientFactory.java:238)Jan 19 18:04:11 at org.testcontainers.DockerClientFactory$1.getDockerClient(DockerClientFactory.java:101)Jan 19 18:04:11 at com.github.dockerjava.api.DockerClientDelegate.authConfig(DockerClientDelegate.java:107)Jan 19 18:04:11 at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:316)Jan 19 18:04:11 at org.testcontainers.containers.GenericContainer.starting(GenericContainer.java:1066)Jan 19 18:04:11 at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:29)Jan 19 18:04:11 at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)Jan 19 18:04:11 at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)Jan 19 18:04:11 at java.util.concurrent.FutureTask.run(FutureTask.java:266)Jan 19 18:04:11 at java.lang.Thread.run(Thread.java:748)https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29722&amp;view=logs&amp;j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&amp;t=160c9ae5-96fd-516e-1c91-deb81f59292a&amp;l=15816</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.TaskManagerRunnerITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ClusterEntrypointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunnerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypointUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="25712" opendate="2022-1-20 00:00:00" fixdate="2022-1-20 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Merge flink-connector-testing into flink-connector-test-utils</summary>
      <description>Both "flink-connector-testing" and "flink-connector-test-utils" modules are designed for providing connector testing infrastructures and helper classes, so merging these two modules could simplify dependencies of connectors and reduce the complexity of Flink project. The plan is to move classes in flink-connector-testing to flink-connector-test-utils, since the latter one have been existing for a longer time in the projects, and flink-connector-testing is in experimental status now.Discussion in mailing list: https://lists.apache.org/thread/r30tjwt3vdbzd5c18q2y3o0toqfmk79o</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.test.java.org.apache.flink.connectors.test.common.utils.TestDataMatchersTest.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.utils.TestDataMatchers.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.testsuites.SourceTestSuiteBase.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.TestResource.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.junit.extensions.TestLoggerExtension.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.junit.extensions.TestCaseInvocationContextProvider.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.junit.extensions.ConnectorTestingExtension.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.junit.annotations.TestEnv.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.junit.annotations.ExternalSystem.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.junit.annotations.ExternalContextFactory.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.external.SourceSplitDataWriter.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.external.ExternalContext.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.external.DefaultContainerizedExternalSystem.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.environment.TestEnvironment.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.environment.RemoteClusterTestEnvironment.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.environment.MiniClusterTestEnvironment.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.src.main.java.org.apache.flink.connectors.test.common.environment.ClusterControllable.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.README.md</file>
      <file type="M">flink-test-utils-parent.flink-connector-testing.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestEnvironment.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.UniformShardAssignerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchDynamicSinkFactoryBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchDynamicSinkBaseITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBuilderBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBaseITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25715" opendate="2022-1-20 00:00:00" fixdate="2022-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Application Mode: Add option to submit a failed job on application error</summary>
      <description>Currently in application mode, any exception happens in the application driver (before submitting an actual job) leads to a fail-over. These errors are usually not retryable and we don't have a good way of reporting them to the user.We'll introduce a new config option `execution.submit-failed-job-on-application-error` that submits a failed job with the `$internal.pipeline.job-id` instead.This is intended to be used in combination with `execution.shutdown-on-application-finish = false` to allow user to retrieve the information about the failed submission.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.DeploymentOptions.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.testjar.BlockingJob.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapITCase.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.java</file>
      <file type="M">docs.layouts.shortcodes.generated.deployment.configuration.html</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingDispatcherGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobSubmitHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.AbstractHandlerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.TestingDispatcherGatewayService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.WebSubmissionExtensionTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarSubmissionITCase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlerParameterTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientSavepointTriggerTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="25719" opendate="2022-1-20 00:00:00" fixdate="2022-1-20 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support General Python UDF in Thread Mode</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PassThroughPythonStreamGroupWindowAggregateOperator.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.utils.CommonPythonUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCalc.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PythonTestUtils.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamTableAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamGroupWindowAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonAggregateFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtilsTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.ArrowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupTableAggregateOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperatorTest.java</file>
      <file type="M">docs.layouts.shortcodes.generated.python.configuration.html</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.runtime.context.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.timerservice.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.datastream.window.window.operator.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.table.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.state.data.view.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.aggregate.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.aggregate.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.context.py</file>
      <file type="M">flink-python.pyflink.fn.execution.table.window.process.function.py</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.process.mode.boot.py</file>
      <file type="M">flink-python.pyflink.fn.execution.utils.operation.utils.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.dependency.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.env.beam.ProcessPythonEnvironmentManager.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.env.ProcessPythonEnvironment.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.env.PythonDependencyInfo.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.env.PythonEnvironmentManager.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonConfig.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.util.PythonEnvironmentManagerUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractDataStreamPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamDataStreamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractOneInputPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamTablePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtils.java</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.env.beam.ProcessPythonEnvironmentManagerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonOverWindowAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonProcTimeBoundedRangeOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonProcTimeBoundedRowsOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonRowTimeBoundedRangeOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonRowTimeBoundedRowsOperatorTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="25726" opendate="2022-1-20 00:00:00" fixdate="2022-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement GlobalCommitter as custom post commit topology</summary>
      <description>The global committer was rarely used before in Sink V1 so we decided to make it an extension that is only exposed to a specific set of users.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.transformations.SinkV1AdapterTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.SinkITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.SinkTransformationTranslatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.SinkTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SinkV1Adapter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SinkTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.connector.sink2.StandardSinkTopologies.java</file>
    </fixedFiles>
  </bug>
  <bug id="25739" opendate="2022-1-20 00:00:00" fixdate="2022-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include changelog jars into distribution</summary>
      <description>Add changelog jars to dist/opt folder: flink-dstl-dfs - so users can add it to plugins/ easily (plugin, cluster level) flink-statebackend-changelog - so that it can be added to lib/ if needed (not plugin, cluster or job-level)Update docs if done after FLINK-25024.cc: chesnay</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25741" opendate="2022-1-21 00:00:00" fixdate="2022-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ignore buffer pools which have no floating buffer in buffer redistributing</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="25742" opendate="2022-1-21 00:00:00" fixdate="2022-2-21 01:00:00" resolution="Done">
    <buginformation>
      <summary>Remove the redundant serialization of RPC invocation at Flink side.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.messages.RpcInvocation.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.MessageSerializationTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActor.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="25755" opendate="2022-1-21 00:00:00" fixdate="2022-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce shared interface for adaptive scheduler state transitions</summary>
      <description>Currently the definitions of the state transitions are scatter between multiple state contexts. We want to: Getting rid of duplicate definitions Attempt to make the state transitions less confusing for the "code reader"This will allow "code reader" to use a type hierarchy for understanding which transitions are allowed from which state. (see the attached screenshot)</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.CreatingExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.WaitingForResources.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.StopWithSavepoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.Restarting.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.Failing.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.Executing.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.CreatingExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.Created.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.java</file>
    </fixedFiles>
  </bug>
  <bug id="25758" opendate="2022-1-22 00:00:00" fixdate="2022-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>GCS Filesystem implementation fails on Java 11 tests due to licensing issues</summary>
      <description>00:33:45,410 DEBUG org.apache.flink.tools.ci.licensecheck.NoticeFileChecker [] - Dependency io.netty:netty-common:4.1.51.Final is mentioned in NOTICE file /__w/2/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency00:33:45,411 ERROR org.apache.flink.tools.ci.licensecheck.NoticeFileChecker [] - Could not find dependency javax.annotation:javax.annotation-api:1.3.2 in NOTICE file /__w/2/s/flink-filesystems/flink-gs-fs-hadoop/src/main/resources/META-INF/NOTICE00:33:45,536 INFO org.apache.flink.tools.ci.licensecheck.JarFileChecker [] - Checking directory /tmp/flink-validation-deployment with a total of 197 jar files.00:34:18,554 ERROR org.apache.flink.tools.ci.licensecheck.JarFileChecker [] - File '/javax/annotation/security/package.html' in jar '/tmp/flink-validation-deployment/org/apache/flink/flink-gs-fs-hadoop/1.15-SNAPSHOT/flink-gs-fs-hadoop-1.15-20220122.001944-1.jar' contains match with forbidden regex 'gnu ?\R?[\s/#]*general ?\R?[\s/#]*public ?\R?[\s/#]*license'.00:34:18,555 ERROR org.apache.flink.tools.ci.licensecheck.JarFileChecker [] - File '/javax/annotation/package.html' in jar '/tmp/flink-validation-deployment/org/apache/flink/flink-gs-fs-hadoop/1.15-SNAPSHOT/flink-gs-fs-hadoop-1.15-20220122.001944-1.jar' contains match with forbidden regex 'gnu ?\R?[\s/#]*general ?\R?[\s/#]*public ?\R?[\s/#]*license'.00:35:46,612 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Found a total of 3 severe license issueshttps://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29932&amp;view=logs&amp;j=946871de-358d-5815-3994-8175615bc253&amp;t=e0240c62-4570-5d1c-51af-dd63d2093da1</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-gs-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25773" opendate="2022-1-24 00:00:00" fixdate="2022-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to flink-shaded 15.0</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25774" opendate="2022-1-24 00:00:00" fixdate="2022-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Restrict the maximum number of buffers can be used per result partition for sort-shuffle</summary>
      <description>Currently, for sort-shuffle, the maximum number of buffers can be used per result partition is Integer.MAX_VALUE. However, if too many buffers are taken by one result partition, other result partitions and input gates may spend too much time waiting for buffers which can influence performance. This ticket aims to restrict the maximum number of buffers can be used per result partition and the selected value is an empirical one based on the TPC-DS test results.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.NettyShuffleUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="25785" opendate="2022-1-24 00:00:00" fixdate="2022-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update com.h2database:h2 to 2.0.210</summary>
      <description>Two security vulnerabilities in H2 Console (CVE-2022-23221 and possible DNS rebinding attack) are fixed in 2.0.120. Flink is currently on 2.0.206 since https://issues.apache.org/jira/browse/FLINK-25576Note: Flink is using this dependency only for testing, so it's not directly impacted by the CVE. We just want to be good citizens and update our dependencies</description>
      <version>1.13.5,1.14.3,1.15.0</version>
      <fixedVersion>1.14.4,1.15.0,1.13.7</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25786" opendate="2022-1-24 00:00:00" fixdate="2022-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adjust the generation of subpartition data storage order for sort-shuffle from random shuffle to random shift</summary>
      <description>Currently, for sort-shuffle the generation of subpartition data storage order  is random shuffle. However, if there is no enough resources to run the downstream consumer tasks in parallel, the performance can be influenced because of the random disk IO caused by the random subpartition data storage order. This ticket aims to improve this scenario.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="25790" opendate="2022-1-24 00:00:00" fixdate="2022-1-24 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support authentication via core-site.xml in GCS FileSystem plugin</summary>
      <description>Add support for authentication via core-site.xml to the new GCS FileSystem connector, recently added via FLINK-11838 Create RecoverableWriter for GCS - ASF JIRA (apache.org).Specifically, make the RecoverableWriter use explicit credentials supplied in core-site.xml in the "google.cloud.auth.service.account.json.keyfile" property. Otherwise, it should use implicit credentials, as it already does.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-gs-fs-hadoop.src.test.java.org.apache.flink.fs.gs.TestUtils.java</file>
      <file type="M">flink-filesystems.flink-gs-fs-hadoop.src.main.java.org.apache.flink.fs.gs.GSFileSystemFactory.java</file>
      <file type="M">flink-filesystems.flink-gs-fs-hadoop.src.main.java.org.apache.flink.fs.gs.GSFileSystem.java</file>
    </fixedFiles>
  </bug>
  <bug id="25791" opendate="2022-1-24 00:00:00" fixdate="2022-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make ObjectIdentifier json representation simpler</summary>
      <description>Use ObjectIdentifier#asSerializableString to serialize the object identifier, rather than serializing it as object.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCorrelateJsonPlanTest.jsonplan.testJoinWithFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.TemporalTableSourceSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexWindowBoundSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LookupKeySerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalWindowSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.JsonSerdeMocks.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.SerdeContext.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testIndividualWindowTVFProcessingTime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testIndividualWindowTVF.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowRank.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowDeduplicate.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowJoinJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testDistinctSplitEnabled.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WatermarkAssignerJsonPlanTest.jsonplan.testWatermarkAssigner.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ValuesJsonPlanTest.jsonplan.testValues.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.UnionJsonPlanTest.jsonplan.testUnion.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortRowTime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortProcessingTime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalJoinJsonPlanTest.jsonplan.testTemporalTableJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalJoinJsonPlanTest.jsonplan.testJoinTemporalFunction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testWatermarkPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testReadingMetadata.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testProjectPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testPartitionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testLimitPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testFilterPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testWritingMetadata.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testPartitioning.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testOverwrite.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.SortLimitJsonPlanTest.jsonplan.testSortLimit.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.RankJsonPlanTest.jsonplan.testRank.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testRowTimeBoundedPartitionedRowsOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupAggregateJsonPlanTest.jsonplan.tesPythonAggCallsWithGroupBy.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCorrelateJsonPlanTest.jsonplan.testPythonTableFunction.out</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalTypeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.ObjectIdentifierJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.ObjectIdentifierJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.jsonplan.testGetJsonPlan.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testComplexCalc.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testSimpleFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testSimpleProject.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ChangelogSourceJsonPlanTest.jsonplan.testChangelogSource.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ChangelogSourceJsonPlanTest.jsonplan.testUpsertSource.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testCrossJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testCrossJoinOverrideParameters.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testJoinWithFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testLeftOuterJoinWithLiteralTrue.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.DeduplicationJsonPlanTest.jsonplan.testDeduplication.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ExpandJsonPlanTest.jsonplan.testExpand.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregate.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregateWithSumCountDistinctAndRetraction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testProcessingTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testRowTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testInnerJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testInnerJoinWithEqualPk.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testInnerJoinWithPk.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testLeftJoinNonEqui.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LimitJsonPlanTest.jsonplan.testLimit.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTable.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTableWithProjectionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.MatchRecognizeJsonPlanTest.jsonplan.testMatch.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctWithNonDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testRowTimeBoundedPartitionedRowsOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.jsonplan.testPythonCalc.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.jsonplan.testPythonFunctionInWhereClause.out</file>
    </fixedFiles>
  </bug>
  <bug id="25792" opendate="2022-1-24 00:00:00" fixdate="2022-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Async Sink Base is too being flushed too frequently resulting in backpressure even when buffer is near empty</summary>
      <description>Bug:Async Sink Base is too being flushed too frequently resulting in backpressure even when buffer is near emptyCause:During a write(), flushIfAble() is called, which checks if the number of buffered elements is greater than a batch size, and if so, insists that the sink flushes immediately, even if the number of inFlightRequests is greater than the maximum allowed number of inFlightRequests, resulting in a yield of the current mailbox thread, and hence blocks.Notice that this can occur even if the buffer is near empty, so the blocking behaviour is unnecessary and undesirable, since we would like the element to be written to the buffer and no blocking to occur.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.sink.writer.TestSinkInitContext.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.sink.writer.AsyncSinkWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.sink.writer.AsyncSinkWriter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkWriterTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="25808" opendate="2022-1-25 00:00:00" fixdate="2022-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JsonTypeInfo property should be valid java identifier</summary>
      <description>Some REST classes use the JsonTypeInfo with the property being named @class. This causes invalid java code to be generated by swagger.Rename it to clazz.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.SubtaskCheckpointStatistics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointStatistics.java</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
    </fixedFiles>
  </bug>
  <bug id="25817" opendate="2022-1-26 00:00:00" fixdate="2022-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FLIP-201: Persist local state in working directory</summary>
      <description>This issue is the umbrella ticket for FLIP-201 which aims at adding support for persisting local state in Flink's working directory. This would enable Flink in certain scenarios to recover locally even in case of process failures.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponentTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskSubmissionTestEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServicesConfiguration.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskLocalStateStoreImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskLocalStateStoreImpl.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.WorkingDirectoryExtension.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerServicesBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.slot.LocalSlotAllocationSnapshot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.WorkingDirectory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="25818" opendate="2022-1-26 00:00:00" fixdate="2022-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add explanation how Kafka Source deals with idleness when parallelism is higher then the number of partitions</summary>
      <description>Add a section to the Kafka Source documentation to explain what happens with the Kafka Source with regards to idleness when parallelism is higher then the number of partitions</description>
      <version>1.15.0</version>
      <fixedVersion>1.13.6,1.14.4,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.datastream.kafka.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="25826" opendate="2022-1-26 00:00:00" fixdate="2022-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle symbols at a central place with serializable format</summary>
      <description>Symbols are quite messy in the code base. We should unify all locations and define a serializable format for the JSON plan.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.MatchRecognizeJsonPlanTest.jsonplan.testMatch.out</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.JsonGenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.JsonValueCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.JsonObjectCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.JsonArrayCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.converter.ExpressionConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.converter.converters.JsonQueryConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.converter.converters.JsonExistsConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.converter.converters.JsonConverterUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="25836" opendate="2022-1-27 00:00:00" fixdate="2022-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>test_keyed_process_function_with_state of BatchModeDataStreamTests faild in PyFlink</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30264&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=c67e71ed-6451-5d26-8920-5a8cf9651901</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="25840" opendate="2022-1-27 00:00:00" fixdate="2022-2-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add semantic test support in the connector testframe</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.utils.TestDataMatchers.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.junit.extensions.TestCaseInvocationContextProvider.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.junit.extensions.ConnectorTestingExtension.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroBulkFormatITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.PulsarSourceUnorderedE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.PulsarSourceOrderedE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.KafkaSourceE2ECase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.PulsarSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.KafkaSourceITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="25845" opendate="2022-1-27 00:00:00" fixdate="2022-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose plan via SQL COMPILE / EXECUTE PLAN</summary>
      <description>This includes:EXECUTE PLAN '/mydir/plan.json';as mentioned in:https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=191336489#FLIP190:SupportVersionUpgradesforTableAPI&amp;SQLPrograms-EXECUTEand https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=191336489#FLIP190:SupportVersionUpgradesforTableAPI&amp;SQLPrograms-COMPILEwith option table.plan.force-recompile</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.utils.JsonPlanTestBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.api.internal.TableEnvironmentInternalTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.ExecNodeGraphCompiledPlan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.command.ExecutePlanOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableEnvironment.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.TableConfigOptions.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.CompiledPlan.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.dml.SqlExecutePlan.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">docs.layouts.shortcodes.generated.table.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="25856" opendate="2022-1-27 00:00:00" fixdate="2022-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix use of UserDefinedType in from_elements</summary>
      <description>If we define a new UserDefinedType, and use it in `from_elements`, it will failed.class VectorUDT(UserDefinedType): @classmethod def sql_type(cls): return DataTypes.ROW( [ DataTypes.FIELD("type", DataTypes.TINYINT()), DataTypes.FIELD("size", DataTypes.INT()), DataTypes.FIELD("indices", DataTypes.ARRAY(DataTypes.INT())), DataTypes.FIELD("values", DataTypes.ARRAY(DataTypes.DOUBLE())), ] ) @classmethod def module(cls): return "pyflink.ml.core.linalg" def serialize(self, obj): if isinstance(obj, SparseVector): indices = [int(i) for i in obj._indices] values = [float(v) for v in obj._values] return 0, obj.size(), indices, values elif isinstance(obj, DenseVector): values = [float(v) for v in obj._values] return 1, None, None, values else: raise TypeError("Cannot serialize %r of type %r".format(obj, type(obj)))self.t_env.from_elements([ (Vectors.dense([1, 2, 3, 4]), 0., 1.), (Vectors.dense([2, 2, 3, 4]), 0., 2.), (Vectors.dense([3, 2, 3, 4]), 0., 3.), (Vectors.dense([4, 2, 3, 4]), 0., 4.), (Vectors.dense([5, 2, 3, 4]), 0., 5.), (Vectors.dense([11, 2, 3, 4]), 1., 1.), (Vectors.dense([12, 2, 3, 4]), 1., 2.), (Vectors.dense([13, 2, 3, 4]), 1., 3.), (Vectors.dense([14, 2, 3, 4]), 1., 4.), (Vectors.dense([15, 2, 3, 4]), 1., 5.), ], DataTypes.ROW([ DataTypes.FIELD("features", VectorUDT()), DataTypes.FIELD("label", DataTypes.DOUBLE()), DataTypes.FIELD("weight", DataTypes.DOUBLE())]))</description>
      <version>1.14.3,1.15.0</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.utils.python.PythonTableUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.api.common.python.PythonBridgeUtils.java</file>
      <file type="M">flink-python.pyflink.table.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
    </fixedFiles>
  </bug>
  <bug id="25872" opendate="2022-1-28 00:00:00" fixdate="2022-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Restoring from non-changelog checkpoint with changelog state-backend enabled in CLAIM mode discards state in use</summary>
      <description>If we restore from checkpoint with changelog state-backend enabled in snapshot CLAIM mode, the restored checkpoint would be discarded on subsume. This invalidates newer/active checkpoints because their materialized part is discarded (for incremental wrapped checkpoints, their private state is discarded). This bug is like FLINK-25478. Design doc: https://docs.google.com/document/d/1KSFWc0gL7HkhC-JNrnsp06TLnsTmZOTHITQDcGMo0cI/edit?usp=sharing,</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.ChangelogPeriodicMaterializationSwitchStateBackendITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.ChangelogPeriodicMaterializationSwitchEnvTestBase.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.AbstractChangelogStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.SharedStateRegistryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.SharedStateRegistryImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.SharedStateRegistry.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.ChangelogStateBackendHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorSubtaskState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.EmbeddedCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointsCleaner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.AbstractCompleteCheckpointStore.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.ChangelogCompatibilityITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="25874" opendate="2022-1-29 00:00:00" fixdate="2022-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink package dependencies conflict</summary>
      <description>I need to install PyFlink with some other libraries in a project, and I found there's dependency conflict with `great-expectations` because PyFlink has pined dependency `python-dateutil==2.8.0`. There are incompatible versions in the resolved dependencies: python-dateutil==2.8.0 (from apache-flink==1.14.3-&gt;-r requirements.in (line 4)) python-dateutil&gt;=2.8.1 (from great-expectations==0.14.4-&gt;-r requirements.in (line 5)) I have to use newer version of great-expectations (&gt;=0.13.1) for some features, so this is blocking me.I found `python-dateutil` v2.8.0 is released in 2019-02-05, see https://github.com/dateutil/dateutil/releases/tag/2.8.0, is there any way to loose the dependency, e.g. &gt;=2.8.0 ?(BTW, `cloudpickle==1.2.2` is also old which released in 2019-09-10, see https://github.com/cloudpipe/cloudpickle/releases/tag/v1.2.2 )</description>
      <version>None</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.README.md</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
    </fixedFiles>
  </bug>
  <bug id="25885" opendate="2022-1-31 00:00:00" fixdate="2022-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClusterEntrypointTest.testWorkingDirectoryIsDeletedIfApplicationCompletes failed on azure</summary>
      <description>2022-01-31T05:00:07.3113870Z Jan 31 05:00:07 java.util.concurrent.CompletionException: org.apache.flink.runtime.rpc.akka.exceptions.AkkaRpcException: Discard message, because the rpc endpoint akka.tcp://flink@127.0.0.1:6123/user/rpc/resourcemanager_2 has not been started yet.2022-01-31T05:00:07.3115008Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)2022-01-31T05:00:07.3115778Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)2022-01-31T05:00:07.3116527Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)2022-01-31T05:00:07.3117267Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)2022-01-31T05:00:07.3118011Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)2022-01-31T05:00:07.3118770Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)2022-01-31T05:00:07.3119608Z Jan 31 05:00:07 at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:251)2022-01-31T05:00:07.3120425Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)2022-01-31T05:00:07.3121199Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)2022-01-31T05:00:07.3121957Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)2022-01-31T05:00:07.3122716Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)2022-01-31T05:00:07.3123457Z Jan 31 05:00:07 at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1387)2022-01-31T05:00:07.3124241Z Jan 31 05:00:07 at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)2022-01-31T05:00:07.3125106Z Jan 31 05:00:07 at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)2022-01-31T05:00:07.3126063Z Jan 31 05:00:07 at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)2022-01-31T05:00:07.3127207Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)2022-01-31T05:00:07.3127982Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)2022-01-31T05:00:07.3128741Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)2022-01-31T05:00:07.3129497Z Jan 31 05:00:07 at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)2022-01-31T05:00:07.3130385Z Jan 31 05:00:07 at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:45)2022-01-31T05:00:07.3131092Z Jan 31 05:00:07 at akka.dispatch.OnComplete.internal(Future.scala:299)2022-01-31T05:00:07.3131695Z Jan 31 05:00:07 at akka.dispatch.OnComplete.internal(Future.scala:297)2022-01-31T05:00:07.3132310Z Jan 31 05:00:07 at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)2022-01-31T05:00:07.3132943Z Jan 31 05:00:07 at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)2022-01-31T05:00:07.3133577Z Jan 31 05:00:07 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)2022-01-31T05:00:07.3134340Z Jan 31 05:00:07 at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)2022-01-31T05:00:07.3135149Z Jan 31 05:00:07 at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)2022-01-31T05:00:07.3135898Z Jan 31 05:00:07 at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)2022-01-31T05:00:07.3136692Z Jan 31 05:00:07 at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)2022-01-31T05:00:07.3137454Z Jan 31 05:00:07 at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)2022-01-31T05:00:07.3138127Z Jan 31 05:00:07 at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)2022-01-31T05:00:07.3138726Z Jan 31 05:00:07 at akka.actor.ActorRef.tell(ActorRef.scala:131)2022-01-31T05:00:07.3139391Z Jan 31 05:00:07 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.sendErrorIfSender(AkkaRpcActor.java:501)2022-01-31T05:00:07.3140173Z Jan 31 05:00:07 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:173)2022-01-31T05:00:07.3140882Z Jan 31 05:00:07 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)2022-01-31T05:00:07.3141535Z Jan 31 05:00:07 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)2022-01-31T05:00:07.3142177Z Jan 31 05:00:07 at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)2022-01-31T05:00:07.3142822Z Jan 31 05:00:07 at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)2022-01-31T05:00:07.3143467Z Jan 31 05:00:07 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)2022-01-31T05:00:07.3144145Z Jan 31 05:00:07 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)2022-01-31T05:00:07.3145019Z Jan 31 05:00:07 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)2022-01-31T05:00:07.3145744Z Jan 31 05:00:07 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)2022-01-31T05:00:07.3146421Z Jan 31 05:00:07 at akka.actor.Actor.aroundReceive(Actor.scala:537)2022-01-31T05:00:07.3147053Z Jan 31 05:00:07 at akka.actor.Actor.aroundReceive$(Actor.scala:535)2022-01-31T05:00:07.3147714Z Jan 31 05:00:07 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)2022-01-31T05:00:07.3148417Z Jan 31 05:00:07 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)2022-01-31T05:00:07.3149072Z Jan 31 05:00:07 at akka.actor.ActorCell.invoke(ActorCell.scala:548)2022-01-31T05:00:07.3149725Z Jan 31 05:00:07 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)2022-01-31T05:00:07.3231566Z Jan 31 05:00:07 at akka.dispatch.Mailbox.run(Mailbox.scala:231)2022-01-31T05:00:07.3232417Z Jan 31 05:00:07 at akka.dispatch.Mailbox.exec(Mailbox.scala:243)2022-01-31T05:00:07.3233367Z Jan 31 05:00:07 at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)2022-01-31T05:00:07.3234208Z Jan 31 05:00:07 at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)2022-01-31T05:00:07.3234909Z Jan 31 05:00:07 at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)2022-01-31T05:00:07.3235609Z Jan 31 05:00:07 at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)2022-01-31T05:00:07.3237023Z Jan 31 05:00:07 Caused by: org.apache.flink.runtime.rpc.akka.exceptions.AkkaRpcException: Discard message, because the rpc endpoint akka.tcp://flink@127.0.0.1:6123/user/rpc/resourcemanager_2 has not been started yet.2022-01-31T05:00:07.3238316Z Jan 31 05:00:07 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:175)2022-01-31T05:00:07.3238886Z Jan 31 05:00:07 ... 20 more2022-01-31T05:00:07.3239220Z Jan 31 05:00:07 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30491&amp;view=logs&amp;j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&amp;t=7b25afdf-cc6c-566f-5459-359dc2585798&amp;l=12987</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent.java</file>
    </fixedFiles>
  </bug>
  <bug id="25892" opendate="2022-1-31 00:00:00" fixdate="2022-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Develop ArchUnit test for connectors</summary>
      <description>ArchUnit test should be developed for connector submodules after the ArchUnit infra for test code has been built.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-gcp-pubsub.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.pom.xml</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25894" opendate="2022-1-31 00:00:00" fixdate="2022-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explicitly configure japicmp oldVersion for flink-streaming-java</summary>
      <description>Since flink-streaming-java lost its scala suffix japicmp can't find the artifact to compare it against, as the 1.14 artifact still has the suffix.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25897" opendate="2022-1-31 00:00:00" fixdate="2022-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update project configuration gradle doc to 7.x version</summary>
      <description>Update the gradle build script and its doc page to 7.x</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.configuration.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="25907" opendate="2022-2-1 00:00:00" fixdate="2022-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add pluggable delegation token manager</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.TaskManagerDisconnectOnShutdownITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.utils.MockResourceManagerRuntimeServices.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.TestingResourceManagerService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.TestingResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.TestingResourceManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.TestingMiniCluster.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.ExecutionGraphInfoStoreTestUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.modules.HadoopModuleFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.contexts.HadoopSecurityContextFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerProcessContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="25910" opendate="2022-2-1 00:00:00" fixdate="2022-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Propagate obtained delegation tokens to TaskManagers</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">tools.ci.flink-ci-tools.src.main.java.org.apache.flink.tools.ci.licensecheck.NoticeFileChecker.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorToResourceManagerConnectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorExecutionDeploymentReconciliationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.utils.TestingResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorRegistrationSuccess.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGatewayDecoratorBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.NoOpDelegationTokenManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.KerberosDelegationTokenManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.DelegationTokenManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="25917" opendate="2022-2-2 00:00:00" fixdate="2022-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Share RpcSystem aross tests</summary>
      <description>We currently create a dedicated RpcSystem for each test, meaning that we go through the whole cycle of extracting the rpc-akka jar and creating a new classloader.For testing purposes we can re-use the same RpcSystem, which will alleviate FLINK-18356 and maybe speed up some test or two.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.MiniClusterResource.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
    </fixedFiles>
  </bug>
  <bug id="25926" opendate="2022-2-2 00:00:00" fixdate="2022-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update org.postgresql:postgresql to 42.3.3</summary>
      <description>Security vulnerability CVE-2022-21724 is fixed in 42.2.25. Flink is currently on 42.2.10.Note: Flink uses this dependency in a Provided scope only.</description>
      <version>1.13.5,1.14.3,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.converter.PostgresRowConverter.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.dialect.psql.PostgresTypeMapper.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25937" opendate="2022-2-3 00:00:00" fixdate="2022-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client end-to-end test e2e fails on AZP</summary>
      <description>The SQL Client end-to-end test e2e tests fails on AZP when using the AdaptiveScheduler because the scheduler expects that the parallelism is set for all vertices:Feb 03 03:45:13 org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.Feb 03 03:45:13 at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.lambda$new$0(DefaultJobMasterServiceProcess.java:97)Feb 03 03:45:13 at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)Feb 03 03:45:13 at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)Feb 03 03:45:13 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)Feb 03 03:45:13 at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1609)Feb 03 03:45:13 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)Feb 03 03:45:13 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)Feb 03 03:45:13 at java.lang.Thread.run(Thread.java:748)Feb 03 03:45:13 Caused by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: The adaptive scheduler expects the parallelism being set for each JobVertex (violated JobVertex: f74b775b58627a33e46b8c155b320255).Feb 03 03:45:13 at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)Feb 03 03:45:13 at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)Feb 03 03:45:13 at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1606)Feb 03 03:45:13 ... 3 moreFeb 03 03:45:13 Caused by: java.lang.IllegalStateException: The adaptive scheduler expects the parallelism being set for each JobVertex (violated JobVertex: f74b775b58627a33e46b8c155b320255).Feb 03 03:45:13 at org.apache.flink.util.Preconditions.checkState(Preconditions.java:215)Feb 03 03:45:13 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.assertPreconditions(AdaptiveScheduler.java:296)Feb 03 03:45:13 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.&lt;init&gt;(AdaptiveScheduler.java:230)Feb 03 03:45:13 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerFactory.createInstance(AdaptiveSchedulerFactory.java:122)Feb 03 03:45:13 at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:115)Feb 03 03:45:13 at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:345)Feb 03 03:45:13 at org.apache.flink.runtime.jobmaster.JobMaster.&lt;init&gt;(JobMaster.java:322)Feb 03 03:45:13 at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:106)Feb 03 03:45:13 at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:94)Feb 03 03:45:13 at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112)Feb 03 03:45:13 at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)Feb 03 03:45:13 ... 3 morehttps://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30662&amp;view=logs&amp;j=fb37c667-81b7-5c22-dd91-846535e99a97&amp;t=39a035c3-c65e-573c-fb66-104c66c28912&amp;l=5782</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.SinkTransformationTranslator.java</file>
    </fixedFiles>
  </bug>
  <bug id="25941" opendate="2022-2-3 00:00:00" fixdate="2022-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaSinkITCase.testAbortTransactionsAfterScaleInBeforeFirstCheckpoint fails on AZP</summary>
      <description>The test KafkaSinkITCase.testAbortTransactionsAfterScaleInBeforeFirstCheckpoint fails on AZP with2022-02-02T17:22:29.5131631Z Feb 02 17:22:29 [ERROR] org.apache.flink.connector.kafka.sink.KafkaSinkITCase.testAbortTransactionsAfterScaleInBeforeFirstCheckpoint Time elapsed: 2.186 s &lt;&lt;&lt; FAILURE!2022-02-02T17:22:29.5146972Z Feb 02 17:22:29 java.lang.AssertionError2022-02-02T17:22:29.5148918Z Feb 02 17:22:29 at org.junit.Assert.fail(Assert.java:87)2022-02-02T17:22:29.5149843Z Feb 02 17:22:29 at org.junit.Assert.assertTrue(Assert.java:42)2022-02-02T17:22:29.5150644Z Feb 02 17:22:29 at org.junit.Assert.assertTrue(Assert.java:53)2022-02-02T17:22:29.5151730Z Feb 02 17:22:29 at org.apache.flink.connector.kafka.sink.KafkaSinkITCase.testAbortTransactionsAfterScaleInBeforeFirstCheckpoint(KafkaSinkITCase.java:267)2022-02-02T17:22:29.5152858Z Feb 02 17:22:29 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2022-02-02T17:22:29.5153757Z Feb 02 17:22:29 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2022-02-02T17:22:29.5155002Z Feb 02 17:22:29 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2022-02-02T17:22:29.5156464Z Feb 02 17:22:29 at java.lang.reflect.Method.invoke(Method.java:498)2022-02-02T17:22:29.5157384Z Feb 02 17:22:29 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)2022-02-02T17:22:29.5158445Z Feb 02 17:22:29 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2022-02-02T17:22:29.5159478Z Feb 02 17:22:29 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)2022-02-02T17:22:29.5160524Z Feb 02 17:22:29 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2022-02-02T17:22:29.5161758Z Feb 02 17:22:29 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2022-02-02T17:22:29.5162775Z Feb 02 17:22:29 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2022-02-02T17:22:29.5163744Z Feb 02 17:22:29 at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)2022-02-02T17:22:29.5164913Z Feb 02 17:22:29 at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)2022-02-02T17:22:29.5166101Z Feb 02 17:22:29 at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)2022-02-02T17:22:29.5167030Z Feb 02 17:22:29 at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)2022-02-02T17:22:29.5167953Z Feb 02 17:22:29 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)2022-02-02T17:22:29.5168956Z Feb 02 17:22:29 at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)2022-02-02T17:22:29.5169936Z Feb 02 17:22:29 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)2022-02-02T17:22:29.5170903Z Feb 02 17:22:29 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)2022-02-02T17:22:29.5171953Z Feb 02 17:22:29 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)2022-02-02T17:22:29.5172919Z Feb 02 17:22:29 at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)2022-02-02T17:22:29.5173811Z Feb 02 17:22:29 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)2022-02-02T17:22:29.5174874Z Feb 02 17:22:29 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)2022-02-02T17:22:29.5175917Z Feb 02 17:22:29 at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)2022-02-02T17:22:29.5176851Z Feb 02 17:22:29 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)2022-02-02T17:22:29.5177816Z Feb 02 17:22:29 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2022-02-02T17:22:29.5178816Z Feb 02 17:22:29 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2022-02-02T17:22:29.5179929Z Feb 02 17:22:29 at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)2022-02-02T17:22:29.5180960Z Feb 02 17:22:29 at org.junit.rules.RunRules.evaluate(RunRules.java:20)2022-02-02T17:22:29.5181827Z Feb 02 17:22:29 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)2022-02-02T17:22:29.5182733Z Feb 02 17:22:29 at org.junit.runners.ParentRunner.run(ParentRunner.java:413)2022-02-02T17:22:29.5183595Z Feb 02 17:22:29 at org.junit.runner.JUnitCore.run(JUnitCore.java:137)2022-02-02T17:22:29.5184586Z Feb 02 17:22:29 at org.junit.runner.JUnitCore.run(JUnitCore.java:115)2022-02-02T17:22:29.5185660Z Feb 02 17:22:29 at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)2022-02-02T17:22:29.5186712Z Feb 02 17:22:29 at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)2022-02-02T17:22:29.5187763Z Feb 02 17:22:29 at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)2022-02-02T17:22:29.5188837Z Feb 02 17:22:29 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)2022-02-02T17:22:29.5190020Z Feb 02 17:22:29 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)2022-02-02T17:22:29.5191469Z Feb 02 17:22:29 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)2022-02-02T17:22:29.5192712Z Feb 02 17:22:29 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)2022-02-02T17:22:29.5193946Z Feb 02 17:22:29 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)2022-02-02T17:22:29.5195325Z Feb 02 17:22:29 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)2022-02-02T17:22:29.5196488Z Feb 02 17:22:29 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)2022-02-02T17:22:29.5197609Z Feb 02 17:22:29 at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)2022-02-02T17:22:29.5198802Z Feb 02 17:22:29 at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)2022-02-02T17:22:29.5199906Z Feb 02 17:22:29 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)2022-02-02T17:22:29.5201084Z Feb 02 17:22:29 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)2022-02-02T17:22:29.5202246Z Feb 02 17:22:29 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)2022-02-02T17:22:29.5203357Z Feb 02 17:22:29 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)2022-02-02T17:22:29.5204504Z Feb 02 17:22:29 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)2022-02-02T17:22:29.5205490Z Feb 02 17:22:29 at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)2022-02-02T17:22:29.5206578Z Feb 02 17:22:29 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30642&amp;view=logs&amp;j=c5f0071e-1851-543e-9a45-9ac140befc32&amp;t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&amp;l=35949</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.sink.CommitterOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.sink.committables.CommittableCollectorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.sink.committables.CheckpointCommittableManagerImplTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.connector.sink2.CommittableSummaryAssert.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.SinkTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperatorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.CommitterOperatorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.CommitterOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.committables.CommittableCollector.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.committables.CheckpointCommittableManagerImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.connector.sink2.GlobalCommitterOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="25947" opendate="2022-2-3 00:00:00" fixdate="2022-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>License checker doesn&amp;#39;t cover flink-table-planner</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25960" opendate="2022-2-4 00:00:00" fixdate="2022-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Distribute the data read buffers more fairly among result partitions for sort-shuffle</summary>
      <description>Currently, the data read buffers for sort-shuffle are allocated in a random way and some result partitions may occupy too many buffers which leads to the starvation of other result partitions. This ticket aims to improve the scenario by not reading data for those result partitions which already occupy more than the average number of read buffers per result partition.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.BatchShuffleReadBufferPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="25961" opendate="2022-2-4 00:00:00" fixdate="2022-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove transport client from Elasticsearch 6/7 connectors (tests only)</summary>
      <description>The Elasticsearch 6/7 connectors still use deprecated transport client for integration tests. This is not really necessary and brings a lot of dependencies, the HighLevelRestClient is fully sufficient and could be used as drop-in replacement.The change affects only tests.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.testutils.SourceSinkDataTestKit.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="25974" opendate="2022-2-7 00:00:00" fixdate="2022-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make cancellation of jobs depend on the JobResultStore</summary>
      <description>JobManagerRunner instances were cancellable as long as the instance was still registered in the Dispatcher.jobManagerRunnerRegistry. With the cleanup being done concurrently (i.e. not relying on the JobManagerRunnerRegistry to be cleaned up anymore), the cancellation of a job should only be possible as long as the job is not globally finished and before cleanup is triggered.We should verify whether a job is listed in the JobResultStore and only enable the user to cancel the job if that's not the case.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherCleanupITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.cleanup.CheckpointResourcesCleanupRunnerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.cleanup.CheckpointResourcesCleanupRunnerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.cleanup.CheckpointResourcesCleanupRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="25976" opendate="2022-2-7 00:00:00" fixdate="2022-2-7 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Update the KDS and KDF Sink&amp;#39;s defaults &amp; update the docs</summary>
      <description>Update:DEFAULT_MAX_IN_FLIGHT_REQUESTS=50 to match with the default threads in the AWS SDK v2 HTTP Client default.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.sink.KinesisDataStreamsSinkBuilder.java</file>
      <file type="M">docs.content.docs.connectors.datastream.kinesis.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug id="25977" opendate="2022-2-7 00:00:00" fixdate="2022-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Close sink client and sink http client for KDS/KDF Sinks</summary>
      <description>We are not closing the AWS SDK and HTTP clients for the new KDS/KDF async sink. The close method needs to be invoked when operator stops.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-data-streams.src.test.java.org.apache.flink.connector.kinesis.table.test.KinesisDataStreamsTableApiIT.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.test.java.org.apache.flink.connector.kinesis.sink.KinesisDataStreamsSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.test.java.org.apache.flink.connectors.kinesis.testutils.KinesaliteContainer.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-data-streams.src.main.java.org.apache.flink.connector.kinesis.sink.KinesisDataStreamsSinkWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="25982" opendate="2022-2-7 00:00:00" fixdate="2022-2-7 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support idleness with watermark alignment</summary>
      <description>In the PoC idleness is ignored. What it means is that even if all splits are idle, probably the last emitted watermark will be kept reported to the coordinator blocking all of the remaining source operators from making progress.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MultipleInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.SourceOperatorAlignmentTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.source.WatermarkToDataOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.source.TimestampsAndWatermarks.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.source.ProgressiveTimestampsAndWatermarks.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.source.NoOpTimestampsAndWatermarks.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorAlignmentTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.connector.source.mocks.MockSourceReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="25984" opendate="2022-2-7 00:00:00" fixdate="2022-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove deprecated API usages in ConfigOptions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowEmitStrategy.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.IncrementalAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalSortRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalSortMergeJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalJoinRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinDeriveNullFilterRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.RelNodeBlock.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.OptimizerConfigOptions.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.ExecutionConfigOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBNativeMetricOptions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.ShuffleServiceOptions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.FileJobGraphRetriever.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.main.java.org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporterOptions.java</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.main.java.org.apache.flink.metrics.influxdb.InfluxdbReporterOptions.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.main.java.org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-stateful-job-upgrade-test.src.main.java.org.apache.flink.streaming.tests.StatefulStreamJobUpgradeTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.TtlTestConfig.java</file>
      <file type="M">flink-end-to-end-tests.flink-netty-shuffle-memory-control-test.src.main.java.org.apache.flink.streaming.tests.NettyShuffleMemoryControlTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-heavy-deployment-stress-test.src.main.java.org.apache.flink.deployment.HeavyDeploymentStressTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestJobFactory.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.ConfigurationTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.ConfigOptionTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.WebOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestartStrategyOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ResourceManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.QueryableStateOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.OptimizerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.NettyShuffleEnvironmentOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.MetricOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JMXServerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HistoryServerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HighAvailabilityOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HeartbeatManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CheckpointingOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.BlobServerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.AlgorithmOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.AkkaOptions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="25995" opendate="2022-2-8 00:00:00" fixdate="2022-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make implicit assumption of SQL local hash explicit</summary>
      <description>If there are multiple consecutive and the same hash shuffles, SQL planner will change them except the first one to use forward partitioner, so that these operators can be chained to reduce unnecessary shuffles.However, sometimes the local hash operators are not chained (e.g. multiple inputs), and this kind of forward partitioners will turn into forward job edges. These forward edges still have the local keyBy assumption, so that they cannot be changed into rescale/rebalance edges, otherwise it can lead to incorrect results. This prevents the adaptive batch scheduler from determining parallelism for other forward edge downstream job vertices (see FLINK-25046).When SQL planner optimizes the case of multiple consecutive the same groupBy, it should use ForwardForConsecutiveHashPartitioner, so that the runtime framework can further decide whether the partitioner can be changed to rescale or not.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.RemoveShuffleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveShuffleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalSortWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalSortAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalRank.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalPythonOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalPythonGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalPythonGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalHashWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalHashAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.MultipleInputNodeCreationProcessor.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.InputProperty.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecMultipleInput.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecExchange.java</file>
    </fixedFiles>
  </bug>
  <bug id="25999" opendate="2022-2-8 00:00:00" fixdate="2022-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecate Per-Job Mode</summary>
      <description>As discussed in &amp;#91;1&amp;#93; and voted on in &amp;#91;2&amp;#93;, the community as decided to deprecate per-job mode.&amp;#91;1&amp;#93; https://lists.apache.org/thread/b8g76cqgtr2c515rd1bs41vy285f317n&amp;#91;2&amp;#93; https://lists.apache.org/thread/v6oz92dfp95qcox45l0f8393089oyjv4</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.executors.YarnJobClusterExecutorFactory.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.executors.YarnJobClusterExecutor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnDeploymentTarget.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.JobClusterEntrypoint.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.DeploymentOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.executors.AbstractJobClusterExecutor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.ClusterDescriptor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.GenericCLI.java</file>
      <file type="M">docs.layouts.shortcodes.generated.deployment.configuration.html</file>
      <file type="M">docs.content.docs.deployment.resource-providers.yarn.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.kubernetes.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.content.docs.deployment.overview.md</file>
      <file type="M">docs.content.docs.deployment.cli.md</file>
      <file type="M">docs.content.docs.concepts.glossary.md</file>
      <file type="M">docs.content.docs.concepts.flink-architecture.md</file>
    </fixedFiles>
  </bug>
  <bug id="2600" opendate="2015-9-1 00:00:00" fixdate="2015-10-1 01:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>Failing ElasticsearchSinkITCase.testNodeClient test case</summary>
      <description>I observed that the ElasticsearchSinkITCase.testNodeClient test case fails on Travis. The stack trace isorg.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1.applyOrElse(JobManager.scala:414) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.testingUtils.TestingJobManager$$anonfun$handleTestingMessage$1.applyOrElse(TestingJobManager.scala:285) at scala.PartialFunction$OrElse.apply(PartialFunction.scala:162) at org.apache.flink.runtime.LeaderSessionMessageFilter$$anonfun$receive$1.applyOrElse(LeaderSessionMessageFilter.scala:36) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:33) at org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:28) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118) at org.apache.flink.runtime.LogMessages$$anon$1.applyOrElse(LogMessages.scala:28) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:104) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.RuntimeException: An error occured in ElasticsearchSink. at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSink.close(ElasticsearchSink.java:307) at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:40) at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:75) at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:243) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:185) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:581) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: IndexMissingException[[my-index] missing] at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSink$1.afterBulk(ElasticsearchSink.java:240) at org.elasticsearch.action.bulk.BulkProcessor.execute(BulkProcessor.java:316) at org.elasticsearch.action.bulk.BulkProcessor.executeIfNeeded(BulkProcessor.java:299) at org.elasticsearch.action.bulk.BulkProcessor.internalAdd(BulkProcessor.java:281) at org.elasticsearch.action.bulk.BulkProcessor.add(BulkProcessor.java:264) at org.elasticsearch.action.bulk.BulkProcessor.add(BulkProcessor.java:260) at org.elasticsearch.action.bulk.BulkProcessor.add(BulkProcessor.java:246) at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSink.invoke(ElasticsearchSink.java:286) at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:37) at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:163) at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:56) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:172) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:581) at java.lang.Thread.run(Thread.java:745)Resources:&amp;#91;1&amp;#93; https://s3.amazonaws.com/archive.travis-ci.org/jobs/78055773/log.txt</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.flink-connector-elasticsearch.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="26001" opendate="2022-2-8 00:00:00" fixdate="2022-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement ProjectableDecodingFormat for avro bulk format</summary>
      <description>Currently BulkDecodingFormat of avro does not also implement ProjectableDecodingFormat and the reader will have to read unused columns. We need to implement ProjectableDecodingFormat to optimize reading from avro files.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroFileFormatFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="26004" opendate="2022-2-8 00:00:00" fixdate="2022-2-8 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce ForwardForConsecutiveHashPartitioner</summary>
      <description>If there are multiple consecutive and the same hash shuffles, SQL planner will change them except the first one to use forward partitioner, so that these operators can be chained to reduce unnecessary shuffles.However, sometimes the consecutive hash operators are not chained (e.g. multiple inputs), and this kind of forward partitioners will turn into forward job edges. These forward edges still have the consecutive hash assumption, so that they cannot be changed into rescale/rebalance edges, otherwise it can lead to incorrect results. This prevents the adaptive batch scheduler from determining parallelism for other forward edge downstream job vertices (see FLINK-25046).To solve it, I propose to introduce a new ForwardForConsecutiveHashPartitioner. When SQL planner optimizes the case of multiple consecutive the same groupBy, it should use the proposed partitioner, so that the runtime framework can further decide whether the partitioner can be changed to hash or not. </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="26014" opendate="2022-2-8 00:00:00" fixdate="2022-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document how to use the working directory for faster local recoveries</summary>
      <description>After having implemented FLIP-198 and FLIP-201, users can now use faster TaskManager failover when using local recovery with persisted volumes. I suggest to add documentation for explaining how to configure Flink to make use of it.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug id="26020" opendate="2022-2-9 00:00:00" fixdate="2022-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unified Pulsar Connector config model</summary>
      <description>PulsarClient has built-in config model named ClientConfigurationData, ConsumerConfigurationData and ProducerConfigurationData. We don't want to expose all the config options. And some config options could conflict with each other.We decide to design a new config model based on Flink's Configuration. Which could provide type checks and better integration with Flink Table.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.connectors.py</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarSourceReaderTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.deserializer.PulsarDeserializationSchemaTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.PulsarSourceBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarUnorderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarUnorderedSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarSourceReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarOrderedSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.PulsarSourceReaderFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.deserializer.PulsarSchemaWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.deserializer.PulsarDeserializationSchemaWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.deserializer.PulsarDeserializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSourceOptions.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSourceBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSource.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.topic.range.RangeGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.SplitsAssignmentState.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.stop.LatestMessageStopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.StopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.config.SourceConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.config.PulsarSourceConfigUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.config.CursorVerification.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.config.PulsarOptions.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.config.PulsarConfigUtils.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.5b9eed8a-5fb6-4373-98ac-3be2a71941b8</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.source.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.client.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="26021" opendate="2022-2-9 00:00:00" fixdate="2022-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pulsar topic deduplicated in both sink and source connector</summary>
      <description>Both topics and partitions are regarded as topics in Pulsar. We have to make the topic configuration more robust for deduplicating the partitions and topics.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.topic.TopicNameUtilsTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSourceBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.topic.TopicNameUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="26022" opendate="2022-2-9 00:00:00" fixdate="2022-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement at-least-once and exactly-once Pulsar Sink</summary>
      <description>Support both three types of DeliveryGuarantee in Pulsar sink.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.PulsarSinkOptions.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.config.SinkConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="26025" opendate="2022-2-9 00:00:00" fixdate="2022-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace MockPulsar with new Pulsar test tools based on PulsarStandalone</summary>
      <description>The old Pulsar connector tests are based on a mocked Pulsar broker which is kinda wired in some behavior. The transaction isn't supported in this mocked Pulsar. So we have to use PulsarStandalone directly.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.PulsarSourceUnorderedE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.PulsarSourceOrderedE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.cases.SharedSubscriptionContext.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.cases.KeySharedSubscriptionContext.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.cases.FailoverSubscriptionContext.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.cases.ExclusiveSubscriptionContext.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.container.FlinkContainers.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.resources.containers.txnStandalone.conf</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.SampleData.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntimeOperator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntime.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.SameThreadOrderedSafeExecutor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.PulsarMockRuntime.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.NonClosableMockBookKeeper.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.MockZooKeeperClientFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.MockPulsarService.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.MockBookKeeperClientFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.BlankBrokerInterceptor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.container.PulsarContainerRuntime.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestSuiteBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.cases.SingleTopicConsumingContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.cases.MultipleTopicTemplateContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.cases.MultipleTopicConsumingContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarSourceReaderTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.PulsarSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.subscriber.PulsarSubscriberTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.common.schema.PulsarSchemaUtilsTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarUnorderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.topic.TopicPartition.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.StopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="26035" opendate="2022-2-9 00:00:00" fixdate="2022-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rework loader-bundle into separate module</summary>
      <description>The flink-table-planner currently creates 2 artifacts. 1 jar containing the planner and various dependencies for the cases where the planner is used directly, and another jar that additionally bundles scala for cases where the loader is used.The latter artifact is purely an intermediate build artifact, and as such we usually wouldn't want to publish it. This is particularly important because this jar doesn't have a correct NOTICE, and having different NOTICE files for different artifacts is surprisingly tricky.We should just rework this into a separate module.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.README.md</file>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-loader.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="26036" opendate="2022-2-9 00:00:00" fixdate="2022-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LocalRecoveryITCase.testRecoverLocallyFromProcessCrashWithWorkingDirectory timeout on azure</summary>
      <description>022-02-09T02:18:17.1827314Z Feb 09 02:18:14 [ERROR] org.apache.flink.test.recovery.LocalRecoveryITCase.testRecoverLocallyFromProcessCrashWithWorkingDirectory Time elapsed: 62.252 s &lt;&lt;&lt; ERROR!2022-02-09T02:18:17.1827940Z Feb 09 02:18:14 java.util.concurrent.TimeoutException2022-02-09T02:18:17.1828450Z Feb 09 02:18:14 at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1784)2022-02-09T02:18:17.1829040Z Feb 09 02:18:14 at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)2022-02-09T02:18:17.1829752Z Feb 09 02:18:14 at org.apache.flink.test.recovery.LocalRecoveryITCase.testRecoverLocallyFromProcessCrashWithWorkingDirectory(LocalRecoveryITCase.java:115)2022-02-09T02:18:17.1830407Z Feb 09 02:18:14 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2022-02-09T02:18:17.1830954Z Feb 09 02:18:14 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2022-02-09T02:18:17.1831582Z Feb 09 02:18:14 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2022-02-09T02:18:17.1832135Z Feb 09 02:18:14 at java.lang.reflect.Method.invoke(Method.java:498)2022-02-09T02:18:17.1832697Z Feb 09 02:18:14 at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)2022-02-09T02:18:17.1833566Z Feb 09 02:18:14 at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)2022-02-09T02:18:17.1834394Z Feb 09 02:18:14 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)2022-02-09T02:18:17.1835125Z Feb 09 02:18:14 at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)2022-02-09T02:18:17.1835875Z Feb 09 02:18:14 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)2022-02-09T02:18:17.1836565Z Feb 09 02:18:14 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)2022-02-09T02:18:17.1837294Z Feb 09 02:18:14 at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)2022-02-09T02:18:17.1838007Z Feb 09 02:18:14 at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)2022-02-09T02:18:17.1838743Z Feb 09 02:18:14 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)2022-02-09T02:18:17.1839499Z Feb 09 02:18:14 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)2022-02-09T02:18:17.1840224Z Feb 09 02:18:14 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)2022-02-09T02:18:17.1840952Z Feb 09 02:18:14 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)2022-02-09T02:18:17.1841616Z Feb 09 02:18:14 at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)2022-02-09T02:18:17.1842257Z Feb 09 02:18:14 at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)2022-02-09T02:18:17.1842951Z Feb 09 02:18:14 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)2022-02-09T02:18:17.1843681Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-02-09T02:18:17.1844782Z Feb 09 02:18:14 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)2022-02-09T02:18:17.1845603Z Feb 09 02:18:14 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)2022-02-09T02:18:17.1846375Z Feb 09 02:18:14 at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)2022-02-09T02:18:17.1847084Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)2022-02-09T02:18:17.1847785Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-02-09T02:18:17.1848490Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)2022-02-09T02:18:17.1849138Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2022-02-09T02:18:17.1849797Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)2022-02-09T02:18:17.1850500Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-02-09T02:18:17.1851169Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)2022-02-09T02:18:17.1851834Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)2022-02-09T02:18:17.1852396Z Feb 09 02:18:14 at java.util.ArrayList.forEach(ArrayList.java:1259)2022-02-09T02:18:17.1853086Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)2022-02-09T02:18:17.1853876Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)2022-02-09T02:18:17.1854746Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-02-09T02:18:17.1855633Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)2022-02-09T02:18:17.1856371Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2022-02-09T02:18:17.1857033Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)2022-02-09T02:18:17.1857722Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-02-09T02:18:17.1858400Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)2022-02-09T02:18:17.1859068Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)2022-02-09T02:18:17.1859632Z Feb 09 02:18:14 at java.util.ArrayList.forEach(ArrayList.java:1259)2022-02-09T02:18:17.1860318Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)2022-02-09T02:18:17.1861122Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)2022-02-09T02:18:17.1861818Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-02-09T02:18:17.1862519Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)2022-02-09T02:18:17.1863169Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)2022-02-09T02:18:17.1863920Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)2022-02-09T02:18:17.1864685Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)2022-02-09T02:18:17.1865773Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)2022-02-09T02:18:17.1866640Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)2022-02-09T02:18:17.1867395Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)2022-02-09T02:18:17.1868198Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)2022-02-09T02:18:17.1868928Z Feb 09 02:18:14 at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)2022-02-09T02:18:17.1869645Z Feb 09 02:18:14 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)2022-02-09T02:18:17.1870359Z Feb 09 02:18:14 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)2022-02-09T02:18:17.1871067Z Feb 09 02:18:14 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)2022-02-09T02:18:17.1871823Z Feb 09 02:18:14 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)2022-02-09T02:18:17.1872551Z Feb 09 02:18:14 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)2022-02-09T02:18:17.1873208Z Feb 09 02:18:14 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)2022-02-09T02:18:17.1873826Z Feb 09 02:18:14 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)2022-02-09T02:18:17.1874572Z Feb 09 02:18:14 at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)2022-02-09T02:18:17.1875289Z Feb 09 02:18:14 at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)2022-02-09T02:18:17.1876343Z Feb 09 02:18:14 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)2022-02-09T02:18:17.1877233Z Feb 09 02:18:14 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)2022-02-09T02:18:17.1877928Z Feb 09 02:18:14 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)2022-02-09T02:18:17.1878584Z Feb 09 02:18:14 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)2022-02-09T02:18:17.1879206Z Feb 09 02:18:14 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)2022-02-09T02:18:17.1879793Z Feb 09 02:18:14 at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)2022-02-09T02:18:17.1880381Z Feb 09 02:18:14 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30956&amp;view=logs&amp;j=5c8e7682-d68f-54d1-16a2-a09310218a49&amp;t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&amp;l=23106</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="26044" opendate="2022-2-9 00:00:00" fixdate="2022-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Properly declare internal UNNEST function</summary>
      <description>UNNEST is not declared as an internal function. We should declare it similar to REPLICATE ROWS.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.SqlUnnestUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.LogicalUnnestRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalUnnestRule.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinition.java</file>
    </fixedFiles>
  </bug>
  <bug id="26047" opendate="2022-2-9 00:00:00" fixdate="2022-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support usrlib in HDFS for YARN application mode</summary>
      <description>In YARN Application mode, we currently support using user jar and lib jar from HDFS. For example, we can run commands like:./bin/flink run-application -t yarn-application \ -Dyarn.provided.lib.dirs="hdfs://myhdfs/my-remote-flink-dist-dir" \ hdfs://myhdfs/jars/my-application.jarFor usrlib, we currently only support local directory. I propose to add HDFS support for usrlib to work with CLASSPATH_INCLUDE_USER_JAR better. It can also benefit cases like using notebook to submit flink job.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.UtilsTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnApplicationFileUploader.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.yarn.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="26060" opendate="2022-2-9 00:00:00" fixdate="2022-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Python specific exec nodes unsupported</summary>
      <description>These nodes are using the old type system, which is going to be removed soon. We should avoid supporting them in the persisted plan, as we cannot commit to support them. Once migrated to the new type system, PyFlink won't need these nodes anymore and will just rely on Table new function stack. For more details, also check https://issues.apache.org/jira/browse/FLINK-25231</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testRowTimeBoundedPartitionedRowsOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupAggregateJsonPlanTest.jsonplan.tesPythonAggCallsWithGroupBy.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCorrelateJsonPlanTest.jsonplan.testPythonTableFunction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCorrelateJsonPlanTest.jsonplan.testJoinWithFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.jsonplan.testPythonFunctionInWhereClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.jsonplan.testPythonCalc.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupAggregateJsonPlanTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCorrelateJsonPlanTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.utils.ExecNodeMetadataUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonCorrelate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonCalc.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCorrelate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCalc.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.udtf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udaf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udaf.py</file>
    </fixedFiles>
  </bug>
  <bug id="26062" opendate="2022-2-9 00:00:00" fixdate="2022-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Changelog] Non-deterministic recovery of PriorityQueue states</summary>
      <description>Currently, InternalPriorityQueue.poll() is logged as a separate operation, without specifying the element that has been polled. On recovery, this recorded poll() is replayed.However, this is not deterministic because the order of PQ elements with equal priorityis not specified. For example, TimerHeapInternalTimer only compares timestamps, which are often equal. This results in polling timers from queue in wrong order =&gt; dropping timers =&gt; and not firing timers. ProcessingTimeWindowCheckpointingITCase.testAggregatingSlidingProcessingTimeWindow fails with materialization enabled and using heap state backend (both in-memory and fs-based implementations). Proposed solution is to replace poll with remove operation (which is based on equality). cc: masteryhx, ym, yunta</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.PriorityQueueStateChangeLoggerImplTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogPqStateTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.StateChangeOperation.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.restore.PriorityQueueStateChangeApplier.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.PriorityQueueStateChangeLoggerImpl.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.PriorityQueueStateChangeLogger.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogKeyGroupedPriorityQueue.java</file>
    </fixedFiles>
  </bug>
  <bug id="26063" opendate="2022-2-9 00:00:00" fixdate="2022-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Changelog] Incorrect key group logged for PQ.poll and remove</summary>
      <description>Key group is logged so that state changes can be re-distributed or shuffled.It is currently obtained from keyContext during poll() and remove() operations.However, keyContext is not updated when dequeing processing time timers.The impact is relatively small for remove(): in the worst case, the operation will be ignored.poll() should probably be replaced with remove() anyways - see FLINK-26062.One way to solve this problem is to extract key group from the polled element - if it is a timer.cc: masteryhx, ym, yunta</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.InternalPriorityQueue.java</file>
    </fixedFiles>
  </bug>
  <bug id="26072" opendate="2022-2-10 00:00:00" fixdate="2022-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mark NiFi connector as deprecated</summary>
      <description>The Flink community has voted to deprecate the NiFi connector in Flink 1.15 and remove it in the version after that one. Details can be found in https://lists.apache.org/thread/gldw588pbdf8hww9jtgfdv5y60v5mt6w</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-nifi.src.main.java.org.apache.flink.streaming.connectors.nifi.NiFiSource.java</file>
      <file type="M">flink-connectors.flink-connector-nifi.src.main.java.org.apache.flink.streaming.connectors.nifi.NiFiSink.java</file>
      <file type="M">docs.content.docs.connectors.datastream.nifi.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.nifi.md</file>
    </fixedFiles>
  </bug>
  <bug id="26073" opendate="2022-2-10 00:00:00" fixdate="2022-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Twitter connector</summary>
      <description>The Flink community has voted and agreed to remove the Twitter connector from the Flink repository. Details can be found in https://lists.apache.org/thread/b9mdwqwdyfyq38j6z86rn0d8b26k96c2</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">tools.ci.java-ci-tools.src.main.resources.modules-skipping-deployment.modulelist</file>
      <file type="M">flink-examples.flink-examples-streaming.src.test.scala.org.apache.flink.streaming.scala.examples.StreamingExamplesITCase.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.test.java.org.apache.flink.streaming.test.StreamingExamplesITCase.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.twitter.TwitterExample.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.twitter.util.TwitterExampleData.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.twitter.TwitterExample.java</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-examples.flink-examples-build-helper.pom.xml</file>
      <file type="M">flink-examples.flink-examples-build-helper.flink-examples-streaming-twitter.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-twitter.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-twitter.src.main.java.org.apache.flink.streaming.connectors.twitter.TwitterSource.java</file>
      <file type="M">flink-connectors.flink-connector-twitter.pom.xml</file>
      <file type="M">docs.content.docs.connectors.datastream.twitter.md</file>
      <file type="M">docs.content.docs.connectors.datastream.overview.md</file>
      <file type="M">docs.content.docs.connectors.datastream.guarantees.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.twitter.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.guarantees.md</file>
    </fixedFiles>
  </bug>
  <bug id="26074" opendate="2022-2-10 00:00:00" fixdate="2022-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve FlameGraphs scalability for high parallelism jobs</summary>
      <description>The FlameGraph feature added in FLINK-13550 issues 1 RPC call per subtask. This may cause performance problems for jobs with high paralleism and a lot of subtask running on the same TaskManager. It should be possible to improve this by grouping thread sampling requests using ThreadMXBean.getThreadInfo(long[] ids, int maxDepth)instead of the currently used individual callsThreadMXBean.getThreadInfo(long id, int maxDepth)</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.threadinfo.ThreadInfoRequestCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.ThreadInfoSampleServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.ThreadInfoRequestCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoStats.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexFlameGraphFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.stats.TaskStatsRequestCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.JvmUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.ThreadInfoSampleService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorThreadInfoGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGatewayDecoratorBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.ThreadInfoSample.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.messages.TaskThreadInfoResponse.java</file>
    </fixedFiles>
  </bug>
  <bug id="26076" opendate="2022-2-10 00:00:00" fixdate="2022-2-10 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Fix ArchUnit violations in Source(Sink)MetricsITCase</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.SinkMetricsITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.InMemoryReporter.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.test.java.org.apache.flink.connector.base.source.reader.SourceMetricsITCase.java</file>
      <file type="M">flink-connectors.flink-connector-base.archunit-violations.8bad5118-af5d-4976-ac57-382ed16f7f7e</file>
    </fixedFiles>
  </bug>
  <bug id="26077" opendate="2022-2-10 00:00:00" fixdate="2022-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support operators send request to Coordinator and return a response</summary>
      <description>Currently, the communitcation between Operator and Coordiator is sigle-way. That means, after Operator sending a message to Coordiator, it can't wait to get the response message. In some senarios, the Operator may need to retrieve some information stored in the Coordinator. Thus, it would be great if we can have a two-way communication.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.NoOpTaskOperatorEventGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorOperatorEventHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.TestingJobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.rpc.RpcTaskOperatorEventGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.CoordinationRequestHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterOperatorEventGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.TaskOperatorEventGateway.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="26079" opendate="2022-2-10 00:00:00" fixdate="2022-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Changelog] Disallow recovery from non-changelog checkpoints</summary>
      <description>Extracted from FLINK-25872. The issue is with the CLAIM mode:&gt; Because discarding an initial checkpoint will invalidate its "private" state which might be in use by future checkpoints.&gt; Normally, changelog backend wraps it and registers with tjhe SharedStateRegistry.&gt; But when recovering from non-changelog checkpoint, it is first added to the Checkpoint store, and wrapping in subsequent checkpoints doesn't help.NO_CLAIM mode is not supported.LEGACY could work.But it's difficult to differentiate between the modes on TM, where backend type is reliably known (see the discussion below).CANONICAL non-changelog savepoints must still be supported.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.migration.StatefulJobWBroadcastStateMigrationITCase.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.AbstractOperatorRestoreTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.ChangelogCompatibilityITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.migration.TypeSerializerSnapshotMigrationITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StatefulJobWBroadcastStateMigrationITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.LegacyStatefulJobSavepointMigrationITCase.java</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.migration.StatefulJobSavepointMigrationITCase.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.util.TestUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogStateBackend.java</file>
      <file type="M">docs.content.docs.ops.state.state.backends.md</file>
      <file type="M">docs.content.zh.docs.ops.state.state.backends.md</file>
    </fixedFiles>
  </bug>
  <bug id="26107" opendate="2022-2-14 00:00:00" fixdate="2022-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CoordinatorEventsExactlyOnceITCase.test failed on azure</summary>
      <description>Feb 14 02:23:11 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 4.135 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCaseFeb 14 02:23:11 [ERROR] org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase.test Time elapsed: 0.72 s &lt;&lt;&lt; ERROR!Feb 14 02:23:11 org.apache.flink.runtime.client.JobExecutionException: Job execution failed.Feb 14 02:23:11 at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)Feb 14 02:23:11 at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:933)Feb 14 02:23:11 at org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase.test(CoordinatorEventsExactlyOnceITCase.java:192)Feb 14 02:23:11 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)Feb 14 02:23:11 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)Feb 14 02:23:11 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)Feb 14 02:23:11 at java.lang.reflect.Method.invoke(Method.java:498)Feb 14 02:23:11 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)Feb 14 02:23:11 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)Feb 14 02:23:11 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)Feb 14 02:23:11 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)Feb 14 02:23:11 at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)Feb 14 02:23:11 at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)Feb 14 02:23:11 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)Feb 14 02:23:11 at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)Feb 14 02:23:11 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)Feb 14 02:23:11 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)Feb 14 02:23:11 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)Feb 14 02:23:11 at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)Feb 14 02:23:11 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)Feb 14 02:23:11 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)Feb 14 02:23:11 at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)Feb 14 02:23:11 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)Feb 14 02:23:11 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)Feb 14 02:23:11 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)Feb 14 02:23:11 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)Feb 14 02:23:11 at org.junit.runners.ParentRunner.run(ParentRunner.java:413)Feb 14 02:23:11 at org.junit.runner.JUnitCore.run(JUnitCore.java:137)Feb 14 02:23:11 at org.junit.runner.JUnitCore.run(JUnitCore.java:115)Feb 14 02:23:11 at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)Feb 14 02:23:11 at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)Feb 14 02:23:11 at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)Feb 14 02:23:11 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)Feb 14 02:23:11 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)Feb 14 02:23:11 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54) https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31347&amp;view=logs&amp;j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&amp;t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&amp;l=9512</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.TestingOperatorCoordinatorHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.CreatingExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerNG.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.State.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.CreatingExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.java</file>
    </fixedFiles>
  </bug>
  <bug id="26117" opendate="2022-2-14 00:00:00" fixdate="2022-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JobManagerMetricGroup needs to implement GloballyCleanableResource as well</summary>
      <description>There's a mistake being done during the introduction of the *CleanableResources interfaces. The JobManagerMetricGroup should also implement the global cleanup since it's called in both cleanup types not only the local one.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.cleanup.DispatcherResourceCleanerFactoryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.cleanup.DispatcherResourceCleanerFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.TestingJobManagerRunnerRegistry.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherCleanupITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.OnMainThreadJobManagerRunnerRegistry.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.JobManagerRunnerRegistry.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="26145" opendate="2022-2-15 00:00:00" fixdate="2022-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>k8s docs jobmanager-pod-template artifacts-fetcher:latest image is not exist, we can use busybox to replace it</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug id="26148" opendate="2022-2-15 00:00:00" fixdate="2022-2-15 01:00:00" resolution="Done">
    <buginformation>
      <summary>Change the format of adaptive batch scheduler config option to "jobmanager.adaptive-batch-scheduler.XXX"</summary>
      <description>Change the format of adaptive batch scheduler config option to jobmanager.adaptive-batch-scheduler.XXX to align the format with the existing scheduler option (For example, jobmanager.adaptive-scheduler.min-parallelism-increase).</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.job.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.scheduling.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.jobmanager.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="26151" opendate="2022-2-15 00:00:00" fixdate="2022-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>inprogressfileRecoverable not be clean up after restoring the bucket</summary>
      <description>In order to clear the previous inProgressFileRecoverable when the checkpoint is successful, inProgressFileRecoverable will be added to inProgressFileRecoverablesPerCheckpoint when the checkpoint is started, but when the bucket is recovered from bucketState, inProgressFileRecoverablesPerCheckpoint does not record inProgressFileRecoverable, resulting in inProgressFileRecoverable not be clean up.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.java</file>
    </fixedFiles>
  </bug>
  <bug id="26160" opendate="2022-2-15 00:00:00" fixdate="2022-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pulsar Connector: stopCursor description should be changed. Connector only stop when auto discovery is disabled.</summary>
      <description>In Pulsar source connector, the stopCursor description can mislead user to believe that the source connector will exit if a BoundedStopCursor is set.  However this might not be true if auto partition discovery is enabled (the discovery loop will always run and expects new partitionSplits). We need to modify the description.</description>
      <version>1.14.3,1.15.0</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSourceBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="26164" opendate="2022-2-15 00:00:00" fixdate="2022-3-15 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Document watermark alignment</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.event-time.generating.watermarks.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.event-time.generating.watermarks.md</file>
    </fixedFiles>
  </bug>
  <bug id="26180" opendate="2022-2-16 00:00:00" fixdate="2022-2-16 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Update docs to introduce the compaction for FileSink</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.datastream.filesystem.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.filesystem.md</file>
    </fixedFiles>
  </bug>
  <bug id="26182" opendate="2022-2-16 00:00:00" fixdate="2022-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>E2E test for PulsarSink on new Flink sink testing tools</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0,1.15.3</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.source.SharedSubscriptionContext.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.source.KeySharedSubscriptionContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.resources.docker.bootstrap.sh</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.source.UnorderedSourceTestSuiteBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.source.cases.MultipleTopicConsumingContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntimeOperator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.PulsarMockRuntime.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.sink.PulsarSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-pulsar.archunit-violations.f4d91193-72ba-4ce4-ad83-98f780dce581</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.resources.containers.txnStandalone.conf</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.container.PulsarContainerRuntime.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.source.enumerator.NoOpEnumStateSerializer.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.PulsarSourceUnorderedE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.PulsarSourceOrderedE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.common.UnorderedSourceTestSuiteBase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.common.KeyedPulsarPartitionDataWriter.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.common.FlinkContainerWithPulsarEnvironment.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.cases.SharedSubscriptionContext.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.cases.KeySharedSubscriptionContext.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.cases.FailoverSubscriptionContext.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.cases.ExclusiveSubscriptionContext.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestSuiteBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestEnvironment.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestContextFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestCommonUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarPartitionDataWriter.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.cases.SingleTopicConsumingContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.cases.SharedSubscriptionConsumingContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.cases.MultipleTopicTemplateContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.cases.MultipleTopicConsumingContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.PulsarUnorderedSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.PulsarSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.topic.TopicRange.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntime.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.embedded.PulsarEmbeddedRuntime.java</file>
    </fixedFiles>
  </bug>
  <bug id="26185" opendate="2022-2-16 00:00:00" fixdate="2022-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>E2E Elasticsearch Tests should use the new Sink interface</summary>
      <description>Currently the E2E tests for Elasticsearch (test_streaming_elasticsearch.sh) is testing the old Sink interface implementation. As we are now moving to the new Sink interface, we should update the tests to test the new implementation</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch6-test.src.main.java.org.apache.flink.streaming.tests.Elasticsearch6SinkExample.java</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch7-test.src.main.java.org.apache.flink.streaming.tests.Elasticsearch7SinkExample.java</file>
    </fixedFiles>
  </bug>
  <bug id="26186" opendate="2022-2-16 00:00:00" fixdate="2022-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update the API_ANNOTATIONS rule</summary>
      <description>Issue 1:It seems that the excluding shaded classes filter does not work in this case since the check is against a valid class whose return type is a shaded external class which somehow was not handled by the filter.  For example, please refer to FLINK-26174.Since there is no way to add Flink API annotation to the 3rd party classes, the rule should be updated.the main ideas is to extend the rule allow external shaded classes without Flink API annotation. Issue 2: It should be fine to let caller method annotated with @VisibleForTesting call target method annotated @VisibleForTesting.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.src.test.java.org.apache.flink.architecture.rules.ApiAnnotationRules.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.e5126cae-f3fe-48aa-b6fb-60ae6cc3fcd5</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.7602816f-5c01-4b7a-9e3e-235dfedec245</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.5b9eed8a-5fb6-4373-98ac-3be2a71941b8</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.18509c9e-3250-4c52-91b9-11ccefc85db1</file>
    </fixedFiles>
  </bug>
  <bug id="26191" opendate="2022-2-16 00:00:00" fixdate="2022-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect license in Elasticsearch connectors</summary>
      <description>The sql-connector-elasticsearc0h 6/7 connector NOTICE lists the elasticsearch dependencies as ASLv2, but they are nowadays (at least in part) licensed differently (dual-licensed under elastic license 2.0 &amp; Server Side Public License (SSPL)).</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.quickstarts.sh</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch7.Elasticsearch7ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch7SinkBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.testutils.ElasticsearchResource.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="26192" opendate="2022-2-16 00:00:00" fixdate="2022-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PulsarOrderedSourceReaderTest fails with exit code 255</summary>
      <description>https://dev.azure.com/wysakowiczdawid/Flink/_build/results?buildId=1367&amp;view=logs&amp;j=f3dc9b18-b77a-55c1-591e-264c46fe44d1&amp;t=2d3cd81e-1c37-5c31-0ee4-f5d5cdb9324d&amp;l=26787Feb 16 13:49:46 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test (default-test) on project flink-connector-pulsar: There are test failures.Feb 16 13:49:46 [ERROR] Feb 16 13:49:46 [ERROR] Please refer to /__w/1/s/flink-connectors/flink-connector-pulsar/target/surefire-reports for the individual test results.Feb 16 13:49:46 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.Feb 16 13:49:46 [ERROR] The forked VM terminated without properly saying goodbye. VM crash or System.exit called?Feb 16 13:49:46 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-connectors/flink-connector-pulsar &amp;&amp; /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:-UseGCOverheadLimit -Duser.country=US -Duser.language=en -jar /__w/1/s/flink-connectors/flink-connector-pulsar/target/surefire/surefirebooter3139517882560779643.jar /__w/1/s/flink-connectors/flink-connector-pulsar/target/surefire 2022-02-16T13-48-34_435-jvmRun1 surefire3358354372075396323tmp surefire_08509996975514960300tmpFeb 16 13:49:46 [ERROR] Error occurred in starting fork, check output in logFeb 16 13:49:46 [ERROR] Process Exit Code: 255Feb 16 13:49:46 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?Feb 16 13:49:46 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-connectors/flink-connector-pulsar &amp;&amp; /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:-UseGCOverheadLimit -Duser.country=US -Duser.language=en -jar /__w/1/s/flink-connectors/flink-connector-pulsar/target/surefire/surefirebooter3139517882560779643.jar /__w/1/s/flink-connectors/flink-connector-pulsar/target/surefire 2022-02-16T13-48-34_435-jvmRun1 surefire3358354372075396323tmp surefire_08509996975514960300tmpFeb 16 13:49:46 [ERROR] Error occurred in starting fork, check output in logFeb 16 13:49:46 [ERROR] Process Exit Code: 255Feb 16 13:49:46 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:748)Feb 16 13:49:46 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:305)Feb 16 13:49:46 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:265)Feb 16 13:49:46 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1314)Feb 16 13:49:46 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1159)Feb 16 13:49:46 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:932)Feb 16 13:49:46 [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)Feb 16 13:49:46 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)Feb 16 13:49:46 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)Feb 16 13:49:46 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)Feb 16 13:49:46 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)Feb 16 13:49:46 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)Feb 16 13:49:46 [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)Feb 16 13:49:46 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)Feb 16 13:49:46 [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)Feb 16 13:49:46 [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)Feb 16 13:49:46 [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)Feb 16 13:49:46 [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)Feb 16 13:49:46 [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)Feb 16 13:49:46 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)Feb 16 13:49:46 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)Feb 16 13:49:46 [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)Feb 16 13:49:46 [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)Feb 16 13:49:46 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)Feb 16 13:49:46 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)Feb 16 13:49:46 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)Feb 16 13:49:46 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)Feb 16 13:49:46 [ERROR] -&gt; [Help 1]Feb 16 13:49:46 [ERROR] Feb 16 13:49:46 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.Feb 16 13:49:46 [ERROR] Re-run Maven using the -X switch to enable full debug logging.Feb 16 13:49:46 [ERROR] Feb 16 13:49:46 [ERROR] For more information about the errors and possible solutions, please read the following articles:Feb 16 13:49:46 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionExceptionFeb 16 13:49:46 [ERROR] Feb 16 13:49:46 [ERROR] After correcting the problems, you can resume the build with the commandFeb 16 13:49:46 [ERROR] mvn &lt;goals&gt; -rf :flink-connector-pulsar</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.embedded.PulsarEmbeddedRuntime.java</file>
    </fixedFiles>
  </bug>
  <bug id="26195" opendate="2022-2-16 00:00:00" fixdate="2022-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka connector tests are mixing JUnit4 and JUnit5</summary>
      <description>In the tests for the Kafka connector there are multiple occurrences of mixing JUnit 4 and JUnit 5. This prevents proper logging from e.g. TestLoggerExtension. There are also tests that run on JUnit 4 but use Assertions or Annotations from JUnit 5</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.KafkaSourceBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.FlinkKafkaInternalProducerITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="26210" opendate="2022-2-17 00:00:00" fixdate="2022-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PulsarSourceUnorderedE2ECase failed on azure due to multiple causes</summary>
      <description>Feb 17 04:58:33 [ERROR] Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 85.664 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.tests.util.pulsar.PulsarSourceUnorderedE2ECaseFeb 17 04:58:33 [ERROR] org.apache.flink.tests.util.pulsar.PulsarSourceUnorderedE2ECase.testOneSplitWithMultipleConsumers(TestEnvironment, DataStreamSourceExternalContext)[1] Time elapsed: 0.571 s &lt;&lt;&lt; ERROR!Feb 17 04:58:33 org.apache.pulsar.client.admin.PulsarAdminException$GettingAuthenticationDataException: Feb 17 04:58:33 java.util.concurrent.ExecutionException: org.apache.pulsar.client.admin.PulsarAdminException$GettingAuthenticationDataException: A MultiException has 2 exceptions. They are:Feb 17 04:58:33 1. java.lang.NoClassDefFoundError: javax/xml/bind/annotation/XmlElementFeb 17 04:58:33 2. java.lang.IllegalStateException: Unable to perform operation: create on org.apache.pulsar.shade.org.glassfish.jersey.jackson.internal.DefaultJacksonJaxbJsonProviderFeb 17 04:58:33 Feb 17 04:58:33 at org.apache.pulsar.client.admin.internal.BaseResource.request(BaseResource.java:70)Feb 17 04:58:33 at org.apache.pulsar.client.admin.internal.BaseResource.asyncPutRequest(BaseResource.java:120)Feb 17 04:58:33 at org.apache.pulsar.client.admin.internal.TopicsImpl.createPartitionedTopicAsync(TopicsImpl.java:430)Feb 17 04:58:33 at org.apache.pulsar.client.admin.internal.TopicsImpl.createPartitionedTopicAsync(TopicsImpl.java:421)Feb 17 04:58:33 at org.apache.pulsar.client.admin.internal.TopicsImpl.createPartitionedTopic(TopicsImpl.java:373)Feb 17 04:58:33 at org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntimeOperator.lambda$createPartitionedTopic$11(PulsarRuntimeOperator.java:504)Feb 17 04:58:33 at org.apache.flink.connector.pulsar.common.utils.PulsarExceptionUtils.sneaky(PulsarExceptionUtils.java:60)Feb 17 04:58:33 at org.apache.flink.connector.pulsar.common.utils.PulsarExceptionUtils.sneakyAdmin(PulsarExceptionUtils.java:50)Feb 17 04:58:33 at org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntimeOperator.createPartitionedTopic(PulsarRuntimeOperator.java:504)Feb 17 04:58:33 at org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntimeOperator.createTopic(PulsarRuntimeOperator.java:184)Feb 17 04:58:33 at org.apache.flink.tests.util.pulsar.cases.KeySharedSubscriptionContext.createSourceSplitDataWriter(KeySharedSubscriptionContext.java:111)Feb 17 04:58:33 at org.apache.flink.tests.util.pulsar.common.UnorderedSourceTestSuiteBase.testOneSplitWithMultipleConsumers(UnorderedSourceTestSuiteBase.java:73)Feb 17 04:58:33 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)Feb 17 04:58:33 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)Feb 17 04:58:33 at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)Feb 17 04:58:33 at java.base/java.lang.reflect.Method.invoke(Method.java:566)Feb 17 04:58:33 at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)Feb 17 04:58:33 at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)Feb 17 04:58:33 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)Feb 17 04:58:33 at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)Feb 17 04:58:33 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31702&amp;view=logs&amp;j=6e8542d7-de38-5a33-4aca-458d6c87066d&amp;t=5846934b-7a4f-545b-e5b0-eb4d8bda32e1&amp;l=15537</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.PulsarSourceOrderedE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.src.test.java.org.apache.flink.tests.util.pulsar.common.FlinkContainerWithPulsarEnvironment.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="26218" opendate="2022-2-17 00:00:00" fixdate="2022-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable JUnit 5 Automatic Extension Detection</summary>
      <description>This allows us to declare the TestLoggerExtension for all tests in a module via the service entry.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="26219" opendate="2022-2-17 00:00:00" fixdate="2022-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-metrics-slf4j</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-slf4j.src.test.java.org.apache.flink.metrics.slf4j.Slf4jReporterTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="26221" opendate="2022-2-17 00:00:00" fixdate="2022-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-metrics-prometheus</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.test.java.org.apache.flink.metrics.prometheus.PrometheusReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.test.java.org.apache.flink.metrics.prometheus.PrometheusReporterTaskScopeTest.java</file>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.test.java.org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporterTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="26223" opendate="2022-2-17 00:00:00" fixdate="2022-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Making ZK-related logs available in tests</summary>
      <description>Recently, we had a few incidents where it appears that ZooKeeper wasn't behaving as expected. It might help to have to the ZooKeeper logs available in these cases.We have multiple options: Introduce an extension to change the ZK log level for specific tests Lower the ZK log level again and make the logs being written to the standard log files Lower the ZK log level again and move the ZK logs into a dedicated file to avoid spoiling the Flink logs</description>
      <version>1.13.6,1.14.3,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0,1.13.7</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug id="26225" opendate="2022-2-17 00:00:00" fixdate="2022-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-metrics-jmx</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.runtime.jobmanager.JMXJobManagerMetricTest.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.metrics.jmx.JMXReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.metrics.jmx.JMXReporterFactoryTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="26226" opendate="2022-2-17 00:00:00" fixdate="2022-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-metrics-influxdb</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.test.java.org.apache.flink.metrics.influxdb.MetricMapperTest.java</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.test.java.org.apache.flink.metrics.influxdb.MeasurementInfoProviderTest.java</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.test.java.org.apache.flink.metrics.influxdb.InfluxdbReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.test.java.org.apache.flink.metrics.influxdb.InfluxdbReporterFactoryTest.java</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="26227" opendate="2022-2-17 00:00:00" fixdate="2022-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-metrics-graphite</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-graphite.src.test.java.org.apache.flink.metrics.graphite.GraphiteReporterFactoryTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="26228" opendate="2022-2-17 00:00:00" fixdate="2022-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-metrics-dropwizard</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.test.java.org.apache.flink.dropwizard.ScheduledDropwizardReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.test.java.org.apache.flink.dropwizard.metrics.FlinkMeterWrapperTest.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.test.java.org.apache.flink.dropwizard.metrics.FlinkCounterWrapperTest.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.test.java.org.apache.flink.dropwizard.metrics.DropwizardMeterWrapperTest.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.test.java.org.apache.flink.dropwizard.metrics.DropwizardFlinkHistogramWrapperTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="26229" opendate="2022-2-17 00:00:00" fixdate="2022-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-metrics-datadog</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-datadog.src.test.java.org.apache.flink.metrics.datadog.DCounterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.test.java.org.apache.flink.metrics.datadog.DatadogHttpReporterFactoryTest.java</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.test.java.org.apache.flink.metrics.datadog.DatadogHttpClientTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="26230" opendate="2022-2-17 00:00:00" fixdate="2022-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-metrics-core</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.DescriptiveStatisticsHistogramTest.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.test.java.org.apache.flink.metrics.MeterViewTest.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.test.java.org.apache.flink.metrics.AbstractHistogramTest.java</file>
      <file type="M">flink-metrics.flink-metrics-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="26232" opendate="2022-2-17 00:00:00" fixdate="2022-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-avro</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.src.test.resources.META-INF.services.org.junit.jupiter.api.extension.Extension</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.table.runtime.batch.AvroTypesITCase.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.typeutils.AvroTypeInfoTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.typeutils.AvroTypeExtractionTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.typeutils.AvroSerializerSnapshotTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.typeutils.AvroSerializerEmptyArrayTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.typeutils.AvroSerializerConcurrencyTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.typeutils.AvroSerializerConcurrencyCheckInactiveITCase.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.typeutils.AvroSchemaConverterTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.RegistryAvroDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.EncoderDecoderTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroStreamingFileSinkITCase.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroSplittableInputFormatTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroRowDeSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroRowDataDeSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroRecordInputFormatTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroOutputFormatTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroOutputFormatITCase.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroKryoSerializerRegistrationsTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroKryoClassloadingTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroInputFormatTypeExtractionTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroExternalJarProgramITCase.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroBulkFormatTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroBulkFormatITCase.java</file>
      <file type="M">flink-formats.flink-avro.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="26244" opendate="2022-2-18 00:00:00" fixdate="2022-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-runtime-web</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.WebMonitorUtilsTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.WebFrontendITCase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.utils.WebFrontendBootstrapTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.testutils.HttpTestClient.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.LeaderRetrievalHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.history.HistoryServerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.history.FsJobArchivistTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtilsTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.ParallelismQueryParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarUploadResponseBodyTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarUploadHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarSubmissionITCase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunResponseBodyTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBodyTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarIdPathParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarDeleteHeadersTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.AllowNonRestoredStateQueryParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.rest.compatibility.RestAPIStabilityTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.rest.compatibility.CompatibilityRoutines.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.rest.compatibility.CompatibilityRoutine.java</file>
    </fixedFiles>
  </bug>
  <bug id="26279" opendate="2022-2-21 00:00:00" fixdate="2022-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide mailbox delay metric</summary>
      <description>Related to FLINK-25414. In order to better debug Task being blocked in busy state, we can periodically enqueue a mailbox action and check how long it took for the task thread to execute such action. We can report this delay as task thread delay/mailbox delay. Would be helpful for debugging why unaligned checkpoints are taking so long to trigger/execute.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxProcessorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailbox.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricNames.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskIOMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.DescriptiveStatisticsHistogram.java</file>
      <file type="M">docs.content.docs.ops.metrics.md</file>
      <file type="M">docs.content.zh.docs.ops.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="2628" opendate="2015-9-7 00:00:00" fixdate="2015-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failing Test: StreamFaultToleranceTestBase.runCheckpointedProgram</summary>
      <description>In pullrequest #1097 the test StreamFaultToleranceTestBase.runCheckpointedProgramThe changes introduced in the pull request are most likely unrelated. I can not reproduce it locally.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.CoStreamCheckpointingITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="26280" opendate="2022-2-21 00:00:00" fixdate="2022-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a flag to disable uid generation</summary>
      <description>We should add a flag to disable uid generation for back-compat. See the discussion here: https://issues.apache.org/jira/browse/FLINK-25932</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.TransformationsTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecIntervalJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.ExecutionConfigOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="26281" opendate="2022-2-21 00:00:00" fixdate="2022-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test Elasticsearch connector End2End</summary>
      <description>Feature introduced in https://issues.apache.org/jira/browse/FLINK-24323Documentation for datastream apiDocumentation for table apiAs 1.15 deprecated the SinkFunction-based Elasticsearch connector and introduces the new connector based on the Sink interface we should test it behaves correctly and as the user expects. Some suggestions what to test: Test delivery guarantees (none, at-least-once) (exactly-once should not run) Write a simple job that is inserting/upserting data into Elasticsearch Write a simple job that is inserting/upserting data into Elasticsearch and use a non-default parallelism Write a simple job in both datastream api and table api Test restarting jobs and scaling up/down Test failure of a simple job that is inserting data with exactly-once delivery guarantee by terminating and restarting Elasticsearch Test against Elasticsearch 6.X and 7.X with the respective connectors</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.connectors.datastream.elasticsearch.md</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch7SinkBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.connector.elasticsearch.sink.Elasticsearch6SinkBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBuilderBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchConnectorOptions.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSinkBuilderBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchSink.java</file>
      <file type="M">docs.content.docs.connectors.datastream.elasticsearch.md</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.ElasticsearchConnectorOptions.java</file>
      <file type="M">docs.content.docs.connectors.table.elasticsearch.md</file>
    </fixedFiles>
  </bug>
  <bug id="26283" opendate="2022-2-21 00:00:00" fixdate="2022-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden AggregateCall serialization in JSON plan</summary>
      <description>Similar to FLINK-25385, we also need to revisit AggregateCall serialization. It should not contain Java serialization. It should support all kinds of agg functions. It should not support legacy stacks.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowJoinJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testDistinctSplitEnabled.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testRowTimeBoundedPartitionedRowsOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctWithNonDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testInnerJoinWithPk.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregateWithSumCountDistinctAndRetraction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregate.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ExpandJsonPlanTest.jsonplan.testExpand.out</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.AggregateCallJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.AggregateCallJsonDeserializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="26289" opendate="2022-2-21 00:00:00" fixdate="2022-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adaptive Scheduler: Exception history can not be queried by the REST API</summary>
      <description>In FLINK-21439, we've started collecting a history of exceptions in the Adaptive Scheduler. We have a good coverage that this part works properly, but we've missed the part that exposes the history via REST API.The problematic part is that execution graph attached with the `ExecutionGraphInfo` does no longer contain a failure info.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.AdaptiveSchedulerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobExceptionsInfoWithHistory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="26295" opendate="2022-2-22 00:00:00" fixdate="2022-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>&amp;#39;HELP ;&amp;#39;, &amp;#39;QUIT ;&amp;#39; and other sql-client commands fail with CalciteException: Non-query expression encountered in illegal context</summary>
      <description>It seems the reason is https://github.com/apache/flink/pull/18363/fileswhere added condition like super(Pattern.compile("HELP;?", DEFAULT_PATTERN_FLAGS))that means HELP; will work however if there is any space between HELP and ; then notand it fails like below (before that it worked without failing)Having a space between command and a semicolon is pretty common since autocompletion inserts it after tab[ERROR] Could not execute SQL statement. Reason:org.apache.calcite.runtime.CalciteException: Non-query expression encountered in illegal context at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490) at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467) at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:560) at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:883) at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:868) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.checkNonQueryExpression(FlinkSqlParserImpl.java:395) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.Expression3(FlinkSqlParserImpl.java:21147) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.Expression2b(FlinkSqlParserImpl.java:20816) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.Expression2(FlinkSqlParserImpl.java:20857) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.Expression(FlinkSqlParserImpl.java:20788) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.LeafQueryOrExpr(FlinkSqlParserImpl.java:20765) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.QueryOrExpr(FlinkSqlParserImpl.java:20213) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.OrderedQueryOrExpr(FlinkSqlParserImpl.java:588) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlStmt(FlinkSqlParserImpl.java:3986) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlStmtList(FlinkSqlParserImpl.java:2915) at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.parseSqlStmtList(FlinkSqlParserImpl.java:287) at org.apache.calcite.sql.parser.SqlParser.parseStmtList(SqlParser.java:193) at org.apache.flink.table.planner.parse.CalciteParser.parseSqlList(CalciteParser.java:77) at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:101) at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$parseStatement$1(LocalExecutor.java:172) at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:88) at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:172) at org.apache.flink.table.client.cli.SqlCommandParserImpl.parseCommand(SqlCommandParserImpl.java:45) at org.apache.flink.table.client.cli.SqlMultiLineParser.parse(SqlMultiLineParser.java:71) at org.jline.reader.impl.LineReaderImpl.acceptLine(LineReaderImpl.java:2731) at org.jline.reader.impl.LineReaderImpl.readLine(LineReaderImpl.java:585) at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:296) at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:281) at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:229) at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:151) at org.apache.flink.table.client.SqlClient.start(SqlClient.java:95) at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187) at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161)</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.parse.SetOperationParseStrategy.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.parse.ResetOperationParseStrategy.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.parse.QuitOperationParseStrategy.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.parse.HelpOperationParseStrategy.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.parse.ClearOperationParseStrategy.java</file>
    </fixedFiles>
  </bug>
  <bug id="26296" opendate="2022-2-22 00:00:00" fixdate="2022-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add missing documentation</summary>
      <description>It appears that the documentation update under Deployment / HA / Overview didn't make it to master. We should mention the JobResultStore and the retryable cleanup here.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.overview.md</file>
      <file type="M">docs.content.docs.deployment.ha.overview.md</file>
      <file type="M">docs.content.docs.concepts.glossary.md</file>
      <file type="M">docs.content.zh.docs.deployment.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.ha.overview.md</file>
      <file type="M">docs.content.zh.docs.concepts.glossary.md</file>
    </fixedFiles>
  </bug>
  <bug id="26298" opendate="2022-2-22 00:00:00" fixdate="2022-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-rpc-core</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-core.src.test.java.org.apache.flink.runtime.concurrent.ScheduledFutureAdapterTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="26302" opendate="2022-2-22 00:00:00" fixdate="2022-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade rat-plugin to 1.13</summary>
      <description>Reduces verbosity of the plugin</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="26303" opendate="2022-2-22 00:00:00" fixdate="2022-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Print rat-plugin violations to the console</summary>
      <description>Violations found by the rat plugin are currently only written to the rat.txt file. If we'd also print them to console then one wouldn't have to re-run things locally to figure out what actually failed.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="26306" opendate="2022-2-22 00:00:00" fixdate="2022-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Changelog] Thundering herd problem with materialization</summary>
      <description>Quick note: CheckpointCleaner is not involved here.When a checkpoint is subsumed, SharedStateRegistry schedules its unused shared state for async deletion. It uses common IO pool for this and adds a Runnable per state handle. ( see SharedStateRegistryImpl.scheduleAsyncDelete)When a checkpoint is started, CheckpointCoordinator uses the same thread pool to initialize the location for it. (see CheckpointCoordinator.initializeCheckpoint)The thread pool is of fixed size jobmanager.io-pool.size; by default it's the number of CPU cores) and uses FIFO queue for tasks.When there is a spike in state deletion, the next checkpoint is delayed waiting for an available IO thread.Back-pressure seems reasonable here (similar to CheckpointCleaner); however, this shared state deletion could be spread across multiple subsequent checkpoints, not neccesarily the next one.---- I believe the issue is an pre-existing one; but it particularly affects changelog state backend, because 1) such spikes are likely there; 2) workloads are latency sensitive.In the tests, checkpoint duration grows from seconds to minutes immediately after the materialization.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogStateBackendTestUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.PeriodicMaterializationManager.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.main.java.org.apache.flink.state.changelog.ChangelogStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="26347" opendate="2022-2-24 00:00:00" fixdate="2022-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should use Flink system Classloader (AppClassloader) when deserializing RPC message</summary>
      <description>FLINK-25742 removed the redundant serialization of RPC invocation at Flink side. However, by accident, it changes the class loading behavior. Before FLINK-25742, Flink system Classloader is used to load RPC message class, but after FLINK-25742, the RpcSystem Classloader (its parent Classloader is not Flink system Classloader) is used which can cause ClassNotFoundException. I encountered this exception when trying to run flink-remote-shuffle on the latest Flink 1.15-SNAPSHOT, the remote shuffle class (shuffle descriptor class) can not be found even when the corresponding jar file is in Flink lib/ directory.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation.java</file>
    </fixedFiles>
  </bug>
  <bug id="26353" opendate="2022-2-24 00:00:00" fixdate="2022-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"flink stop --help" does not list "--type" option</summary>
      <description>./bin/flink stop --help Action "stop" stops a running program with a savepoint (streaming jobs only).  Syntax: stop [OPTIONS] &lt;Job ID&gt;  "stop" action options:     -d,--drain                           Send MAX_WATERMARK before taking the                                          savepoint and stopping the pipelne.     -p,--savepointPath &lt;savepointPath&gt;   Path to the savepoint (for example                                          hdfs:///flink/savepoint-1537). If no                                          directory is specified, the configured                                          default will be used                                          ("state.savepoints.dir").  Options for Generic CLI mode:     -D &lt;property=value&gt;   Allows specifying multiple generic configuration                           options. The available options can be found at                           https://nightlies.apache.org/flink/flink-docs-stable/                           ops/config.html     -e,--executor &lt;arg&gt;   DEPRECATED: Please use the -t option instead which is                           also available with the "Application Mode".                           The name of the executor to be used for executing the                           given job, which is equivalent to the                           "execution.target" config option. The currently                           available executors are: "remote", "local",                           "kubernetes-session", "yarn-per-job" (deprecated),                           "yarn-session".     -t,--target &lt;arg&gt;     The deployment target for the given application,                           which is equivalent to the "execution.target" config                           option. For the "run" action the currently available                           targets are: "remote", "local", "kubernetes-session",                           "yarn-per-job" (deprecated), "yarn-session". For the                           "run-application" action the currently available                           targets are: "kubernetes-application".  Options for yarn-cluster mode:     -m,--jobmanager &lt;arg&gt;            Set to yarn-cluster to use YARN execution                                      mode.     -yid,--yarnapplicationId &lt;arg&gt;   Attach to running YARN session     -z,--zookeeperNamespace &lt;arg&gt;    Namespace to create the Zookeeper                                      sub-paths for high availability mode  Options for default mode:     -D &lt;property=value&gt;             Allows specifying multiple generic                                     configuration options. The available                                     options can be found at                                     https://nightlies.apache.org/flink/flink-do                                     cs-stable/ops/config.html     -m,--jobmanager &lt;arg&gt;           Address of the JobManager to which to                                     connect. Use this flag to connect to a                                     different JobManager than the one specified                                     in the configuration. Attention: This                                     option is respected only if the                                     high-availability configuration is NONE.     -z,--zookeeperNamespace &lt;arg&gt;   Namespace to create the Zookeeper sub-paths                                     for high availability mode</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendStopWithSavepointTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendSavepointTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="26354" opendate="2022-2-24 00:00:00" fixdate="2022-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"-restoreMode" should be "--restoreMode" and should have a shorthand</summary>
      <description>-restoreMode &lt;arg&gt; Defines how should we restore from the given savepoint. Supported options: [claim - claim ownership of the savepoint and delete once it is subsumed, no_claim (default) - do not claim ownership, the first checkpoint will not reuse any files from the restored one, legacy - the old behaviour, do not assume ownership of the savepoint files, but can reuse some shared files.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendRunTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="26374" opendate="2022-2-25 00:00:00" fixdate="2022-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JSON_OBJECT may throw NullPointerException on nullable column</summary>
      <description>From ML:  Using the latest SNAPSHOT BUILD. If I have a column definition as  .column(                "events",                DataTypes.ARRAY(                    DataTypes.ROW(                        DataTypes.FIELD("status", DataTypes.STRING().notNull()),                        DataTypes.FIELD("timestamp", DataTypes.STRING().notNull()),                        DataTypes.FIELD("increment_identifier", DataTypes.STRING().nullable())))) And a query as JSON_OBJECT('events' VALUE events) event_json Will generate JSON correctly ONLY if increment_identifier is NOT NULL but will throw a NullPointerException on the first record that has that column as null. Exception is not helpful. Exception in thread "main" org.apache.flink.runtime.client.JobExecutionException: Job execution failed.at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642)at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:259)at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)at akka.dispatch.OnComplete.internal(Future.scala:300)at akka.dispatch.OnComplete.internal(Future.scala:297)at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategyat org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739)at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)at java.base/java.lang.reflect.Method.invoke(Method.java:566)at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)at akka.actor.Actor.aroundReceive(Actor.scala:537)at akka.actor.Actor.aroundReceive$(Actor.scala:535)at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)at akka.actor.ActorCell.invoke(ActorCell.scala:548)at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)at akka.dispatch.Mailbox.run(Mailbox.scala:231)at akka.dispatch.Mailbox.exec(Mailbox.scala:243)... 5 moreCaused by: java.lang.NullPointerExceptionat StreamExecCalc$422.convertRow$317$(Unknown Source)at StreamExecCalc$422.convertArray$316$(Unknown Source)at StreamExecCalc$422.processElement_split71(Unknown Source)at StreamExecCalc$422.processElement(Unknown Source)at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51)at org.apache.flink.streaming.api.operators.async.queue.StreamRecordQueueEntry.emitResult(StreamRecordQueueEntry.java:64)at org.apache.flink.streaming.api.operators.async.queue.OrderedStreamElementQueue.emitCompletedElement(OrderedStreamElementQueue.java:71)at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.outputCompletedElement(AsyncWaitOperator.java:302)at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.access$100(AsyncWaitOperator.java:79)at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.processResults(AsyncWaitOperator.java:381)at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.lambda$processInMailbox$0(AsyncWaitOperator.java:362)at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:338)at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324)at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)at java.base/java.lang.Thread.run(Thread.java:829)</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.JsonFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.JsonGenerateUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug id="26388" opendate="2022-2-28 00:00:00" fixdate="2022-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Release Testing: Repeatable Cleanup (FLINK-25433)</summary>
      <description>Repeatable cleanup got introduced with FLIP-194 but should be considered as an independent feature of the JobResultStore (JRS) from a user's point of view.Repeatable cleanup can be triggered by running into an error while cleaning up. This can be achieved by disabling access to S3 after the job finished, e.g.: Setting a reasonable enough checkpointing time (checkpointing should be enabled to allow cleanup of s3) Disable s3 (removing permissions or shutting down the s3 server) Stop job with savepointStopping the job should work but the logs should show failure with repeating retries. Enabling S3 again should fix the issue.Keep in mind that if testing this in with HA, you should use a different bucket for the file-based JRS artifacts only change permissions for the bucket that holds JRS-unrelated artifacts. Flink would fail fatally if the JRS is not able to access it's backend storage.Documentation and configuration is still in the process of being updated in FLINK-26296 and FLINK-26331</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="26395" opendate="2022-2-28 00:00:00" fixdate="2022-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The description of RAND_INTEGER is wrong in SQL function documents</summary>
      <description>RAND_INTEGER will returns a integer value, but document of SQL function shows it will return a double value.</description>
      <version>1.13.5,1.14.3,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.Expressions.java</file>
      <file type="M">flink-python.pyflink.table.expressions.py</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="26396" opendate="2022-2-28 00:00:00" fixdate="2022-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Changelog] Upload is not failed even if all attempts timeout</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.resources.log4j2.properties</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.RetryingExecutorTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.BatchingStateChangeUploaderTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.RetryingExecutor.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.BatchingStateChangeUploader.java</file>
    </fixedFiles>
  </bug>
  <bug id="26418" opendate="2022-3-1 00:00:00" fixdate="2022-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tests on flink-table-planner produce tmp_XXX dirs which are not cleaned up</summary>
      <description>Running tests in flink-table-planner produces a bunch of tmp_XXXXXX directories in the flink-table-planner root module dir, (not inside the build dirs). As a result, if you don't change the global gitignore show as new dirs/files to commit, and they are not cleaned up when one runs mvn clean. On top, if you try to build the whole flink project you get: [ERROR] Failed to execute goal org.apache.rat:apache-rat-plugin:0.12:check (default) on project flink-parent: Too many files with unapproved license: 6 See RAT report in: /home/matriv/ververica/flink/target/rat.txt -&gt; [Help 1]and you need to manually remove those dirs.It would be great to keep these directories under the build and maybe also automatically remove each one, once the test producing it finishes successfully. </description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.TestingTaskManagerRuntimeInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="2642" opendate="2015-9-9 00:00:00" fixdate="2015-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala Table API crashes when executing word count example</summary>
      <description>I tried to run the examples provided in the documentation of Flink's Table API. Unfortunately, the Scala word count example provided in the documentation doesn't work and does not give a meaningful exception.(Other examples work fine)Here my code:package org.apache.flink.examples.scalaimport org.apache.flink.api.scala._import org.apache.flink.api.scala.table._object WordCount { def main(args: Array[String]): Unit = { // set up execution environment val env = ExecutionEnvironment.getExecutionEnvironment case class WC(word: String, count: Int) val input = env.fromElements(WC("hello", 1), WC("hello", 1), WC("ciao", 1)) val expr = input.toTable val result = expr.groupBy('word).select('word, 'count.sum as 'count).toDataSet[WC] result.print() }}Here the thrown exception:Exception in thread "main" org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1.applyOrElse(JobManager.scala:414) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.LeaderSessionMessageFilter$$anonfun$receive$1.applyOrElse(LeaderSessionMessageFilter.scala:36) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:33) at org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:28) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118) at org.apache.flink.runtime.LogMessages$$anon$1.applyOrElse(LogMessages.scala:28) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:104) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.Exception: The user defined 'open(Configuration)' method in class org.apache.flink.api.table.runtime.ExpressionSelectFunction caused an exception: null at org.apache.flink.runtime.operators.RegularPactTask.openUserCode(RegularPactTask.java:1368) at org.apache.flink.runtime.operators.chaining.ChainedMapDriver.openTask(ChainedMapDriver.java:47) at org.apache.flink.runtime.operators.RegularPactTask.openChainedTasks(RegularPactTask.java:1408) at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:142) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:581) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.NullPointerException at org.apache.flink.api.table.codegen.IndentStringContext$$anonfun$j$2.apply(Indenter.scala:30) at org.apache.flink.api.table.codegen.IndentStringContext$$anonfun$j$2.apply(Indenter.scala:23) at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771) at org.apache.flink.api.table.codegen.IndentStringContext.j(Indenter.scala:23) at org.apache.flink.api.table.codegen.GenerateSelect.generateInternal(GenerateSelect.scala:55) at org.apache.flink.api.table.codegen.GenerateSelect.generateInternal(GenerateSelect.scala:32) at org.apache.flink.api.table.codegen.ExpressionCodeGenerator.generate(ExpressionCodeGenerator.scala:66) at org.apache.flink.api.table.runtime.ExpressionSelectFunction.open(ExpressionSelectFunction.scala:46) at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:33) at org.apache.flink.runtime.operators.RegularPactTask.openUserCode(RegularPactTask.java:1366) ... 5 more</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.plan.PlanTranslator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="26422" opendate="2022-3-1 00:00:00" fixdate="2022-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Chinese documentation with the new TablePipeline docs</summary>
      <description>Chinese docs needs to be updated with the content of this commit: https://github.com/apache/flink/commit/4f65c7950f2c3ef849f2094deab0e199ffedf57b</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.common.md</file>
      <file type="M">docs.content.zh.docs.dev.table.tableApi.md</file>
      <file type="M">docs.content.zh.docs.dev.table.data.stream.api.md</file>
      <file type="M">docs.content.zh.docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  <bug id="26434" opendate="2022-3-1 00:00:00" fixdate="2022-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove &amp;#39;table.planner&amp;#39; table config option</summary>
      <description>We kept the old API around for another release but it should be safe now to remove all references left to the planner switch.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.PlannerType.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.EnvironmentSettings.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.TableConfigOptions.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.context.ExecutionContext.java</file>
      <file type="M">flink-python.pyflink.table.environment.settings.py</file>
    </fixedFiles>
  </bug>
  <bug id="26460" opendate="2022-3-3 00:00:00" fixdate="2022-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Unsupported type when convertTypeToSpec: MAP</summary>
      <description>CREATE TABLE zm_test ( `a` BIGINT, `m` MAP&lt;STRING,BIGINT&gt;);if we insert into zm_test useINSERT INTO zm_test(`a`) SELECT `a` FROM MyTable;then will throw ExceptionUnsupported type when convertTypeToSpec: MAPwe must useINSERT INTO zm_test SELECT `a`, cast(null AS MAP&lt;STRING,BIGINT&gt;) FROM MyTable;</description>
      <version>1.13.1,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.PreValidateReWriter.scala</file>
    </fixedFiles>
  </bug>
  <bug id="26469" opendate="2022-3-3 00:00:00" fixdate="2022-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adaptive job shows error in WebUI when not enough resource are available</summary>
      <description>When there is no resource and job is in CREATED state, the job page shows the error: "Job failed during initialization of JobManager".</description>
      <version>1.15.0</version>
      <fixedVersion>1.16.0,1.17.0,1.15.3</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="26484" opendate="2022-3-4 00:00:00" fixdate="2022-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FileSystem.delete is not implemented consistently</summary>
      <description>The BlobServer cleanup does not work for the Presto S3 filesystem in case of failure due to some bug in the recursive delete implementation. The false return value is not processed which leads to an error case being "swallowed", i.e. recursive cleanups do not work in this case (see PrestoS3FileSystem:496)</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.java.org.apache.flink.fs.s3presto.S3FileSystemFactory.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.main.java.org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.fs.FileSystemBehaviorTestSuite.java</file>
    </fixedFiles>
  </bug>
  <bug id="26485" opendate="2022-3-4 00:00:00" fixdate="2022-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Changelog] State not discarded after multiple retries</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.RetryingExecutorTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.BatchingStateChangeUploadSchedulerTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.StateChangeUploader.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.RetryingExecutor.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.main.java.org.apache.flink.changelog.fs.BatchingStateChangeUploadScheduler.java</file>
    </fixedFiles>
  </bug>
  <bug id="26494" opendate="2022-3-4 00:00:00" fixdate="2022-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing logs during retry</summary>
      <description>The FutureRetry.retry functionality doesn't log the errors but just trigger a retry. This makes it harder for the user to figure out what's wrong.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.cleanup.DefaultResourceCleanerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.cleanup.DispatcherResourceCleanerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.cleanup.DefaultResourceCleaner.java</file>
    </fixedFiles>
  </bug>
  <bug id="26496" opendate="2022-3-5 00:00:00" fixdate="2022-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-yarn-test</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBaseTest.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOSecuredITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnPrioritySchedulingITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNHighAvailabilityITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNFileReplicationITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnConfigurationITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNApplicationITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.UtilsTest.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.CliFrontendRunWithYarnTest.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.SecureTestEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="26501" opendate="2022-3-6 00:00:00" fixdate="2022-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Quickstarts Scala nightly end-to-end test failed on azure due to checkponts failed and logs contains exceptions</summary>
      <description>2022-03-05T02:35:36.4040037Z Mar 05 02:35:36 2022-03-05 02:35:34,334 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Triggering checkpoint 1 (type=CHECKPOINT) @ 1646447734295 for job b236087395260dc34648b84c2b86d6e8.2022-03-05T02:35:36.4041701Z Mar 05 02:35:36 2022-03-05 02:35:34,387 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Decline checkpoint 1 by task e8a324cae6bf452d32db6797bbbafad0 of job b236087395260dc34648b84c2b86d6e8 at 127.0.0.1:45911-0a50f5 @ localhost (dataPort=44047).2022-03-05T02:35:36.4043279Z Mar 05 02:35:36 org.apache.flink.util.SerializedThrowable: Task name with subtask : Source: Sequence Source (Deprecated) -&gt; Map -&gt; Sink: Unnamed (1/1)#0 Failure reason: Checkpoint was declined (task is closing)2022-03-05T02:35:36.4044531Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1389) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4045729Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1382) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4047172Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.triggerCheckpointBarrier(Task.java:1348) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4049092Z Mar 05 02:35:36 at org.apache.flink.runtime.taskexecutor.TaskExecutor.triggerCheckpoint(TaskExecutor.java:956) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4050158Z Mar 05 02:35:36 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_322]2022-03-05T02:35:36.4050929Z Mar 05 02:35:36 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_322]2022-03-05T02:35:36.4051776Z Mar 05 02:35:36 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_322]2022-03-05T02:35:36.4052559Z Mar 05 02:35:36 at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_322]2022-03-05T02:35:36.4053373Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[?:?]2022-03-05T02:35:36.4054849Z Mar 05 02:35:36 at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]2022-03-05T02:35:36.4055685Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[?:?]2022-03-05T02:35:36.4056461Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[?:?]2022-03-05T02:35:36.4057219Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[?:?]2022-03-05T02:35:36.4057899Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]2022-03-05T02:35:36.4059666Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]2022-03-05T02:35:36.4061005Z Mar 05 02:35:36 at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4062324Z Mar 05 02:35:36 at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4063941Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]2022-03-05T02:35:36.4065009Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4066205Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4067514Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4068255Z Mar 05 02:35:36 at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]2022-03-05T02:35:36.4069019Z Mar 05 02:35:36 at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]2022-03-05T02:35:36.4069638Z Mar 05 02:35:36 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]2022-03-05T02:35:36.4070271Z Mar 05 02:35:36 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) ~[?:?]2022-03-05T02:35:36.4070862Z Mar 05 02:35:36 at akka.actor.ActorCell.invoke(ActorCell.scala:548) ~[?:?]2022-03-05T02:35:36.4071453Z Mar 05 02:35:36 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]2022-03-05T02:35:36.4072430Z Mar 05 02:35:36 at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]2022-03-05T02:35:36.4073023Z Mar 05 02:35:36 at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]2022-03-05T02:35:36.4073687Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_322]2022-03-05T02:35:36.4074596Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_322]2022-03-05T02:35:36.4075712Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_322]2022-03-05T02:35:36.4076437Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) ~[?:1.8.0_322]2022-03-05T02:35:36.4077754Z Mar 05 02:35:36 2022-03-05 02:35:34,410 WARN org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger checkpoint 1 for job b236087395260dc34648b84c2b86d6e8. (0 consecutive failed attempts so far)2022-03-05T02:35:36.4078865Z Mar 05 02:35:36 org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint was declined (task is closing)2022-03-05T02:35:36.4080161Z Mar 05 02:35:36 at org.apache.flink.runtime.messages.checkpoint.SerializedCheckpointException.unwrap(SerializedCheckpointException.java:51) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4081619Z Mar 05 02:35:36 at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage(CheckpointCoordinator.java:988) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4083063Z Mar 05 02:35:36 at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$declineCheckpoint$2(ExecutionGraphHandler.java:103) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4085407Z Mar 05 02:35:36 at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4086635Z Mar 05 02:35:36 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_322]2022-03-05T02:35:36.4087419Z Mar 05 02:35:36 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_322]2022-03-05T02:35:36.4088438Z Mar 05 02:35:36 at java.lang.Thread.run(Thread.java:750) [?:1.8.0_322]2022-03-05T02:35:36.4089614Z Mar 05 02:35:36 Caused by: org.apache.flink.util.SerializedThrowable: Task name with subtask : Source: Sequence Source (Deprecated) -&gt; Map -&gt; Sink: Unnamed (1/1)#0 Failure reason: Checkpoint was declined (task is closing)2022-03-05T02:35:36.4090937Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1389) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4092177Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1382) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4093430Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.triggerCheckpointBarrier(Task.java:1348) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4094740Z Mar 05 02:35:36 at org.apache.flink.runtime.taskexecutor.TaskExecutor.triggerCheckpoint(TaskExecutor.java:956) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4095836Z Mar 05 02:35:36 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_322]2022-03-05T02:35:36.4096579Z Mar 05 02:35:36 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_322]2022-03-05T02:35:36.4097766Z Mar 05 02:35:36 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_322]2022-03-05T02:35:36.4098684Z Mar 05 02:35:36 at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_322]2022-03-05T02:35:36.4101381Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[?:?]2022-03-05T02:35:36.4102353Z Mar 05 02:35:36 at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]2022-03-05T02:35:36.4103218Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[?:?]2022-03-05T02:35:36.4104019Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[?:?]2022-03-05T02:35:36.4104801Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[?:?]2022-03-05T02:35:36.4105719Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]2022-03-05T02:35:36.4108356Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]2022-03-05T02:35:36.4110333Z Mar 05 02:35:36 at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4112523Z Mar 05 02:35:36 at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4113601Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]2022-03-05T02:35:36.4114790Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4116110Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4117636Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4118641Z Mar 05 02:35:36 at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]2022-03-05T02:35:36.4119307Z Mar 05 02:35:36 at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]2022-03-05T02:35:36.4120161Z Mar 05 02:35:36 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]2022-03-05T02:35:36.4120842Z Mar 05 02:35:36 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) ~[?:?]2022-03-05T02:35:36.4121482Z Mar 05 02:35:36 at akka.actor.ActorCell.invoke(ActorCell.scala:548) ~[?:?]2022-03-05T02:35:36.4122113Z Mar 05 02:35:36 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]2022-03-05T02:35:36.4122736Z Mar 05 02:35:36 at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]2022-03-05T02:35:36.4123332Z Mar 05 02:35:36 at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]2022-03-05T02:35:36.4123984Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_322]2022-03-05T02:35:36.4124749Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_322]2022-03-05T02:35:36.4125750Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_322]2022-03-05T02:35:36.4126591Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) ~[?:1.8.0_322]2022-03-05T02:35:36.4128133Z Mar 05 02:35:36 2022-03-05 02:35:34,430 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Sequence Source (Deprecated) -&gt; Map -&gt; Sink: Unnamed (1/1) (e8a324cae6bf452d32db6797bbbafad0) switched from RUNNING to FINISHED. https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32553&amp;view=logs&amp;j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&amp;t=cc5499f8-bdde-5157-0d76-b6528ecd808e&amp;l=18735</description>
      <version>1.14.3,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="26506" opendate="2022-3-7 00:00:00" fixdate="2022-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support StreamExecutionEnvironment.registerCachedFile in Python DataStream API</summary>
      <description>This API is missed in Python DataStream API.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  <bug id="26516" opendate="2022-3-7 00:00:00" fixdate="2022-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sink V2 is not state compatible with Sink V1</summary>
      <description>While working https://issues.apache.org/jira/browse/FLINK-26173 we decided to split off the state compatibility issue to lower the priority of the behavioral issue since it does not affect many users. This ticket is solely responsible to fix the state incompatibility between Sink V1 and Sink V2.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.sink.committables.CommittableCollectorSerializerTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.SinkTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.committables.CommittableCollectorSerializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.sink.committables.CommittableCollector.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SinkV1Adapter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.connector.sink2.GlobalCommitterOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="26517" opendate="2022-3-7 00:00:00" fixdate="2022-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Normalize the decided parallelism to power of 2 when using adaptive batch scheduler</summary>
      <description>As describe in FLINK-26330, in order to make the number of subpartitoins evenly consumed by downstream tasks, we need to normalize the decided parallelism to 2^N.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.AdaptiveBatchSchedulerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismDeciderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismDecider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchSchedulerFactory.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.job.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.scheduling.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.jobmanager.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="26518" opendate="2022-3-7 00:00:00" fixdate="2022-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support the new type inference in Scala Table API table functions</summary>
      <description>Currently, we cannot distinguish between old and new type inference for Scala Table API because those functions are not registered in a catalog but are used "inline". We should support them as well.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.CorrelateSortToRankRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalWindowTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.calcite.WindowTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.calcite.WindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.calcite.LogicalWindowTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.calcite.LogicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.expressions.windowProperties.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.expressions.fieldExpression.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalPythonGroupWindowAggregateRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalPythonWindowAggregateRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.SubQueryDecorrelator.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.delegation.PlannerContext.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.catalog.QueryOperationCatalogViewTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.UserDefinedTableFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamingTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.table.validation.CorrelateValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.table.CorrelateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.table.validation.CorrelateValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.table.CorrelateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TemporalTableFunctionJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CorrelateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.ColumnFunctionsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RewriteMinusAllRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RewriteIntersectAllRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.common.PartialInsertTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.CorrelateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedTableFunctions.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.table.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.SetOpRewriteUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.bridging.BridgingSqlFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.calcite.FlinkRelBuilder.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql.validate.ProcedureNamespace.java</file>
      <file type="M">flink-table.flink-table-api-scala.src.main.scala.org.apache.flink.table.api.ImplicitExpressionConversions.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.utils.CalculatedTableFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="26520" opendate="2022-3-7 00:00:00" fixdate="2022-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement SEARCH operator</summary>
      <description>The codegen right now is not implementing the SEARCH operator, but it's using the rex builder to circumvent it. We should implement the SEARCH operator directly, to remove the usage of the flink type factory</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.ConvertToNotInOrInRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.ConvertToNotInOrInRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.LookupJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.LikeCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.CastRuleProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="26534" opendate="2022-3-8 00:00:00" fixdate="2022-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>shuffle by sink&amp;#39;s primary key should cover the case that input changelog stream has a different parallelism</summary>
      <description>FLINK-20370 fix the wrong result when sink primary key is not the same with query and introduced a new auto-keyby sink's primary key strategy for append stream if the sink's parallelism differs from input stream's.But still exists one case to be solved:for a changelog stream, its changelog upsert key same as sink's primary key, but sink's parallelism changed by user (via those sinks which implement the `ParallelismProvider` interface, e.g., KafkaDynamicSink), we should fix it.And a minor change: keyby canbe omitted when sink has single parallism (because none partitioner will cause worse disorder)</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="26543" opendate="2022-3-9 00:00:00" fixdate="2022-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the issue that exceptions generated in startup are missed in Python loopback mode</summary>
      <description></description>
      <version>1.14.3,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.boot.py</file>
    </fixedFiles>
  </bug>
  <bug id="26547" opendate="2022-3-9 00:00:00" fixdate="2022-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Accepting slots without a matching requirement leads to unfulfillable requirements</summary>
      <description>To allow recovered TMs to eagerly re-offer their slots we allowed the registration of slots without a matching requirement if the job is currently restarting.All slots that the pool accepts are mapped to a certain requirement, in order to determine whether sufficient slots were received yet. If a slot is reserved for a requirement that does not coincide with the mapping the pool come up with, then the mapping and requirements are changed accordingly to ensure we still request sufficient slots.This leads to issues with slots that were accepted without a matching requirement. Those were mapped to the actual resource profile of the slot (to fit into the book-keeping). With the above logic in place this could lead to a specific resource requirement being added, which the remaining JM components are not aware of (and thus will never get rid of).</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPoolTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.ResourceProfile.java</file>
    </fixedFiles>
  </bug>
  <bug id="26549" opendate="2022-3-9 00:00:00" fixdate="2022-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>INSERT INTO with VALUES leads to wrong type inference with nested types</summary>
      <description>While working on casting, I've found out we have an interesting bug in the insert values type inference. This comes from the KafkaTableITCase#testKafkaSourceSinkWithMetadata (look at this version in particular https://github.com/apache/flink/blob/567440115bcacb5aceaf3304e486281c7da8c14f/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaTableITCase.java).The test scenario is an INSERT INTO VALUES query which is also pushing some metadata to a Kafka table, in particular is writing the headers metadata.The table is declared like that: CREATE TABLE kafka ( `physical_1` STRING, `physical_2` INT, `timestamp-type` STRING METADATA VIRTUAL, `timestamp` TIMESTAMP(3) METADATA, `leader-epoch` INT METADATA VIRTUAL, `headers` MAP&lt;STRING, BYTES&gt; METADATA, `partition` INT METADATA VIRTUAL, `topic` STRING METADATA VIRTUAL, `physical_3` BOOLEAN) WITH ( 'connector' = 'kafka', [...])The insert into query looks like:INSERT INTO kafka VALUES('data 1', 1, TIMESTAMP '2020-03-08 13:12:11.123', MAP['k1', x'C0FFEE', 'k2', x'BABE'], TRUE),('data 2', 2, TIMESTAMP '2020-03-09 13:12:11.123', CAST(NULL AS MAP&lt;STRING, BYTES&gt;), FALSE),('data 3', 3, TIMESTAMP '2020-03-10 13:12:11.123', MAP['k1', X'10', 'k2', X'20'], TRUE)Note that in the first row, the byte literal is of length 3, while in the last row the byte literal is of length 1.The generated plan of this INSERT INTO is:== Abstract Syntax Tree ==LogicalSink(table=[default_catalog.default_database.kafka], fields=[physical_1, physical_2, physical_3, headers, timestamp])+- LogicalProject(physical_1=[$0], physical_2=[$1], physical_3=[$4], headers=[CAST($3):(VARCHAR(2147483647) CHARACTER SET "UTF-16LE", VARBINARY(2147483647)) MAP], timestamp=[CAST($2):TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)]) +- LogicalUnion(all=[true]) :- LogicalProject(EXPR$0=[_UTF-16LE'data 1'], EXPR$1=[1], EXPR$2=[2020-03-08 13:12:11.123:TIMESTAMP(3)], EXPR$3=[MAP(_UTF-16LE'k1', X'c0ffee':VARBINARY(3), _UTF-16LE'k2', X'babe':VARBINARY(3))], EXPR$4=[true]) : +- LogicalValues(tuples=[[{ 0 }]]) :- LogicalProject(EXPR$0=[_UTF-16LE'data 2'], EXPR$1=[2], EXPR$2=[2020-03-09 13:12:11.123:TIMESTAMP(3)], EXPR$3=[null:(VARCHAR(2147483647) CHARACTER SET "UTF-16LE", VARBINARY(2147483647)) MAP], EXPR$4=[false]) : +- LogicalValues(tuples=[[{ 0 }]]) +- LogicalProject(EXPR$0=[_UTF-16LE'data 3'], EXPR$1=[3], EXPR$2=[2020-03-10 13:12:11.123:TIMESTAMP(3)], EXPR$3=[MAP(_UTF-16LE'k1', X'10':BINARY(1), _UTF-16LE'k2', X'20':BINARY(1))], EXPR$4=[true]) +- LogicalValues(tuples=[[{ 0 }]])== Optimized Physical Plan ==Sink(table=[default_catalog.default_database.kafka], fields=[physical_1, physical_2, physical_3, headers, timestamp])+- Union(all=[true], union=[physical_1, physical_2, physical_3, headers, timestamp]) :- Calc(select=[_UTF-16LE'data 1' AS physical_1, 1 AS physical_2, true AS physical_3, CAST(CAST(MAP(_UTF-16LE'k1', X'c0ffee':VARBINARY(3), _UTF-16LE'k2', X'babe':VARBINARY(3)) AS (CHAR(2) CHARACTER SET "UTF-16LE" NOT NULL, BINARY(1) NOT NULL) MAP) AS (VARCHAR(2147483647) CHARACTER SET "UTF-16LE", VARBINARY(2147483647)) MAP) AS headers, CAST(2020-03-08 12:12:11.123:TIMESTAMP_WITH_LOCAL_TIME_ZONE(3) AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS timestamp]) : +- Values(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]]) :- Calc(select=[_UTF-16LE'data 2' AS physical_1, 2 AS physical_2, false AS physical_3, null:(VARCHAR(2147483647) CHARACTER SET "UTF-16LE", VARBINARY(2147483647)) MAP AS headers, CAST(2020-03-09 12:12:11.123:TIMESTAMP_WITH_LOCAL_TIME_ZONE(3) AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS timestamp]) : +- Values(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]]) +- Calc(select=[_UTF-16LE'data 3' AS physical_1, 3 AS physical_2, true AS physical_3, CAST(MAP(_UTF-16LE'k1', X'10':BINARY(1), _UTF-16LE'k2', X'20':BINARY(1)) AS (VARCHAR(2147483647) CHARACTER SET "UTF-16LE", VARBINARY(2147483647)) MAP) AS headers, CAST(2020-03-10 12:12:11.123:TIMESTAMP_WITH_LOCAL_TIME_ZONE(3) AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS timestamp]) +- Values(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]])== Optimized Execution Plan ==Sink(table=[default_catalog.default_database.kafka], fields=[physical_1, physical_2, physical_3, headers, timestamp])+- Union(all=[true], union=[physical_1, physical_2, physical_3, headers, timestamp]) :- Calc(select=['data 1' AS physical_1, 1 AS physical_2, true AS physical_3, CAST(CAST(MAP('k1', X'c0ffee', 'k2', X'babe') AS (CHAR(2), BINARY(1)) MAP) AS (VARCHAR(2147483647), VARBINARY(2147483647)) MAP) AS headers, CAST(2020-03-08 12:12:11.123 AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS timestamp]) : +- Values(tuples=[[{ 0 }]])(reuse_id=[1]) :- Calc(select=['data 2' AS physical_1, 2 AS physical_2, false AS physical_3, null:(VARCHAR(2147483647), VARBINARY(2147483647)) MAP AS headers, CAST(2020-03-09 12:12:11.123 AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS timestamp]) : +- Reused(reference_id=[1]) +- Calc(select=['data 3' AS physical_1, 3 AS physical_2, true AS physical_3, CAST(MAP('k1', X'10', 'k2', X'20') AS (VARCHAR(2147483647), VARBINARY(2147483647)) MAP) AS headers, CAST(2020-03-10 12:12:11.123 AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS timestamp]) +- Reused(reference_id=[1])As you see, in the Abstract Syntax Tree section a casting for the headers is injected (although unnecessary, as it should be an identity cast), but then in Optimized Physical Plan another casting is injected:CAST(CAST(MAP(_UTF-16LE'k1', X'c0ffee':VARBINARY(3), _UTF-16LE'k2', X'babe':VARBINARY(3)) AS (CHAR(2) CHARACTER SET "UTF-16LE" NOT NULL, BINARY(1) NOT NULL) MAP) AS (VARCHAR(2147483647) CHARACTER SET "UTF-16LE", VARBINARY(2147483647)) MAP) AS headersWhich makes no sense, as it's casting the values of the map first to BINARY(1) and then to BYTES, causing to trim the last 2 bytes. Removing the last row to insert makes the VALUES type inference work properly:== Abstract Syntax Tree ==LogicalSink(table=[default_catalog.default_database.kafka], fields=[physical_1, physical_2, physical_3, headers, timestamp])+- LogicalProject(physical_1=[$0], physical_2=[$1], physical_3=[$4], headers=[$3], timestamp=[CAST($2):TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)]) +- LogicalUnion(all=[true]) :- LogicalProject(EXPR$0=[_UTF-16LE'data 1'], EXPR$1=[1], EXPR$2=[2020-03-08 13:12:11.123:TIMESTAMP(3)], EXPR$3=[MAP(_UTF-16LE'k1', X'c0ffee':VARBINARY(3), _UTF-16LE'k2', X'babe':VARBINARY(3))], EXPR$4=[true]) : +- LogicalValues(tuples=[[{ 0 }]]) +- LogicalProject(EXPR$0=[_UTF-16LE'data 2'], EXPR$1=[2], EXPR$2=[2020-03-09 13:12:11.123:TIMESTAMP(3)], EXPR$3=[null:(VARCHAR(2147483647) CHARACTER SET "UTF-16LE", VARBINARY(2147483647)) MAP], EXPR$4=[false]) +- LogicalValues(tuples=[[{ 0 }]])== Optimized Physical Plan ==Sink(table=[default_catalog.default_database.kafka], fields=[physical_1, physical_2, physical_3, headers, timestamp])+- Union(all=[true], union=[physical_1, physical_2, physical_3, headers, timestamp]) :- Calc(select=[_UTF-16LE'data 1' AS physical_1, 1 AS physical_2, true AS physical_3, CAST(MAP(_UTF-16LE'k1', X'c0ffee':VARBINARY(3), _UTF-16LE'k2', X'babe':VARBINARY(3)) AS (VARCHAR(2147483647) CHARACTER SET "UTF-16LE", VARBINARY(2147483647)) MAP) AS headers, CAST(2020-03-08 12:12:11.123:TIMESTAMP_WITH_LOCAL_TIME_ZONE(3) AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS timestamp]) : +- Values(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]]) +- Calc(select=[_UTF-16LE'data 2' AS physical_1, 2 AS physical_2, false AS physical_3, null:(VARCHAR(2147483647) CHARACTER SET "UTF-16LE", VARBINARY(2147483647)) MAP AS headers, CAST(2020-03-09 12:12:11.123:TIMESTAMP_WITH_LOCAL_TIME_ZONE(3) AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS timestamp]) +- Values(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]])== Optimized Execution Plan ==Sink(table=[default_catalog.default_database.kafka], fields=[physical_1, physical_2, physical_3, headers, timestamp])+- Union(all=[true], union=[physical_1, physical_2, physical_3, headers, timestamp]) :- Calc(select=['data 1' AS physical_1, 1 AS physical_2, true AS physical_3, CAST(MAP('k1', X'c0ffee', 'k2', X'babe') AS (VARCHAR(2147483647), VARBINARY(2147483647)) MAP) AS headers, CAST(2020-03-08 12:12:11.123 AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS timestamp]) : +- Values(tuples=[[{ 0 }]])(reuse_id=[1]) +- Calc(select=['data 2' AS physical_1, 2 AS physical_2, false AS physical_3, null:(VARCHAR(2147483647), VARBINARY(2147483647)) MAP AS headers, CAST(2020-03-09 12:12:11.123 AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS timestamp]) +- Reused(reference_id=[1])</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.calcite.FlinkTypeFactoryTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="26580" opendate="2022-3-10 00:00:00" fixdate="2022-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FileSink Compactor is not properly processing in-progress files.</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.compactor.CompactorOperatorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.sink.compactor.operator.CompactorOperatorStateHandler.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.StreamingCompactingFileSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.FileSinkITBase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.FileSinkCompactionSwitchITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.compactor.CompactCoordinatorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.BatchCompactingFileSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.sink.compactor.operator.CompactCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="26582" opendate="2022-3-10 00:00:00" fixdate="2022-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run the assertj conversion script to convert assertions in flink-table</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.TableFormatFactoryBaseTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.util.TimeWindowUtilTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.util.RowDataHarnessAssertor.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.util.ResettableExternalBufferTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.util.collections.binary.BytesMultiMapTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.util.collections.binary.BytesHashMapTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.typeutils.SerializerTestUtil.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.typeutils.RowDataSerializerTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.types.LogicalTypeAssignableTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.types.DataTypePrecisionFixerTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperatorTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.wmassigners.RowTimeMiniBatchAssginerOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.wmassigners.ProcTimeMiniBatchAssignerOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.window.WindowTestUtils.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.window.WindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.window.WindowOperatorContractTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.window.triggers.TriggersTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.window.slicing.WindowedSliceAssignerTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.window.slicing.TumblingSliceAssignerTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.window.slicing.SliceAssignerTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.window.slicing.HoppingSliceAssignerTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.window.slicing.CumulativeSliceAssignerTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.window.MergingWindowSetTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.window.grouping.HeapWindowsGroupingTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.window.assigners.TumblingWindowAssignerTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.window.assigners.SlidingWindowAssignerTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.window.assigners.SessionWindowAssignerTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.window.assigners.CumulativeWindowAssignerTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.source.InputConversionOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.sort.SortUtilTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.sort.BufferedKVExternalSorterTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.sort.BinaryMergeIteratorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.sort.BinaryExternalSorterTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.sink.SinkUpsertMaterializerTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.rank.window.WindowRankOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.over.RowTimeRowsUnboundedPrecedingFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.over.RowTimeRowsBoundedPrecedingFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.over.RowTimeRangeUnboundedPrecedingFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.over.RowTimeRangeBoundedPrecedingFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.over.ProcTimeRangeBoundedPrecedingFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.over.NonBufferOverWindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.over.BufferDataOverWindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.multipleinput.TableOperatorWrapperTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.multipleinput.TableOperatorWrapperGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.multipleinput.output.OutputTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.multipleinput.input.InputTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.multipleinput.input.InputSelectionHandlerTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.multipleinput.BatchMultipleInputStreamOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.window.WindowJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.temporal.TemporalRowTimeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.SortMergeJoinIteratorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.RandomSortMergeOuterJoinTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.RandomSortMergeInnerJoinTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.interval.RowTimeIntervalJoinTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.interval.ProcTimeIntervalJoinTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.Int2SortMergeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.Int2HashJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.AsyncLookupJoinHarnessTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.window.RowTimeWindowDeduplicateOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.RowTimeMiniBatchLatestChangeDeduplicateFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.ProcTimeMiniBatchDeduplicateKeepLastRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.ProcTimeMiniBatchDeduplicateKeepFirstRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.bundle.trigger.CountCoBundleTriggerTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.bundle.trigger.CountBundleTriggerTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.bundle.MapBundleOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.aggregate.window.SlicingWindowAggOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.aggregate.HashAggTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.io.CompressedHeaderlessChannelTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.hashtable.LongHashTableTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.hashtable.BinaryHashTableTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.generated.CompileUtilsTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.formats.raw.RawFormatSerDeSchemaTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.formats.raw.RawFormatFactoryTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.data.util.DataFormatTestUtil.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.data.TimestampDataTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.data.RowDataTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.data.NestedRowDataTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.data.DecimalDataTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.data.DataStructureConvertersTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.data.DataFormatConvertersTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.data.binary.BinarySegmentUtilsTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.data.BinaryStringDataTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.data.BinaryRowDataTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.data.BinaryArrayDataTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.utils.StreamExchangeModeUtilsTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.utils.JsonPlanTestBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.utils.InternalConfigOptionsTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.utils.CatalogTableStatisticsConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.typeutils.RowTypeUtilsTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.table.ValuesITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.table.PrintConnectorITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.table.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.table.DataGeneratorConnectorITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.DataStreamJavaITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.CompactionITCaseBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.module.ModuleITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.TemporalSortJsonITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.CompactManagedTableITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.ParallelismSettingTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.SortSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexWindowBoundSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RankTypeSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RankRangeSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RankProcessStrategySerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.PartitionSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LookupKeySerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalWindowSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.processor.utils.TopologyGraphTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.processor.utils.InputPriorityGraphGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.processor.utils.InputPriorityConflictResolverTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.processor.utils.InputOrderCalculatorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.processor.MultipleInputNodeCreationProcessorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSinkITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.FlinkCalciteCatalogReaderTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.MergeTableLikeUtilTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.ConstructedAccessFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionMiscITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.BuiltInAggregateFunctionTestBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.AggFunctionTestBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.expressions.converter.ExpressionConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.delegation.ParserImplTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.delegation.DefaultExecutorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.codegen.SortCodeGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.codegen.LongHashJoinGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.codegen.EqualiserCodeGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.codegen.CodeSplitTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.codegen.calls.BuiltInMethodsTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.catalog.DatabaseCalciteSchemaTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.catalog.CatalogStatisticsTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.catalog.CatalogITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.catalog.CatalogConstraintTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.api.internal.StatementSetImplTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.api.EnvironmentTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.connector.file.table.FileSystemTableFactoryTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.TypeStringUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.TypeMappingUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.TableSchemaUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.print.TableauStyleTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.PartitionPathUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.EncodingUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LegacyTypeInfoDataTypeConverterTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.TypeTransformationsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.sources.TableSourceTestBase.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.plan.stats.TableStatsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.TableSinkFactoryServiceTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliTableauResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliUtilsTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.BaseMaterializedResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.ChangelogCollectResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectBatchResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.SqlClientTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.CreateTableLikeTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkDDLDataTypeTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.TableApiIdentifierParsingTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.connector.datagen.table.types.DecimalDataRandomGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImplTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.BlackHoleSinkFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.CsvTableSinkFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.DataGenTableSourceFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.factories.PrintSinkFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.api.EnvironmentSettingsTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.api.TableConfigTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.api.TableDescriptorTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.CatalogBaseTableResolutionTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.CatalogTableImpTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.GenericInMemoryCatalogFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.GenericInMemoryCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.SchemaResolutionTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.expressions.ObjectToExpressionTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.expressions.resolver.ExpressionResolverTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.operations.QueryOperationTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.typeutils.FieldInfoUtilsTest.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.AddBooleanBeforeReturnRewriterTest.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.CodeRewriterTestBase.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.JavaCodeSplitterTest.java</file>
      <file type="M">flink-table.flink-table-code-splitter.src.test.java.org.apache.flink.table.codesplit.JavaParserTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.api.constraints.UniqueConstraintTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.api.TableSchemaTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.catalog.CatalogTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.catalog.CatalogTestUtil.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.connector.ProjectionTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.connector.sink.TestManagedSinkCommitter.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.connector.sink.TestManagedSinkWriter.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.data.columnar.vector.ColumnVectorTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.data.columnar.vector.VectorizedColumnBatchTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.descriptors.DescriptorPropertiesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.descriptors.DescriptorTestBase.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.expressions.ExpressionTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.module.CoreModuleFactoryTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="26583" opendate="2022-3-10 00:00:00" fixdate="2022-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The user is not informed in any way when a job is resubmitted but already globally-terminated</summary>
      <description>-We experience some unwanted behavior if a clean JobResult is listed in the JobResultStore and a job with the same Job ID is submitted in Application Mode.We would expect that the second submission fails with a DuplicateJobSubmissionException. Instead, the submission succeeds with the job not running anymore.-Update:We reiterated over the problem and decided that the Exception is not the desired failure because it would cause a failover in k8s setups, for instance. We rather want to inform the user through a warning because Flink still behaves as expected.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="26588" opendate="2022-3-10 00:00:00" fixdate="2022-5-10 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Translate the new SQL CAST documentation to Chinese</summary>
      <description>Since FLINK-26125 is now merged, this content change should also be translated to Chinese. Relevant PR is https://github.com/apache/flink/pull/18813</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.dev.table.types.md</file>
    </fixedFiles>
  </bug>
  <bug id="26604" opendate="2022-3-11 00:00:00" fixdate="2022-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update AvroParquet format user-facing document</summary>
      <description>add minimal mvn dependency setup describe the namespace use case in the Avro schema reduce the redundant information w.r.t. the bunded/unbunded data</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.datastream.formats.parquet.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.formats.parquet.md</file>
    </fixedFiles>
  </bug>
  <bug id="26613" opendate="2022-3-11 00:00:00" fixdate="2022-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stateful unified Sink V2 upgrades only work when operator uids are given</summary>
      <description>As part of documentation &amp;#91;1&amp;#93;we guarantee that in case a stateful migration fails and no uids are used that it is possible to bind the recovered state via the uidHash. This ticket should add a uidHash setter for the operators that are affected by the migration (writer, committer, global committer). &amp;#91;1&amp;#93; https://nightlies.apache.org/flink/flink-docs-master/docs/ops/upgrading/#preconditions</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.SinkTransformationTranslatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.SinkTransformationTranslator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SinkTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="26621" opendate="2022-3-14 00:00:00" fixdate="2022-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ChangelogPeriodicMaterializationITCase crashes JVM on CI in RocksDB cleanup</summary>
      <description>2022-03-11T16:20:12.6929558Z Mar 11 16:20:12 [WARNING] The requested profile "skip-webui-build" could not be activated because it does not exist.2022-03-11T16:20:12.6939269Z Mar 11 16:20:12 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test (integration-tests) on project flink-tests: There are test failures.2022-03-11T16:20:12.6940062Z Mar 11 16:20:12 [ERROR] 2022-03-11T16:20:12.6940954Z Mar 11 16:20:12 [ERROR] Please refer to /__w/2/s/flink-tests/target/surefire-reports for the individual test results.2022-03-11T16:20:12.6941875Z Mar 11 16:20:12 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.2022-03-11T16:20:12.6942966Z Mar 11 16:20:12 [ERROR] ExecutionException Error occurred in starting fork, check output in log2022-03-11T16:20:12.6943919Z Mar 11 16:20:12 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException Error occurred in starting fork, check output in log2022-03-11T16:20:12.6945023Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:532)2022-03-11T16:20:12.6945878Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:479)2022-03-11T16:20:12.6946761Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:322)2022-03-11T16:20:12.6947532Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:266)2022-03-11T16:20:12.6953051Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1314)2022-03-11T16:20:12.6954035Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1159)2022-03-11T16:20:12.6954917Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:932)2022-03-11T16:20:12.6955749Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)2022-03-11T16:20:12.6956542Z Mar 11 16:20:12 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)2022-03-11T16:20:12.6957456Z Mar 11 16:20:12 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)2022-03-11T16:20:12.6958232Z Mar 11 16:20:12 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)2022-03-11T16:20:12.6959038Z Mar 11 16:20:12 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)2022-03-11T16:20:12.6960553Z Mar 11 16:20:12 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)2022-03-11T16:20:12.6962116Z Mar 11 16:20:12 [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)2022-03-11T16:20:12.6963009Z Mar 11 16:20:12 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)2022-03-11T16:20:12.6963737Z Mar 11 16:20:12 [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)2022-03-11T16:20:12.6964644Z Mar 11 16:20:12 [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)2022-03-11T16:20:12.6965647Z Mar 11 16:20:12 [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)2022-03-11T16:20:12.6966732Z Mar 11 16:20:12 [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)2022-03-11T16:20:12.6967818Z Mar 11 16:20:12 [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)2022-03-11T16:20:12.6968857Z Mar 11 16:20:12 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2022-03-11T16:20:12.6969986Z Mar 11 16:20:12 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2022-03-11T16:20:12.6971491Z Mar 11 16:20:12 [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2022-03-11T16:20:12.6972207Z Mar 11 16:20:12 [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)2022-03-11T16:20:12.6973134Z Mar 11 16:20:12 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)2022-03-11T16:20:12.6974067Z Mar 11 16:20:12 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)2022-03-11T16:20:12.6974828Z Mar 11 16:20:12 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)2022-03-11T16:20:12.6975753Z Mar 11 16:20:12 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)2022-03-11T16:20:12.6976620Z Mar 11 16:20:12 [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: Error occurred in starting fork, check output in log2022-03-11T16:20:12.6977893Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:662)2022-03-11T16:20:12.6978949Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$700(ForkStarter.java:121)2022-03-11T16:20:12.6980306Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:465)2022-03-11T16:20:12.6981716Z Mar 11 16:20:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:442)2022-03-11T16:20:12.6982449Z Mar 11 16:20:12 [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)2022-03-11T16:20:12.6983156Z Mar 11 16:20:12 [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2022-03-11T16:20:12.6983904Z Mar 11 16:20:12 [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2022-03-11T16:20:12.6984533Z Mar 11 16:20:12 [ERROR] at java.lang.Thread.run(Thread.java:748)2022-03-11T16:20:12.6985373Z Mar 11 16:20:12 [ERROR] -&gt; [Help 1]2022-03-11T16:20:12.6985741Z Mar 11 16:20:12 [ERROR] 2022-03-11T16:20:12.6986424Z Mar 11 16:20:12 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.2022-03-11T16:20:12.6987456Z Mar 11 16:20:12 [ERROR] Re-run Maven using the -X switch to enable full debug logging.2022-03-11T16:20:12.6987935Z Mar 11 16:20:12 [ERROR] 2022-03-11T16:20:12.6988446Z Mar 11 16:20:12 [ERROR] For more information about the errors and possible solutions, please read the following articles:2022-03-11T16:20:12.6989125Z Mar 11 16:20:12 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException2022-03-11T16:20:12.6989631Z Mar 11 16:20:12 [ERROR] 2022-03-11T16:20:12.6990284Z Mar 11 16:20:12 [ERROR] After correcting the problems, you can resume the build with the command2022-03-11T16:20:12.6991194Z Mar 11 16:20:12 [ERROR] mvn &lt;goals&gt; -rf :flink-tests2022-03-11T16:20:13.5874703Z Mar 11 16:20:13 Process exited with EXIT CODE: 1.https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32927&amp;view=logs&amp;j=5c8e7682-d68f-54d1-16a2-a09310218a49&amp;t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&amp;l=6255</description>
      <version>1.15.0</version>
      <fixedVersion>1.16.0,1.15.2</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="26633" opendate="2022-3-14 00:00:00" fixdate="2022-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elasticsearch connector does not report recordsSend metric</summary>
      <description>As part of a unified sink, it is recommended to report a recordSend metric to let users track the number of outgoing records from Flink.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriterITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="26634" opendate="2022-3-14 00:00:00" fixdate="2022-3-14 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Update Chinese version of Elasticsearch connector docs</summary>
      <description>In https://github.com/apache/flink/pull/19035 we made some smaller changes to the documentation for the Elasticsearch connector with regards to the delivery guarantee. These changes still not to be ported over to the chinese docs.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0,1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.connectors.datastream.elasticsearch.md</file>
    </fixedFiles>
  </bug>
  <bug id="26642" opendate="2022-3-15 00:00:00" fixdate="2022-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pulsar sink fails with non-partitioned topic</summary>
      <description>Flink support pulsar sink now in FLINK-20732. I encountered a problem when using pulsar sink in master branch, when I use non-partitioned topic.The current test found that both partitioned topics and non-partitioned topics ending with -partition-i can be supported, but ordinary non-partitioned topics without -partition-i will have problems, such as 'test_topic'. Reproducing the problem requires writing to a non-partitioned topic. Below is the stack information when the exception is encountered. I briefly communicated with Jianyun Zhao , this may be a bug.  2022-03-08 21:39:13,622 - INFO - [flink-akka.actor.default-dispatcher-13:Execution@1419] - Source: Pulsar Source -&gt; (Sink: Writer -&gt; Sink: Committer, Sink: Print to Std. Out) (1/6) (44af5e8a2b9d553952c7ed3e5d40e672) switched from RUNNING to FAILED on 54284e57-42a9-4e2e-9c41-54b0ad559832 @ 127.0.0.1 (dataPort=-1).java.lang.IllegalArgumentException: You should provide topics for routing topic by message key hash.at org.apache.flink.shaded.guava30.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144)at org.apache.flink.connector.pulsar.sink.writer.router.RoundRobinTopicRouter.route(RoundRobinTopicRouter.java:54)at org.apache.flink.connector.pulsar.sink.writer.PulsarWriter.write(PulsarWriter.java:138)at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.processElement(SinkWriterOperator.java:124)at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)at org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.collect(BroadcastingOutputCollector.java:77)at org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.collect(BroadcastingOutputCollector.java:32)at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask$AsyncDataOutputToOutput.emitRecord(SourceOperatorStreamTask.java:205)at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:110)at org.apache.flink.connector.pulsar.source.reader.emitter.PulsarRecordEmitter.emitRecord(PulsarRecordEmitter.java:41)at org.apache.flink.connector.pulsar.source.reader.emitter.PulsarRecordEmitter.emitRecord(PulsarRecordEmitter.java:33)at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:143)at org.apache.flink.connector.pulsar.source.reader.source.PulsarOrderedSourceReader.pollNext(PulsarOrderedSourceReader.java:106)at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:382)at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)at java.lang.Thread.run(Thread.java:748)   </description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.sink.writer.topic.TopicMetadataListenerTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.topic.TopicNameUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.topic.TopicMetadataListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="26645" opendate="2022-3-15 00:00:00" fixdate="2022-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pulsar Source subscribe to a single topic partition will consume all partitions from that topic</summary>
      <description>Say users subscribe to 4 partitions of a topic with 16 partitions, current Pulsar sourcewill actually consume from all 16 partitions. Expect to consume from 4 partitions only.</description>
      <version>1.14.4,1.15.0</version>
      <fixedVersion>1.14.5,1.15.1,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntimeOperator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.subscriber.PulsarSubscriberTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.sink.writer.topic.TopicMetadataListenerTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.topic.TopicNameUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.subscriber.impl.TopicListSubscriber.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.subscriber.impl.BasePulsarSubscriber.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.topic.TopicMetadataListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="26652" opendate="2022-3-15 00:00:00" fixdate="2022-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failing to cleanup a job should not fail the Flink Cluster in Session Mode</summary>
      <description>We introduced the option to disable the retryable cleanup in FLINK-26331. This should make Flink fall back to the 1.14- functionality with just printing a warning in session mode.Instead, a RetryException is thrown which causes Flink to fail fatally. For Job and Application Mode failing fatally is ok because it doesn't affect other builds. But for session mode, we want to print a warning, instead.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherCleanupITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CleanupOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.fixed.delay.cleanup.strategy.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.exponential.delay.cleanup.strategy.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.cleanup.configuration.html</file>
      <file type="M">docs.content.docs.deployment.overview.md</file>
      <file type="M">docs.content.docs.deployment.ha.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.ha.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="26658" opendate="2022-3-15 00:00:00" fixdate="2022-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate documentation build to Github Actions</summary>
      <description>INFRA recently setup the required credentials to rsync content to nightlies.apache.org via github actions. This means we can migrate out buildbot setup to github actions instead.This should make maintenance a lot easier, as we'd have more control over the environment. It'd also make it way easier to discover.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.docs.sh</file>
      <file type="M">tools.ci.build.docs.sh</file>
      <file type="M">.github.workflows.docs.yml</file>
    </fixedFiles>
  </bug>
  <bug id="2666" opendate="2015-9-14 00:00:00" fixdate="2015-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow custom Timestamp extractors for Flink sources</summary>
      <description>When record timestamps are turned on users currently have 2 ways of specifying record timestamps.They can either chose to automatically attach ingress timestamps (and send watermarks), or custom implement a sourcefunction to manually assign timestamps and emit watermarks.It would be good if users could define a Timestamp extractor function that will attach a timestamp for every record generated using any of the current Flink sources. Also watermarks for these records should be automatically generated based on the extracted event time (assuming monotonicity per source) periodically.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.timestamp.TimestampITCase.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="26681" opendate="2022-3-16 00:00:00" fixdate="2022-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support sql statement end with ";" for Hive dialect</summary>
      <description>In FLINK-25600, the sql client won't remove ';' at the end of command, so the sql statement will keep the semicolon. When using Hive dialect, it'll be passed to HiveParser and then throw the ParseException like org.apache.flink.table.planner.delegation.hive.copy.HiveASTParseException: line 1:28 cannot recognize input near ';' '&lt;EOF&gt;' ..So we need to support thesql statement end with ";" for Hive dialect. </description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.set.q</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveASTParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="26701" opendate="2022-3-17 00:00:00" fixdate="2022-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Relocation of connector-base might break user jars due to changed imports</summary>
      <description>With the introduction of FLINK-25927, every connector now relocates connector-base to better support connectors compatibility with different Flink versions. Unfortunately, not all classes in connector-base are only used by connector but some are supposed to be used inside the user jar directly i.e. DeliveryGuarantee, HybridSource...Since the connector now relocates the module the existing imports are broken.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.shade.sh</file>
      <file type="M">tools.ci.compile.sh</file>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
      <file type="M">flink-formats.flink-orc.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-streams.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-aws-kinesis-streams.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-files.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="26709" opendate="2022-3-17 00:00:00" fixdate="2022-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace TableConfig.getConfiguration().get(Option) with TableConfig.get(Option)</summary>
      <description>Replace TableConfig.getConfiguration().get(Option) with TableConfig.get(Option) and use the complete view of rootConfiguration + configuration (environment config + app specific config)</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.reuse.SubplanReuser.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamingWithMiniBatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.RankTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.ModifiedMonotonicityTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.TwoStageAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.IncrementalAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.physical.stream.ExpandWindowTableFunctionTransposeRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.physical.stream.ChangelogModeInferenceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.physical.batch.RemoveRedundantLocalHashAggRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.physical.batch.EnforceLocalSortAggRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.physical.batch.EnforceLocalHashAggRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.JoinDeriveNullFilterRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.hint.OptionsHintTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.common.TableFactoryTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.common.JoinReorderTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.SortTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.SortLimitTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.RemoveShuffleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.MultipleInputCreationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.LegacySinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.SortMergeSemiAntiJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.SortMergeJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.ShuffledHashSemiAntiJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.ShuffledHashJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopSemiAntiJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashSemiAntiJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.DeadlockBreakupTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.agg.SortAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.agg.HashAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.agg.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.physical.batch.PushLocalAggIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalTypeJsonSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.BuiltInAggregateFunctionTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.python.PythonFunctionFactoryTest.java</file>
      <file type="M">flink-end-to-end-tests.flink-tpcds-test.src.main.java.org.apache.flink.table.tpcds.TpcdsTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-python-test.src.main.java.org.apache.flink.python.tests.BatchPythonUdfSqlJob.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaChangelogTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.MySqlCatalogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveSinkCompactionITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.table.ElasticsearchDynamicSinkBaseITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.codegen.agg.AggTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.calcite.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.RankUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.IntervalJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.schema.LegacyCatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.TwoStageOptimizedAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.MiniBatchIntervalInferRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.IncrementalAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalSortRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalSortMergeJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalJoinRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalHashJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoLegacyTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.JoinDeriveNullFilterRule.scala</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveLookupJoinITCase.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonFunctionFactory.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-table-api-bridge-base.src.main.java.org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableResult.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.ExpressionResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.TableEnvironmentMock.java</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.WatermarkPushDownSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.SerdeContext.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.spec.DynamicTableSinkSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.spec.DynamicTableSourceSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.spec.DynamicTableSpecBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.utils.DummyStreamExecutionEnvironment.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.utils.ShortcutUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalJoinBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.PartitionPruner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LookupKeySerdeTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedWatermarkGeneratorSupplier.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.ArrowUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.CompiledPlanImpl.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSourceUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoSourceScanRuleBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.physical.batch.PushLocalAggIntoScanRuleBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.utils.TableConfigUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnNullCount.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdDistinctRowCount.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdDistribution.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.SelectivityEstimator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalHashAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalOverAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalPythonGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalRank.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalSortAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
    </fixedFiles>
  </bug>
  <bug id="26712" opendate="2022-3-17 00:00:00" fixdate="2022-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metadata keys should not conflict with physical columns</summary>
      <description>If you have an field called timestamp and in addition want to read the timestamp from the metadata:CREATE TABLE animal_sightings_with_metadata ( `timestamp` TIMESTAMP(3), `name` STRING, `country` STRING, `number` INT, `append_time` TIMESTAMP(3) METADATA FROM 'timestamp', `partition` BIGINT METADATA VIRTUAL, `offset` BIGINT METADATA VIRTUAL, `headers` MAP&lt;STRING, BYTES&gt; METADATA, `timestamp-type` STRING METADATA, `leader-epoch` INT METADATA, `topic` STRING METADATA)This gives:[ERROR] Could not execute SQL statement. Reason:org.apache.flink.table.api.ValidationException: Field names must be unique. Found duplicates: [timestamp]</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.PushLocalAggIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testReadingMetadata.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testWritingMetadata.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.connector.file.table.FileSystemTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSourceUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSinkUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.abilities.SupportsReadingMetadata.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.sink.abilities.SupportsWritingMetadata.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemTableSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="26727" opendate="2022-3-18 00:00:00" fixdate="2022-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the implementation of sub-interpreter in Thread Mode</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.EmbeddedPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractEmbeddedPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.env.embedded.EmbeddedPythonEnvironmentManager.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.dependency.py</file>
      <file type="M">docs.layouts.shortcodes.generated.python.configuration.html</file>
      <file type="M">docs.content.docs.dev.python.python.execution.mode.md</file>
      <file type="M">docs.content.zh.docs.dev.python.python.execution.mode.md</file>
    </fixedFiles>
  </bug>
  <bug id="26728" opendate="2022-3-18 00:00:00" fixdate="2022-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support min max min_by max_by operation in KeyedStream</summary>
      <description>Support min max min_by max_by operation in KeyedStream </description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug id="26739" opendate="2022-3-19 00:00:00" fixdate="2022-4-19 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Hive 2.3.8 and 2.3.9</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="26771" opendate="2022-3-21 00:00:00" fixdate="2022-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix incomparable exception between boolean type and numeric type in Hive dialect</summary>
      <description>Hive support compare boolean type with numeric type, for example such sql can be excuted in Hive:// the data type for `status` is `int`select * from employee where status = true; But in Flink, with Hive dialect, it'll throw  "Incomparable types: BOOLEAN and INT NOT NULL" exception.For such case, it should be consistent with Hive while using Hive dialect in Flink.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserRexNodeConverter.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserDMLHelper.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserSqlFunctionConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="26780" opendate="2022-3-21 00:00:00" fixdate="2022-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an option to force serialization for rpc invocations</summary>
      <description>In the past releases we had a several issues due to non-serializable RPC invocations. We should add an option to force serialization, that we could enabled on CI.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.MessageSerializationTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.FencedAkkaInvocationHandler.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceConfiguration.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcService.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.AkkaOptions.java</file>
      <file type="M">tools.ci.test.controller.sh</file>
    </fixedFiles>
  </bug>
  <bug id="26783" opendate="2022-3-21 00:00:00" fixdate="2022-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Restore from a stop-with-savepoint if failed during committing</summary>
      <description>We decided stop-with-savepoint should commit side-effects and thus we should fail over to those savepoints if a failure happens when committing side effects.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.stopwithsavepoint.StopWithSavepointTerminationHandlerImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.stopwithsavepoint.StopWithSavepointTerminationHandlerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="26798" opendate="2022-3-22 00:00:00" fixdate="2022-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JobMaster.testJobFailureWhenTaskExecutorHeartbeatTimeout failed due to missing Execution</summary>
      <description>This build failed due to an ExecutionGraphException indicating that an expected Execution wasn't around:[...]Caused by: org.apache.flink.util.FlinkException: Execution 48dbc880c8225256b8bc112ea36e9082 is unexpectedly no longer running on task executor bbad15fcb93d4b2b4f80fe2c35e03e6d. at org.apache.flink.runtime.jobmaster.JobMaster$1.onMissingDeploymentsOf(JobMaster.java:250) ~[classes/:?] ... 35 more</description>
      <version>1.14.4,1.15.0,1.16.0</version>
      <fixedVersion>1.14.5,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="26810" opendate="2022-3-22 00:00:00" fixdate="2022-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The local time zone does not take effect when the dynamic index uses a field of type timestamp_ltz</summary>
      <description>When using  TIMESTAMP_WITH_LOCAL_TIMEZONE field to generate a dynamic index,  it will alway use UTC timezone.     </description>
      <version>None</version>
      <fixedVersion>1.15.0,elasticsearch-3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.IndexGeneratorFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.connector.elasticsearch.table.IndexGeneratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.IndexGeneratorFactory.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.IndexGeneratorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="26853" opendate="2022-3-24 00:00:00" fixdate="2022-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HeapStateBackend ignores metadata updates in certain cases</summary>
      <description>On recovery, HeapRestoreOperation reads state handles one by one; each handle contains metadata at the beginning; the metadata is always read, but not actually used if a state with the corresponding name was already registeredIn a rare case of downscaling + multiple checkpoints with different metadata; this might lead to data being deserialized incorrectly (always using the initial metadata).It also prevents incremental checkpoints with schema evolution.On first access, however, the backend itself will update (merge) metadata; so that it doesn't affect new state updates.</description>
      <version>1.14.4,1.15.0,1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.CopyOnWriteStateTableTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.CopyOnWriteStateMap.java</file>
    </fixedFiles>
  </bug>
  <bug id="26855" opendate="2022-3-25 00:00:00" fixdate="2022-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ImportError: cannot import name &amp;#39;environmentfilter&amp;#39; from &amp;#39;jinja2&amp;#39;</summary>
      <description>ar 24 17:38:39 ===========mypy checks... [SUCCESS]===========Mar 24 17:38:39 rm -rf _build/*Mar 24 17:38:39 /__w/2/s/flink-python/dev/.conda/bin/sphinx-build -b html -d _build/doctrees -a -W . _build/htmlMar 24 17:38:40 Traceback (most recent call last):Mar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/bin/sphinx-build", line 6, in &lt;module&gt;Mar 24 17:38:40 from sphinx.cmd.build import mainMar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/cmd/build.py", line 23, in &lt;module&gt;Mar 24 17:38:40 from sphinx.application import SphinxMar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/application.py", line 42, in &lt;module&gt;Mar 24 17:38:40 from sphinx.highlighting import lexer_classes, lexersMar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/highlighting.py", line 30, in &lt;module&gt;Mar 24 17:38:40 from sphinx.ext import doctestMar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/ext/doctest.py", line 28, in &lt;module&gt;Mar 24 17:38:40 from sphinx.builders import BuilderMar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/builders/__init__.py", line 24, in &lt;module&gt;Mar 24 17:38:40 from sphinx.io import read_docMar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/io.py", line 42, in &lt;module&gt;Mar 24 17:38:40 from sphinx.util.rst import append_epilog, docinfo_re, prepend_prologMar 24 17:38:40 File "/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/sphinx/util/rst.py", line 22, in &lt;module&gt;Mar 24 17:38:40 from jinja2 import environmentfilterMar 24 17:38:40 ImportError: cannot import name 'environmentfilter' from 'jinja2' (/__w/2/s/flink-python/dev/.conda/lib/python3.7/site-packages/jinja2/__init__.py)Mar 24 17:38:40 Makefile:76: recipe for target 'html' failedMar 24 17:38:40 make: *** [html] Error 1Mar 24 17:38:40 ==========sphinx checks... [FAILED]===========https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33717&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=c67e71ed-6451-5d26-8920-5a8cf9651901&amp;l=23450</description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.14.5,1.15.0,1.13.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug id="26865" opendate="2022-3-25 00:00:00" fixdate="2022-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the potential failure of loading library in Thread Mode</summary>
      <description>The failure occurs in session mode.</description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.15.0,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
    </fixedFiles>
  </bug>
  <bug id="26928" opendate="2022-3-30 00:00:00" fixdate="2022-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary Docker network creation in Kafka connector tests</summary>
      <description>Currently each Kafka test class will create a Docker network, which could flush the network usage on Docker host, and test would fail if all IP address in the pool of Docker are occupied. </description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.15.0,1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="26929" opendate="2022-3-30 00:00:00" fixdate="2022-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce adaptive hash join for batch sql optimization</summary>
      <description>We propose an optimization method adaptive hash join for the batch join scenario, hoping to integrate the advantages of sorted-merge join and hash join according to the characteristics of runtime data. The adaptive hash join will try to use hash join strategy firstly, if it failed, will fall back to sort merge join.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.util.UniformBinaryRowGenerator.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.String2SortMergeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.String2HashJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.SortMergeJoinIteratorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.Int2SortMergeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.join.Int2HashJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.hashtable.LongHashTableTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.hashtable.BinaryHashTableTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.join.SortMergeJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.join.HashJoinOperator.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.io.BinaryRowChannelInputViewIterator.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.hashtable.LongHybridHashTable.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.hashtable.LongHashPartition.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.hashtable.BinaryHashTable.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.hashtable.BinaryHashPartition.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.hashtable.BaseHybridHashTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.codegen.LongHashJoinGeneratorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.SortUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.LongHashJoinGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeConfig.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSortMergeJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecHashJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="26931" opendate="2022-3-30 00:00:00" fixdate="2022-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pulsar sink&amp;#39;s producer name should be unique</summary>
      <description>Pulsar's new sink interface didn't make the producer name unique. Which would make the pulsar fail to consume messages.</description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.16.0,1.15.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.sink.PulsarSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSourceBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.config.PulsarSourceConfigUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.PulsarWriter.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.PulsarSinkBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.config.PulsarSinkConfigUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.config.PulsarConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.config.PulsarClientFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="26961" opendate="2022-3-31 00:00:00" fixdate="2022-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update multiple Jackson dependencies to v2.13.2 and v2.13.2.1</summary>
      <description>There is a High CVE-2020-36518, https://github.com/advisories/GHSA-57j2-w4cx-62h2which was fixed with 2.13.2.1</description>
      <version>None</version>
      <fixedVersion>1.14.5,1.15.0,elasticsearch-3.0.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-gs-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="26986" opendate="2022-4-1 00:00:00" fixdate="2022-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove deprecated string expressions in Python Table API</summary>
      <description>In FLINK-26704, it has removed the string expressions in Table API. However, there are still some APIs still using string expressions in Python Table API, however, they should not work any more as the string expressions have already been removed in the Java Table API.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.window.py</file>
      <file type="M">flink-python.pyflink.table.table.py</file>
      <file type="M">flink-python.pyflink.table.schema.py</file>
      <file type="M">flink-python.pyflink.examples.table.windowing.tumble.window.py</file>
      <file type="M">flink-python.pyflink.examples.table.windowing.sliding.window.py</file>
      <file type="M">flink-python.pyflink.examples.table.windowing.session.window.py</file>
      <file type="M">flink-python.pyflink.examples.table.windowing.over.window.py</file>
      <file type="M">flink-python.pyflink.examples.table.pandas.pandas.udaf.py</file>
      <file type="M">docs.content.docs.dev.table.tableApi.md</file>
      <file type="M">docs.content.docs.dev.table.catalogs.md</file>
      <file type="M">docs.content.docs.dev.python.table.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.content.docs.dev.python.table.udfs.python.udfs.md</file>
      <file type="M">docs.content.docs.dev.python.table.python.table.api.connectors.md</file>
      <file type="M">docs.content.zh.docs.dev.table.tableApi.md</file>
      <file type="M">docs.content.zh.docs.dev.table.catalogs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.udfs.python.udfs.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.python.table.api.connectors.md</file>
    </fixedFiles>
  </bug>
  <bug id="26994" opendate="2022-4-1 00:00:00" fixdate="2022-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merge libraries CI profile into core</summary>
      <description>The libraries profile spends more time on setting up the environment than actually running tests. Merge it with Core for more efficiency.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="26995" opendate="2022-4-1 00:00:00" fixdate="2022-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable fork-reuse for all unit tests</summary>
      <description>pnowojski made the great discovery that flink-runtime, flink-streaming-java and flink-tests run all unit tests without reusing forks.We can save a lot of time if we'd rectify this.This will require re-branding some tests as IT cases.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-streaming-java.pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionTest.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SynchronousCheckpointTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.LocalStateForwardingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.TestProcessingTimeServiceTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamTaskTimerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.SinkTransformationTranslatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.FileUploadHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.MemoryExecutionGraphInfoStoreTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketAssignerITCases.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ZooKeeperUtilsITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.AbstractHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.AbstractHandlerITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="27024" opendate="2022-4-2 00:00:00" fixdate="2022-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup surefire configuration</summary>
      <description>We have a few redundant surefire configurations in some connector modules, and overall a lot of duplication and stuff defined on the argLine which could be systemEnvironmentVariables (which are easier to extend in sub-modules).</description>
      <version>None</version>
      <fixedVersion>1.16.0,elasticsearch-3.0.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27031" opendate="2022-4-3 00:00:00" fixdate="2022-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ChangelogRescalingITCase.test failed due to IllegalStateException</summary>
      <description>This build failed in ChangelogRescalingITCase.test:Apr 01 20:26:53 Caused by: java.lang.IllegalArgumentException: Key group 94 is not in KeyGroupRange{startKeyGroup=96, endKeyGroup=127}. Unless you're directly using low level state access APIs, this is most likely caused by non-deterministic shuffle key (hashCode and equals implementation).Apr 01 20:26:53 at org.apache.flink.runtime.state.KeyGroupRangeOffsets.newIllegalKeyGroupException(KeyGroupRangeOffsets.java:37)Apr 01 20:26:53 at org.apache.flink.runtime.state.heap.StateTable.getMapForKeyGroup(StateTable.java:305)Apr 01 20:26:53 at org.apache.flink.runtime.state.heap.StateTable.get(StateTable.java:261)Apr 01 20:26:53 at org.apache.flink.runtime.state.heap.StateTable.get(StateTable.java:143)Apr 01 20:26:53 at org.apache.flink.runtime.state.heap.HeapListState.add(HeapListState.java:94)Apr 01 20:26:53 at org.apache.flink.state.changelog.ChangelogListState.add(ChangelogListState.java:78)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.processElement(WindowOperator.java:404)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:99)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:80)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)Apr 01 20:26:53 at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)Apr 01 20:26:53 at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)Apr 01 20:26:53 at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:38)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:531)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:227)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:841)Apr 01 20:26:53 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:767)Apr 01 20:26:53 at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)Apr 01 20:26:53 at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)Apr 01 20:26:53 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)Apr 01 20:26:53 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)Apr 01 20:26:53 at java.lang.Thread.run(Thread.java:748)</description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.16.0,1.15.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.TaskStateAssignment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="27059" opendate="2022-4-5 00:00:00" fixdate="2022-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-compress</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-compress.src.test.java.org.apache.flink.formats.compress.CompressWriterFactoryTest.java</file>
      <file type="M">flink-formats.flink-compress.src.test.java.org.apache.flink.formats.compress.CompressionFactoryITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="27069" opendate="2022-4-6 00:00:00" fixdate="2022-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the potential memory corruption in Thread Mode</summary>
      <description>Apr 02 12:24:54 *** Error in `/usr/lib/jvm/adoptopenjdk-11-hotspot-amd64/bin/java': malloc(): memory corruption: 0x00007ff7c43bb820 ***Apr 02 12:24:54 ======= Backtrace: =========Apr 02 12:24:54 /lib/x86_64-linux-gnu/libc.so.6(+0x777f5)[0x7ff7f90be7f5]Apr 02 12:24:54 /lib/x86_64-linux-gnu/libc.so.6(+0x8215e)[0x7ff7f90c915e]Apr 02 12:24:54 /lib/x86_64-linux-gnu/libc.so.6(__libc_malloc+0x54)[0x7ff7f90cb1d4]Apr 02 12:24:54 /root/flink/flink-python/dev/.conda/envs/3.8/lib/libpython3.8.so.1.0(PyObject_Malloc+0x166)[0x7ff78c16c636]Apr 02 12:24:54 /root/flink/flink-python/dev/.conda/envs/3.8/lib/libpython3.8.so.1.0(PyBytes_FromStringAndSize+0x76)[0x7ff78c1b9316]Apr 02 12:24:54 /root/flink/flink-python/.tox/py38-cython/lib/python3.8/site-packages/pemja_core.cpython-38-x86_64-linux-gnu.so(JcpPyBytes_FromJByteArray+0x46)[0x7ff7f400c706]Apr 02 12:24:54 /root/flink/flink-python/.tox/py38-cython/lib/python3.8/site-packages/pemja_core.cpython-38-x86_64-linux-gnu.so(JcpPyObject_FromJObject+0x3db)[0x7ff7f400d2ab]Apr 02 12:24:54 /root/flink/flink-python/.tox/py38-cython/lib/python3.8/site-packages/pemja_core.cpython-38-x86_64-linux-gnu.so(JcpPyObject_SetJObject+0x2e)[0x7ff7f400ee6e]Apr 02 12:24:54 /root/flink/flink-python/.tox/py38-cython/lib/python3.8/site-packages/pemja_core.cpython-38-x86_64-linux-gnu.so(Java_pemja_core_PythonInterpreter_set__JLjava_lang_String_2Ljava_lang_Object_2+0x35)[0x7ff7f400a765]Apr 02 12:24:54 [0x7ff7d8887630]</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2707" opendate="2015-9-18 00:00:00" fixdate="2015-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set state checkpointer before default state for PartitionedStreamOperatorState</summary>
      <description>Currently the default state is set before the passed StateCheckpointer instance for operator states.What currently happens because of this is that the default value is serialized with Java serialization and then deserialized on the opstate.value() call using the StateCheckpointer most likely causing a failure.This can be trivially fixed by swaping the order of the 2 calls.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.state.StatefulOperatorTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamingRuntimeContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="27108" opendate="2022-4-7 00:00:00" fixdate="2022-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>State cache clean up doesn&amp;#39;t work as expected</summary>
      <description>The test case test_session_window_late_merge failed when working on FLINK-26190. After digging into this problem, I found that the reason should be that the logic to determine whether a key &amp; namespace exists in state cache is wrong is wrong. It causes the state cache isn't clean up when it becomes invalidate.</description>
      <version>1.13.0,1.14.0,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0,1.13.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.pyflink.fn.execution.state.impl.py</file>
    </fixedFiles>
  </bug>
  <bug id="27111" opendate="2022-4-7 00:00:00" fixdate="2022-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update docs regarding EnvironmentSettings / TableConfig</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="27119" opendate="2022-4-7 00:00:00" fixdate="2022-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup JobMasters</summary>
      <description>Several tests create JobMasters but don't make sure that it is shut down. We should change the tests to use a try-with-resource statement.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterQueryableStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterExecutionDeploymentReconciliationTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="27140" opendate="2022-4-8 00:00:00" fixdate="2022-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move JobResultStore dirty entry creation into ioExecutor</summary>
      <description>The FileSystemJobResultStore is thread-safe and, therefore, we can move the dirty entry creation into the ioExecutor</description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.MiniDispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="27174" opendate="2022-4-11 00:00:00" fixdate="2022-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Non-null check for bootstrapServers field is incorrect in KafkaSink</summary>
      <description>If the user-supplied kafkaProducerConfig contains bootstrapServers information, there is no need to define the value of this field separately through the setBootstrapServers method. Obviously, the current code doesn't notice this. Perhaps we can check bootstrapServers as follows: Or check bootstrapServers like KafkaSourceBuilder.  </description>
      <version>1.14.4,1.15.0,1.16.0</version>
      <fixedVersion>1.14.5,1.15.1,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.sink.KafkaSinkBuilderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.KafkaSinkBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="27199" opendate="2022-4-12 00:00:00" fixdate="2022-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Pulsar to 2.10.0 for fixing the unstable Pulsar test environment.</summary>
      <description>Pulsar's transaction is not stable. The standalone cluster often hangs the test, then we will meet a timeout for the tests at last.The latest Pulsar 2.10.0 drops the zookeeper and fixes a lot of issues in the Pulsar transaction. Bump to this version would resolve the current test issues.</description>
      <version>1.14.4,1.15.0,1.16.0</version>
      <fixedVersion>1.16.0,1.15.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.PulsarMockRuntime.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.MockZooKeeperClientFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.MockPulsarService.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.mock.MockBookKeeperClientFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.PulsarSinkOptions.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.config.SinkConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.config.PulsarSinkConfigUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.config.PulsarOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.sink.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.producer.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.client.configuration.html</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-pulsar.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.split.PulsarPartitionSplit.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.start.MessageIdStartCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.config.SourceConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.router.MessageKeyHash.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27217" opendate="2022-4-13 00:00:00" fixdate="2022-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support partition filter push down when there exists default_parition</summary>
      <description>Using Hive dialect, when the table's partition column is nullable, and when insert a row whose value of partition column is null,  the  row will be assigned to the partition with value 'HiveConf.ConfVars.DEFAULTPARTITIONNAME',  default is  "_HIVE_DEFAULT_PARTITION_".Like this : create table ptestfilter (a string) partitioned by (c int);INSERT OVERWRITE TABLE ptestfilter PARTITION (c) select 'Col1', null;INSERT OVERWRITE TABLE ptestfilter PARTITION (c) select 'Col2', 5;select * from ptestfilter where c between 2 and 6 ;It'll try to do partition filter push down, and  cast the '_HIVE_DEFAULT_PARTITION_' to int type, then it will fail with the following exception:java.lang.NumberFormatException: For input string: "__HIVE_DEFAULT_PARTITION__"    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)    at java.lang.Integer.parseInt(Integer.java:580)    at java.lang.Integer.parseInt(Integer.java:615)    at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:273)    at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)    at org.apache.flink.table.planner.plan.utils.PartitionPruner$.org$apache$flink$table$planner$plan$utils$PartitionPruner$$convertPartitionFieldValue(PartitionPruner.scala:173)  </description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ShowPartitionsOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="27229" opendate="2022-4-13 00:00:00" fixdate="2022-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cassandra overrides netty version in tests</summary>
      <description>flink-connector-cassandra declares: &lt;dependency&gt; &lt;!-- Bump cassandra netty dependency --&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;4.1.46.Final&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;which overrides the project wide version of netty just for tests.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27230" opendate="2022-4-13 00:00:00" fixdate="2022-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unnecessary entries in connector-kinesis NOTICE file</summary>
      <description>flink-connector-kinesis lists but does not bundle:- commons-logging:commons-logging:1.1.3- com.fasterxml.jackson.core:jackson-core:2.13.2[INFO] Excluding commons-logging:commons-logging:jar:1.1.3 from the shaded jar.[INFO] Excluding com.fasterxml.jackson.core:jackson-core:jar:2.13.2 from the shaded jar.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="27231" opendate="2022-4-13 00:00:00" fixdate="2022-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL pulsar connector lists dependencies under wrong license</summary>
      <description>Pulsar sql connector lists following dependencies under ASL2 license while they are licensed with Bouncy Castle license (variant of MIT?).- org.bouncycastle:bcpkix-jdk15on:1.69- org.bouncycastle:bcprov-ext-jdk15on:1.69- org.bouncycastle:bcprov-jdk15on:1.69- org.bouncycastle:bcutil-jdk15on:1.69</description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.15.0,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-pulsar.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="27233" opendate="2022-4-13 00:00:00" fixdate="2022-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unnecessary entries in connector-elasticsearch7 in NOTICE file</summary>
      <description>flink-sql-connector-elasticsearch7 lists following dependencies in the NOTICE file, which are not bundled in the jar:- com.fasterxml.jackson.core:jackson-databind:2.13.2.2- com.fasterxml.jackson.core:jackson-annotations:2.13.2- org.apache.lucene:lucene-spatial:8.7.0- org.elasticsearch:elasticsearch-plugin-classloader:7.10.2- org.lz4:lz4-java:1.8.0</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="27234" opendate="2022-4-13 00:00:00" fixdate="2022-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable fork-reuse for connector-jdbc</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProviderDriverClassConcurrentLoadingTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-jdbc.archunit-violations.6b9ab1b0-c14d-4667-bab5-407b81fba98b</file>
    </fixedFiles>
  </bug>
  <bug id="27251" opendate="2022-4-14 00:00:00" fixdate="2022-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Timeout aligned to unaligned checkpoint barrier in the output buffers of an upstream subtask</summary>
      <description>After FLINK-23041, the downstream task can be switched UC when currentTime - triggerTime &gt; timeout. But the downstream task still needs wait for all barriers of upstream. If the back pressure is serve, the downstream task cannot receive all barrier within CP timeout, causes CP to fail. Can we support upstream Task switching from Aligned to UC? It means that when the barrier cannot be sent from the output buffer to the downstream task within the execution.checkpointing.aligned-checkpoint-timeout, the upstream task switches to UC and takes a snapshot of the data before the barrier in the output buffer. Hi akalashnikov , please help take a look in your free time, thanks a lot.</description>
      <version>1.14.0,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TestSubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MockSubtaskCheckpointCoordinatorBuilder.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.RecordWriterOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.AlternatingCollectingBarriers.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlternatingAlignedBarrierHandlerState.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ChannelPersistenceITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.MockResultPartitionWriter.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.InputChannelTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.MockChannelStateWriter.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.RecordWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointOptions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.channel.ChannelStateWriter.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.TestBarrierHandlerFactory.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.checkpointing.AlternatingCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.InputProcessorUtil.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="27252" opendate="2022-4-14 00:00:00" fixdate="2022-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove surefire fork options from connector-hive</summary>
      <description>Cleanup of unnecessary settings, that will also slightly speed up testing.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27253" opendate="2022-4-14 00:00:00" fixdate="2022-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove custom surefire config from connector-cassandra</summary>
      <description>With the recent improvements around the cassandra test stability we can clean up some technical debt.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27282" opendate="2022-4-18 00:00:00" fixdate="2022-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the bug of wrong positions mapping in RowCoder</summary>
      <description></description>
      <version>1.13.6,1.14.4,1.15.0</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.table.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonMapMergeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.PythonUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PythonMapMergeRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.utils.OperationTreeBuilder.java</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.examples.table.basic.operations.py</file>
      <file type="M">flink-python.pyflink.common.types.py</file>
      <file type="M">docs.content.docs.dev.python.table.operations.row.based.operations.md</file>
      <file type="M">docs.content.docs.dev.python.table.intro.to.table.api.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.operations.row.based.operations.md</file>
      <file type="M">docs.content.zh.docs.dev.python.table.intro.to.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="27297" opendate="2022-4-19 00:00:00" fixdate="2022-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the StreamExecutionEnvironment#getExecutionEnvironment(Configuration) method in PyFlink</summary>
      <description>StreamExecutionEnvironment#getExecutionEnvironment(Configuration) method has been added in Java side since release-1.12, we need to add this method in Python too</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.common.tests.test.execution.config.py</file>
      <file type="M">docs.content.docs.dev.python.python.config.md</file>
      <file type="M">docs.content.zh.docs.dev.python.python.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="27304" opendate="2022-4-19 00:00:00" fixdate="2022-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink doesn&amp;#39;t support Hive primitive type void yet</summary>
      <description>We can reproduce through a UTAdd test case in HiveDialectITCase @Test public void testHiveVoidType() { tableEnv.loadModule("hive", new HiveModule(hiveCatalog.getHiveVersion())); tableEnv.executeSql( "create table src (a int, b string, c int, sample array&lt;binary&gt;)"); tableEnv.executeSql("select a, one from src lateral view explode(sample) samples as one where a &gt; 0 "); }</description>
      <version>1.13.1,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserTypeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="27308" opendate="2022-4-19 00:00:00" fixdate="2022-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the Hadoop implementation for filesystems to 3.3.2</summary>
      <description>Flink currently uses Hadoop version 3.2.2 for the Flink filesystem implementations. Upgrading this to version 3.3.2 would provide users the features listed in HADOOP-17566</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.java.org.apache.flink.fs.s3hadoop.HadoopS3AccessHelper.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.pom.xml</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.pom.xml</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="27319" opendate="2022-4-20 00:00:00" fixdate="2022-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Duplicated "-t" option for savepoint format and deployment target</summary>
      <description>The two options savepoint format and deployment target have the same short option which causes a clash and the CLI to fail.I suggest to drop the short "-t" for savepoint format.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendStopWithSavepointTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendSavepointTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="27341" opendate="2022-4-21 00:00:00" fixdate="2022-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TaskManager running together with JobManager are bind to 127.0.0.1</summary>
      <description>If some TaskManagers running with JobManager on the same machine while some other TaskManager not, the TaskManagers running together with JobManager would bind to localhost or 127.0.01, which makes the Netty connections across the TaskManagers fail.</description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.17.0,1.16.1,1.15.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.ConnectionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="27368" opendate="2022-4-24 00:00:00" fixdate="2022-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL CAST(&amp;#39; 1 &amp;#39; as BIGINT) returns wrong result</summary>
      <description>Flink SQL&gt; select&gt; cast(' 1 ' as tinyint),&gt; cast(' 1 ' as smallint),&gt; cast(' 1 ' as int),&gt; cast(' 1 ' as bigint),&gt; cast(' 1 ' as float),&gt; cast(' 1 ' as double);+----+--------+--------+-------------+----------------------+--------------------------------+--------------------------------+| op | EXPR$0 | EXPR$1 | EXPR$2 | EXPR$3 | EXPR$4 | EXPR$5 |+----+--------+--------+-------------+----------------------+--------------------------------+--------------------------------+[ERROR] Could not execute SQL statement. Reason:java.lang.NumberFormatException: For input string: ' 1 '. Invalid character found. at org.apache.flink.table.data.binary.BinaryStringDataUtil.numberFormatExceptionFor(BinaryStringDataUtil.java:585) at org.apache.flink.table.data.binary.BinaryStringDataUtil.toInt(BinaryStringDataUtil.java:518) at org.apache.flink.table.data.binary.BinaryStringDataUtil.toByte(BinaryStringDataUtil.java:568) at StreamExecCalc$392.processElement(Unknown Source) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418) at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513) at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103) at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67) at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:332)Setting CAST behavior to legacy but got null result :Flink SQL&gt; set table.exec.legacy-cast-behaviour=enabled;[INFO] Session property has been set.Flink SQL&gt; select&gt; cast(' 1 ' as tinyint),&gt; cast(' 1 ' as smallint),&gt; cast(' 1 ' as int),&gt; cast(' 1 ' as bigint),&gt; cast(' 1 ' as float),&gt; cast(' 1 ' as double);+----+--------+--------+-------------+----------------------+--------------------------------+--------------------------------+| op | EXPR$0 | EXPR$1 | EXPR$2 | EXPR$3 | EXPR$4 | EXPR$5 |+----+--------+--------+-------------+----------------------+--------------------------------+--------------------------------+[ERROR] Could not execute SQL statement. Reason:org.apache.flink.table.api.TableException: Column 'EXPR$0' is NOT NULL, however, a null value is being written into it. You can set job configuration 'table.exec.sink.not-null-enforcer'='DROP' to suppress this exception and drop such records silently. at org.apache.flink.table.runtime.operators.sink.ConstraintEnforcer.processNotNullConstraint(ConstraintEnforcer.java:261) at org.apache.flink.table.runtime.operators.sink.ConstraintEnforcer.processElement(ConstraintEnforcer.java:241) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) at StreamExecCalc$591.processElement_split1(Unknown Source) at StreamExecCalc$591.processElement(Unknown Source) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57) at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418) at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513) at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103) at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67) at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:332)In 1.14 the result should be &amp;#91;1, 1, 1, 1, 1.0, 1.0&amp;#93;. In Postgres:postgres=# select cast(' 1 ' as int), cast(' 1 ' as bigint), cast(' 1 ' as float); int4 | int8 | float8------+------+-------- 1 | 1 | 1(1 row)</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.casting.CastRulesTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.StringToNumericPrimitiveCastRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="27382" opendate="2022-4-25 00:00:00" fixdate="2022-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Job mode wait with cluster shutdown until the cleanup is done</summary>
      <description>The shutdown is triggered as soon as the job terminates globally without waiting for any cleanup. This behavior was ok'ish in 1.14- because we didn't bother so much about the cleanup. In 1.15+ we might want to wait for the cleanup to finish.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.MiniDispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.MiniDispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="27386" opendate="2022-4-25 00:00:00" fixdate="2022-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>throw NPE if multi MAPJOIN hint union all</summary>
      <description>We can reproduce through a UTAdd test case in HiveDialectITCase@Testpublic void testHiveMultiMapJoinUnionAll() { tableEnv.executeSql("create table t1 (id bigint, name string)"); tableEnv.executeSql("create table t2 (id bigint, name string)"); tableEnv.executeSql("select /*+ mapjoin(t2) */ t1.id from t1 join t2 on t1.id = t2.id union all select /*+ mapjoin(t2) */ t1.id from t1 join t2 on t1.name = t2.name");}</description>
      <version>1.13.1,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveASTParseUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="27387" opendate="2022-4-25 00:00:00" fixdate="2022-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Insert Multi-Table</summary>
      <description>We can reproduce through a UTAdd test case in HiveDialectITCase@Testpublic void testInsertMultiTable() { tableEnv.executeSql("create table t1 (id bigint, name string)"); tableEnv.executeSql("create table t2 (id bigint, name string)"); tableEnv.executeSql("create table t3 (id bigint, name string, age int)"); tableEnv.executeSql("from (select id, name, age from t3) t " + "insert overwrite table t1 select id, name where age &lt; 20 " + "insert overwrite table t2 select id, name where age &gt; 20 ");} This is a very common case for batch.</description>
      <version>1.13.1,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="27399" opendate="2022-4-25 00:00:00" fixdate="2022-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pulsar connector didn&amp;#39;t set start consuming position correctly</summary>
      <description>The Pulsar connector didn't use the consuming position from the checkpoint. They just commit the position to Pulsar after the checkpoint is complete. And the connector starts to consume messages from Pulsar directly by the offset stored on the Pulsar subscription.This causes the test could be failed in some situations. The start cursor (position on Pulsar) would be reset to the wrong position, which caused the results didn't match the desired records.How to fix this issueChange the start position seeking mechanism from Pulsar consumer API to Pulsar admin API. Don't reset the start position when the topic has a subscription.This issue fixes FLINK-23944 FLINK-24872 FLINK-25815 FLINK-25884 FLINK-26177 FLINK-26721 FLINK-27833</description>
      <version>1.14.4,1.15.0,1.16.0</version>
      <fixedVersion>1.16.0,1.15.2,1.14.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.runtime.PulsarRuntimeOperator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.testutils.PulsarTestContext.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.PulsarSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.SplitsAssignmentStateTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumStateSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.StopCursorTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.split.PulsarPartitionSplit.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarUnorderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.PulsarSourceReaderFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSourceOptions.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSourceBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.PulsarSource.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.subscriber.PulsarSubscriber.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.SplitsAssignmentState.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumState.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumerator.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.stop.PublishTimestampStopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.stop.NeverStopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.stop.MessageIdStopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.stop.LatestMessageStopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.stop.EventTimestampStopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.StopCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.start.TimestampStartCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.start.MessageIdStartCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.StartCursor.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.CursorPosition.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.config.SourceConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.config.PulsarSourceConfigUtils.java</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.consumer.configuration.html</file>
      <file type="M">docs.content.docs.connectors.datastream.pulsar.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.pulsar.md</file>
    </fixedFiles>
  </bug>
  <bug id="2740" opendate="2015-9-22 00:00:00" fixdate="2015-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create data consumer for Apache NiFi</summary>
      <description>Create a connector to Apache NiFi to create Flink DataStreams from NiFi flows</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27441" opendate="2022-4-28 00:00:00" fixdate="2022-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scrollbar is missing for particular UI elements (Accumulators, Backpressure, Watermarks)</summary>
      <description>The angular version bump introduced a bug, where for nzScroll does not support percentage in CSS calc, so the scrollbar will be invisible. There is an easy workaround, the linked Angular discussion covers it.Angular issue: https://github.com/NG-ZORRO/ng-zorro-antd/issues/3090</description>
      <version>1.14.3,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.watermarks.job-overview-drawer-watermarks.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.taskmanagers.job-overview-drawer-taskmanagers.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.accumulators.job-overview-drawer-accumulators.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="27450" opendate="2022-4-29 00:00:00" fixdate="2022-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JDK 11] Hive SessionState can&amp;#39;t be initialized due to classloader problem</summary>
      <description>If we use jdk11 to run Hive related jobs, there will be a problem that the Hive SessionState cannot be initializedjava.lang.ClassCastException: class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap') at org.apache.hadoop.hive.ql.session.SessionState.&lt;init&gt;(SessionState.java:394) at org.apache.hadoop.hive.ql.session.SessionState.&lt;init&gt;(SessionState.java:370) at org.apache.flink.table.planner.delegation.hive.HiveParser$HiveParserSessionState.&lt;init&gt;(HiveParser.java:382) at org.apache.flink.table.planner.delegation.hive.HiveParser.startSessionState(HiveParser.java:306) at org.apache.flink.table.planner.delegation.hive.HiveParser.parse(HiveParser.java:205) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:695) at org.apache.flink.connectors.hive.HiveDialectQueryITCase.setup(HiveDialectQueryITCase.java:74) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)Refer to HIVE-21584, choose hive version 2.3.9 to support jdk11We have been running in production for a long time</description>
      <version>1.13.1,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27466" opendate="2022-5-2 00:00:00" fixdate="2022-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC metaspace leak fix is misleading</summary>
      <description>To ensure that these classes are only loaded once you should either add the driver jars to Flink’s lib/ folder, or add the driver classes to the list of parent-first loaded class via classloader.parent-first-patterns-additional.This reads as if adding the driver to classloader.parent-first-patterns-additional can solve the issue in all cases, but this only works if the driver is already in lib/.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.debugging.debugging.classloading.md</file>
      <file type="M">docs.content.zh.docs.ops.debugging.debugging.classloading.md</file>
    </fixedFiles>
  </bug>
  <bug id="27487" opendate="2022-5-4 00:00:00" fixdate="2022-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaMetricWrappers do incorrect cast</summary>
      <description>In FLINK-24765 the kafka metric wrappers that bridge kafka metrics into our metric system were migrated from the deprecated KafkaMetric#value to #metricValue.This migration was done incorrectly. It was assumed that #metricValue behaves similar to #value in that it always returns a double, but this is not the case, as they may also return longs or strings.This results in these metrics throwing exceptions, which generally breaks the reporter mechanism completely.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.metrics.KafkaMetricMutableWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="27544" opendate="2022-5-9 00:00:00" fixdate="2022-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Example code in &amp;#39;Structure of Table API and SQL Programs&amp;#39; is out of date and cannot run</summary>
      <description>The example code in Structure of Table API and SQL Programs of 'Concepts &amp; Common API' is out of date and when user run this piece of code, they will get the following result:Exception in thread "main" org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.SinkTable'.Table options are:'connector'='blackhole''rows-per-second'='1' at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:262) at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:421) at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:222) at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:178) at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) at scala.collection.Iterator.foreach(Iterator.scala:937) at scala.collection.Iterator.foreach$(Iterator.scala:937) at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) at scala.collection.IterableLike.foreach(IterableLike.scala:70) at scala.collection.IterableLike.foreach$(IterableLike.scala:69) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike.map(TraversableLike.scala:233) at scala.collection.TraversableLike.map$(TraversableLike.scala:226) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:178) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1656) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:782) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:861) at org.apache.flink.table.api.internal.TablePipelineImpl.execute(TablePipelineImpl.java:56) at com.yck.TestTableAPI.main(TestTableAPI.java:43)Caused by: org.apache.flink.table.api.ValidationException: Unsupported options found for 'blackhole'.Unsupported options:rows-per-secondSupported options:connectorproperty-version at org.apache.flink.table.factories.FactoryUtil.validateUnconsumedKeys(FactoryUtil.java:624) at org.apache.flink.table.factories.FactoryUtil$FactoryHelper.validate(FactoryUtil.java:914) at org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.validate(FactoryUtil.java:978) at org.apache.flink.connector.blackhole.table.BlackHoleTableSinkFactory.createDynamicTableSink(BlackHoleTableSinkFactory.java:64) at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:259) ... 19 moreI think this mistake would drive users crazy when they first fry Table API &amp; Flink SQL since this is the very first code they see.Overall this code is outdated in two places:1. The Query creating temporary table should be CREATE TEMPORARY TABLE SinkTable WITH ('connector' = 'blackhole') LIKE SourceTable (EXCLUDING OPTIONS) instead of CREATE TEMPORARY TABLE SinkTable WITH ('connector' = 'blackhole') LIKE SourceTable which missed (EXCLUDING OPTIONS) sql_like_pattern2. The part creating a source table should be tableEnv.createTemporaryTable("SourceTable", TableDescriptor.forConnector("datagen") .schema(Schema.newBuilder() .column("f0", DataTypes.STRING()) .build()) .option(DataGenConnectorOptions.ROWS_PER_SECOND, 1L) .build());instead of tableEnv.createTemporaryTable("SourceTable", TableDescriptor.forConnector("datagen") .schema(Schema.newBuilder() .column("f0", DataTypes.STRING()) .build()) .option(DataGenOptions.ROWS_PER_SECOND, 100) .build());since the class DataGenOptions was replaced by class DataGenConnectorOptions in this commitThe test code is in my github Repository(version 1.15) and version 1.14The affected versions are 1.15 and 1.14.</description>
      <version>1.14.0,1.14.2,1.14.3,1.14.4,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.common.md</file>
      <file type="M">docs.content.zh.docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  <bug id="27561" opendate="2022-5-10 00:00:00" fixdate="2022-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-connector-aws-base</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-aws-base.src.test.java.org.apache.flink.connector.aws.util.AWSGeneralUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-base.src.test.java.org.apache.flink.connector.aws.util.AWSAsyncSinkUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-base.src.test.java.org.apache.flink.connector.aws.table.util.AWSOptionsUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-base.src.test.java.org.apache.flink.connector.aws.table.util.AsyncClientOptionsUtilsTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27565" opendate="2022-5-10 00:00:00" fixdate="2022-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump minimist from 1.2.5 to 1.2.6 in /flink-runtime-web/web-dashboard</summary>
      <description>It's recommended to use version 1.2.6 or later: https://security.snyk.io/vuln/SNYK-JS-MINIMIST-2429795 (version &lt;=1.2.5) https://snyk.io/vuln/SNYK-JS-MINIMIST-559764 (version &lt;=1.2.3)</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
    </fixedFiles>
  </bug>
  <bug id="27566" opendate="2022-5-10 00:00:00" fixdate="2022-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-connector-aws-kinesis-firehose</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.table.KinesisFirehoseDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseStateSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkElementConverterTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.test.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkBuilderTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="27567" opendate="2022-5-10 00:00:00" fixdate="2022-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-connector-aws-kinesis-streams</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.test.java.org.apache.flink.connector.kinesis.table.util.KinesisProducerOptionsMapperTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.test.java.org.apache.flink.connector.kinesis.table.RowDataFieldsKinesisPartitionKeyGeneratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.test.java.org.apache.flink.connector.kinesis.table.KinesisDynamicTableSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.test.java.org.apache.flink.connector.kinesis.sink.KinesisStreamsStateSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.test.java.org.apache.flink.connector.kinesis.sink.KinesisStreamsSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.test.java.org.apache.flink.connector.kinesis.sink.KinesisStreamsSinkBuilderTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="27570" opendate="2022-5-11 00:00:00" fixdate="2022-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkpoint path error does not cause the job to stop</summary>
      <description>I configured the wrong checkpoint path when starting the job, and set：conf.set (executioncheckpointingoptions. Tolerable_failure_number, 0);env setRestartStrategy(RestartStrategies.noRestart());The job is expected to stop due to a checkpoint error, but the job is still running.Here is my job configuration and environment：</description>
      <version>1.14.4,1.15.0,1.16.0</version>
      <fixedVersion>1.16.0,1.15.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.CheckpointFailureManagerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointFailureManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStorageAccess.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.FinishedTaskStateProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.DefaultCheckpointPlan.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="27607" opendate="2022-5-13 00:00:00" fixdate="2022-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-connector-files</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.LegacyRowResource.java</file>
      <file type="M">flink-connectors.flink-connector-files.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.BatchCompactingFileSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.BatchExecutionFileSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.committer.FileCommitterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.compactor.AbstractCompactTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.compactor.CompactCoordinatorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.compactor.CompactorOperatorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.FileCommittableSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.FileSinkCommittableSerializerMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.FileSinkCompactionSwitchITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.FileSinkITBase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.StreamingCompactingFileSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.StreamingExecutionFileSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.utils.IntegerFileSinkTestDataUtils.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.writer.FileSinkMigrationITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.writer.FileWriterBucketStateSerializerMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.writer.FileWriterBucketStateSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.writer.FileWriterBucketTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.sink.writer.FileWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.assigners.LocalityAwareSplitAssignerTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.enumerate.BlockSplittingRecursiveEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.enumerate.NonSplittingRecursiveEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.FileSourceHeavyThroughputTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.FileSourceSplitSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.FileSourceSplitStateTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.FileSourceSplitTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.FileSourceTextLinesITCase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.impl.AdapterTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.impl.ContinuousFileSplitEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.impl.FileRecordFormatAdapterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.impl.FileRecordsTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.impl.FileSourceReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.impl.StaticFileSplitEnumeratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.impl.StreamFormatAdapterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.PendingSplitsCheckpointSerializerTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.util.ArrayResultIteratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.util.IteratorResultIteratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.src.util.SingletonResultIteratorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.BinPackingTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.EnrichedRowDataTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.FileSystemCommitterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.FileSystemOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.LimitableBulkFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.PartitionWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.RowPartitionComputerTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.stream.compact.AbstractCompactTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.stream.compact.CompactCoordinatorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.stream.compact.CompactFileWriterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.stream.compact.CompactOperatorTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.stream.StreamingFileWriterTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="27699" opendate="2022-5-19 00:00:00" fixdate="2022-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Align atPublishTime method of StopCursor class for Pulsar connector</summary>
      <description>StopCursor#atEventTime is deprecated, align to StopCursor#atPublishTime.</description>
      <version>1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.connectors.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.pulsar.py</file>
    </fixedFiles>
  </bug>
  <bug id="27711" opendate="2022-5-20 00:00:00" fixdate="2022-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct the typo of set_topics_pattern by changing it to set_topic_pattern for Pulsar Connector</summary>
      <description>Update set_topics_pattern to set_topic_pattern</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.connectors.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.pulsar.py</file>
    </fixedFiles>
  </bug>
  <bug id="27718" opendate="2022-5-20 00:00:00" fixdate="2022-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix fail to count mutiple fields excpetion in Hive dialect</summary>
      <description>In Hive, it's support to count multiple fields using the following sql: select count(distinct a, b) from srcWe are also expected to support it while using Hive dialect in Flink.  </description>
      <version>None</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.module.hive.HiveModule.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveGenericUDAF.java</file>
    </fixedFiles>
  </bug>
  <bug id="27734" opendate="2022-5-22 00:00:00" fixdate="2022-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Not showing checkpoint interval properly in WebUI when checkpoint is disabled</summary>
      <description>Not showing checkpoint interval properly  in WebUI when checkpoint is disabled</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="27735" opendate="2022-5-23 00:00:00" fixdate="2022-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update testcontainers dependency to v1.17.2</summary>
      <description>testcontainers 1.17.2 is releasedAmong others there is a fix for connection leak in jdbc, performanceMain benefits (based on https://github.com/testcontainers/testcontainers-java/releases/tag/1.17.2)</description>
      <version>None</version>
      <fixedVersion>1.16.0,elasticsearch-3.0.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27757" opendate="2022-5-24 00:00:00" fixdate="2022-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elasticsearch connector should not use flink-table-planner but flink-table-planner-loader</summary>
      <description>Connectors should not rely on flink-table-planner but on flink-table-planner-loader by default. We can should change this for the Elasticsearch connector as this is being externalized at the moment</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="27776" opendate="2022-5-25 00:00:00" fixdate="2022-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Throw exception when UDAF used in sliding window does not implement merge method in PyFlink</summary>
      <description>We use the pane state to optimize the result of calculating the window state, which requires udaf to implement the merge method. However, due to the lack of detection of whether the merge method of udaf is implemented, the user's output result did not meet his expectations and there is no exception. Below is an example of a UDAF that implements the merge method:class SumAggregateFunction(AggregateFunction): def get_value(self, accumulator): return accumulator[0] def create_accumulator(self): return [0] def accumulate(self, accumulator, *args): accumulator[0] = accumulator[0] + args[0] def retract(self, accumulator, *args): accumulator[0] = accumulator[0] - args[0] def merge(self, accumulator, accumulators): for other_acc in accumulators: accumulator[0] = accumulator[0] + other_acc[0] def get_accumulator_type(self): return DataTypes.ARRAY(DataTypes.BIGINT()) def get_result_type(self): return DataTypes.BIGINT()</description>
      <version>1.13.6,1.14.4,1.15.0</version>
      <fixedVersion>1.14.5,1.15.1,1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.udf.py</file>
    </fixedFiles>
  </bug>
  <bug id="2779" opendate="2015-9-29 00:00:00" fixdate="2015-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation to reflect new Stream/Window API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.navbar.html</file>
      <file type="M">docs.internals.general.arch.md</file>
      <file type="M">docs.index.md</file>
      <file type="M">docs.apis.streaming.guide.md</file>
      <file type="M">docs.apis.programming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="27818" opendate="2022-5-27 00:00:00" fixdate="2022-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Model enums as references in OpenAPI spec</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.OpenApiSpecGenerator.java</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
    </fixedFiles>
  </bug>
  <bug id="27819" opendate="2022-5-27 00:00:00" fixdate="2022-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate better operationIds for OpenAPI spec</summary>
      <description>There is an easy way to generate operation ids that are significantly better than the defaults.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.MessageHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.savepoints.stop.StopWithSavepointTriggerHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.savepoints.SavepointTriggerHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.savepoints.SavepointDisposalTriggerHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.JobSubmitHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobCancellationHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.dataset.ClusterDataSetDeleteTriggerHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.cluster.ShutdownHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.rescaling.RescalingTriggerHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarUploadHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanGetHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarDeleteHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.AbstractJarPlanHeaders.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.rest.OpenApiSpecGeneratorTest.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.rest.data.TestEmptyMessageHeaders.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.OpenApiSpecGenerator.java</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
    </fixedFiles>
  </bug>
  <bug id="27822" opendate="2022-5-28 00:00:00" fixdate="2022-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate the doc of checkpoint/savepoint guarantees</summary>
      <description>Translate the change of FLINK-26134 </description>
      <version>1.15.0,1.16.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.ops.state.savepoints.md</file>
      <file type="M">docs.content.zh.docs.ops.state.checkpoints.vs.savepoints.md</file>
      <file type="M">docs.content.zh.docs.ops.state.checkpoints.md</file>
      <file type="M">docs.content.zh.docs.concepts.stateful-stream-processing.md</file>
    </fixedFiles>
  </bug>
  <bug id="27837" opendate="2022-5-30 00:00:00" fixdate="2022-12-30 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support statement set in the SQL Gateway</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.AbstractSqlGatewayStatementITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.SessionContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="27865" opendate="2022-6-1 00:00:00" fixdate="2022-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add guide and example for configuring SASL and SSL in Kafka SQL connector document</summary>
      <description>Using SASL and SSL in Kafka connector is a common case and usually quite complex for new users that not quite familiar with the design of Kafka connector, so it would be helpful to add a guidance of how to enable these security options in Kafka connector.</description>
      <version>1.14.4,1.15.0,1.16.0</version>
      <fixedVersion>1.16.0,1.15.2,1.14.6</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.kafka.md</file>
      <file type="M">docs.content.docs.connectors.datastream.kafka.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.kafka.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="27903" opendate="2022-6-6 00:00:00" fixdate="2022-6-6 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce and support HYBRID resultPartitionType</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorBatchExecutionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulingPipelinedRegion.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategyTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.StreamExchangeMode.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.GlobalStreamExchangeMode.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionType.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ExecutionOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.BatchShuffleMode.java</file>
    </fixedFiles>
  </bug>
  <bug id="27904" opendate="2022-6-6 00:00:00" fixdate="2022-7-6 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce HsMemoryDataManager</summary>
      <description>Introduce HsDataBuffer to manage memory data of hybrid shuffle mode.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSpillingStrategyUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSpillingStrategyTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsSelectiveSpillingStrategyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsFullSpillingStrategyTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsMemoryDataSpiller.java</file>
    </fixedFiles>
  </bug>
  <bug id="27905" opendate="2022-6-6 00:00:00" fixdate="2022-7-6 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce HsSpillingStrategy and Implement SelectiveSpillingStrategy、FullSpillingStrategy</summary>
      <description>Introduce HsSpillingStrategy and Implement HsSelectiveSpillingStrategy to support make a spilling decision for hybrid shuffle.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartitionReadSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HybridShuffleConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="27907" opendate="2022-6-6 00:00:00" fixdate="2022-7-6 01:00:00" resolution="Done">
    <buginformation>
      <summary>Implement disk read and write logic for hybrid shuffle</summary>
      <description>Implement disk read and write logic for hybrid shuffle. In order to access the disk as sequentially as possible, Introduce HsMemoryDataSpiller, HsResultPartitionReadScheduler and HsSubpartitionFileReader.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.BufferReaderWriterUtilTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BufferReaderWriterUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="27944" opendate="2022-6-7 00:00:00" fixdate="2022-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>IO metrics collision happens if a task has union inputs</summary>
      <description>When a task has union inputs, some IO metrics(numBytesIn* and numBuffersIn*) of the different inputs may collide and failed to be registered. The problem can be reproduced with a simple job like:DataStream&lt;String&gt; source1 = env.fromElements("abc");DataStream&lt;String&gt; source2 = env.fromElements("123");source1.union(source2).print(); Logs of collisions:2022-06-08 00:59:01,629 WARN org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'numBytesInLocal'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]2022-06-08 00:59:01,629 WARN org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'numBytesInLocalPerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]2022-06-08 00:59:01,629 WARN org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'numBytesInLocal'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]2022-06-08 00:59:01,629 WARN org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'numBytesInLocalPerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]2022-06-08 00:59:01,630 WARN org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'numBytesInRemote'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]2022-06-08 00:59:01,630 WARN org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'numBytesInRemotePerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]2022-06-08 00:59:01,630 WARN org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'numBytesInRemote'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]2022-06-08 00:59:01,630 WARN org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'numBytesInRemotePerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]2022-06-08 00:59:01,630 WARN org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'numBuffersInLocal'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]2022-06-08 00:59:01,630 WARN org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'numBuffersInLocalPerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]2022-06-08 00:59:01,630 WARN org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'numBuffersInLocal'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]2022-06-08 00:59:01,630 WARN org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'numBuffersInLocalPerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]2022-06-08 00:59:01,630 WARN org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'numBuffersInRemote'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]2022-06-08 00:59:01,630 WARN org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'numBuffersInRemotePerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]2022-06-08 00:59:01,630 WARN org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'numBuffersInRemote'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]2022-06-08 00:59:01,630 WARN org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'numBuffersInRemotePerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]</description>
      <version>1.15.0</version>
      <fixedVersion>1.17.0,1.16.1,1.15.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkBenchmarkEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NettyShuffleEnvironmentTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NettyShuffleEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="27955" opendate="2022-6-8 00:00:00" fixdate="2022-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink installation failure on Windows OS</summary>
      <description>Because pemja doesn't support windows os, it makes installation failed in windows os in release-1.15. We need to fix it asap.</description>
      <version>1.15.0</version>
      <fixedVersion>1.15.1,1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
    </fixedFiles>
  </bug>
  <bug id="27976" opendate="2022-6-9 00:00:00" fixdate="2022-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[WebUi] Allow order by jobname</summary>
      <description>Allow to order jobs (running and canceled) by job name</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-list.job-list.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-list.job-list.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="28003" opendate="2022-6-10 00:00:00" fixdate="2022-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive sql is wrongly modified by SqlCompleter in SQL client when using -f {file}</summary>
      <description>When I run the following sql in SqlClient using 'sql-client.sh -f zj_test.sql'create table if not exists db.zj_test(pos                   int,rank_cmd              string)partitioned by (`p_date` string,`p_hourmin` string);INSERT OVERWRITE TABLE db.zj_test PARTITION (p_date='20220605', p_hourmin = '0100')SELECTpos ,rank_cmdFROM db.sourceTwhere p_date = '20220605' and p_hourmin = '0100'; An error would be thrown out because the 'pos' field is changed to 'POSITION'. I guess `SqlCompleter` in sqlClient module might do something wrong.The error could be reproduced using the attached file.  </description>
      <version>1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.utils.SqlParserHelper.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="28004" opendate="2022-6-10 00:00:00" fixdate="2022-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce CI heap space by 25%</summary>
      <description>I've recently seen several builds where docker crashes again with out of memory errors.Our current memory configuration is a bit optimistic and relies on all forks not hitting the upper limit.If we'd reduce the heap space by 25% everything could run at maximum without memory running out.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="28005" opendate="2022-6-10 00:00:00" fixdate="2022-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce network memory in UnalignedCheckpointITCase</summary>
      <description>This test has some cases where it spawns 25 task managers, each allocation 64mb for the network buffer pool.This is likely way more than necessary.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="28006" opendate="2022-6-10 00:00:00" fixdate="2022-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run architecture tests in single JVM</summary>
      <description>The architecture tests, in particular the production ones, load a lot of classes into memory (~700mb). To prevent this from failing in the future I propose to run these tests in a single JVM that gets the entire memory budget.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2802" opendate="2015-10-2 00:00:00" fixdate="2015-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Watermark triggered operators cannot progress with cyclic flows</summary>
      <description>The problem is that we can easily create a cyclic watermark (time) dependency in the stream graph which will result in a deadlock for watermark triggered operators such as the `WindowOperator`.A solution to this could be to emit a Long.MAX_VALUE watermark from the iteration sources.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamMockEnvironment.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamIterationHead.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.io.StreamInputProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="28083" opendate="2022-6-15 00:00:00" fixdate="2022-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PulsarSource cannot work with object-reusing DeserializationSchema.</summary>
      <description>This issue is the same as Kafka's https://issues.apache.org/jira/browse/FLINK-25132</description>
      <version>1.14.4,1.15.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.test.java.org.apache.flink.connector.pulsar.source.enumerator.cursor.StopCursorTest.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarUnorderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarUnorderedSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarSourceReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarOrderedSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.PulsarSourceReaderFactory.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.message.PulsarMessageCollector.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.message.PulsarMessage.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.fetcher.PulsarUnorderedFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.fetcher.PulsarOrderedFetcherManager.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.fetcher.PulsarFetcherManagerBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.emitter.PulsarRecordEmitter.java</file>
    </fixedFiles>
  </bug>
  <bug id="28084" opendate="2022-6-15 00:00:00" fixdate="2022-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pulsar unordered reader should disable retry and delete reconsume logic.</summary>
      <description>UnroderdPulsarSourceReader currently calls reconsume, but this feature relys on retry topic. But if retry topic is enabled the initial search will only support earliest and lates (because it will be a multiconsumer impl). We plan to delete the reconsume logic to get rid of dependency on retry topic and should disable retry.</description>
      <version>1.14.4,1.15.0</version>
      <fixedVersion>1.16.0,1.14.6,1.15.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarUnorderedPartitionSplitReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="28085" opendate="2022-6-15 00:00:00" fixdate="2022-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Close all the pending Pulsar transactions when flink shutdown the pipeline.</summary>
      <description>Currently transactionId is not persisted. After a job restart we lose handle to the transaction which is still not aborted in Pulsar broker. Pulsar broker will abort these hanging transactions after a timeout but this is not desirable. We need to close all the pending transactionId.</description>
      <version>1.14.4,1.15.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.split.PulsarUnorderedPartitionSplitReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarUnorderedSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarSourceReaderBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.source.PulsarOrderedSourceReader.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.source.reader.fetcher.PulsarFetcherManagerBase.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.utils.PulsarTransactionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="28089" opendate="2022-6-16 00:00:00" fixdate="2022-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive dialect support "tablesample (xx rows)"</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="2809" opendate="2015-10-2 00:00:00" fixdate="2015-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DataSet[Unit] doesn&amp;#39;t work</summary>
      <description>The following code creates a DataSet[Unit]: val env = ExecutionEnvironment.createLocalEnvironment() val a = env.fromElements(1,2,3) val b = a.map (_ =&gt; ()) b.writeAsText("/tmp/xxx") env.execute()This doesn't work, because a VoidSerializer is created, which can't cope with a BoxedUnit. See exception below.I'm now thinking about creating a UnitSerializer class.org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1.applyOrElse(JobManager.scala:314) at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:36) at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:29) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at org.apache.flink.runtime.ActorLogMessages$$anon$1.applyOrElse(ActorLogMessages.scala:29) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:92) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.ClassCastException: scala.runtime.BoxedUnit cannot be cast to java.lang.Void at org.apache.flink.api.common.typeutils.base.VoidSerializer.serialize(VoidSerializer.java:26) at org.apache.flink.runtime.plugable.SerializationDelegate.write(SerializationDelegate.java:51) at org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.addRecord(SpanningRecordSerializer.java:76) at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:83) at org.apache.flink.runtime.operators.shipping.OutputCollector.collect(OutputCollector.java:65) at org.apache.flink.runtime.operators.chaining.ChainedMapDriver.collect(ChainedMapDriver.java:78) at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:177) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:564) at java.lang.Thread.run(Thread.java:745)</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.types.TypeInformationGenTest.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.codegen.TypeInformationGen.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.codegen.TypeDescriptors.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.codegen.TypeAnalyzer.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.CollectionInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="28090" opendate="2022-6-16 00:00:00" fixdate="2022-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support attachAsDatastream in Python Table API</summary>
      <description>Implement attachAsDatastream. A pull request is submitted as this issue is created.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.util.java.utils.py</file>
      <file type="M">flink-python.pyflink.table.statement.set.py</file>
      <file type="M">docs.content.docs.dev.table.data.stream.api.md</file>
      <file type="M">docs.content.zh.docs.dev.table.data.stream.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="28136" opendate="2022-6-20 00:00:00" fixdate="2022-7-20 01:00:00" resolution="Done">
    <buginformation>
      <summary>Implement ExecutionTimeBasedSlowTaskDetector</summary>
      <description>In the first version of speculative execution, an ExecutionTimeBasedSlowTaskDetector will be used to detect slow tasks. For ExecutionTimeBasedSlowTaskDetector, if a task's execution time is much longer than that of most tasks of the same JobVertex, the task will be identified as slow. More specifically, it will compute an execution time baseline for each JobVertex. Tasks which execute longer than or equals to the baseline will be identified as slow tasks.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.scheduling.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="28138" opendate="2022-6-20 00:00:00" fixdate="2022-7-20 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add metrics for speculative execution</summary>
      <description>Following two metrics will be added to expose job problems and show the effectiveness of speculative execution: numSlowExecutionVertices: Number of slow execution vertices at the moment. numEffectiveSpeculativeExecutions: Number of speculative executions which finish before their corresponding original executions finish.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptivebatch.SpeculativeSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.SpeculativeScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricNames.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.SpeculativeExecutionVertex.java</file>
    </fixedFiles>
  </bug>
  <bug id="28139" opendate="2022-6-20 00:00:00" fixdate="2022-8-20 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add documentation for speculative execution</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.metrics.md</file>
      <file type="M">docs.content.zh.docs.ops.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="28147" opendate="2022-6-20 00:00:00" fixdate="2022-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update httplib2 to at least 0.19.0</summary>
      <description>We should update httplib2 to at least 0.19.0 to address CVE-2021-21240 and avoid false flags about Flink being vulnerable.</description>
      <version>1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
    </fixedFiles>
  </bug>
  <bug id="28161" opendate="2022-6-21 00:00:00" fixdate="2022-8-21 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce the session related API for REST endpoint</summary>
      <description>It includes openSession, closeSession and configure session. Please refer to FLIP-91 for API details.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.SqlGatewayRestEndpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="28162" opendate="2022-6-21 00:00:00" fixdate="2022-8-21 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce the operation related API for REST endpoint</summary>
      <description>It includes getOperationStatus, cancelOperation, closeOperation.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.SqlGatewayRestEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.HttpMethodWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="28222" opendate="2022-6-23 00:00:00" fixdate="2022-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add sql-csv/json modules</summary>
      <description>For consistency and maintainability the csv/json formats should have a dedicated sql-jar module.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-csv.pom.xml</file>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">flink-formats.flink-json.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="28246" opendate="2022-6-24 00:00:00" fixdate="2022-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Store classifier in dependency</summary>
      <description>The classifier is required to properly differentiate between jars and test-jars.In the future we could also improve the accuracy of the notice checker (which currently doesn't care about classifiers).</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.flink-ci-tools.src.test.java.org.apache.flink.tools.ci.utils.notice.NoticeParserTest.java</file>
      <file type="M">tools.ci.flink-ci-tools.src.test.java.org.apache.flink.tools.ci.utils.dependency.DependencyParserTreeTest.java</file>
      <file type="M">tools.ci.flink-ci-tools.src.test.java.org.apache.flink.tools.ci.utils.dependency.DependencyParserCopyTest.java</file>
      <file type="M">tools.ci.flink-ci-tools.src.test.java.org.apache.flink.tools.ci.licensecheck.NoticeFileCheckerTest.java</file>
      <file type="M">tools.ci.flink-ci-tools.src.main.java.org.apache.flink.tools.ci.utils.shared.Dependency.java</file>
      <file type="M">tools.ci.flink-ci-tools.src.main.java.org.apache.flink.tools.ci.utils.notice.NoticeParser.java</file>
      <file type="M">tools.ci.flink-ci-tools.src.main.java.org.apache.flink.tools.ci.utils.dependency.DependencyParser.java</file>
      <file type="M">tools.ci.flink-ci-tools.src.main.java.org.apache.flink.tools.ci.licensecheck.NoticeFileChecker.java</file>
    </fixedFiles>
  </bug>
  <bug id="28296" opendate="2022-6-29 00:00:00" fixdate="2022-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive&amp;#39;s UDAF which implement GenericUDAFResolver</summary>
      <description>Some Hive's udaf implementt GenericUDAFResolver, but Flink only support the function implement UDAF and GenericUDAFResolver2</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDAFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveGenericUDAF.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.factories.HiveFunctionDefinitionFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="28298" opendate="2022-6-29 00:00:00" fixdate="2022-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support left and right built-in function in the Table API</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.converter.DirectConvertRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.BaseExpressions.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.expression.py</file>
      <file type="M">flink-python.pyflink.table.expression.py</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="28324" opendate="2022-6-30 00:00:00" fixdate="2022-10-30 01:00:00" resolution="Done">
    <buginformation>
      <summary>JUnit5 Migration] Module: flink-sql-client</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.SqlClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectBatchResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.ChangelogCollectResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.utils.SqlParserHelper.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliUtilsTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliTableauResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientMultiStatementITTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="28347" opendate="2022-7-1 00:00:00" fixdate="2022-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update testcontainers dependency to v1.17.6</summary>
      <description>Changelog: https://github.com/testcontainers/testcontainers-java/releases/tag/1.17.6Main benefits for Flink: Elasticsearch and Pulsar improvements</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="28353" opendate="2022-7-1 00:00:00" fixdate="2022-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude unschedulable nodes using IP addresses of kubernetes nodes</summary>
      <description>when the job is submitted to the k8s cluster and the parameter -Dkubernetes.rest-service.exposed.type=NodePort is used, the web ui address of the job obtained at this time is the IP of any machine in the k8s cluster.but client will throw connect refuse exception when the node's schedule status is unschedulable. We should exclude those node IPs to choose web ui address</description>
      <version>1.14.4,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesClientTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClientTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.services.NodePortService.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.services.LoadBalancerService.java</file>
    </fixedFiles>
  </bug>
  <bug id="28354" opendate="2022-7-1 00:00:00" fixdate="2022-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support instr and locate bulit-in function in Table API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.converter.DirectConvertRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.BaseExpressions.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.expression.py</file>
      <file type="M">flink-python.pyflink.table.expression.py</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="28357" opendate="2022-7-1 00:00:00" fixdate="2022-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Watermark issue when recovering Finished sources</summary>
      <description>Copied mostly from email trail on the flink user mailing list:I done a lot of experimentation and I’m convinced there is a problem with Flink handling Finished sources and recovery. The program consists of: Two sources: One “Long Running Source” – stays alive and emits a watermark of DateTime.now() every 10 seconds. Prints the console a message saying the watermark has been emitted. Throws an exception every 5 or 10 iterations to force a recovery. One “Short Lived Source” – emits a Long.MAX_VALUE watermark, prints a message to the console and returns. The “Short Live Source” feeds into a map() and then it joins with the “Long Running Source” with a KeyedCoProcessFunction. Moves to “FINISHED” state by Flink.The problem here is that the “Join” receives no Long.MAX_VALUE watermark from the map() in some situations after a recovery. The dashboard goes from showing this:To the below after a recovery (with the currentInput1/2Watermark metrics showing input 2 having not received a watermark from the map, saying –Long.MAX_VALUE):The program is currently set to checkpoint every 5 seconds. By experimenting with 70 seconds, it seems that if only one checkpoint has been taken with the “Short Lived Source” in a FINISHED state since the last recovery then everything works fine and the restarted “Short Lived Source” emits its watermark and I see the “ShortedLivedEmptySource emitting Long.MAX_VALUE watermark” message on the console meaning the run() definitely executed. However, I found that if 2 or more checkpoints are taken since the last recovery with the source in a FINISHED state then the console message does not appear and the watermark is not emitted.To repeat – the Join does not get a Long.MAX_VALUE watermark from my source or Flink if I see two or more checkpoints logged in between recoveries. If zero or checkpoints are made, everything is fine – the join gets the watermark and I see my console message. You can play with the checkpointing frequency as per the code comments:        // Useful checkpoint interval options:        //    5 - see the problem after the first recovery        //   70 - useful to see bad behaviour kick in after a recovery or two        //  120 - won't see the problem as we don't have 2 checkpoints within a single recovery sessionIf I merge the Triggering/Completed checkpoint messages in the log with my console output I see something like this clearly showing the “Short Lived Source” run() method is not executed after 2 checkpoints with the operators marked as FINISHED: 2022-06-29T11:52:31.268Z: ShortLivedEmptySource emitting Long.MAX_VALUE watermark.2022-06-29T11:52:31.293Z: LongRunningSource emitting initial watermark=16565035512682022-06-29T11:52:41.302Z: LongRunningSource emitting loop watermark=16565035613022022-06-29T11:52:51.302Z: LongRunningSource emitting loop watermark=16565035713022022-06-29T11:53:01.303Z: LongRunningSource emitting loop watermark=16565035813032022-06-29 11:53:02.772 INFO  &amp;#91;Checkpoint Timer&amp;#93; o.a.f.r.c.CheckpointCoordinator           Triggering checkpoint 1 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD})2022-06-29 11:53:02.870 INFO  &amp;#91;jobmanager-io-thread-10&amp;#93; o.a.f.r.c.CheckpointCoordinator    Completed checkpoint 1 for job 877656d7752bc1304c2cb92790e6aefb2022-06-29T11:53:11.303Z: LongRunningSource emitting loop watermark=16565035913032022-06-29T11:53:21.304Z: LongRunningSource emitting loop watermark=16565036013042022-06-29T11:53:21.304Z: ------------------ Recovery ------------------2022-06-29T11:53:22.405Z: LongRunningSource emitting initial watermark=16565036024052022-06-29T11:53:22.408Z: ShortLivedEmptySource emitting Long.MAX_VALUE watermark.2022-06-29T11:53:32.406Z: LongRunningSource emitting loop watermark=16565036124062022-06-29T11:53:42.406Z: LongRunningSource emitting loop watermark=16565036224062022-06-29 11:53:51.048 INFO  &amp;#91;Checkpoint Timer&amp;#93; o.a.f.r.c.CheckpointCoordinator           Triggering checkpoint 2 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD})2022-06-29 11:53:51.067 INFO  &amp;#91;jobmanager-io-thread-4&amp;#93; o.a.f.r.c.CheckpointCoordinator     Completed checkpoint 2 for job 877656d7752bc1304c2cb92790e6aefb2022-06-29T11:53:52.407Z: LongRunningSource emitting loop watermark=16565036324072022-06-29T11:54:02.407Z: LongRunningSource emitting loop watermark=16565036424072022-06-29T11:54:12.408Z: LongRunningSource emitting loop watermark=16565036524082022-06-29T11:54:22.408Z: LongRunningSource emitting loop watermark=16565036624082022-06-29T11:54:32.409Z: LongRunningSource emitting loop watermark=16565036724092022-06-29T11:54:42.409Z: LongRunningSource emitting loop watermark=16565036824092022-06-29T11:54:52.410Z: LongRunningSource emitting loop watermark=16565036924102022-06-29 11:55:01.048 INFO  &amp;#91;Checkpoint Timer&amp;#93; o.a.f.r.c.CheckpointCoordinator           Triggering checkpoint 3 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD})2022-06-29 11:55:01.057 INFO  &amp;#91;jobmanager-io-thread-10&amp;#93; o.a.f.r.c.CheckpointCoordinator    Completed checkpoint 3 for job 877656d7752bc1304c2cb92790e6aefb2022-06-29T11:55:02.410Z: LongRunningSource emitting loop watermark=16565037024102022-06-29T11:55:02.411Z: ------------------ Recovery ------------------2022-06-29T11:55:03.445Z: LongRunningSource emitting initial watermark=1656503703444       &lt;&lt;&lt;&lt;&lt; NO “ShortLivedEmptySource” message after recovery2022-06-29T11:55:13.446Z: LongRunningSource emitting loop watermark=16565037134452022-06-29T11:55:23.446Z: LongRunningSource emitting loop watermark=16565037234462022-06-29T11:55:33.446Z: LongRunningSource emitting loop watermark=1656503733446 I have also attached a longer example with shows everything working fine after 5 recoveries, and then breaking after the 6th.I am guessing here it has something to do with the checkpointing and recovery of a FINISHED source.Finally, here are some ways that allows the code to work: Change the code so the “Short Lived Source” doesn’t return from run() and stays RUNNING (uncomment the Thread.sleep) As I mentioned before, if I remove the map() operator the problem in the join also goes away. (I don’t see the console output but the join is happy) Use a long enough checkpoint interval (e.g. 120 seconds) so we don’t have two checkpoints with FINISHED state per recovery.The fact these changes prevent the issue means I really think there’s some bug or inconsistency here – if somebody could explain I would really appreciate it. </description>
      <version>1.15.0</version>
      <fixedVersion>1.16.0,1.15.2,1.14.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamNode.java</file>
    </fixedFiles>
  </bug>
  <bug id="28360" opendate="2022-7-2 00:00:00" fixdate="2022-10-2 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support stop job statement in SQL client</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.utils.ParserResource.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.context.ExecutionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="28369" opendate="2022-7-4 00:00:00" fixdate="2022-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support PARSE_URL bulit-in function in Table API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.converter.DirectConvertRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.BaseExpressions.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.expression.py</file>
      <file type="M">flink-python.pyflink.table.expression.py</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="28373" opendate="2022-7-4 00:00:00" fixdate="2022-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Read a full buffer of data per file IO read request for sort-shuffle</summary>
      <description>Currently, for sort blocking shuffle, the corresponding data readers read shuffle data in buffer granularity. Before compression, each buffer is 32K by default, after compression the size will become smaller (may less than 10K). For file IO, this is pretty smaller. To achieve better performance and reduce IOPS, we can read more data per IO read request and parse buffer header and data in memory.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriteReadTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.CompositeBuffer.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SortMergeSubpartitionReaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.ReadOnlySlicedBufferTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeSubpartitionReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFileReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyMessage.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.ReadOnlySlicedNetworkBuffer.java</file>
    </fixedFiles>
  </bug>
  <bug id="28376" opendate="2022-7-4 00:00:00" fixdate="2022-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Restrict the number of threads for sort-shuffle data read</summary>
      <description>Currently, the number of IO threads for shuffle data reading is relevant to the size of reading memory and the number of CPU cores. We should also consider the number of slots and the number of disks.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SharedPoolNettyShuffleServiceFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NettyShuffleEnvironmentBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.ShuffleEnvironmentContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NettyShuffleServiceFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="28377" opendate="2022-7-4 00:00:00" fixdate="2022-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Decrease the memory size per request for sort-shuffle data read from 8M to 4M</summary>
      <description>Currently, for sort blocking shuffle, the corresponding data reader always allocate a fixed size of 8M buffers for shuffle data reading. FLINK-28373 can increase buffer utilization, after which, we can reduce the buffer size per request to reduce the time waiting for buffers. (This change is guarded by TPC-DS test that there is no performance regression)</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.BatchShuffleReadBufferPoolTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.BatchShuffleReadBufferPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="28386" opendate="2022-7-5 00:00:00" fixdate="2022-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Trigger an immediate checkpoint after all tasks has reached end-of-data</summary>
      <description>Currently for bounded job in streaming mode, by default it will wait for one more checkpoint to commit the last piece of data. If the checkpoint period is long, the waiting time might also be long. to optimize this situation, we could eagerly trigger a checkpoint after tasks have reached end-of-data.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamMockEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TestingTaskManagerActions.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.NoOpTaskManagerActions.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestTaskManagerActions.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.TestingSchedulerNG.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DummyEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.TestingJobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.TaskManagerActions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.RuntimeEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerNG.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.Environment.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointEnvironment.java</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.checkpointing.md</file>
    </fixedFiles>
  </bug>
  <bug id="28475" opendate="2022-7-9 00:00:00" fixdate="2022-1-9 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>kafka connector won&amp;#39;t stop when the stopping offset is zero</summary>
      <description>when use kafka connector in bounded mode,and the stopping offset hapends to be 0,the kafka connector won't stop,which is not expected.I had traced the code, and found the stopping offset will be set to empty when it is zero, and an empty stopping offset means no stopping offset when serialized. This leads to a wrong execution.I had fixed this in my personal branch,now I am logging this issue in Jira so that I can make merge request.</description>
      <version>1.15.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReaderTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.source.split.KafkaPartitionSplit.java</file>
    </fixedFiles>
  </bug>
  <bug id="28508" opendate="2022-7-12 00:00:00" fixdate="2022-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support SPLIT_INDEX and STR_TO_MAP built-in function in Table API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.converter.DirectConvertRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.BaseExpressions.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.expression.py</file>
      <file type="M">flink-python.pyflink.table.expression.py</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="28509" opendate="2022-7-12 00:00:00" fixdate="2022-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support REVERSE built-in function in Table API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.expressions.converter.DirectConvertRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.BaseExpressions.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.expression.py</file>
      <file type="M">flink-python.pyflink.table.expression.py</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug id="28617" opendate="2022-7-21 00:00:00" fixdate="2022-12-21 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support stop job statement in SqlGatewayService</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.TestUtils.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.utils.Constants.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="28636" opendate="2022-7-21 00:00:00" fixdate="2022-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add utility to test POJO compliance</summary>
      <description>Users should be encouraged to eagerly verify that their POJOs satisfy all the requirements that Flink imposes, however we provide no convenient way to test that.They currently have to resort to something like below, which isn't obvious at all:TypeSerializer&lt;Event&gt; eventSerializer = TypeInformation.of(Event.class).createSerializer(new ExecutionConfig());assertThat(eventSerializer).isInstanceOf(PojoSerializer.class);</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.java</file>
      <file type="M">docs.content.docs.dev.datastream.fault-tolerance.serialization.types.serialization.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.fault-tolerance.serialization.types.serialization.md</file>
    </fixedFiles>
  </bug>
  <bug id="28655" opendate="2022-7-24 00:00:00" fixdate="2022-1-24 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support show jobs statement in SqlGatewayService</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.utils.Constants.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="28658" opendate="2022-7-24 00:00:00" fixdate="2022-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for job lifecycle statements</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.17.0,1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.show.md</file>
      <file type="M">docs.content.docs.dev.table.sql.jar.md</file>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.docs.dev.table.sql-gateway.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.show.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.jar.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="28696" opendate="2022-7-26 00:00:00" fixdate="2022-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Notify all newlyAdded/Merged blocked nodes to BlocklistListener</summary>
      <description>This bug was introduced by FLINK-28660. Our newly added logic results in that blocklist listener will not be notified when there are no newly added nodes (only merge nodes) 。 </description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blocklist.DefaultBlocklistHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blocklist.DefaultBlocklistHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="28743" opendate="2022-7-29 00:00:00" fixdate="2022-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support validating the determinism for StreamPhysicalMatchRecognize</summary>
      <description>MatchRecognize has complex expressions and is not commonly used in traditional SQLs, so mark this as a minor issue (for 1.16)</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.NonDeterministicDagTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.NonDeterministicDagTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.optimize.StreamNonDeterministicUpdatePlanVisitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="28744" opendate="2022-7-29 00:00:00" fixdate="2022-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Calcite version to 1.31</summary>
      <description>We should upgrade to Calcite 1.31 so we can benefit from https://issues.apache.org/jira/browse/CALCITE-4865</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.subquery.SubqueryCorrelateVariablesValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.subquery.SubqueryCorrelateVariablesValidationTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushFilterInCalcIntoTableSourceRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.hints.batch.ShuffleMergeJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.hints.batch.ShuffleHashJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.hints.batch.NestLoopJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.hints.batch.BroadcastJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.tools.RelBuilder.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql.validate.SqlValidatorImpl.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql2rel.SqlToRelConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql2rel.RelDecorrelator.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.logical.LogicalWindow.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.logical.LogicalValues.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.logical.LogicalUnion.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.logical.LogicalSort.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.logical.LogicalMinus.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.logical.LogicalIntersect.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.logical.LogicalFilter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.hint.HintPredicates.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.core.Window.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.core.Values.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.core.Union.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.core.Sort.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.core.Snapshot.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.core.SetOp.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.core.Minus.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.core.Intersect.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.rel.core.Filter.java</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-calcite-bridge.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkDDLDataTypeTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.validate.FlinkSqlConformance.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.pom.xml</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="28759" opendate="2022-8-1 00:00:00" fixdate="2022-8-1 01:00:00" resolution="Done">
    <buginformation>
      <summary>Enable speculative execution for in AdaptiveBatchScheduler TPC-DS e2e tests</summary>
      <description>To verify the correctness of speculative execution, we can enabled it in AdaptiveBatchScheduler TPC-DS e2e tests, which runs a lot of different batch jobs and verifies the result.Note that we need to disable the blocklist (by setting block duration to 0) in such single machine e2e tests.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="28807" opendate="2022-8-4 00:00:00" fixdate="2022-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Various components don&amp;#39;t respect schema lifecycle</summary>
      <description>A surprising number of components never call (De)SerializationSchema#open making life very difficult for people who want to make use of said method.</description>
      <version>1.15.0</version>
      <fixedVersion>1.16.0,elasticsearch-3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.connector.upserttest.sink.UpsertTestSink.java</file>
      <file type="M">flink-python.pyflink.pyflink.gateway.server.py</file>
      <file type="M">flink-python.pyflink.datastream.connectors.tests.test.kafka.py</file>
      <file type="M">flink-python.pyflink.common.tests.test.serialization.schemas.py</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.utils.SerializationSchemaMatcher.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.utils.DeserializationSchemaMatcher.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.ogg.OggJsonSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.maxwell.MaxwellJsonSerDerTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonRowSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonRowDataSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.debezium.DebeziumJsonSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.canal.CanalJsonSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.ogg.OggJsonSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.ogg.OggJsonDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.maxwell.MaxwellJsonSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.maxwell.MaxwellJsonDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.debezium.DebeziumJsonSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.debezium.DebeziumJsonDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.canal.CanalJsonSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.canal.CanalJsonDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvRowDeSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvRowDataSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-csv.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.JSONKeyValueDeserializationSchemaTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.connector.kafka.sink.DefaultKafkaSinkContext.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.RowElasticsearchSinkFunction.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkFunction.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.connector.elasticsearch.table.RowElasticsearchEmitter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.src.main.java.org.apache.flink.connector.kinesis.sink.KinesisStreamsSinkElementConverter.java</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.src.main.java.org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkElementConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="28820" opendate="2022-8-5 00:00:00" fixdate="2022-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pulsar Connector PulsarSink performance issue when delivery guarantee is not NONE</summary>
      <description>Pulsar Sink writes at a speed of a dozen messages per second when At-Least-Once and Exactly-Once are enabled. When None is used, the message write speed reaches the Pulsar write bottleneck.</description>
      <version>1.15.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.writer.PulsarWriter.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.PulsarSinkOptions.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.config.SinkConfiguration.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.sink.config.PulsarSinkConfigUtils.java</file>
      <file type="M">docs.layouts.shortcodes.generated.pulsar.sink.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="28821" opendate="2022-8-5 00:00:00" fixdate="2022-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adjust join cost for dpp query pattern which could help more plans use dpp</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.DynamicPartitionPruningRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.physical.batch.DynamicPartitionPruningRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.physical.batch.DynamicPartitionPruningRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="28841" opendate="2022-8-5 00:00:00" fixdate="2022-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document dynamic property support for startup scripts</summary>
      <description>The support for dynamic properties in startup scripts isn't documented anywhere.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.overview.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.docker.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.standalone.docker.md</file>
    </fixedFiles>
  </bug>
  <bug id="29198" opendate="2022-9-5 00:00:00" fixdate="2022-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RetryExtension doesn&amp;#39;t make the test fail if the retries are exhausted</summary>
      <description>FLINK-24627 introduced retry functionality for JUnit5-based tests. It appears that the retry mechanism doesn't have the desired behavior: If the retries are exhausted without the test ever succeeding will result in the test being ignored. I would expect the test to fail in that case. Otherwise, a CI run would succeed without anyone noticing the malfunctioning of the test.</description>
      <version>1.15.0,1.16.0,1.17.0</version>
      <fixedVersion>1.17.0,1.15.3,1.16.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.test.java.org.apache.flink.testutils.junit.RetryOnFailureExtensionTest.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.test.java.org.apache.flink.testutils.junit.RetryOnExceptionExtensionTest.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.testutils.junit.extensions.retry.strategy.RetryOnExceptionStrategy.java</file>
    </fixedFiles>
  </bug>
  <bug id="29347" opendate="2022-9-20 00:00:00" fixdate="2022-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failed to restore from list state with empty protobuf object</summary>
      <description>I use protobuf generated class in an union list state.When my flink job restores from checkpoint, I get exception:Caused by: java.lang.RuntimeException: Could not create class com.MY_PROTOBUF_GENERATED_CLASS at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:76) ~[my-lib-0.1.1-SNAPSHOT.jar:?] at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:40) ~[my-lib-0.1.1-SNAPSHOT.jar:?] at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:679) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:354) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.state.OperatorStateRestoreOperation.deserializeOperatorStateValues(OperatorStateRestoreOperation.java:217) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.state.OperatorStateRestoreOperation.restore(OperatorStateRestoreOperation.java:188) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.state.DefaultOperatorStateBackendBuilder.build(DefaultOperatorStateBackendBuilder.java:80) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createOperatorStateBackend(EmbeddedRocksDBStateBackend.java:482) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$operatorStateBackend$0(StreamTaskStateInitializerImpl.java:277) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.operatorStateBackend(StreamTaskStateInitializerImpl.java:286) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:174) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:109) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:711) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:687) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292] Caused by: com.esotericsoftware.kryo.KryoException: java.io.EOFException: No more bytes left. at org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.readBytes(NoFetchingInput.java:128) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.esotericsoftware.kryo.io.Input.readBytes(Input.java:314) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:73) ~[my-lib-0.1.1-SNAPSHOT.jar:?] at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:40) ~[my-lib-0.1.1-SNAPSHOT.jar:?] at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:679) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:354) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.state.OperatorStateRestoreOperation.deserializeOperatorStateValues(OperatorStateRestoreOperation.java:217) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.state.OperatorStateRestoreOperation.restore(OperatorStateRestoreOperation.java:188) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.state.DefaultOperatorStateBackendBuilder.build(DefaultOperatorStateBackendBuilder.java:80) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createOperatorStateBackend(EmbeddedRocksDBStateBackend.java:482) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$operatorStateBackend$0(StreamTaskStateInitializerImpl.java:277) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.operatorStateBackend(StreamTaskStateInitializerImpl.java:286) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:174) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:109) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:711) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:687) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292] Caused by: java.io.EOFException: No more bytes left. at org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.readBytes(NoFetchingInput.java:128) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.esotericsoftware.kryo.io.Input.readBytes(Input.java:314) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:73) ~[my-lib-0.1.1-SNAPSHOT.jar:?] at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:40) ~[my-lib-0.1.1-SNAPSHOT.jar:?] at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:679) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:354) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.state.OperatorStateRestoreOperation.deserializeOperatorStateValues(OperatorStateRestoreOperation.java:217) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.state.OperatorStateRestoreOperation.restore(OperatorStateRestoreOperation.java:188) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.state.DefaultOperatorStateBackendBuilder.build(DefaultOperatorStateBackendBuilder.java:80) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createOperatorStateBackend(EmbeddedRocksDBStateBackend.java:482) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$operatorStateBackend$0(StreamTaskStateInitializerImpl.java:277) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.operatorStateBackend(StreamTaskStateInitializerImpl.java:286) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:174) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:109) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:711) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:687) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.12-1.14.4.jar:1.14.4] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292]  I find it is because when protobuf serializer serializes an object, which is built directly with builder without assign any value to field, the serializer will generate a zero length byte[] and then write it into state with content '\0'（indicates zero length data).When recovered from checkpoint, protobuf seralizer deserialize the data. It get length 0, and call InputStream#read(byte[] bytes, int offset, int count) with count = 0.The underlying Input implementation is NoFetchingInput. It will call Inputsteam#read(byte[] bytes, int offset, int count) with count = 0.The InputStream implementation is ByteStateHandleInputStream, It will return -1 as long as no data left in memory，even if count is 0.A simple fix is add check before return -1. If caller reads 0 bytes, it should always return 0 instead of -1.</description>
      <version>1.14.2,1.15.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.memory.ByteStreamStateHandleTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="29531" opendate="2022-10-6 00:00:00" fixdate="2022-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump protoc and protobuf-java dependencies to 3.21.7</summary>
      <description>Bump protoc and protobuf-java dependencies to at least 3.21.7 to avoid false positive scans on Protobuf vulnerabilities</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
      <file type="M">flink-formats.flink-sql-protobuf.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-json-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29532" opendate="2022-10-6 00:00:00" fixdate="2022-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Pulsar dependency to 2.10.1</summary>
      <description>Update the Pulsar dependency to 2.10.1 to benefit of the fixes highlights at https://github.com/apache/pulsar/releases/tag/v2.10.1</description>
      <version>None</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-pulsar.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29803" opendate="2022-10-31 00:00:00" fixdate="2022-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table API Scala APIs lack proper source jars</summary>
      <description></description>
      <version>1.15.0</version>
      <fixedVersion>1.17.0,1.15.3,1.16.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-scala.pom.xml</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="29913" opendate="2022-11-7 00:00:00" fixdate="2022-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shared state would be discarded by mistake when maxConcurrentCheckpoint&gt;1</summary>
      <description>When maxConcurrentCheckpoint&gt;1, the shared state of Incremental rocksdb state backend would be discarded by registering the same name handle. See https://github.com/apache/flink/pull/21050#discussion_r1011061072cc roman </description>
      <version>1.15.0,1.16.0,1.17.0</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.SharedStateRegistry.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.SharedStateRegistryImpl.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StateHandleReuseITCase.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategyTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateUploaderTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateDownloaderTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksSnapshotUtil.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksNativeFullSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksDBSnapshotStrategyBase.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateUploader.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateDownloader.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBRestoreResult.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.SharedStateRegistryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.IncrementalRemoteKeyedStateHandleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SchedulerUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.metadata.CheckpointTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.PlaceholderStreamStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.IncrementalRemoteKeyedStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.IncrementalLocalKeyedStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.IncrementalKeyedStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.changelog.ChangelogStateBackendHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="30250" opendate="2022-11-30 00:00:00" fixdate="2022-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The flame graph type is wrong</summary>
      <description>When the flame graph type is switched from On-CPU to Mixed. It still show the graph of On-CPU.Root cause:When click the other types, the web frontend will call the requestFlameGraph and update the graphType. However, the graphType is the old type during requestFlameGraph. So the graph type show the new type, but the flame graph is the result of old type. code link</description>
      <version>1.15.0,1.16.0,1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.flamegraph.job-overview-drawer-flamegraph.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.flamegraph.job-overview-drawer-flamegraph.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-flamegraph.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.components.flame-graph.flame-graph.component.ts</file>
    </fixedFiles>
  </bug>
  <bug id="31272" opendate="2023-3-1 00:00:00" fixdate="2023-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Duplicate operators appear in the StreamGraph for Python DataStream API jobs</summary>
      <description>For the following job:import argparseimport jsonimport sysimport timefrom typing import Iterable, castfrom pyflink.common import Types, Time, Encoderfrom pyflink.datastream import StreamExecutionEnvironment, ProcessWindowFunction, EmbeddedRocksDBStateBackend, \ PredefinedOptions, FileSystemCheckpointStorage, CheckpointingMode, ExternalizedCheckpointCleanupfrom pyflink.datastream.connectors.file_system import FileSink, RollingPolicy, OutputFileConfigfrom pyflink.datastream.state import ReducingState, ReducingStateDescriptorfrom pyflink.datastream.window import TimeWindow, Trigger, TriggerResult, T, TumblingProcessingTimeWindows, \ ProcessingTimeTriggerclass CountWithProcessTimeoutTrigger(ProcessingTimeTrigger): def __init__(self, window_size: int): self._window_size = window_size self._count_state_descriptor = ReducingStateDescriptor( "count", lambda a, b: a + b, Types.LONG()) @staticmethod def of(window_size: int) -&gt; 'CountWithProcessTimeoutTrigger': return CountWithProcessTimeoutTrigger(window_size) def on_element(self, element: T, timestamp: int, window: TimeWindow, ctx: 'Trigger.TriggerContext') -&gt; TriggerResult: count_state = cast(ReducingState, ctx.get_partitioned_state(self._count_state_descriptor)) count_state.add(1) # print("element arrive:", element, "count_state:", count_state.get(), window.max_timestamp(), # ctx.get_current_watermark()) if count_state.get() &gt;= self._window_size: # 必须fire&amp;purge！！！！ print("fire element count", element, count_state.get(), window.max_timestamp(), ctx.get_current_watermark()) count_state.clear() return TriggerResult.FIRE_AND_PURGE if timestamp &gt;= window.end: count_state.clear() return TriggerResult.FIRE_AND_PURGE else: return TriggerResult.CONTINUE def on_processing_time(self, timestamp: int, window: TimeWindow, ctx: Trigger.TriggerContext) -&gt; TriggerResult: if timestamp &gt;= window.end: return TriggerResult.CONTINUE else: print("fire with process_time:", timestamp) count_state = cast(ReducingState, ctx.get_partitioned_state(self._count_state_descriptor)) count_state.clear() return TriggerResult.FIRE_AND_PURGE def on_event_time(self, timestamp: int, window: TimeWindow, ctx: 'Trigger.TriggerContext') -&gt; TriggerResult: return TriggerResult.CONTINUE def clear(self, window: TimeWindow, ctx: 'Trigger.TriggerContext') -&gt; None: count_state = ctx.get_partitioned_state(self._count_state_descriptor) count_state.clear()def to_dict_map(v): time.sleep(1) dict_value = json.loads(v) return dict_valuedef get_group_key(value, keys): group_key_values = [] for key in keys: one_key_value = 'null' if key in value: list_value = value[key] if list_value: one_key_value = str(list_value[0]) group_key_values.append(one_key_value) group_key = '_'.join(group_key_values) # print("group_key=", group_key) return group_keyclass CountWindowProcessFunction(ProcessWindowFunction[dict, dict, str, TimeWindow]): def __init__(self, uf): self._user_function = uf def process(self, key: str, context: ProcessWindowFunction.Context[TimeWindow], elements: Iterable[dict]) -&gt; Iterable[dict]: result_list = self._user_function.process_after_group_by_function(elements) return result_listif __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument( '--output', dest='output', required=False, help='Output file to write results to.') argv = sys.argv[1:] known_args, _ = parser.parse_known_args(argv) output_path = known_args.output env = StreamExecutionEnvironment.get_execution_environment() # write all the data to one file env.set_parallelism(1) # process time env.get_config().set_auto_watermark_interval(0) state_backend = EmbeddedRocksDBStateBackend(True) state_backend.set_predefined_options(PredefinedOptions.FLASH_SSD_OPTIMIZED) env.set_state_backend(state_backend) config = env.get_checkpoint_config() # config.set_checkpoint_storage(FileSystemCheckpointStorage("hdfs://ha-nn-uri/tmp/checkpoint/")) config.set_checkpoint_storage(FileSystemCheckpointStorage("file:///Users/10030122/Downloads/pyflink_checkpoint/")) config.set_checkpointing_mode(CheckpointingMode.AT_LEAST_ONCE) config.set_checkpoint_interval(5000) config.set_externalized_checkpoint_cleanup(ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION) # define the source data_stream1 = env.from_collection(['{"user_id": ["0"], "goods_id": [0,0]}', '{"user_id": ["1"], "goods_id": [1,0]}', '{"user_id": ["2"], "goods_id": [2,0]}', '{"user_id": ["1"], "goods_id": [3,0]}', '{"user_id": ["2"], "goods_id": [4,0]}', '{"user_id": ["1"], "goods_id": [5,0]}', '{"user_id": ["2"], "goods_id": [6,0]}', '{"user_id": ["1"], "goods_id": [7,0]}', '{"user_id": ["2"], "goods_id": [8,0]}', '{"user_id": ["1"], "goods_id": [9,0]}', '{"user_id": ["2"], "goods_id": [10,0]}', '{"user_id": ["1"], "goods_id": [11,0]}', '{"user_id": ["2"], "goods_id": [12,0]}', '{"user_id": ["1"], "goods_id": [13,0]}', '{"user_id": ["2"], "goods_id": [14,0]}', '{"user_id": ["1"], "goods_id": [15,0]}', '{"user_id": ["2"], "goods_id": [16,0]}', '{"user_id": ["1"], "goods_id": [17,0]}', '{"user_id": ["2"], "goods_id": [18,0]}', '{"user_id": ["1"], "goods_id": [19,0]}', '{"user_id": ["2"], "goods_id": [20,0]}', '{"user_id": ["1"], "goods_id": [21,0]}', '{"user_id": ["2"], "goods_id": [22,0]}', '{"user_id": ["1"], "goods_id": [23,0]}', '{"user_id": ["2"], "goods_id": [24,0]}', '{"user_id": ["1"], "goods_id": [25,0]}', '{"user_id": ["2"], "goods_id": [26,0]}', '{"user_id": ["1"], "goods_id": [27,0]}', '{"user_id": ["2"], "goods_id": [28,0]}', '{"user_id": ["1"], "goods_id": [29,0]}', '{"user_id": ["2"], "goods_id": [30,0]}']) data_stream2 = env.from_collection(['{"user_id": ["0"], "goods_id": [0,0]}', '{"user_id": ["1"], "goods_id": [1,0]}', '{"user_id": ["2"], "goods_id": [2,0]}', '{"user_id": ["1"], "goods_id": [3,0]}', '{"user_id": ["2"], "goods_id": [4,0]}', '{"user_id": ["1"], "goods_id": [5,0]}', '{"user_id": ["2"], "goods_id": [6,0]}', '{"user_id": ["1"], "goods_id": [7,0]}', '{"user_id": ["2"], "goods_id": [8,0]}', '{"user_id": ["1"], "goods_id": [9,0]}', '{"user_id": ["2"], "goods_id": [10,0]}', '{"user_id": ["1"], "goods_id": [11,0]}', '{"user_id": ["2"], "goods_id": [12,0]}', '{"user_id": ["1"], "goods_id": [13,0]}', '{"user_id": ["2"], "goods_id": [14,0]}', '{"user_id": ["1"], "goods_id": [15,0]}', '{"user_id": ["2"], "goods_id": [16,0]}', '{"user_id": ["1"], "goods_id": [17,0]}', '{"user_id": ["2"], "goods_id": [18,0]}', '{"user_id": ["1"], "goods_id": [19,0]}', '{"user_id": ["2"], "goods_id": [20,0]}', '{"user_id": ["1"], "goods_id": [21,0]}', '{"user_id": ["2"], "goods_id": [22,0]}', '{"user_id": ["1"], "goods_id": [23,0]}', '{"user_id": ["2"], "goods_id": [24,0]}', '{"user_id": ["1"], "goods_id": [25,0]}', '{"user_id": ["2"], "goods_id": [26,0]}', '{"user_id": ["1"], "goods_id": [27,0]}', '{"user_id": ["2"], "goods_id": [28,0]}', '{"user_id": ["1"], "goods_id": [29,0]}', '{"user_id": ["2"], "goods_id": [30,0]}']) # group_keys = ['user_id', 'goods_id'] group_keys = ['user_id'] sink_to_file_flag = True data_stream = data_stream1.union(data_stream2) # user_function = __import__("UserFunction") ds = data_stream.map(lambda v: to_dict_map(v)) \ .filter(lambda v: v) \ .map(lambda v: v) \ .key_by(lambda v: get_group_key(v, group_keys)) \ .window(TumblingProcessingTimeWindows.of(Time.seconds(12))) \ .process(CountWindowProcessFunction(lambda v: v), Types.STRING()) ds = ds.map(lambda v: v, Types.PRIMITIVE_ARRAY(Types.BYTE())) base_path = "/tmp/1.txt" encoder = Encoder.simple_string_encoder() file_sink_builder = FileSink.for_row_format(base_path, encoder) file_sink = file_sink_builder \ .with_bucket_check_interval(1000) \ .with_rolling_policy(RollingPolicy.on_checkpoint_rolling_policy()) \ .with_output_file_config( OutputFileConfig.builder().with_part_prefix("pre").with_part_suffix("suf").build()) \ .build() ds.sink_to(file_sink) # submit for execution env.execute()The stream graph is as following:{ "nodes" : [ { "id" : 1, "type" : "Source: Collection Source", "pact" : "Data Source", "contents" : "Source: Collection Source", "parallelism" : 1 }, { "id" : 2, "type" : "Source: Collection Source", "pact" : "Data Source", "contents" : "Source: Collection Source", "parallelism" : 1 }, { "id" : 9, "type" : "TumblingProcessingTimeWindows", "pact" : "Operator", "contents" : "Window(TumblingProcessingTimeWindows(12000, 0), ProcessingTimeTrigger, CountWindowProcessFunction)", "parallelism" : 1, "predecessors" : [ { "id" : 15, "ship_strategy" : "HASH", "side" : "second" } ] }, { "id" : 10, "type" : "Map", "pact" : "Operator", "contents" : "Map", "parallelism" : 1, "predecessors" : [ { "id" : 9, "ship_strategy" : "FORWARD", "side" : "second" } ] }, { "id" : 15, "type" : "Map, Filter, Map, _stream_key_by_map_operator", "pact" : "Operator", "contents" : "Map, Filter, Map, _stream_key_by_map_operator", "parallelism" : 1, "predecessors" : [ { "id" : 1, "ship_strategy" : "FORWARD", "side" : "second" }, { "id" : 2, "ship_strategy" : "FORWARD", "side" : "second" } ] }, { "id" : 16, "type" : "TumblingProcessingTimeWindows, Map", "pact" : "Operator", "contents" : "Window(TumblingProcessingTimeWindows(12000, 0), ProcessingTimeTrigger, CountWindowProcessFunction)", "parallelism" : 1, "predecessors" : [ { "id" : 15, "ship_strategy" : "HASH", "side" : "second" } ] }, { "id" : 18, "type" : "Sink: Writer", "pact" : "Operator", "contents" : "Sink: Writer", "parallelism" : 1, "predecessors" : [ { "id" : 10, "ship_strategy" : "FORWARD", "side" : "second" } ] }, { "id" : 20, "type" : "Sink: Committer", "pact" : "Operator", "contents" : "Sink: Committer", "parallelism" : 1, "predecessors" : [ { "id" : 18, "ship_strategy" : "FORWARD", "side" : "second" } ] } ]}The plan is incorrect as we can see that TumblingProcessingTimeWindows appears twice.</description>
      <version>1.15.0</version>
      <fixedVersion>1.17.0,1.15.4,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.chain.PythonOperatorChainingOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3216" opendate="2016-1-11 00:00:00" fixdate="2016-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Define pattern specification</summary>
      <description>In order to detect event patterns we first have to define the pattern. This issue tracks the progress of implementing a user facing API to define event patterns. Patterns should support the following operations next(): The given event has to follow directly after the preceding eventfollowedBy(): The given event has to follow the preceding event. There might occur other events in-between every(): In a follow-by relationship a starting event can be matched with multiple successive events. Consider the pattern a → b where → denotes the follow-by relationship. The event sequence a, b, b can be matched as a, b or a, (b), b where the first b is left out. The essential question is whether a is allowed to match multiple times or only the first time. The method every specifies exactly that. Every events in a pattern can match with multiple successive events. This makes only sense in a follow-by relationship, though. followedByEvery(): Similar to followedBy just that the specified element can be matched with multiple successive events or(): Alternative event which can be matched instead of the original event: every(“e1”).where().or(“e2”).where() within(): Defines a time interval in which the pattern has to be completed, otherwise an incomplete pattern can be emitted (timeout case)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
      <file type="M">flink-libraries.pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>
