<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="27925" opendate="2022-6-7 00:00:00" fixdate="2022-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid to create watcher without the resourceVersion</summary>
      <description>Currently, we create the watcher in KubernetesResourceManager. But it do not pass the resourceVersion parameter, it will trigger a request to etcd. It will bring the burden to the etcd in large scale cluster (which have been seen in our internal k8s cluster). More detail can be found here I think we could use the informer to improve it (which will spawn a list-watch and maintain the resourceVersion internally)</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.NoOpWatchCallbackHandler.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformerITCase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesPodsWatcherTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesClientTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClientTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="30972" opendate="2023-2-9 00:00:00" fixdate="2023-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>E2e tests always fail in phase "Prepare E2E run"</summary>
      <description>Installing required softwareReading package lists...Building dependency tree...Reading state information...bc is already the newest version (1.07.1-2build1).bc set to manually installed.libapr1 is already the newest version (1.6.5-1ubuntu1).libapr1 set to manually installed.0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.--2023-02-09 04:38:47-- http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.10_amd64.debResolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 185.125.190.36, 185.125.190.39, ...Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.HTTP request sent, awaiting response... 404 Not Found2023-02-09 04:38:47 ERROR 404: Not Found.WARNING: apt does not have a stable CLI interface. Use with caution in scripts.Reading package lists...E: Unsupported file ./libssl1.0.0_1.0.2n-1ubuntu5.10_amd64.deb given on commandline##[error]Bash exited with code '100'.Finishing: Prepare E2E run</description>
      <version>1.17.0,1.15.4,1.16.2,1.18.0</version>
      <fixedVersion>1.17.0,1.15.4,1.16.2,1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="31092" opendate="2023-2-15 00:00:00" fixdate="2023-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive ITCases fail with OutOfMemoryError</summary>
      <description>We're experiencing an OutOfMemoryError where the heap space reaches the upper limit:https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46161&amp;view=logs&amp;j=fc5181b0-e452-5c8f-68de-1097947f6483&amp;t=995c650b-6573-581c-9ce6-7ad4cc038461&amp;l=23142Feb 15 05:05:14 [INFO] Running org.apache.flink.table.catalog.hive.HiveCatalogITCaseFeb 15 05:05:17 [INFO] java.lang.OutOfMemoryError: Java heap spaceFeb 15 05:05:17 [INFO] Dumping heap to java_pid9669.hprof ...Feb 15 05:05:28 [INFO] Heap dump file created [1957090051 bytes in 11.718 secs]java.lang.OutOfMemoryError: Java heap space at org.apache.maven.surefire.booter.ForkedBooter.cancelPingScheduler(ForkedBooter.java:209) at org.apache.maven.surefire.booter.ForkedBooter.acknowledgedExit(ForkedBooter.java:419) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:186) at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)</description>
      <version>1.17.0,1.16.1,1.18.0</version>
      <fixedVersion>1.17.0,1.16.2,1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.FactoryUtilTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.ServiceLoaderUtil.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FactoryUtil.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.operation.OperationManagerTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationManager.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.utils.ThreadUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="31166" opendate="2023-2-21 00:00:00" fixdate="2023-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>array_contains does NOT work when haystack elements are not nullable and needle is nullable</summary>
      <description>ARRAY_CONTAINS works ok for the case when both haystack elements and needle are not nullable e.g.SELECT array_contains(ARRAY[0, 1], 0);it works ok when both haystack elements and needle are nullable e.g.SELECT array_contains(ARRAY[0, 1, NULL], CAST(NULL AS INT));it works ok when haystack elements are nullable and needle is not nullable e.g.SELECT array_contains(ARRAY[0, 1, NULL], 1);and it does NOT work when haystack elements are not nullable and needle is nullable e.g.SELECT array_contains(ARRAY[0, 1], CAST(NULL AS INT));  </description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CollectionFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.InputTypeStrategiesTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.strategies.ArrayElementArgumentTypeStrategy.java</file>
    </fixedFiles>
  </bug>
  <bug id="31168" opendate="2023-2-21 00:00:00" fixdate="2023-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JobManagerHAProcessFailureRecoveryITCase failed due to job not being found</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46342&amp;view=logs&amp;j=b0a398c0-685b-599c-eb57-c8c2a771138e&amp;t=747432ad-a576-5911-1e2a-68c6bedc248a&amp;l=12706We see this build failure because a job couldn't be found:java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error while waiting for job to be initialized at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:319) at org.apache.flink.api.java.ExecutionEnvironment.executeAsync(ExecutionEnvironment.java:1061) at org.apache.flink.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:958) at org.apache.flink.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:942) at org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase.testJobManagerFailure(JobManagerHAProcessFailureRecoveryITCase.java:235) at org.apache.flink.test.recovery.JobManagerHAProcessFailureRecoveryITCase$4.run(JobManagerHAProcessFailureRecoveryITCase.java:336)Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error while waiting for job to be initialized at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999) at org.apache.flink.api.java.ExecutionEnvironment.executeAsync(ExecutionEnvironment.java:1056) ... 4 moreCaused by: java.lang.RuntimeException: Error while waiting for job to be initialized at org.apache.flink.client.ClientUtils.waitUntilJobInitializationFinished(ClientUtils.java:160) at org.apache.flink.client.deployment.executors.AbstractSessionClusterExecutor.lambda$execute$2(AbstractSessionClusterExecutor.java:82) at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedFunction$2(FunctionUtils.java:73) at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642) at java.base/java.util.concurrent.CompletableFuture$Completion.exec(CompletableFuture.java:479) at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290) at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020) at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656) at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594) at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.rest.util.RestClientException: [org.apache.flink.runtime.rest.NotFoundException: Job 865dcd87f4828dbeb3d93eb52e2636b1 not found at org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.lambda$handleRequest$1(AbstractExecutionGraphHandler.java:99) at java.base/java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:986) at java.base/java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:970) at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088) at org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCache.lambda$getExecutionGraphInternal$0(DefaultExecutionGraphCache.java:109) at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088) at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:252) at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088) at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1387) at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$1(ClassLoadingUtils.java:93) at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92) at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088) at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:45) at akka.dispatch.OnComplete.internal(Future.scala:299) at akka.dispatch.OnComplete.internal(Future.scala:297) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65) at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68) at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284) at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284) at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284) at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621) at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:25) at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23) at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532) at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29) at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63) at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12) at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81) at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48) at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290) at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020) at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656) at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594) at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)Caused by: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (865dcd87f4828dbeb3d93eb52e2636b1) at org.apache.flink.runtime.dispatcher.Dispatcher.requestExecutionGraphInfo(Dispatcher.java:840) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304) at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) at akka.actor.Actor.aroundReceive(Actor.scala:537) at akka.actor.Actor.aroundReceive$(Actor.scala:535) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) at akka.actor.ActorCell.invoke(ActorCell.scala:548) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) at akka.dispatch.Mailbox.run(Mailbox.scala:231) at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ... 5 more</description>
      <version>1.15.3,1.16.1,1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.TestJvmProcess.java</file>
    </fixedFiles>
  </bug>
  <bug id="31182" opendate="2023-2-22 00:00:00" fixdate="2023-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CompiledPlan cannot deserialize BridgingSqlFunction with MissingTypeStrategy</summary>
      <description>This issue is reported from the user mail list.The stacktrace is Unable to find source-code formatter for language: text. Available languages are: actionscript, ada, applescript, bash, c, c#, c++, cpp, css, erlang, go, groovy, haskell, html, java, javascript, js, json, lua, none, nyan, objc, perl, php, python, r, rainbow, ruby, scala, sh, sql, swift, visualbasic, xml, yamlCaused by: org.apache.flink.table.api.TableException: Could not resolve internal system function '$UNNEST_ROWS$1'. This is a bug, please file an issue.    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeInternalFunction(RexNodeJsonDeserializer.java:392)    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeSqlOperator(RexNodeJsonDeserializer.java:337)    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeCall(RexNodeJsonDeserializer.java:307)    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:146)    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:128)    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:115) The root cause is that although ModuleManager can resolve '$UNNEST_ROWS$1', the output type strategy is "Missing"; as a result, FunctionCatalogOperatorTable#convertToBridgingSqlFunction returns empty.</description>
      <version>1.17.0,1.18.0,1.17.1</version>
      <fixedVersion>1.17.0,1.16.2,1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.CorrelateJsonPlanITCase.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.strategies.SpecificTypeStrategies.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
    </fixedFiles>
  </bug>
  <bug id="31237" opendate="2023-2-27 00:00:00" fixdate="2023-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix possible bug of array_distinct</summary>
      <description>as talked here https://github.com/apache/flink/pull/19623, we should use builtin expressions/functions. because the sql semantic is different from  java equals</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.scalar.ArrayDistinctFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="31250" opendate="2023-2-28 00:00:00" fixdate="2023-2-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parquet format supports MULTISET type</summary>
      <description>Parquet format supports ARRAY, MAP and ROW type, doesn't support MULTISET type. Parquet format should support MULTISET type.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.vector.ParquetColumnarRowSplitReaderTest.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.utils.ParquetSchemaConverter.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.row.ParquetRowDataWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="3126" opendate="2015-12-6 00:00:00" fixdate="2015-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove accumulator type from "value" in web frontend</summary>
      <description>The web frontend shows the type of the counter twice:http://i.imgur.com/yBWT8GR.pngIt would be nicer to just show the value "42" instead of "LongValue 42" there.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.accumulators.StringifiedAccumulatorResult.java</file>
    </fixedFiles>
  </bug>
  <bug id="31279" opendate="2023-3-1 00:00:00" fixdate="2023-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix MULTIPLY(TIMES) function doesn&amp;#39;t support interval types</summary>
      <description>// code placeholderFlink SQL&gt; select 2 * interval '3'  day;[ERROR] Could not execute SQL statement. Reason:org.apache.flink.table.planner.codegen.CodeGenException: Interval expression type expected.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.MathFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug id="31286" opendate="2023-3-2 00:00:00" fixdate="2023-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python processes are still alive when shutting down a session cluster directly without stopping the jobs</summary>
      <description>Reproduce steps:1) start a standalone cluster: ./bin/start_cluster.sh2) submit a PyFlink job which contains Python UDFs3) stop the cluster: ./bin/stop_cluster.sh4) Check if Python process still exists: ps aux | grep -i beam_boot</description>
      <version>None</version>
      <fixedVersion>1.17.0,1.15.4,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonFunctionRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="31351" opendate="2023-3-6 00:00:00" fixdate="2023-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2EndpointITCase.testExecuteStatementInSyncModeWithRuntimeException2 times out on CI</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46872&amp;view=logs&amp;j=fc5181b0-e452-5c8f-68de-1097947f6483&amp;t=995c650b-6573-581c-9ce6-7ad4cc038461&amp;l=24908 Mar 06 18:28:56 "ForkJoinPool-1-worker-25" #27 daemon prio=5 os_prio=0 tid=0x00007ff4b1832000 nid=0x21b2 waiting on condition [0x00007ff3a8c3e000]Mar 06 18:28:56 java.lang.Thread.State: TIMED_WAITING (sleeping)Mar 06 18:28:56 at java.lang.Thread.sleep(Native Method)Mar 06 18:28:56 at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.waitUntilJobIsRunning(HiveServer2EndpointITCase.java:1004)Mar 06 18:28:56 at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.lambda$testExecuteStatementInSyncModeWithRuntimeException2$37(HiveServer2EndpointITCase.java:711)Mar 06 18:28:56 at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase$$Lambda$2018/2127600974.accept(Unknown Source)Mar 06 18:28:56 at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.runExecuteStatementInSyncModeWithRuntimeException(HiveServer2EndpointITCase.java:999)Mar 06 18:28:56 at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.testExecuteStatementInSyncModeWithRuntimeException2(HiveServer2EndpointITCase.java:709)Mar 06 18:28:56 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)Mar 06 18:28:56 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)Mar 06 18:28:56 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)Mar 06 18:28:56 at java.lang.reflect.Method.invoke(Method.java:498)</description>
      <version>1.17.0,1.16.1,1.18.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.operation.OperationManagerTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="31378" opendate="2023-3-9 00:00:00" fixdate="2023-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation fails to build due to lack of package</summary>
      <description>In Project Configuration Section it shows that "If you want to run your job by simply executing the main class, you will need flink-runtime in your classpath". However, when I just add flink-runtime in my classPath, an error is thrown like this:"No ExecutorFactory found to execute the application".It seems that flink-clients is also needed to supply an excutor through Java Service Load.Could you please add this in official article for beginners like me? </description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.configuration.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.configuration.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="31383" opendate="2023-3-9 00:00:00" fixdate="2023-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for documenting additionProperties of the REST API payloads.</summary>
      <description>For implementing the request and response body of the resource requirements endpoint, we need to be able to document "additionalProperties" because these payloads have only top-level dynamic properties of the same type.This affects both classic (HTML docs) and OpenAPI generators.An example of what we want to be able to document is:@JsonAnySetter@JsonAnyGetter@JsonSerialize(keyUsing = JobVertexIDKeySerializer.class)@JsonDeserialize(keyUsing = JobVertexIDKeyDeserializer.class)private final Map&lt;JobVertexID, JobVertexResourceRequirements&gt; jobVertexResourceRequirements;</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.rest.RestAPIDocGeneratorTest.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.rest.OpenApiSpecGeneratorTest.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.OpenApiSpecGenerator.java</file>
      <file type="M">docs.static.generated.rest.v1.sql.gateway.yml</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
    </fixedFiles>
  </bug>
  <bug id="31385" opendate="2023-3-9 00:00:00" fixdate="2023-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce extended Assertj Matchers for completable futures</summary>
      <description>Introduce extended Assertj Matchers for completable futures that don't rely on timeouts.In general, we want to avoid relying on timeouts in the Flink test suite to get additional context (thread dump) in case something gets stuck.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.FlinkAssertions.java</file>
    </fixedFiles>
  </bug>
  <bug id="31396" opendate="2023-3-10 00:00:00" fixdate="2023-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Occasional inaccurate timeout time calculation with System.nanotime in batch read buffer pool</summary>
      <description>When running TPC-DS tests, I encountered the read buffer request timeout because of configuring too less read buffers. But I found the timeout time may be less than 5m occasionally, 5m is the expected time. I read the docs of System.nanotime, the docs say that t1 &lt; t0 should not be used, because of the possibility of numerical overflow. I tested the System.currentTimeMillis and it can work as expected.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsFileDataManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.BatchShuffleReadBufferPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="3140" opendate="2015-12-8 00:00:00" fixdate="2015-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NULL value data layout in Row Serializer/Comparator</summary>
      <description>To store/materialize NULL value in Row objects, we should need new Row Serializer/Comparator which is aware of NULL value fields.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.CaseClassTypeInfo.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.GroupedAggreagationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeinfo.RowTypeInfo.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeinfo.RowSerializer.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorTTT2Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorTTT1Test.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.ComparatorTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="31405" opendate="2023-3-11 00:00:00" fixdate="2023-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor tests to git rid of timeout of CompletableFuture assertions.</summary>
      <description>In general, we want to avoid relying on local timeouts in the Flink test suite to get additional context (thread dump) in case something gets stuck(see Code Style and Quality Guide).Some of timeout in tests are introduced by assertions for CompleteFuture. After FLINK-31385, we can refactor these tests to git rid of timeout for CompletableFuture assertions.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.test.java.org.apache.flink.core.testutils.FlinkAssertionsTest.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.FlinkCompletableFutureAssert.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.testsuites.SinkTestSuiteBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.ThreadInfoSampleServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsMemoryDataSpillerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HsMemoryDataManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerRegistryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.cleanup.DefaultResourceCleanerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.cleanup.CheckpointResourcesCleanupRunnerTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.PerJobMiniClusterFactoryTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ClientTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.ClientHeartbeatTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="31409" opendate="2023-3-13 00:00:00" fixdate="2023-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive dialect should use public interfaces in Hive connector</summary>
      <description>Currently, for the Hive dialect part in Hive connector, it depends much internal interfaces in flink-table-planner or other module. We should avoid it and use public interfaces proposed in  [FLIP-216|https://cwiki.apache.org/confluence/display/FLINK/FLIP-216%3A++Introduce+pluggable+dialect+and+plan+for+migrating+Hive+dialect]</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveSessionState.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserDMLHelper.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.copy.HiveParserBaseSemanticAnalyzer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.delegation.DialectFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.delegation.DefaultDialectFactory.java</file>
      <file type="M">flink-table.flink-table-calcite-bridge.src.main.java.org.apache.flink.table.calcite.bridge.CalciteContext.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.PlannerMock.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.delegation.Planner.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.delegation.Parser.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.delegation.ExtendedOperationExecutor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserLoadSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveOperationExecutor.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveDialectFactory.java</file>
      <file type="M">flink-table.README.md</file>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="31447" opendate="2023-3-14 00:00:00" fixdate="2023-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Aligning unit tests of FineGrainedSlotManager with DeclarativeSlotManager</summary>
      <description>There's the DeclarativeSlotManagerTest that covers some specific issues that should be ported to the fine grained slot manager.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.utils.MockResourceManagerRuntimeServices.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TestingResourceAllocatorBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TestingResourceAllocator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerDefaultResourceAllocationStrategyITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.DefaultSlotStatusSyncerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.AbstractFineGrainedSlotManagerITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="31448" opendate="2023-3-14 00:00:00" fixdate="2023-5-14 01:00:00" resolution="Done">
    <buginformation>
      <summary>Use FineGrainedSlotManager as the default SlotManager</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.test.controller.sh</file>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRuntimeServicesConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.scheduling.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.cluster.configuration.html</file>
      <file type="M">docs.content.docs.deployment.finegrained.resource.md</file>
      <file type="M">docs.content.zh.docs.deployment.finegrained.resource.md</file>
    </fixedFiles>
  </bug>
  <bug id="31450" opendate="2023-3-14 00:00:00" fixdate="2023-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce ExecutableOperation for operations to execute</summary>
      <description>Colocating the execution logic within the Operation, just like how RunnableCommand and V2CommandExec do in Spark. We can introduce a class like:public interface ExecutableOperation { TableResultInternal execute(Context ctx); interface Context { CatalogManager getCatalogManager(); FunctionCatalog getFunctionCatalog(); ResourceManager getResourceManager(); Configuration getConfiguration(); }}Many base interfaces can extend it (AlterOperation, CreateOperation, DropOperation, etc.). This approach improves code readability (not spread code across different classes) and make supporting a new statement by just adding an Operation class instead of 3 classes (Operation class, Executor class, and the mapping class).</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.UseOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.UseModulesOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.UseDatabaseOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.UseCatalogOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="31498" opendate="2023-3-17 00:00:00" fixdate="2023-5-17 01:00:00" resolution="Done">
    <buginformation>
      <summary>DeclartiveSlotManager always request redundant task manager when resource is not enough</summary>
      <description>Currently redundant task manager check in DeclarativeSlotManager only compare free slots with required redundant slots. when there are no enough resources in YARN/Kubernetes, this mechanism will always try to request new task manager. there are two way to address this.1. maintain the state of redundant workers to avoid request twice2. only try to request redundant workers when there is no pending workerThe first way will make the logic of redundant worker too complicated, I would like to choose the second wayLooking forward to any suggestion.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TaskExecutorManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.TaskExecutorManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="31501" opendate="2023-3-17 00:00:00" fixdate="2023-1-17 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Move SqlCreateView conversion logic to SqlCreateViewConverter</summary>
      <description>Introduce SqlCreateViewConverter and move the conversion logic of SqlCreateView -&gt; CreateViewOperation to it.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConverters.java</file>
    </fixedFiles>
  </bug>
  <bug id="31510" opendate="2023-3-18 00:00:00" fixdate="2023-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use getMemorySize instead of getMemory</summary>
      <description>In YARN-4844, use getMemorySize instead of getMemory, because using int to represent memory may exceed the bounds in some cases and produce negative numbers.This change was merged in HADOOP-2.8.0, we should use getMemorySize instead of getMemory.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.TaskExecutorProcessSpecContainerResourcePriorityAdapterTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.TaskExecutorProcessSpecContainerResourcePriorityAdapter.java</file>
    </fixedFiles>
  </bug>
  <bug id="31536" opendate="2023-3-21 00:00:00" fixdate="2023-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to set watermark emit strategy in sql layer</summary>
      <description>Support to configure the watermark emit strategy for the source which implements the `SupportsWatermarkPushDown` interface. User could configure the watermark emit strategy in flink sql job with table options or 'OPTIONS' hint，as discussed in  https://cwiki.apache.org/confluence/display/FLINK/FLIP-296%3A+Extend+watermark-related+features+for+SQL </description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.GeneratedWatermarkGeneratorSupplier.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testWatermarkPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DynamicTableSourceSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.WatermarkPushDownSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.hint.FlinkHintStrategies.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.FactoryUtilTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FactoryUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="31538" opendate="2023-3-21 00:00:00" fixdate="2023-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Supports parse catalog/database and properties for uri</summary>
      <description>Supports parse catalog/database and properties for uri</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.FlinkDriver.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="31539" opendate="2023-3-21 00:00:00" fixdate="2023-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to configure watermark alignment in sql layer</summary>
      <description>Support to configure the watermark alignment for the source which implements the `SupportsWatermarkPushDown` interface.User can configure watermark alignment in flink sql job with table options or 'OPTIONS' hint, as disgussedin https://cwiki.apache.org/confluence/display/FLINK/FLIP-296%3A+Extend+watermark-related+features+for+SQL</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.FilterableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testWatermarkPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DynamicTableSourceSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.WatermarkPushDownSpec.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.FactoryUtilTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.watermark.WatermarkParams.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FactoryUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="31546" opendate="2023-3-21 00:00:00" fixdate="2023-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Close all statements when connection is closed</summary>
      <description>Close all statements when connection is closed</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.test.java.org.apache.flink.table.jdbc.FlinkStatementTest.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.FlinkStatement.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.FlinkConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="31547" opendate="2023-3-21 00:00:00" fixdate="2023-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce FlinkResultSetMetaData for jdbc driver</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-jdbc-driver-bundle.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="31549" opendate="2023-3-21 00:00:00" fixdate="2023-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add doc for jdbc driver</summary>
      <description>1. How to use jdbc driver in java code2. How to use jdbc driver in tools</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="31609" opendate="2023-3-24 00:00:00" fixdate="2023-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fatal error in ResourceManager caused YARNSessionFIFOSecuredITCase.testDetachedMode to fail</summary>
      <description>This looks like FLINK-30908. I created a follow-up ticket because we reached a new minor version.https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47547&amp;view=logs&amp;j=fc5181b0-e452-5c8f-68de-1097947f6483&amp;t=995c650b-6573-581c-9ce6-7ad4cc038461Mar 24 09:32:29 2023-03-24 09:31:50,001 ERROR org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl [] - Exception on heartbeatMar 24 09:32:29 java.io.InterruptedIOException: Interrupted waiting to send RPC request to serverMar 24 09:32:29 java.io.InterruptedIOException: Interrupted waiting to send RPC request to serverMar 24 09:32:29 at org.apache.hadoop.ipc.Client.call(Client.java:1461) ~[hadoop-common-2.10.2.jar:?]Mar 24 09:32:29 at org.apache.hadoop.ipc.Client.call(Client.java:1403) ~[hadoop-common-2.10.2.jar:?]Mar 24 09:32:29 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230) ~[hadoop-common-2.10.2.jar:?]Mar 24 09:32:29 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118) ~[hadoop-common-2.10.2.jar:?]Mar 24 09:32:29 at com.sun.proxy.$Proxy33.allocate(Unknown Source) ~[?:?]Mar 24 09:32:29 at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77) ~[hadoop-yarn-common-2.10.2.jar:?]Mar 24 09:32:29 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]Mar 24 09:32:29 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]Mar 24 09:32:29 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]Mar 24 09:32:29 at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]Mar 24 09:32:29 at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:433) ~[hadoop-common-2.10.2.jar:?]Mar 24 09:32:29 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:166) ~[hadoop-common-2.10.2.jar:?]Mar 24 09:32:29 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:158) ~[hadoop-common-2.10.2.jar:?]Mar 24 09:32:29 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:96) ~[hadoop-common-2.10.2.jar:?]Mar 24 09:32:29 at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:362) ~[hadoop-common-2.10.2.jar:?]Mar 24 09:32:29 at com.sun.proxy.$Proxy34.allocate(Unknown Source) ~[?:?]Mar 24 09:32:29 at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:297) ~[hadoop-yarn-client-2.10.2.jar:?]Mar 24 09:32:29 at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$HeartbeatThread.run(AMRMClientAsyncImpl.java:274) [hadoop-yarn-client-2.10.2.jar:?]Mar 24 09:32:29 Caused by: java.lang.InterruptedExceptionMar 24 09:32:29 at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:404) ~[?:1.8.0_292]Mar 24 09:32:29 at java.util.concurrent.FutureTask.get(FutureTask.java:191) ~[?:1.8.0_292]Mar 24 09:32:29 at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1177) ~[hadoop-common-2.10.2.jar:?]Mar 24 09:32:29 at org.apache.hadoop.ipc.Client.call(Client.java:1456) ~[hadoop-common-2.10.2.jar:?]Mar 24 09:32:29 ... 17 more</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="31635" opendate="2023-3-28 00:00:00" fixdate="2023-5-28 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support writing records to the new tiered storage architecture</summary>
      <description>Support writing records to the new tiered store architecture.To achieve the goal, this mainly includes the following two parts. 1. Introduces the tiered storage architecture.2. The producer-side implementation of the architecture</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionID.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.TierFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.common.TieredStorageConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.NettyShuffleMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="31636" opendate="2023-3-28 00:00:00" fixdate="2023-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upstream supports reading buffers from tiered storage</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.EventSerializerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.Buffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.EventSerializer.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.TestInputChannel.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannelTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannel.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.shuffle.TieredResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.shuffle.TieredResultPartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="31638" opendate="2023-3-28 00:00:00" fixdate="2023-5-28 01:00:00" resolution="Done">
    <buginformation>
      <summary>Downstream supports reading buffers from tiered storage</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.TierConsumerAgent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.TieredStorageConsumerClient.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.benchmark.SingleInputGateBenchmarkFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.InputGateFairnessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NettyShuffleEnvironmentBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.NettyShuffleEnvironmentConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NettyShuffleServiceFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="31639" opendate="2023-3-28 00:00:00" fixdate="2023-5-28 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce tiered storage memory manager</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.LocalBufferPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="31640" opendate="2023-3-28 00:00:00" fixdate="2023-6-28 01:00:00" resolution="Done">
    <buginformation>
      <summary>Write the accumulated buffers to the right storage tier</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.TestingBufferAccumulator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.HashBufferAccumulatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.shuffle.TieredResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.TierProducerAgent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.TieredStorageProducerClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.shuffle.TieredResultPartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="31641" opendate="2023-3-28 00:00:00" fixdate="2023-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce memory tier</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.TierFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="31642" opendate="2023-3-28 00:00:00" fixdate="2023-6-28 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce memory tier reader</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.TestingPartitionRequestClient.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.netty.NettyConnectionReaderImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.PartitionRequestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NetworkSequenceViewReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestServerHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyMessage.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.CreditBasedSequenceNumberingViewReader.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkBenchmarkEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.InputGateFairnessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.netty.TestingTieredStorageNettyService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.TieredStorageConsumerClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.netty.TieredStorageNettyServiceImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.netty.TieredStorageNettyService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NettyShuffleEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.TierFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.TierConsumerAgent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.memory.MemoryTierFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="31644" opendate="2023-3-28 00:00:00" fixdate="2023-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce disk tier</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.TestingTierFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.TierFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.memory.MemoryTierFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.common.TieredStorageUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="31645" opendate="2023-3-28 00:00:00" fixdate="2023-7-28 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce disk tier reader</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.disk.DiskTierProducerAgentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.TestingTierFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.netty.TieredStorageConsumerClientTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.TierFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.memory.MemoryTierFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.disk.DiskTierProducerAgent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.disk.DiskTierFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.netty.TieredStorageResultSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.netty.TieredStorageNettyServiceImpl.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.file.ProducerMergedPartitionFileWriterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.file.ProducerMergedPartitionFileReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="31646" opendate="2023-3-28 00:00:00" fixdate="2023-7-28 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce remote storage tier</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.common.TieredStorageTestUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.common.TieredStorageUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.common.TieredStorageConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="31647" opendate="2023-3-28 00:00:00" fixdate="2023-7-28 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce remote storage tier reader</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.netty.TieredStorageConsumerClientTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.remote.RemoteTierFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.file.SegmentPartitionFileReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.file.PartitionFileReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BufferReaderWriterUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="31656" opendate="2023-3-29 00:00:00" fixdate="2023-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Obtain delegation tokens early to support external file system usage in blob server</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.token.ExceptionThrowingDelegationTokenReceiver.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.FileSystemJobResultStoreFileOperationsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.entrypoint.ClusterEntrypointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.FileSystemBlobStoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.NoOpDelegationTokenManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.DelegationTokenManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.FileSystemJobResultStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.FileSystemBlobStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="31687" opendate="2023-4-2 00:00:00" fixdate="2023-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink jdbc driver get rid of flink core</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.FlinkConnection.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver-bundle.pom.xml</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.DefaultContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.ExecutorImpl.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.parser.SqlClientSyntaxHighlighter.java</file>
    </fixedFiles>
  </bug>
  <bug id="31693" opendate="2023-4-3 00:00:00" fixdate="2023-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump http-cache-semantics from 4.1.0 to 4.1.1</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
    </fixedFiles>
  </bug>
  <bug id="31694" opendate="2023-4-3 00:00:00" fixdate="2023-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump ua-parser-js from 0.7.31 to 0.7.33</summary>
      <description>Dependabot PR: https://github.com/apache/flink/pull/21767</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
    </fixedFiles>
  </bug>
  <bug id="31767" opendate="2023-4-11 00:00:00" fixdate="2023-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the implementation for "analyze table" execution on partitioned table</summary>
      <description>Currently, for partitioned table, the "analyze table" command will generate a separate SQL statement for each partition. When there are too many partitions, the compilation/submission/execution time will be very long. This issue aims to improve it: we can combine the sql statements for each partition into one with "union all", and just need to execution one sql.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.AnalyzeTableUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="31804" opendate="2023-4-14 00:00:00" fixdate="2023-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ITCase MiniCluster test architecture rule should accept MiniClusterTestEnvironment</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-parquet.archunit-violations.1b8e145e-3f0a-4415-b463-37a87dd0a998</file>
      <file type="M">flink-formats.flink-orc.archunit-violations.8a4ac4b1-94ba-4a1d-a639-b2bd22868374</file>
      <file type="M">flink-formats.flink-json.archunit-violations.62c5e4e5-2b0e-41ed-a268-ee33d5edd162</file>
      <file type="M">flink-formats.flink-csv.archunit-violations.41d59928-b3fa-4aa2-8b1b-221c1031bd0c</file>
      <file type="M">flink-formats.flink-avro.archunit-violations.337c411e-1885-4f85-8070-aabe3638eb43</file>
      <file type="M">flink-connectors.flink-connector-kafka.archunit-violations.97dda445-f6bc-43e2-8106-5876ca0cd052</file>
      <file type="M">flink-connectors.flink-connector-hive.archunit-violations.26d337fc-45c4-4d03-a84a-6692c37fafbc</file>
      <file type="M">flink-connectors.flink-connector-files.archunit-violations.f5e3e868-8d92-4258-9654-a605dc9c550f</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-test.src.main.java.org.apache.flink.architecture.rules.ITCaseRules.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="31806" opendate="2023-4-14 00:00:00" fixdate="2023-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prod architecture tests didn&amp;#39;t detect non-public API usage</summary>
      <description>FLINK-31805 wasn't detected by the production architecture tests.Not sure if this is an issue on the cassandra or Flink side.</description>
      <version>cassandra-3.0.0,1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.src.main.java.org.apache.flink.architecture.rules.ConnectorRules.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.stored.rules</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.b8900323-6aab-4e7e-9b17-f53b3c3dca46</file>
    </fixedFiles>
  </bug>
  <bug id="31819" opendate="2023-4-17 00:00:00" fixdate="2023-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for using watermark advanced functions in sql</summary>
      <description>Add document for using watermark advanced functions in sql  </description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.concepts.time.attributes.md</file>
      <file type="M">docs.content.zh.docs.dev.table.concepts.time.attributes.md</file>
    </fixedFiles>
  </bug>
  <bug id="31832" opendate="2023-4-18 00:00:00" fixdate="2023-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add benchmarks for end to end  restarting tasks</summary>
      <description>As discussed in https://issues.apache.org/jira/browse/FLINK-31771. We need a benchmark for job failover and end to end restarting tasks</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.benchmark.SchedulerBenchmarkBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.benchmark.JobConfiguration.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.benchmark.e2e.SchedulerEndToEndBenchmarkBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="31834" opendate="2023-4-18 00:00:00" fixdate="2023-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Azure Warning: no space left on device</summary>
      <description>In this CI run: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48213&amp;view=logs&amp;j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&amp;t=841082b6-1a93-5908-4d37-a071f4387a5f&amp;l=21There was this warning:Loaded image: confluentinc/cp-kafka:6.2.2Loaded image: testcontainers/ryuk:0.3.3ApplyLayer exit status 1 stdout: stderr: write /opt/jdk-15.0.1+9/lib/modules: no space left on device##[error]Bash exited with code '1'.Finishing: Restore docker images</description>
      <version>None</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="31859" opendate="2023-4-19 00:00:00" fixdate="2023-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update maven cyclonedx plugin to 2.7.7</summary>
      <description>there are at least 2 related improvements1. current version depends on jackson-databind 2.14.0 and has a memory issue described at https://github.com/FasterXML/jackson-databind/issues/3665 which is fixed in later versions2. current version leads to lots of traces in logs (e.g. mvn clean verify for flink-core) which is fixed in later versions[ERROR] An error occurred attempting to read POMorg.codehaus.plexus.util.xml.pull.XmlPullParserException: UTF-8 BOM plus xml decl of ISO-8859-1 is incompatible (position: START_DOCUMENT seen &lt;?xml version="1.0" encoding="ISO-8859-1"... @1:42) at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDeclWithVersion (MXParser.java:3423) at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDecl (MXParser.java:3345) at org.codehaus.plexus.util.xml.pull.MXParser.parsePI (MXParser.java:3197) at org.codehaus.plexus.util.xml.pull.MXParser.parseProlog (MXParser.java:1828) at org.codehaus.plexus.util.xml.pull.MXParser.nextImpl (MXParser.java:1757) at org.codehaus.plexus.util.xml.pull.MXParser.next (MXParser.java:1375) at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:3940) at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:612) at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:627) at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:759) at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:746) at org.cyclonedx.maven.BaseCycloneDxMojo.retrieveParentProject (BaseCycloneDxMojo.java:694) at org.cyclonedx.maven.BaseCycloneDxMojo.getClosestMetadata (BaseCycloneDxMojo.java:524) at org.cyclonedx.maven.BaseCycloneDxMojo.convert (BaseCycloneDxMojo.java:481) at org.cyclonedx.maven.CycloneDxMojo.execute (CycloneDxMojo.java:70) at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137) at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:370) at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:351) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:215) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:171) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:163) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81) at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56) at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128) at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:294) at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192) at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105) at org.apache.maven.cli.MavenCli.execute (MavenCli.java:960) at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:293) at org.apache.maven.cli.MavenCli.main (MavenCli.java:196) at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke (Method.java:498) at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282) at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225) at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406) at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)</description>
      <version>1.17.0,1.18.0</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="31876" opendate="2023-4-21 00:00:00" fixdate="2023-5-21 01:00:00" resolution="Done">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-queryable-state</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.test.java.org.apache.flink.queryablestate.client.state.ImmutableValueStateTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.test.java.org.apache.flink.queryablestate.client.state.ImmutableReducingStateTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.test.java.org.apache.flink.queryablestate.client.state.ImmutableMapStateTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.test.java.org.apache.flink.queryablestate.client.state.ImmutableListStateTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.test.java.org.apache.flink.queryablestate.client.state.ImmutableAggregatingStateTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.MessageSerializerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KvStateServerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KvStateServerHandlerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KvStateRequestSerializerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KVStateRequestSerializerRocksDBTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KvStateClientHandlerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.AbstractServerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.NonHAQueryableStateRocksDBBackendITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.NonHAQueryableStateFsBackendITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.HAQueryableStateRocksDBBackendITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.HAQueryableStateFsBackendITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.AbstractQueryableStateTestBase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.client.proxy.KvStateClientProxyImplTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="31888" opendate="2023-4-23 00:00:00" fixdate="2023-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce interfaces and utility classes related to enrichment/labelling of failures leading to job restart</summary>
      <description>We need to introduce new interfaces/implementations for FailureEnricher / Context / FailureEnricherUtils</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.job.manager.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.jobmanager.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="31950" opendate="2023-4-27 00:00:00" fixdate="2023-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce StateMetadata</summary>
      <description>According to the FLIP design, we're about to introduce StateMetadata, which describes the TTL attribute of the stateful stream operator.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="31957" opendate="2023-4-27 00:00:00" fixdate="2023-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for the user story</summary>
      <description>Add documentation on how to use compiled plan to configure operator-level state TTL.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.concepts.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.concepts.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="31962" opendate="2023-4-27 00:00:00" fixdate="2023-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>libssl not found when running CI</summary>
      <description>Installed Maven 3.2.5 to /home/vsts/maven_cache/apache-maven-3.2.5Installing required softwareReading package lists...Building dependency tree...Reading state information...bc is already the newest version (1.07.1-2build1).bc set to manually installed.libapr1 is already the newest version (1.6.5-1ubuntu1).libapr1 set to manually installed.0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.--2023-04-27 11:42:53-- http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.11_amd64.debResolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 185.125.190.36, 185.125.190.39, ...Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.HTTP request sent, awaiting response... 404 Not Found2023-04-27 11:42:53 ERROR 404: Not Found.</description>
      <version>1.16.2,1.18.0,1.17.1</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="31963" opendate="2023-4-28 00:00:00" fixdate="2023-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>java.lang.ArrayIndexOutOfBoundsException when scaling down with unaligned checkpoints</summary>
      <description>I'm testing Autoscaler through Kubernetes Operator and I'm facing the following issue.As you know, when a job is scaled down through the autoscaler, the job manager and task manager go down and then back up again.When this happens, an index out of bounds exception is thrown and the state is not restored from a checkpoint.gyfora told me via the Flink Slack troubleshooting channel that this is likely an issue with Unaligned Checkpoint and not an issue with the autoscaler, but I'm opening a ticket with Gyula for more clarification.Please see the attached JM and TM error logs.Thank you.</description>
      <version>1.17.0,1.16.1,1.15.4,1.18.0</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.TaskStateAssignment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="31974" opendate="2023-4-28 00:00:00" fixdate="2023-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JobManager crashes after KubernetesClientException exception with FatalExitExceptionHandler</summary>
      <description>When resource quota limit is reached JobManager will throw org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods "my-namespace-flink-cluster-taskmanager-1-2" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13. In 1.16.1 , this is handled gracefully:2023-04-28 22:07:24,631 WARN  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Failed requesting worker with resource spec WorkerResourceSpec \{cpuCores=1.0, taskHeapSize=25.600mb (26843542 bytes), taskOffHeapSize=0 bytes, networkMemSize=64.000mb (67108864 bytes), managedMemSize=230.400mb (241591914 bytes), numSlots=4}, current pending count: 0java.util.concurrent.CompletionException: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods "my-namespace-flink-cluster-taskmanager-1-138" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.        at java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]        at java.util.concurrent.CompletableFuture.completeThrowable(Unknown Source) ~[?:?]        at java.util.concurrent.CompletableFuture$AsyncRun.run(Unknown Source) ~[?:?]        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]        at java.lang.Thread.run(Unknown Source) ~[?:?]Caused by: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods "my-namespace-flink-cluster-taskmanager-1-138" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:684) ~[flink-dist-1.16.1.jar:1.16.1]        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:664) ~[flink-dist-1.16.1.jar:1.16.1]        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:613) ~[flink-dist-1.16.1.jar:1.16.1]        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:558) ~[flink-dist-1.16.1.jar:1.16.1]        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:521) ~[flink-dist-1.16.1.jar:1.16.1]        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:308) ~[flink-dist-1.16.1.jar:1.16.1]        at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:644) ~[flink-dist-1.16.1.jar:1.16.1]        at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:83) ~[flink-dist-1.16.1.jar:1.16.1]        at io.fabric8.kubernetes.client.dsl.base.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:61) ~[flink-dist-1.16.1.jar:1.16.1]        at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$createTaskManagerPod$1(Fabric8FlinkKubeClient.java:163) ~[flink-dist-1.16.1.jar:1.16.1]        ... 4 moreBut , in Flink 1.17.0 , Job Manager crashes:2023-04-28 20:50:50,534 ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.actor.default-dispatcher-15' produced an uncaught exception. Stopping the process...java.util.concurrent.CompletionException: org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods "my-namespace-flink-cluster-taskmanager-1-2" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.        at java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]        at java.util.concurrent.CompletableFuture.completeThrowable(Unknown Source) ~[?:?]        at java.util.concurrent.CompletableFuture$AsyncRun.run(Unknown Source) ~[?:?]        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]        at java.lang.Thread.run(Unknown Source) ~[?:?]Caused by: org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods "my-namespace-flink-cluster-taskmanager-1-2" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:684) ~[flink-dist-1.17.0.jar:1.17.0]        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:664) ~[flink-dist-1.17.0.jar:1.17.0]        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:613) ~[flink-dist-1.17.0.jar:1.17.0]        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:558) ~[flink-dist-1.17.0.jar:1.17.0]        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:521) ~[flink-dist-1.17.0.jar:1.17.0]        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:308) ~[flink-dist-1.17.0.jar:1.17.0]        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:644) ~[flink-dist-1.17.0.jar:1.17.0]        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:83) ~[flink-dist-1.17.0.jar:1.17.0]        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:61) ~[flink-dist-1.17.0.jar:1.17.0]        at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$createTaskManagerPod$1(Fabric8FlinkKubeClient.java:163) ~[flink-dist-1.17.0.jar:1.17.0]        ... 4 more</description>
      <version>1.17.0,1.18.0</version>
      <fixedVersion>1.18.0,1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriverTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="31996" opendate="2023-5-4 00:00:00" fixdate="2023-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Chaining operators with different max parallelism prevents rescaling</summary>
      <description>We might chain operators with different max parallelism together if they are set to have the same parallelism initially.When we decide to rescale the JobGraph vertices (using AdaptiveScheduler), we're gapped by the lowest maxParallelism of the operator chain. This is especially visible with things like CollectSink, TwoPhaseCommitSink, CDC, and a GlobalCommiter with maxParallelism set to 1. An obvious solution would be to prevent the chaining of operators with different maxParallelism, but we need to double-check this doesn't introduce a breaking change.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.StreamingScalaAPICompletenessTest.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.PipelineOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.pipeline.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="31997" opendate="2023-5-4 00:00:00" fixdate="2023-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update to Fabric8 6.5.1+ in flink-kubernetes</summary>
      <description>We should update the fabric8 version in flink-kubernetes to at least 6.5.1. Flink currently uses a very old fabric8 version. The fabric8 library dependencies have since been revised and greately improved to make them more moduler and allow eliminating securitiy vulnerabilities more easily like: https://issues.apache.org/jira/browse/FLINK-31815The newer versions especially 6.5.1 + also add some improvement stability fixes for watches and other parts.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.MixedKubernetesServerExtension.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesPodTemplateTestUtils.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesClusterDescriptorTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClientTest.java</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesClusterDescriptor.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesClusterClientFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.resources.KubernetesLeaderElector.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClientFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32043" opendate="2023-5-10 00:00:00" fixdate="2023-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SqlClient session unrecoverable once one wrong setting occurred</summary>
      <description>In sql client, it can not work normally once one wrong setting occurred// wrong setting hereFlink SQL&gt; SET table.sql-dialect = flink;[INFO] Execute statement succeed.Flink SQL&gt; select '' AS f1, a from t1;[ERROR] Could not execute SQL statement. Reason:java.lang.IllegalArgumentException: No enum constant org.apache.flink.table.api.SqlDialect.FLINKFlink SQL&gt; SET table.sql-dialect = default;[ERROR] Could not execute SQL statement. Reason:java.lang.IllegalArgumentException: No enum constant org.apache.flink.table.api.SqlDialect.FLINKFlink SQL&gt; RESET table.sql-dialect;[ERROR] Could not execute SQL statement. Reason:java.lang.IllegalArgumentException: No enum constant org.apache.flink.table.api.SqlDialect.FLINKFlink SQL&gt; RESET;[ERROR] Could not execute SQL statement. Reason:java.lang.IllegalArgumentException: No enum constant org.apache.flink.table.api.SqlDialect.FLINK</description>
      <version>1.17.0,1.18.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.AbstractSqlGatewayStatementITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.SessionContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="32058" opendate="2023-5-11 00:00:00" fixdate="2023-5-11 01:00:00" resolution="Done">
    <buginformation>
      <summary>Migrate subclasses of BatchAbstractTestBase in runtime.batch.sql to JUnit5</summary>
      <description>Migrate subclasses of BatchAbstractTestBase in runtime.batch.sql to JUnit5.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.testutils.junit.extensions.parameterized.ParameterizedTestExtension.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.WindowTableFunctionITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.ValuesITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.UnnestITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.UnionITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableScanITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.SortLimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.RankITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.OverAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.MultipleInputITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.MiscITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.LimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.Limit0RemoveITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.LegacyTableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.LegacyLimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CorrelateITCase2.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CodeSplitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="32059" opendate="2023-5-11 00:00:00" fixdate="2023-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate subclasses of BatchAbstractTestBase in batch.sql.agg and batch.sql.join to JUnit5</summary>
      <description>Migrate subclasses of BatchAbstractTestBase in batch.sql.agg and batch.sql.join to JUnit5.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.SemiJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.ScalarQueryITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.OuterJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.JoinWithoutKeyITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.JoinConditionTypeCoerceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.InnerJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.SortDistinctAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.PruneAggregateCallITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.HashAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.GroupingSetsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.DistinctAggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateRemoveITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateJoinTransposeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-gs-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="32060" opendate="2023-5-11 00:00:00" fixdate="2023-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate subclasses of BatchAbstractTestBase in table and other modules to JUnit5</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.StreamFileSystemTestCsvITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.StreamFileSystemITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.FileSystemITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.SortITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.OverAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.LimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.LegacyTableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.LegacyLimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.AggregationITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.FileSystemTestCsvITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase.scala</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonBatchFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvFilesystemStreamITCase.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvFilesystemBatchITCase.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroFilesystemITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="3208" opendate="2016-1-6 00:00:00" fixdate="2016-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename Gelly vertex-centric model to scatter-gather</summary>
      <description>The idea is to have the following naming: Pregel model: vertex-centric iteration Spargel model: scatter-gather iteration GSA model: as isOpen to suggestions!</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.test.VertexCentricConfigurationITCase.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.test.example.IncrementalSSSPITCase.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.test.CollectionModeSuperstepITCase.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.spargel.SpargelTranslationTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.spargel.SpargelCompilerTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.spargel.VertexUpdateFunction.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.spargel.VertexCentricIteration.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.spargel.VertexCentricConfiguration.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.spargel.MessagingFunction.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.SingleSourceShortestPaths.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.PageRank.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.LabelPropagation.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.ConnectedComponents.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.CommunityDetection.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.GraphCsvReader.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.Graph.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.example.SingleSourceShortestPaths.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.example.IncrementalSSSP.java</file>
      <file type="M">flink-libraries.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.Graph.scala</file>
      <file type="M">flink-libraries.flink-gelly-scala.src.main.scala.org.apache.flink.graph.scala.example.SingleSourceShortestPaths.scala</file>
      <file type="M">docs.libs.gelly.guide.md</file>
    </fixedFiles>
  </bug>
  <bug id="32094" opendate="2023-5-14 00:00:00" fixdate="2023-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>startScheduling.BATCH performance regression since May 11th</summary>
      <description>http://codespeed.dak8s.net:8000/timeline/#/?exe=5&amp;ben=startScheduling.BATCH&amp;extr=on&amp;quarts=on&amp;equid=off&amp;env=2&amp;revs=200</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.NettyShuffleMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="32123" opendate="2023-5-17 00:00:00" fixdate="2023-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avro Confluent Schema Registry nightly end-to-end test failed due to timeout</summary>
      <description>For the past few hours, E2E tests fail with: 'Avro Confluent Schema Registry nightly end-to-end test' failed after 9 minutes and 53 seconds! Test exited with exit code 1Looks like https://archive.apache.org/dist/kafka/  mirror is overloaded – download locally took more than 30minLets switch to  https://downloads.apache.org mirror </description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="3216" opendate="2016-1-11 00:00:00" fixdate="2016-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Define pattern specification</summary>
      <description>In order to detect event patterns we first have to define the pattern. This issue tracks the progress of implementing a user facing API to define event patterns. Patterns should support the following operations next(): The given event has to follow directly after the preceding eventfollowedBy(): The given event has to follow the preceding event. There might occur other events in-between every(): In a follow-by relationship a starting event can be matched with multiple successive events. Consider the pattern a → b where → denotes the follow-by relationship. The event sequence a, b, b can be matched as a, b or a, (b), b where the first b is left out. The essential question is whether a is allowed to match multiple times or only the first time. The method every specifies exactly that. Every events in a pattern can match with multiple successive events. This makes only sense in a follow-by relationship, though. followedByEvery(): Similar to followedBy just that the specified element can be matched with multiple successive events or(): Alternative event which can be matched instead of the original event: every(“e1”).where().or(“e2”).where() within(): Defines a time interval in which the pattern has to be completed, otherwise an incomplete pattern can be emitted (timeout case)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
      <file type="M">flink-libraries.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32162" opendate="2023-5-23 00:00:00" fixdate="2023-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Misleading log message due to missing null check</summary>
      <description>Updating the job requirements always logs "Failed to update requirements for job {}." because we don't check whether the error is not null.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="32166" opendate="2023-5-23 00:00:00" fixdate="2023-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show unassigned/total TM resources in web ui</summary>
      <description>It is important to know how many resources of a TM are currently assigned to jobs.This is different to what resources currently used, since you can have assigned 1gb memory to a job with it only using 10mb at this time.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.task-manager.ts</file>
    </fixedFiles>
  </bug>
  <bug id="32167" opendate="2023-5-23 00:00:00" fixdate="2023-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log dynamic slot creation on task manager</summary>
      <description>When a slot is dynamically allocated on the TM we should log that this happens, what resources it consumes and what the remaining resources are.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.ResourceBudgetManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="32168" opendate="2023-5-23 00:00:00" fixdate="2023-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log required/available resources in RM</summary>
      <description>When matching requirements against available resource the RM currently doesn't log anything apart from whether it could fulfill the resources or not.We can make the system easier to audit by logging the current requirements, available resources, and how many resources are left after the matching.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="32169" opendate="2023-5-23 00:00:00" fixdate="2023-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show allocated slots on TM page</summary>
      <description>Show the allocated slogs on the TM page, so that you can better understand which job is consuming what resources.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.task-manager.ts</file>
    </fixedFiles>
  </bug>
  <bug id="32186" opendate="2023-5-25 00:00:00" fixdate="2023-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support subtask stack auto-search when redirecting from subtask backpressure tab</summary>
      <description>Note that we have introduced a dump link on the backpressure page in FLINK-29996(Figure 1), which helps to check what are the corresponding subtask doing more easily.But we still have to search for the corresponding call stack of the back-pressured subtask from the whole TaskManager thread dumps, it's not convenient enough.Therefore, I would like to trigger the search for the editor automatically after redirecting from the backpressure tab, which will help to scroll the thread dumps to the corresponding call stack of the back-pressured subtask (As shown in Figure 2).Figure 1. ThreadDump Link in Backpressure TabFigure 2. Trigger Auto-search after Redirecting from Backpressure Tab</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0,1.17.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.thread-dump.task-manager-thread-dump.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.thread-dump.task-manager-thread-dump.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="32201" opendate="2023-5-26 00:00:00" fixdate="2023-7-26 01:00:00" resolution="Done">
    <buginformation>
      <summary>Enable the distribution of shuffle descriptors via the blob server by connection number</summary>
      <description>Flink support distributes shuffle descriptors via the blob server to reduce JobManager overhead. But the default threshold to enable it is 1MB, which never reaches. Users need to set a proper value for this, but it requires advanced knowledge before configuring it.I would like to enable this feature by the number of connections of a group of shuffle descriptors. For examples, a simple streaming job with two operators, each with 10,000 parallelism and connected via all-to-all distribution. In this job, we only get one set of shuffle descriptors, and this group has 10000 * 10000 connections. This means that JobManager needs to send this set of shuffle descriptors to 10000 tasks.Since it is also difficult for users to configure, I would like to give it a default value. The serialized shuffle descriptors sizes for different parallelism are shown below. Producer parallelism serialized shuffle descriptor size consumer parallelism total data size that JM needs to send 5000 100KB 5000 500MB 10000 200KB 10000 2GB 20000 400Kb 20000 8GB So, I would like to set the default value to 10,000 * 10,000. Any suggestions or concerns are appreciated.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SchedulerTestingUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.RemoveCachedShuffleDescriptorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.BlockingResultPartitionReleaseTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptorFactoryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobWriter.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.CachedShuffleDescriptorsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptorFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.CachedShuffleDescriptors.java</file>
    </fixedFiles>
  </bug>
  <bug id="32213" opendate="2023-5-30 00:00:00" fixdate="2023-6-30 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add get off heap buffer in memory segment</summary>
      <description>When flink job writes data to data lake such as paimon, iceberg and hudi, the sink will write data to writer buffer first, then flush the data to file system. To manage the writer buffer better, we'd like to allocate segment from managed memory in flink and get off heap buffer to create writer buffer</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.memory.MemorySegmentSimpleTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.MemorySegment.java</file>
    </fixedFiles>
  </bug>
  <bug id="32231" opendate="2023-5-31 00:00:00" fixdate="2023-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>libssl not found when running CI</summary>
      <description>--2023-05-31 19:10:13-- http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.12_amd64.debResolving security.ubuntu.com (security.ubuntu.com)... 185.125.190.39, 91.189.91.38, 91.189.91.39, ...Connecting to security.ubuntu.com (security.ubuntu.com)|185.125.190.39|:80... connected.HTTP request sent, awaiting response... 404 Not Found2023-05-31 19:10:13 ERROR 404: Not Found.e.g.https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49523&amp;view=logs&amp;j=bea52777-eaf8-5663-8482-18fbc3630e81&amp;t=d6e79740-7cf7-5407-2e69-ca34c9be0efb&amp;l=265</description>
      <version>1.18.0,1.16.3,1.17.2</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="32232" opendate="2023-6-1 00:00:00" fixdate="2023-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Supports parse truncate table statement</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug id="32234" opendate="2023-6-1 00:00:00" fixdate="2023-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support execute truncate table statement</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestUpdateDeleteTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.DeletePushDownUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.TruncateTableOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ExecutableOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.ExecutableOperationContextImpl.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlDmlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConverters.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="3226" opendate="2016-1-12 00:00:00" fixdate="2016-3-12 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Translate optimized logical Table API plans into physical plans representing DataSet programs</summary>
      <description>This issue is about translating an (optimized) logical Table API (see FLINK-3225) query plan into a physical plan. The physical plan is a 1-to-1 representation of the DataSet program that will be executed. This means: Each Flink RelNode refers to exactly one Flink DataSet or DataStream operator. All (join and grouping) keys of Flink operators are correctly specified. The expressions which are to be executed in user-code are identified. All fields are referenced with their physical execution-time index. Flink type information is available. Optional: Add physical execution hints for joinsThe translation should be the final part of Calcite's optimization process.For this task we need to: implement a set of Flink DataSet RelNodes. Each RelNode corresponds to one Flink DataSet operator (Map, Reduce, Join, ...). The RelNodes must hold all relevant operator information (keys, user-code expression, strategy hints, parallelism). implement rules to translate optimized Calcite RelNodes into Flink RelNodes. We start with a straight-forward mapping and later add rules that merge several relational operators into a single Flink operator, e.g., merge a join followed by a filter. Timo implemented some rules for the first SQL implementation which can be used as a starting point. Integrate the translation rules into the Calcite optimization process</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataset.DataSetProjectRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.typeinfo.RowSerializerTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.typeinfo.RowComparatorTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.typeinfo.RenamingProxyTypeInfoTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.PojoGroupingITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.JoinITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeinfo.RowTypeInfo.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeinfo.RenamingProxyTypeInfo.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeinfo.RenameOperator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableConfig.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.package.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.ExpressionSelectFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.ExpressionJoinFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.ExpressionFilterFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.ExpressionAggregateFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetSort.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetMap.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetExchange.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.literals.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.Indenter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.GenerateSelect.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.GenerateResultAssembler.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.GenerateJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.GenerateFilter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.ExpressionCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.TableConversions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetReduce.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetFlatMap.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.CastingITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.JavaBatchTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.ScalaBatchTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetSource.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.PlanTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataset.DataSetScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.schema.DataSetTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.TypeConverter.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.AsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.FilterITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.SelectITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.AsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.SelectITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetGroupReduce.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.RexNodeTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.TranslationContext.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.AggregationsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.ExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.OperatorCodeGen.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.parser.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.CastingITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.ExpressionsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.StringExpressionsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.FilterITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.StringExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.test.TableProgramsTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.functions.AggregateFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.functions.aggregate.Aggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.functions.aggregate.AggregateFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.functions.aggregate.AvgAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.functions.aggregate.CountAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.functions.aggregate.MaxAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.functions.aggregate.MinAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.functions.aggregate.SumAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.functions.FunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.logical.FlinkAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataset.DataSetAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataset.DataSetJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.GroupedAggregationsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.UnionITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.GroupedAggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataset.DataSetFilterRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="32265" opendate="2023-6-6 00:00:00" fixdate="2023-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use default classloader in jobmanager when there are no user jars for job</summary>
      <description>Currently job manager will create a new class loader for each flink job even it has no user jars, which may cause metaspace increasing. Flink can use system classloader for the jobs without jars.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobManagerSharedServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="32271" opendate="2023-6-6 00:00:00" fixdate="2023-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Report RECOMMENDED_PARALLELISM as an autoscaler metric</summary>
      <description>It is beneficial to report the recommended parallelism and overlay it with the current parallelism on the same chart when auto scaler is running in advisor mode.</description>
      <version>None</version>
      <fixedVersion>kubernetes-operator-1.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="32288" opendate="2023-6-8 00:00:00" fixdate="2023-6-8 01:00:00" resolution="Done">
    <buginformation>
      <summary>Improve the scheduling performance of AdaptiveBatchScheduler</summary>
      <description>After adding the benchmark of AdaptiveBatchScheduler in FLINK-30480, we noticed a regression in the performance of SchedulingDownstreamTasksInBatchJobBenchmark#SchedulingDownstreamTasks. When scheduling a batch job with a parallelism of 4000*4000, the time spent increased from 32ms to 1336ms on my local PC.To improve the performance, we can optimize the traversal by checking if the consumedPartitionGroups have finished all its partitions.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingInputConsumableDecider.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.DefaultInputConsumableDeciderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.VertexwiseSchedulingStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.PartialFinishedInputConsumableDecider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.InputConsumableDecider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.DefaultInputConsumableDecider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.AllFinishedInputConsumableDecider.java</file>
    </fixedFiles>
  </bug>
  <bug id="32294" opendate="2023-6-9 00:00:00" fixdate="2023-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The CI fails due to HiveITCase</summary>
      <description>2 ITCases fail: HiveITCase.testHiveDialect HiveITCase.testReadWriteHivehttps://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49766&amp;view=logs&amp;j=ef799394-2d67-5ff4-b2e5-410b80c9c0af&amp;t=9e5768bc-daae-5f5f-1861-e58617922c7a&amp;l=14346 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49766&amp;view=logs&amp;j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&amp;s=ae4f8708-9994-57d3-c2d7-b892156e7812&amp;t=0f3adb59-eefa-51c6-2858-3654d9e0749d&amp;l=14652  </description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-hive.src.test.java.org.apache.flink.tests.hive.containers.HiveContainer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3230" opendate="2016-1-13 00:00:00" fixdate="2016-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kinesis streaming producer</summary>
      <description>Add a FlinkKinesisProducer for the Flink Kinesis streaming connector. We will be using AWS SDK implementation for code consistency with the FlinkKinesisConsumer.The features of FlinkKinesisProducer is rather straightforward:1. Partition put records based on partition key.2. Configurable put mode: Bulk put for higher throughput vs. sequential single record puts. Size of bulk should also be configurable.3. For bulk put, user can also choose to enforce strict ordering of the result with the tradeoff of higher put latency. Ref: https://brandur.org/kinesis-order</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.create.release.files.sh</file>
      <file type="M">flink-streaming-connectors.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-twitter.pom.xml</file>
      <file type="M">docs.apis.streaming.connectors.hdfs.md</file>
    </fixedFiles>
  </bug>
  <bug id="32300" opendate="2023-6-9 00:00:00" fixdate="2023-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support get object for result set</summary>
      <description>Support get object for result set</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.test.java.org.apache.flink.table.jdbc.FlinkStatementTest.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.test.java.org.apache.flink.table.jdbc.FlinkResultSetTest.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.FlinkStatement.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.FlinkResultSet.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.BaseStatement.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver-bundle.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32302" opendate="2023-6-9 00:00:00" fixdate="2023-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable Hbase 2.x tests on Java 17</summary>
      <description>Lacking support on the HBase side. Version bumps may solve it, but that's out of scope of this issue since the connector is being externalized.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase-2.2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32304" opendate="2023-6-9 00:00:00" fixdate="2023-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce rpc-akka jar size</summary>
      <description>We bundle unnecessary dependencies in the rpc-akka jar; we can easily shave of 15mb of dependencies.</description>
      <version>None</version>
      <fixedVersion>1.18.0,1.17.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-rpc.flink-rpc-akka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32314" opendate="2023-6-12 00:00:00" fixdate="2023-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ignore class-loading errors after RPC system shutdown</summary>
      <description>In tests we occasionally see the akka rpc service throwing class loading errors after it was shut down.AFAICT our shutdown procedure is correct, and it's just akka shutting down some things asynchronously.I couldn't figure out why/what is still running, so as a bandaid I suggest to ignore classloading errors after the rpc service shutdown has completed.</description>
      <version>None</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.RobustActorSystemTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.RobustActorSystem.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.concurrent.TestingUncaughtExceptionHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="32337" opendate="2023-6-14 00:00:00" fixdate="2023-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL array_union could return wrong result</summary>
      <description>This is was mentioned at https://github.com/apache/flink/pull/22717#issuecomment-1587333488 how to reproduceSELECT array_union(ARRAY[CAST(NULL AS INT)], ARRAY[1]); -- returns [NULL, 1], this is OKSELECT array_union(ARRAY[1], ARRAY[CAST(NULL AS INT)]); -- returns [1, 0], this is NOT OK</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.functions.scalar.ArrayUnionFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CollectionFunctionsITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="32338" opendate="2023-6-14 00:00:00" fixdate="2023-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add FailsOnJava17 annotation</summary>
      <description>Add an annotation for disabling specific tests on Java 17, similar to FailsOnJava11.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-hbase.src.test.java.org.apache.flink.tests.util.hbase.SQLClientHBaseITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="32343" opendate="2023-6-15 00:00:00" fixdate="2023-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix exception for jdbc tools</summary>
      <description>Fix exception for jdbc tools</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.FlinkStatement.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.FlinkConnection.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.BaseStatement.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.BaseConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="32369" opendate="2023-6-16 00:00:00" fixdate="2023-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setup cron build</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.compile.sh</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
    </fixedFiles>
  </bug>
  <bug id="32370" opendate="2023-6-16 00:00:00" fixdate="2023-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC SQl gateway e2e test is unstable</summary>
      <description>The client is failing while trying to collect data when the job already finished on the cluster.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.AbstractHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="32385" opendate="2023-6-20 00:00:00" fixdate="2023-7-20 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce a struct SerializedShuffleDescriptorAndIndices to identify a group of ShuffleDescriptorAndIndex</summary>
      <description>Introduce a new struct named SerializedShuffleDescriptorAndIndices to identify a group of ShuffleDescriptorAndIndex. Then we could cache these ShuffleDescriptorAndIndex in TaskExecutor side</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.RemoveCachedShuffleDescriptorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptorTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptorFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.deployment.CachedShuffleDescriptorsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.IntermediateResult.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.TaskDeploymentDescriptorFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.InputGateDeploymentDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.CachedShuffleDescriptors.java</file>
    </fixedFiles>
  </bug>
  <bug id="32388" opendate="2023-6-20 00:00:00" fixdate="2023-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the ability to pass parameters to CUSTOM PartitionCommitPolicy</summary>
      <description>By allowing the passing of parameters, the custom PartitionCommitPolicy becomes more flexible and customizable. This enables user to enhance their custom PartitionCommitPolicy by including additional functionality, such as passing monitoring parameters to track the files associated with each commit.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.connector.file.table.FileSystemTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableCompactSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectQueryITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveOptions.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.FileSystemCommitterTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.test.java.org.apache.flink.connector.file.table.batch.compact.BatchPartitionCommitterSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.stream.PartitionCommitter.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.PartitionCommitPolicyFactory.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemConnectorOptions.java</file>
      <file type="M">docs.content.docs.connectors.table.filesystem.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.filesystem.md</file>
    </fixedFiles>
  </bug>
  <bug id="32389" opendate="2023-6-20 00:00:00" fixdate="2023-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Dependabot] Bump guava from 27.0.1-jre to 32.0.0-jre</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
      <file type="M">flink-formats.flink-orc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32390" opendate="2023-6-20 00:00:00" fixdate="2023-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Dependabot] Bump socket.io-parser and socket.io and engine.io</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
    </fixedFiles>
  </bug>
  <bug id="32391" opendate="2023-6-20 00:00:00" fixdate="2023-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>YARNSessionFIFOITCase.checkForProhibitedLogContents fails on AZP on Java 17</summary>
      <description>This build failed https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50217&amp;view=logs&amp;j=d871f0ce-7328-5d00-023b-e7391f5801c8&amp;t=77cbea27-feb9-5cf5-53f7-3267f9f9c6b6&amp;l=27971as Jun 20 01:30:32 01:30:32.994 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 79.015 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.yarn.YARNSessionFIFOSecuredITCaseJun 20 01:30:32 01:30:32.994 [ERROR] org.apache.flink.yarn.YARNSessionFIFOSecuredITCase.testDetachedMode Time elapsed: 17.586 s &lt;&lt;&lt; FAILURE!Jun 20 01:30:32 java.lang.AssertionError: Jun 20 01:30:32 Found a file /__w/2/s/flink-yarn-tests/target/test/data/flink-yarn-tests-fifo-secured/yarn-23119131678/flink-yarn-tests-fifo-secured-logDir-nm-1_0/application_1687224557882_0002/container_1687224557882_0002_01_000002/taskmanager.log with a prohibited string (one of [Exception, Started SelectChannelConnector@0.0.0.0:8081]). Excerpts:Jun 20 01:30:32 [Jun 20 01:30:32 2023-06-20 01:29:57,749 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Stopping Akka RPC service.Jun 20 01:30:32 2023-06-20 01:29:57,767 WARN akka.actor.CoordinatedShutdown [] - Could not addJvmShutdownHook, due to: Shutdown in progressJun 20 01:30:32 2023-06-20 01:29:57,767 INFO akka.actor.CoordinatedShutdown [] - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]Jun 20 01:30:32 2023-06-20 01:29:57,768 WARN akka.actor.CoordinatedShutdown [] - Could not addJvmShutdownHook, due to: Shutdown in progressJun 20 01:30:32 2023-06-20 01:29:57,768 INFO akka.actor.CoordinatedShutdown [] - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]Jun 20 01:30:32 2023-06-20 01:29:57,781 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator [] - Shutting down remote daemon.Jun 20 01:30:32 2023-06-20 01:29:57,781 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator [] - Shutting down remote daemon.Jun 20 01:30:32 2023-06-20 01:29:57,782 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator [] - Remote daemon shut down; proceeding with flushing remote transports.Jun 20 01:30:32 2023-06-20 01:29:57,782 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator [] - Remote daemon shut down; proceeding with flushing remote transports.Jun 20 01:30:32 2023-06-20 01:29:57,788 WARN akka.remote.transport.netty.NettyTransport [] - Remote connection to [264d5b384bcc/192.168.224.2:42920] failed with java.net.SocketException: Connection resetJun 20 01:30:32 2023-06-20 01:29:57,788 WARN akka.remote.transport.netty.NettyTransport [] - Remote connection to [264d5b384bcc/192.168.224.2:42920] failed with java.net.SocketException: Connection resetJun 20 01:30:32 ]</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="32392" opendate="2023-6-20 00:00:00" fixdate="2023-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Several jobs failed on AZP with No space left on device</summary>
      <description>This Build failed with no space left https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50162&amp;view=logs&amp;j=585d8b77-fa33-51bc-8163-03e54ba9ce5b##[error]Unhandled exception. System.IO.IOException: No space left on device : '/home/vsts/agents/3.220.5/_diag/Worker_20230619-021757-utc.log' at System.IO.RandomAccess.WriteAtOffset(SafeFileHandle handle, ReadOnlySpan`1 buffer, Int64 fileOffset) at System.IO.Strategies.BufferedFileStreamStrategy.FlushWrite() at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder) at System.Diagnostics.TextWriterTraceListener.Flush() at Microsoft.VisualStudio.Services.Agent.HostTraceListener.WriteHeader(String source, TraceEventType eventType, Int32 id) in /home/vsts/work/1/s/src/Microsoft.VisualStudio.Services.Agent/HostTraceListener.cs:line 151 at Microsoft.VisualStudio.Services.Agent.HostTraceListener.TraceEvent(TraceEventCache eventCache, String source, TraceEventType eventType, Int32 id, String message) in /home/vsts/work/1/s/src/Microsoft.VisualStudio.Services.Agent/HostTraceListener.cs:line 81 at System.Diagnostics.TraceSource.TraceEvent(TraceEventType eventType, Int32 id, String message) at Microsoft.VisualStudio.Services.Agent.Util.ProcessInvoker.ProcessExitedHandler(Object sender, EventArgs e) in /home/vsts/work/1/s/src/Agent.Sdk/ProcessInvoker.cs:line 496 at System.Diagnostics.Process.OnExited() at System.Diagnostics.Process.RaiseOnExited() at System.Diagnostics.Process.CompletionCallback(Object waitHandleContext, Boolean wasSignaled) at System.Threading._ThreadPoolWaitOrTimerCallback.WaitOrTimerCallback_Context_f(Object state) at System.Threading.ExecutionContext.RunInternal(ExecutionContext executionContext, ContextCallback callback, Object state)--- End of stack trace from previous location --- at System.Threading._ThreadPoolWaitOrTimerCallback.PerformWaitOrTimerCallback(_ThreadPoolWaitOrTimerCallback helper, Boolean timedOut) at System.Threading.PortableThreadPool.CompleteWait(RegisteredWaitHandle handle, Boolean timedOut) at System.Threading.ThreadPoolWorkQueue.Dispatch() at System.Threading.PortableThreadPool.WorkerThread.WorkerThreadStart(),##[error]The hosted runner encountered an error while running your job. (Error Type: Failure).for 1.16, 1.17 it happens while 'Upload artifacts to S3'for 1.18 while 'Deploy maven snapshot'</description>
      <version>1.18.0,1.16.3,1.17.2</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
      <file type="M">tools.azure-pipelines.build-nightly-dist.yml</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug id="32396" opendate="2023-6-20 00:00:00" fixdate="2023-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support timestamp for jdbc driver and gateway</summary>
      <description>Support timestamp and timestamp_ltz data type for jdbc driver and sql-gateway</description>
      <version>1.18.0</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.test.java.org.apache.flink.table.jdbc.FlinkStatementTest.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.utils.ArrayFieldGetter.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.FlinkResultSet.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.serde.ResultInfo.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.handler.statement.FetchResultsHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="32403" opendate="2023-6-21 00:00:00" fixdate="2023-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add database related operations in catalog manager</summary>
      <description>Add database operations in catalog manager for different sql operations</description>
      <version>1.18.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlDdlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ddl.DropDatabaseOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ddl.CreateDatabaseOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ddl.AlterDatabaseOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="32404" opendate="2023-6-21 00:00:00" fixdate="2023-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce catalog modification listener and factory interfaces</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.api.EnvironmentTest.java</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.factories.TableFactoryUtil.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.TableConfigOptions.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.SessionContext.java</file>
      <file type="M">docs.layouts.shortcodes.generated.table.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="32406" opendate="2023-6-21 00:00:00" fixdate="2023-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Notify catalog listener for database events</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.listener.CatalogContext.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="32407" opendate="2023-6-21 00:00:00" fixdate="2023-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Notify catalog listener for table events</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.CatalogManagerTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.listener.CatalogContext.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="32426" opendate="2023-6-25 00:00:00" fixdate="2023-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix adaptive local hash agg can&amp;#39;t work when auxGrouping exist</summary>
      <description>For the following case, the field `a` is primary key,  we select from `AuxGroupingTable` and group by a, b. Since a is primary key, it also guarantee the unique, so planner will extract b as auxGrouping field.registerCollection( "AuxGroupingTable", data2, type2, "a, b, c, d, e", nullablesOfData2, FlinkStatistic.builder().uniqueKeys(Set(Set("a").asJava).asJava).build())checkResult( "SELECT a, b, COUNT(c) FROM AuxGroupingTable GROUP BY a, b", Seq( row(1, 1, 1), row(2, 3, 2), row(3, 4, 3), row(4, 10, 4), row(5, 11, 5) ))  Due to the generated code doesn't get auxGrouping fields from input RowData and then setting it to aggBuffer, the aggBuffer RowData loses some fields, and it will throw an index Exception when get the field from it. As following:Caused by: java.lang.AssertionError: index (1) should &lt; 1    at org.apache.flink.table.data.binary.BinaryRowData.assertIndexIsValid(BinaryRowData.java:127)    at org.apache.flink.table.data.binary.BinaryRowData.isNullAt(BinaryRowData.java:156)    at org.apache.flink.table.data.utils.JoinedRowData.isNullAt(JoinedRowData.java:113)    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.toBinaryRow(RowDataSerializer.java:201)    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.serialize(RowDataSerializer.java:103)    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.serialize(RowDataSerializer.java:48)    at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.serialize(StreamElementSerializer.java:165)    at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.serialize(StreamElementSerializer.java:43)    at org.apache.flink.runtime.plugable.SerializationDelegate.write(SerializationDelegate.java:54)    at org.apache.flink.runtime.io.network.api.writer.RecordWriter.serializeRecord(RecordWriter.java:141)    at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:107)    at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:55)    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:134)    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collectAndCheckIfChained(RecordWriterOutput.java:114)    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:95)    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:48)    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:59)    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:31)    at LocalHashAggregateWithKeys$39.processElement_split2(Unknown Source)    at LocalHashAggregateWithKeys$39.processElement(Unknown Source)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:108)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:77)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)    at BatchExecCalc$10.processElement(Unknown Source)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:108)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:77)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)    at SourceConversion$6.processElement(Unknown Source)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:108)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:77)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)    at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)    at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)    at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)    at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)</description>
      <version>1.18.0,1.17.1</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.HashAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ProjectionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="32428" opendate="2023-6-25 00:00:00" fixdate="2023-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce base interfaces for CatalogStore</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.FactoryUtilTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FactoryUtil.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
    </fixedFiles>
  </bug>
  <bug id="3246" opendate="2016-1-17 00:00:00" fixdate="2016-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Consolidate maven project names with *-parent suffix</summary>
      <description>The projects flink-streaming-connectors-parent and flink-contrib parent carry the unnecessary -parent suffix.I suspect that was mistakenly added when looking at the root project called flink-parent. The suffix was added there to not have a project with an unqualified name flink. However, for the projects mentioned here, that suffix is not necessary and can be dropped.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-twitter.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-rabbitmq.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-nifi.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-flume.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-elasticsearch.pom.xml</file>
      <file type="M">flink-contrib.pom.xml</file>
      <file type="M">flink-contrib.flink-tweet-inputformat.pom.xml</file>
      <file type="M">flink-contrib.flink-streaming-contrib.pom.xml</file>
      <file type="M">flink-contrib.flink-storm.pom.xml</file>
      <file type="M">flink-contrib.flink-storm-examples.pom.xml</file>
      <file type="M">flink-contrib.flink-operator-stats.pom.xml</file>
      <file type="M">flink-contrib.flink-connector-wikiedits.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32460" opendate="2023-6-28 00:00:00" fixdate="2023-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add doc for list procedures</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.show.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.show.md</file>
    </fixedFiles>
  </bug>
  <bug id="32466" opendate="2023-6-28 00:00:00" fixdate="2023-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalid input strategy for many functions which allows BINARY strings</summary>
      <description>"string" in SQL terms covers both character strings and binary strings. The author of CONCAT might not have known this. In any case, the code gen instead of the validator fails when executing:TableEnvironment t = TableEnvironment.create(EnvironmentSettings.inStreamingMode());t.createTemporaryView("t", t.fromValues(lit(new byte[] {97})));t.executeSql("SELECT CONCAT(f0, '-magic') FROM t").print();As future work, we should also allow binary strings.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="32468" opendate="2023-6-28 00:00:00" fixdate="2023-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Akka by Pekko</summary>
      <description>Akka 2.6.x will not receive security fixes from September 2023 onwards (see https://discuss.lightbend.com/t/2-6-x-maintenance-proposal/9949). A mid-term plan to replace Akka is described in FLINK-29281. In the meantime, we suggest to replace Akka by Apache Pekko (incubating), which is a fork of Akka 2.6.x under the Apache 2.0 license. This way - if needed - we at least have the ability to release security fixes ourselves in collaboration with the Pekko community.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaUtils.java</file>
      <file type="M">tools.ci.test.controller.sh</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnTaskExecutorRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManagerDriver.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBaseTest.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.IPv6HostnamesITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.cancelling.CancelingTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.accumulators.AccumulatorLiveITCase.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.container.FlinkContainersSettings.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecDynamicFilteringDataCollector.java</file>
      <file type="M">flink-table.flink-table-planner-loader.pom.xml</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.RpcSSLAuthITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.StandaloneLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.TestingJobMasterGatewayBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.SlotCountExceedingParallelismTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartialConsumePipelinedResultTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.LeaderRetrievalUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.ConnectionUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.netty.TieredStorageNettyServiceImpl.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.RpcUtils.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.TimeoutCallStackTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.SupervisorActorTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.RobustActorSystemTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.RemoteAkkaRpcActorTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.MessageSerializationTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.MainThreadValidationTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.ContextClassLoadingSettingTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaUtilsTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcSerializedValueTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorOversizedResponseMessageTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorHandshakeTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaBootstrapToolsTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaActorSystemTest.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.test.java.org.apache.flink.runtime.rpc.akka.ActorSystemExtension.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.resources.META-INF.services.org.apache.flink.runtime.rpc.RpcSystem</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.SupervisorActor.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.RobustActorSystem.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.RemoteAddressExtension.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.PriorityThreadsDispatcher.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.PrioritySettingThreadFactory.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.HostAndPort.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.FencedAkkaInvocationHandler.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.exceptions.AkkaUnknownMessageException.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.exceptions.AkkaRpcRuntimeException.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.exceptions.AkkaRpcInvalidStateException.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.exceptions.AkkaRpcException.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.exceptions.AkkaHandshakeException.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.EscalatingSupervisorStrategy.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.DeadLettersActor.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.CustomSSLEngineProvider.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.ControlMessages.java</file>
      <file type="M">docs.content.zh.docs.deployment.config.md</file>
      <file type="M">docs.content.zh.docs.deployment.memory.mem.setup.jobmanager.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.standalone.kubernetes.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.yarn.md</file>
      <file type="M">docs.content.zh.docs.deployment.security.security-ssl.md</file>
      <file type="M">docs.content.docs.deployment.config.md</file>
      <file type="M">docs.content.docs.deployment.memory.mem.setup.jobmanager.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.kubernetes.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.yarn.md</file>
      <file type="M">docs.content.docs.deployment.security.security-ssl.md</file>
      <file type="M">docs.content.docs.ops.state.checkpoints.md</file>
      <file type="M">docs.layouts.shortcodes.generated.akka.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.taskmanager.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.metric.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.task.manager.configuration.html</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.AkkaOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.MetricOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.NetUtils.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.testutils.TestingUtils.java</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j-console.properties</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j.properties</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.logback-console.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.logback.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.heavy.deployment.sh</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.cli.KubernetesSessionCliTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionAndRetrievalITCase.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-rpc.flink-rpc-akka-loader.pom.xml</file>
      <file type="M">flink-rpc.flink-rpc-akka-loader.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcSystemLoader.java</file>
      <file type="M">flink-rpc.flink-rpc-akka-loader.src.main.resources.META-INF.services.org.apache.flink.runtime.rpc.RpcSystemLoader</file>
      <file type="M">flink-rpc.flink-rpc-akka-loader.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcSystemLoaderITCase.java</file>
      <file type="M">flink-rpc.flink-rpc-akka-loader.src.test.java.org.apache.flink.runtime.rpc.akka.FallbackAkkaRpcSystemLoader.java</file>
      <file type="M">flink-rpc.flink-rpc-akka-loader.src.test.resources.META-INF.services.org.apache.flink.runtime.rpc.RpcSystemLoader</file>
      <file type="M">flink-rpc.flink-rpc-akka.pom.xml</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.concurrent.akka.ActorSystemScheduledExecutorAdapter.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaBasedEndpoint.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaBootstrapTools.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActor.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcSerializedValue.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcService.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceConfiguration.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils.java</file>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcSystem.java</file>
    </fixedFiles>
  </bug>
  <bug id="32479" opendate="2023-6-29 00:00:00" fixdate="2023-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tests revoke leadership too early</summary>
      <description>There are a few tests issue a request to the dispatcher and immediately revoke leadership. In this case there is no guarantee that the request arrived before leadership was revoked, so it could fail if it arrives afterwards since we reject requests if we aren't the leader anymore.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderChangeClusterComponentsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.JobDispatcherITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="32491" opendate="2023-6-29 00:00:00" fixdate="2023-7-29 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce RuntimeFilterOperator to support runtime filter which can reduce the shuffle data size before shuffle join</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.runtimefilter.LocalRuntimeFilterBuilderOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.runtimefilter.GlobalRuntimeFilterBuilderOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.runtimefilter.util.RuntimeFilterUtils.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.runtimefilter.LocalRuntimeFilterBuilderOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="32492" opendate="2023-6-30 00:00:00" fixdate="2023-7-30 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce FlinkRuntimeFilterProgram to inject runtime filter</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.DynamicFilteringITCase.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.OptimizerConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.optimizer.config.configuration.html</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.JoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalJoinRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkBatchProgram.scala</file>
    </fixedFiles>
  </bug>
  <bug id="32498" opendate="2023-6-30 00:00:00" fixdate="2023-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>array_max return type should always nullable</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CollectionFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.inference.strategies.ArrayElementOutputTypeStrategyTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.strategies.SpecificTypeStrategies.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.strategies.ArrayElementOutputTypeStrategy.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
    </fixedFiles>
  </bug>
  <bug id="3250" opendate="2016-1-18 00:00:00" fixdate="2016-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Savepoint coordinator requires too strict parallelism match</summary>
      <description>The savepoint coordinator requires that the number of collected states of an operator and the parallelism of the new job (the one to be restored) match exactly. This is too strict.For example a Kafka source with parallelism 2 and a single Kafka partition does not collect state for one of the two sources (hence it is not part of the savepoint state). Currently, restoring the same job with the same parallelism fails, which should not happen.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.SavepointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.SavepointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="32506" opendate="2023-6-30 00:00:00" fixdate="2023-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the watermark aggregation benchmark for source coordinator</summary>
      <description>FLINK-32420 is improving the watermark aggregation performance.We want to add a benchmark for it first, and then we can see the official performance change.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorAlignmentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.source.coordinator.CoordinatorTestUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="3251" opendate="2016-1-18 00:00:00" fixdate="2016-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkpoint stats show ghost numbers</summary>
      <description>StephanEwen reported an issue with the display of checkpoint stats. A pipeline with a stateful source and stateless intermediate operator shows stats for the stateless intermediate operator. The numbers are most likely the same as for the source operator.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.stats.SimpleCheckpointStatsTrackerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.stats.SimpleCheckpointStatsTracker.java</file>
    </fixedFiles>
  </bug>
  <bug id="32526" opendate="2023-7-4 00:00:00" fixdate="2023-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update apache parquet to 1.13.1</summary>
      <description>Now 1.13.1 is availablehttps://parquet.apache.org/blog/</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">flink-formats.flink-sql-parquet.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="32536" opendate="2023-7-4 00:00:00" fixdate="2023-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python tests fail with Arrow DirectBuffer exception</summary>
      <description>https://dev.azure.com/chesnay/flink/_build/results?buildId=3674&amp;view=logs&amp;j=fba17979-6d2e-591d-72f1-97cf42797c11&amp;t=727942b6-6137-54f7-1ef9-e66e706ea0682023-07-04T12:54:15.5296754Z Jul 04 12:54:15 E py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame.2023-07-04T12:54:15.5299579Z Jul 04 12:54:15 E : java.lang.RuntimeException: Arrow depends on DirectByteBuffer.&lt;init&gt;(long, int) which is not available. Please set the system property 'io.netty.tryReflectionSetAccessible' to 'true'.2023-07-04T12:54:15.5302307Z Jul 04 12:54:15 E at org.apache.flink.table.runtime.arrow.ArrowUtils.checkArrowUsable(ArrowUtils.java:184)2023-07-04T12:54:15.5302859Z Jul 04 12:54:15 E at org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame(ArrowUtils.java:546)2023-07-04T12:54:15.5303177Z Jul 04 12:54:15 E at jdk.internal.reflect.GeneratedMethodAccessor287.invoke(Unknown Source)2023-07-04T12:54:15.5303515Z Jul 04 12:54:15 E at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2023-07-04T12:54:15.5303929Z Jul 04 12:54:15 E at java.base/java.lang.reflect.Method.invoke(Method.java:568)2023-07-04T12:54:15.5307338Z Jul 04 12:54:15 E at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)2023-07-04T12:54:15.5309888Z Jul 04 12:54:15 E at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)2023-07-04T12:54:15.5310306Z Jul 04 12:54:15 E at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)2023-07-04T12:54:15.5337220Z Jul 04 12:54:15 E at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)2023-07-04T12:54:15.5341859Z Jul 04 12:54:15 E at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)2023-07-04T12:54:15.5342363Z Jul 04 12:54:15 E at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)2023-07-04T12:54:15.5344866Z Jul 04 12:54:15 E at java.base/java.lang.Thread.run(Thread.java:833)2023-07-04T12:54:15.5663559Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::BatchPandasConversionTests::test_empty_to_pandas2023-07-04T12:54:15.5663891Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::BatchPandasConversionTests::test_from_pandas2023-07-04T12:54:15.5664299Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::BatchPandasConversionTests::test_to_pandas2023-07-04T12:54:15.5664655Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::BatchPandasConversionTests::test_to_pandas_for_retract_table2023-07-04T12:54:15.5665003Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::StreamPandasConversionTests::test_empty_to_pandas2023-07-04T12:54:15.5665360Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::StreamPandasConversionTests::test_from_pandas2023-07-04T12:54:15.5665704Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::StreamPandasConversionTests::test_to_pandas2023-07-04T12:54:15.5666045Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::StreamPandasConversionTests::test_to_pandas_for_retract_table2023-07-04T12:54:15.5666415Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_conversion.py::StreamPandasConversionTests::test_to_pandas_with_event_time2023-07-04T12:54:15.5666840Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::BatchPandasUDAFITTests::test_group_aggregate_function2023-07-04T12:54:15.5667189Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::BatchPandasUDAFITTests::test_group_aggregate_with_aux_group2023-07-04T12:54:15.5667526Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::BatchPandasUDAFITTests::test_group_aggregate_without_keys2023-07-04T12:54:15.5667882Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::BatchPandasUDAFITTests::test_over_window_aggregate_function2023-07-04T12:54:15.5668242Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::BatchPandasUDAFITTests::test_slide_group_window_aggregate_function2023-07-04T12:54:15.5668607Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::BatchPandasUDAFITTests::test_tumble_group_window_aggregate_function2023-07-04T12:54:15.5668961Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_execute_over_aggregate_from_json_plan2023-07-04T12:54:15.5669334Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_proc_time_over_rows_window_aggregate_function2023-07-04T12:54:15.5669714Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_row_time_over_range_window_aggregate_function2023-07-04T12:54:15.5670085Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_row_time_over_rows_window_aggregate_function2023-07-04T12:54:15.5670450Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_sliding_group_window_over_count2023-07-04T12:54:15.5670804Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_sliding_group_window_over_time2023-07-04T12:54:15.5671168Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_tumbling_group_window_over_count2023-07-04T12:54:15.5671510Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udaf.py::StreamPandasUDAFITTests::test_tumbling_group_window_over_time2023-07-04T12:54:15.5671847Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::BatchPandasUDFITTests::test_all_data_types2023-07-04T12:54:15.5672171Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::BatchPandasUDFITTests::test_basic_functionality2023-07-04T12:54:15.5672544Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::BatchPandasUDFITTests::test_data_types2023-07-04T12:54:15.5672864Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::BatchPandasUDFITTests::test_invalid_pandas_udf2023-07-04T12:54:15.5673188Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::StreamPandasUDFITTests::test_all_data_types2023-07-04T12:54:15.5673501Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::StreamPandasUDFITTests::test_basic_functionality2023-07-04T12:54:15.5673881Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::StreamPandasUDFITTests::test_data_types2023-07-04T12:54:15.5674201Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_pandas_udf.py::StreamPandasUDFITTests::test_invalid_pandas_udf2023-07-04T12:54:15.5674544Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_row_based_operation.py::BatchRowBasedOperationITTests::test_aggregate_with_pandas_udaf2023-07-04T12:54:15.5674941Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_row_based_operation.py::BatchRowBasedOperationITTests::test_aggregate_with_pandas_udaf_without_keys2023-07-04T12:54:15.5675324Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_row_based_operation.py::BatchRowBasedOperationITTests::test_map_with_pandas_udf2023-07-04T12:54:15.5675684Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_row_based_operation.py::StreamRowBasedOperationITTests::test_aggregate2023-07-04T12:54:15.5676033Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_row_based_operation.py::StreamRowBasedOperationITTests::test_flat_aggregate2023-07-04T12:54:15.5676401Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_row_based_operation.py::StreamRowBasedOperationITTests::test_flat_aggregate_list_view2023-07-04T12:54:15.5676771Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_row_based_operation.py::StreamRowBasedOperationITTests::test_map_with_pandas_udf2023-07-04T12:54:15.5677224Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_sql.py::JavaSqlTests::test_java_sql_ddl - sub...2023-07-04T12:54:15.5677536Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_data_view_clear2023-07-04T12:54:15.5677856Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_distinct_and_filter2023-07-04T12:54:15.5678162Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_double_aggregate2023-07-04T12:54:15.5678476Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_list_view2023-07-04T12:54:15.5678784Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_map_view2023-07-04T12:54:15.5679082Z Jul 04 12:54:15 FAILED pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_map_view_iterate</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
    </fixedFiles>
  </bug>
  <bug id="32539" opendate="2023-7-5 00:00:00" fixdate="2023-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Archunit violations started to fail in test_misc</summary>
      <description>blocker since now it fails on every buildto reproduce jdk 8 is requiredmvn clean install -DskipTestsmvn verify -pl flink-architecture-tests/flink-architecture-tests-production/ -Darchunit.freeze.store.default.allowStoreUpdate=falseIt seems the reason is FLINK-27415where it was removed line checkArgument(fileLength &gt; 0L);at the same time it was mentioned in achunit violations and now should be removed as wellexample of failurehttps://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50946&amp;view=logs&amp;j=fc5181b0-e452-5c8f-68de-1097947f6483&amp;t=995c650b-6573-581c-9ce6-7ad4cc038461&amp;l=23064</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.f7a4e6fa-e7de-48c9-a61e-c13e83f0c72e</file>
    </fixedFiles>
  </bug>
  <bug id="32541" opendate="2023-7-5 00:00:00" fixdate="2023-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the buffer leaking in buffer accumulators when a failover occurs</summary>
      <description>When a failover occurs, the buffers in the sort/hash accumulators should be released correctly to avoid buffers leaking. </description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.disk.SubpartitionDiskCacheManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.SortBufferAccumulatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.HashBufferAccumulatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.disk.SubpartitionDiskCacheManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.HashSubpartitionBufferAccumulator.java</file>
    </fixedFiles>
  </bug>
  <bug id="32544" opendate="2023-7-5 00:00:00" fixdate="2023-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PythonFunctionFactoryTest fails on Java 17</summary>
      <description>https://dev.azure.com/chesnay/flink/_build/results?buildId=3676&amp;view=logs&amp;j=fba17979-6d2e-591d-72f1-97cf42797c11&amp;t=727942b6-6137-54f7-1ef9-e66e706ea068Jul 05 10:17:23 Exception in thread "main" java.lang.reflect.InaccessibleObjectException: Unable to make field private static java.util.IdentityHashMap java.lang.ApplicationShutdownHooks.hooks accessible: module java.base does not "opens java.lang" to unnamed module @1880a322Jul 05 10:17:23 at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)Jul 05 10:17:23 at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)Jul 05 10:17:23 at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)Jul 05 10:17:23 at java.base/java.lang.reflect.Field.setAccessible(Field.java:172)Jul 05 10:17:23 at org.apache.flink.client.python.PythonFunctionFactoryTest.closeStartedPythonProcess(PythonFunctionFactoryTest.java:115)Jul 05 10:17:23 at org.apache.flink.client.python.PythonFunctionFactoryTest.cleanEnvironment(PythonFunctionFactoryTest.java:79)Jul 05 10:17:23 at org.apache.flink.client.python.PythonFunctionFactoryTest.main(PythonFunctionFactoryTest.java:52)Side-notes: maybe re-evaluate if the test could be run through maven now The shutdown hooks business is quite sketchy, and AFAICT would be unnecessary if the test were an ITCase</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.sql.py</file>
    </fixedFiles>
  </bug>
  <bug id="32549" opendate="2023-7-6 00:00:00" fixdate="2023-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tiered storage memory manager supports ownership transfer for buffers</summary>
      <description>Currently, the accumulator is responsible for requesting all buffers, leading to an inaccurate number of requested buffers for each tier. To address this issue, buffer ownership must be transferred from the accumulator to the tiers when writing them, which will enable the memory manager to maintain a correct number of requested buffers for different owners.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.memory.MemoryTierProducerAgentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.disk.DiskTierProducerAgentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.TestingTierProducerAgent.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.TestingTieredStorageMemoryManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.TieredStorageMemoryManagerImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.TierProducerAgent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.remote.RemoteTierProducerAgent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.memory.MemoryTierProducerAgent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.disk.DiskTierProducerAgent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.disk.DiskCacheManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.TieredStorageProducerClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.TieredStorageMemoryManagerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.TieredStorageMemoryManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.SortBufferAccumulator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.ReadOnlySlicedNetworkBuffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.NetworkBuffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.FileRegionBuffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.CompositeBuffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.Buffer.java</file>
    </fixedFiles>
  </bug>
  <bug id="32568" opendate="2023-7-10 00:00:00" fixdate="2023-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure that all subtasks are sorted by busy ratio at the backpressure tab by default</summary>
      <description>After FLINK-29998 and FLINK-30468, all subtasks are sorted by busy ratio at the backpressure tab by default.FLINK-30829 makes the backpressure tab could be sort by busy/backpressure/idle seperately. However, the default sort rule is changed.Following is the picture about it, all subtask are sorted by 3 columns be default:</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
    </fixedFiles>
  </bug>
  <bug id="32584" opendate="2023-7-12 00:00:00" fixdate="2023-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make it possible to unset default catalog and/or database</summary>
      <description>In certain scenarios it might make sense to unset the default catalog and/or database. For example in a situation when there is no sane default one, but we want the user make that decision consciously. This change has a narrow scope and changes only some checks in the API surface.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.delegation.PlannerContext.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.Catalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="32585" opendate="2023-7-12 00:00:00" fixdate="2023-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filter javax.xml.bind:jaxb-api false positive for Pulsar connector</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.flink-ci-tools.src.main.java.org.apache.flink.tools.ci.licensecheck.JarFileChecker.java</file>
    </fixedFiles>
  </bug>
  <bug id="32592" opendate="2023-7-14 00:00:00" fixdate="2023-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>(Stream)ExEnv#initializeContextEnvironment isn&amp;#39;t thread-safe</summary>
      <description>ContextWe are using the flink-k8s-operator to deploy multiple jobs (up to 32) to a single session cluster. The job submissions done by the operator happen concurrently, basically at the same time.Operator version: 1.5.0Flink version:  1.15.4, 1.7.1, 1.18 (master@f37d41cf)ProblemRarely (~once every 50 deployments) one of the jobs will not be executed. In the following incident 4 jobs are deployed at the same time: gorner-task-staging-e5730831 gorner-facility-staging-e5730831 gorner-aepp-staging-e5730831 gorner-session-staging-e5730831 The operator submits the job, they all get a reasonable jobID:2023-07-14 10:25:35,295 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/gorner-task-staging-e5730831] Submitting job: 4968b186061e44390000000000000002 to session cluster.2023-07-14 10:25:35,297 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/gorner-facility-staging-e5730831] Submitting job: 91a5260d916c4dff0000000000000002 to session cluster.2023-07-14 10:25:35,301 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/gorner-aepp-staging-e5730831] Submitting job: 103c0446e14749a10000000000000002 to session cluster.2023-07-14 10:25:35,302 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/gorner-session-staging-e5730831] Submitting job: de59304d370b4b8e0000000000000002 to session cluster.In the cluster the JarRunHandler's handleRequest() method will get the request, all 4 jobIDs are present (also all args, etc are correct):2023-07-14 10:25:35,320 WARN org.apache.flink.runtime.webmonitor.handlers.JarRunHandler [] - handleRequest - requestBody.jobId: 4968b186061e443900000000000000022023-07-14 10:25:35,321 WARN org.apache.flink.runtime.webmonitor.handlers.JarRunHandler [] - handleRequest - requestBody.jobId: de59304d370b4b8e00000000000000022023-07-14 10:25:35,321 WARN org.apache.flink.runtime.webmonitor.handlers.JarRunHandler [] - handleRequest - requestBody.jobId: 91a5260d916c4dff00000000000000022023-07-14 10:25:35,321 WARN org.apache.flink.runtime.webmonitor.handlers.JarRunHandler [] - handleRequest - requestBody.jobId: 103c0446e14749a10000000000000002But once the EmbeddedExecutor's submitAndGetJobClientFuture() method is called instead of getting 1 call per jobID we have 4 calls but one of the jobIDs twice:2023-07-14 10:25:35,616 WARN org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - optJobId: Optional[4968b186061e44390000000000000002]2023-07-14 10:25:35,616 WARN org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - optJobId: Optional[103c0446e14749a10000000000000002]2023-07-14 10:25:35,616 WARN org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - optJobId: Optional[de59304d370b4b8e0000000000000002]2023-07-14 10:25:35,721 WARN org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - optJobId: Optional[de59304d370b4b8e0000000000000002]If this is important: the jobGraph obtained does not match the jobID. We get 2 times de59304d370b4b8e0000000000000002 but the jobgraph for this jobID is never returned by getJobGraph() in EmbeddedExecutor.submitAndGetJobClientFuture().This will then lead to the job already existing:2023-07-14 10:25:35,616 WARN org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - submittedJobIds: []2023-07-14 10:25:35,616 WARN org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - submittedJobIds: []2023-07-14 10:25:35,616 WARN org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - submittedJobIds: []2023-07-14 10:25:35,721 WARN org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - submittedJobIds: [de59304d370b4b8e0000000000000002]But since the jobs are completely different the execution will fail. Depending on the timing with one of the following exceptions: RestHandlerException: No jobs included in application ClassNotFoundException: io.dectris.aelps.pipelines.gorner.facility.FacilityEventProcessor </description>
      <version>1.15.4,1.18.0,1.17.1</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.StreamExecutionEnvironmentTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="32608" opendate="2023-7-17 00:00:00" fixdate="2023-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve source reusing with projection push down</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.NonDeterministicDagTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.optimize.program.DynamicPartitionPruningProgramTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.hints.batch.ShuffleMergeJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.hints.batch.ShuffleHashJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.hints.batch.NestLoopJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.hints.batch.BroadcastJoinHintTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.ForwardHashExchangeTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DynamicFilteringTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.analyze.NonDeterministicUpdateAnalyzerTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.analyze.GroupAggregationAnalyzerTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.FlinkCalcMergeRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.reuse.SubplanReuser.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.common.CommonPhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.utils.FlinkRelUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.spec.DynamicTableSourceSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.WatermarkPushDownSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.SourceWatermarkSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.SourceAbilitySpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.ReadingMetadataSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.ProjectPushDownSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.PartitionPushDownSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.LimitPushDownSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.FilterPushDownSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.AggregatePushDownSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSourceUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FactoryUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="32617" opendate="2023-7-18 00:00:00" fixdate="2023-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FlinkResultSetMetaData throw exception for most methods</summary>
      <description>I think most methods, e.g. ```boolean supportsMultipleResultSets() throws SQLException;boolean supportsMultipleTransactions() throws SQLException;boolean supportsMinimumSQLGrammar() throws SQLException;```We can just return true or false.   </description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.test.java.org.apache.flink.table.jdbc.FlinkDatabaseMetaDataTest.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.FlinkDatabaseMetaData.java</file>
      <file type="M">flink-table.flink-sql-jdbc-driver.src.main.java.org.apache.flink.table.jdbc.BaseDatabaseMetaData.java</file>
    </fixedFiles>
  </bug>
  <bug id="32623" opendate="2023-7-18 00:00:00" fixdate="2023-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rest api doesn&amp;#39;t return minimum resource requirements correctly</summary>
      <description>The resource requirements returned by the rest api always return a hardcoded 1 lower bound for each vertex.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.java</file>
    </fixedFiles>
  </bug>
  <bug id="32640" opendate="2023-7-21 00:00:00" fixdate="2023-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hard backpressure time should also be measured in hybrid shuffle</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.TestingTieredStorageMemoryManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.TieredStorageMemoryManagerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.TieredStorageMemoryManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.shuffle.TieredResultPartition.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HybridShuffleTestUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsOutputMetrics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsMemoryDataManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="32656" opendate="2023-7-24 00:00:00" fixdate="2023-8-24 01:00:00" resolution="Done">
    <buginformation>
      <summary>Deprecate ManagedTable related APIs</summary>
      <description>Please refer to FLIP-346: Deprecate ManagedTable related APIs for more details.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.ManagedTableFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.RequireCatalogLock.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.CatalogLock.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.Catalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.ManagedTableListener.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableDescriptor.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalogLock.java</file>
    </fixedFiles>
  </bug>
  <bug id="32657" opendate="2023-7-24 00:00:00" fixdate="2023-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert upgrading ExecNode versions for StateMetadata</summary>
      <description>In theory, introducing a new attribute in ExecNode requires upgrading the version of ExecNodeMetadata. However, since this is currently an experimental feature, the attributes that need to be serialized for the final exec node are still being iterated, and upgrading the version will make the serialization scheme of the lower version become immutable (unless a patch is applied to the old version), and the testing framework is not perfect either. Therefore, upgrading the ExecNode version is not necessary if state compatibility can be maintained at the implementation level.It should be okay to roll back ExecNodeMetadata to version 1 because compatibility handling is enabled at the code level.Long-term we need a larger testing framework, per-Flink and per-ExecNode version that validates all attributes.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LimitJsonPlanTest.jsonplan.testLimit.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testIndividualWindowTVFProcessingTime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testIndividualWindowTVF.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowRank.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowDeduplicate.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowJoinJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testDistinctSplitEnabled.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WatermarkAssignerJsonPlanTest.jsonplan.testWatermarkAssigner.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ValuesJsonPlanTest.jsonplan.testValues.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.UnionJsonPlanTest.jsonplan.testUnion.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortRowTime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortProcessingTime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalJoinJsonPlanTest.jsonplan.testTemporalTableJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalJoinJsonPlanTest.jsonplan.testJoinTemporalFunction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testWatermarkPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testReadingMetadata.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testProjectPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testPartitionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testLimitPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testFilterPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testWritingMetadata.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testPartitioning.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testPartialInsert.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testOverwrite.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest.jsonplan.testCdcWithNonDeterministicFuncSinkWithDifferentPk.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.SortLimitJsonPlanTest.jsonplan.testSortLimit.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.RankJsonPlanTest.jsonplan.testRank.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testRowTimeBoundedPartitionedRowsOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonOverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonGroupAggregateJsonPlanTest.jsonplan.tesPythonAggCallsWithGroupBy.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCorrelateJsonPlanTest.jsonplan.testPythonTableFunction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCorrelateJsonPlanTest.jsonplan.testJoinWithFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.jsonplan.testPythonFunctionInWhereClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.jsonplan.testPythonCalc.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testRowTimeBoundedPartitionedRowsOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctWithNonDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.MatchRecognizeJsonPlanTest.jsonplan.testMatch.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTableWithRetryHint.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTableWithProjectionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTableWithAsyncRetryHint2.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTableWithAsyncRetryHint.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTableWithAsyncHint2.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTableWithAsyncHint.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTable.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testAggAndLeftJoinWithTryResolveMode.out</file>
      <file type="M">docs.content.zh.docs.dev.table.concepts.overview.md</file>
      <file type="M">docs.content.docs.dev.table.concepts.overview.md</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecChangelogNormalize.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecDeduplicate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGlobalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecIncrementalGroupAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLimit.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLookupJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecRank.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSortLimit.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.TransformationsTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.jsonplan.testGetJsonPlan.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testComplexCalc.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testProjectPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testSarg.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testSimpleFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CalcJsonPlanTest.jsonplan.testSimpleProject.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ChangelogSourceJsonPlanTest.jsonplan.testChangelogSource.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ChangelogSourceJsonPlanTest.jsonplan.testUpsertSource.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testCrossJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testCrossJoinOverrideParameters.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testJoinWithFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.CorrelateJsonPlanTest.jsonplan.testLeftOuterJoinWithLiteralTrue.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.DeduplicationJsonPlanTest.jsonplan.testDeduplication.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.ExpandJsonPlanTest.jsonplan.testExpand.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggCallsWithGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testSimpleAggWithoutGroupBy[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=false].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testUserDefinedAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregate.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregateWithSumCountDistinctAndRetraction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testProcessingTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testRowTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testInnerJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testInnerJoinWithEqualPk.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testInnerJoinWithPk.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.JoinJsonPlanTest.jsonplan.testLeftJoinNonEqui.out</file>
    </fixedFiles>
  </bug>
  <bug id="32663" opendate="2023-7-24 00:00:00" fixdate="2023-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RescalingITCase.testSavepointRescalingInPartitionedOperatorStateList fails on AZP</summary>
      <description>This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51501&amp;view=logs&amp;j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&amp;t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&amp;l=8665fails asJul 21 01:24:54 01:24:54.146 [ERROR] RescalingITCase.testSavepointRescalingInPartitionedOperatorStateList Time elapsed: 1.485 s &lt;&lt;&lt; FAILURE!Jul 21 01:24:54 java.lang.AssertionError: expected:&lt;530&gt; but was:&lt;30&gt;Jul 21 01:24:54 at org.junit.Assert.fail(Assert.java:89)Jul 21 01:24:54 at org.junit.Assert.failNotEquals(Assert.java:835)Jul 21 01:24:54 at org.junit.Assert.assertEquals(Assert.java:647)Jul 21 01:24:54 at org.junit.Assert.assertEquals(Assert.java:633)Jul 21 01:24:54 at org.apache.flink.test.checkpointing.RescalingITCase.testSavepointRescalingPartitionedOperatorState(RescalingITCase.java:621)Jul 21 01:24:54 at org.apache.flink.test.checkpointing.RescalingITCase.testSavepointRescalingInPartitionedOperatorStateList(RescalingITCase.java:508)Jul 21 01:24:54 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)Jul 21 01:24:54 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)Jul 21 01:24:54 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:4...</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.CheckpointCoordinatorConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="32670" opendate="2023-7-25 00:00:00" fixdate="2023-8-25 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Cascade deprecation to classes that implement SourceFunction</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.windowing.GroupedProcessingTimeWindowExample.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.statemachine.StateMachineExample.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.statemachine.KafkaEventsGeneratorJob.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.statemachine.generator.EventsGeneratorSource.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.gpu.MatrixVectorMul.java</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-common.src.main.java.org.apache.flink.walkthrough.common.source.TransactionSource.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.FiniteTestSource.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.StatefulSequenceSource.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.SocketTextStreamFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.RichSourceFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ParallelSourceFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.MultipleIdsMessageAcknowledgingSourceBase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.MessageAcknowledgingSourceBase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.FromSplittableIteratorFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.FromIteratorFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.FromElementsFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.FileMonitoringFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.checkpoint.ExternallyInducedSource.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.sources.ArrowSourceFunction.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.TtlStateUpdateSource.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-sql-test.src.main.java.org.apache.flink.sql.tests.StreamSQLTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-state-evolution-test.src.main.java.org.apache.flink.test.StatefulStreamingJob.java</file>
      <file type="M">flink-end-to-end-tests.flink-queryable-state-test.src.main.java.org.apache.flink.streaming.tests.queryablestate.QsStateProducer.java</file>
      <file type="M">flink-end-to-end-tests.flink-netty-shuffle-memory-control-test.src.main.java.org.apache.flink.streaming.tests.NettyShuffleMemoryControlTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-local-recovery-and-allocation-test.src.main.java.org.apache.flink.streaming.tests.StickyAllocationAndLocalRecoveryTestJob.java</file>
      <file type="M">flink-end-to-end-tests.flink-heavy-deployment-stress-test.src.main.java.org.apache.flink.deployment.HeavyDeploymentStressTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-file-sink-test.src.main.java.org.apache.flink.connector.file.sink.FileSinkProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.SequenceGeneratorSource.java</file>
      <file type="M">flink-end-to-end-tests.flink-cli-test.src.main.java.org.apache.flink.streaming.tests.PeriodicStreamingJob.java</file>
      <file type="M">flink-contrib.flink-connector-wikiedits.src.main.java.org.apache.flink.streaming.connectors.wikiedits.WikipediaEditsSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="32674" opendate="2023-7-26 00:00:00" fixdate="2023-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for the new Context.getTargetColumns</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.insert.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.insert.md</file>
    </fixedFiles>
  </bug>
  <bug id="32676" opendate="2023-7-26 00:00:00" fixdate="2023-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add doc for catalog modification listener</summary>
      <description>Add doc for catalog modification listener</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.catalogs.md</file>
      <file type="M">docs.content.zh.docs.dev.table.catalogs.md</file>
    </fixedFiles>
  </bug>
  <bug id="32680" opendate="2023-7-26 00:00:00" fixdate="2023-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Job vertex names get messed up once there is a source vertex chained with a MultipleInput vertex in job graph</summary>
      <description>Take the following test(put it to MultipleInputITCase) as example: @Test public void testMultipleInputDoesNotChainedWithSource() throws Exception { testJobVertexName(false); } @Test public void testMultipleInputChainedWithSource() throws Exception { testJobVertexName(true); } public void testJobVertexName(boolean chain) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); TestListResultSink&lt;Long&gt; resultSink = new TestListResultSink&lt;&gt;(); DataStream&lt;Long&gt; source1 = env.fromSequence(0L, 3L).name("source1"); DataStream&lt;Long&gt; source2 = env.fromElements(4L, 6L).name("source2"); DataStream&lt;Long&gt; source3 = env.fromElements(7L, 9L).name("source3"); KeyedMultipleInputTransformation&lt;Long&gt; transform = new KeyedMultipleInputTransformation&lt;&gt;( "MultipleInput", new KeyedSumMultipleInputOperatorFactory(), BasicTypeInfo.LONG_TYPE_INFO, 1, BasicTypeInfo.LONG_TYPE_INFO); if (chain) { transform.setChainingStrategy(ChainingStrategy.HEAD_WITH_SOURCES); } KeySelector&lt;Long, Long&gt; keySelector = (KeySelector&lt;Long, Long&gt;) value -&gt; value % 3; env.addOperator( transform .addInput(source1.getTransformation(), keySelector) .addInput(source2.getTransformation(), keySelector) .addInput(source3.getTransformation(), keySelector)); new MultipleConnectedStreams(env).transform(transform).rebalance().addSink(resultSink).name("sink"); env.execute(); } When we run testMultipleInputDoesNotChainedWithSource , all job vertex names are normal:When we run testMultipleInputChainedWithSource (the MultipleInput chained with source1), job vertex names get messed up (all job vertex names contain Source: source1): I think it's a bug.</description>
      <version>1.16.2,1.18.0,1.17.1</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="32683" opendate="2023-7-26 00:00:00" fixdate="2023-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Pekko from 1.0.0 to 1.0.1</summary>
      <description>Updates Pekko dependency to 1.0.1 which contains the following bugfix https://github.com/apache/incubator-pekko/pull/492 . See https://github.com/apache/incubator-pekko/issues/491 for more info</description>
      <version>1.18.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-rpc.flink-rpc-akka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32685" opendate="2023-7-26 00:00:00" fixdate="2023-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression on sortedMultiInput and sortedTwoInput since 2023-07-18</summary>
      <description>http://codespeed.dak8s.net:8000/timeline/#/?exe=1&amp;ben=sortedMultiInput&amp;extr=on&amp;quarts=on&amp;equid=off&amp;env=2&amp;revs=200http://codespeed.dak8s.net:8000/timeline/#/?exe=1&amp;ben=sortedTwoInput&amp;extr=on&amp;quarts=on&amp;equid=off&amp;env=2&amp;revs=200</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="32686" opendate="2023-7-26 00:00:00" fixdate="2023-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression on startScheduling.BATCH and startScheduling.STREAMING since 2023-07-24</summary>
      <description>http://codespeed.dak8s.net:8000/timeline/#/?exe=5&amp;ben=startScheduling.STREAMING&amp;extr=on&amp;quarts=on&amp;equid=off&amp;env=2&amp;revs=200http://codespeed.dak8s.net:8000/timeline/#/?exe=5&amp;ben=startScheduling.BATCH&amp;extr=on&amp;quarts=on&amp;equid=off&amp;env=2&amp;revs=200</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.shuffle.NettyShuffleMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.common.TieredStoragePartitionId.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.common.TieredStorageIdMappingUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="32689" opendate="2023-7-26 00:00:00" fixdate="2023-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Insufficient validation for table.local-time-zone</summary>
      <description>There are still cases where timezone information is lost silently due to the interaction between java.util.TimeZone and java.time.ZoneId.This might be theoretical problem, but I would feel safer if we change the check to:if (!java.util.TimeZone.getTimeZone(zoneId).toZoneId().equals(ZoneId.of(zoneId))) { throw new ValidationException(errorMessage);}</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.connector.file.table.FileSystemTableFactoryTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.utils.TableConfigUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.api.TableConfigTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableConfig.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemTableFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="32691" opendate="2023-7-26 00:00:00" fixdate="2023-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make it possible to use builtin functions without catalog/db set</summary>
      <description>Relative to https://issues.apache.org/jira/browse/FLINK-32584, function lookup fails without the catalog and database set.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.catalog.UnknownCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="3270" opendate="2016-1-21 00:00:00" fixdate="2016-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add example for reading and writing to Kafka</summary>
      <description>The Kafka connector of Flink is the most used streaming connector. I would like to add two examples showing how to read and write data into a Kafka topic.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.examples.WriteIntoKafka.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.examples.ReadFromKafka.java</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32703" opendate="2023-7-27 00:00:00" fixdate="2023-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hotfix] flink-python POM has a typo for protobuf-java in shading config</summary>
      <description>Fix typo. `inculde` -&gt; `include`                                  &lt;includes combine.children="append"&gt;                                    &lt;include&gt;net.razorvine:*&lt;/include&gt;                                    &lt;include&gt;net.sf.py4j:*&lt;/include&gt;                                    &lt;include&gt;org.apache.beam:*&lt;/include&gt;                                    &lt;include&gt;com.fasterxml.jackson.core:*&lt;/include&gt;                                    &lt;include&gt;joda-time:*&lt;/include&gt;                                    &lt;inculde&gt;com.google.protobuf:*&lt;/inculde&gt;                                    &lt;include&gt;org.apache.arrow:*&lt;/include&gt;                                    &lt;include&gt;io.netty:*&lt;/include&gt;                                    &lt;include&gt;com.google.flatbuffers:*&lt;/include&gt;                                    &lt;include&gt;com.alibaba:pemja&lt;/include&gt;                                &lt;/includes&gt;</description>
      <version>1.16.2,1.18.0,1.17.1</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32708" opendate="2023-7-28 00:00:00" fixdate="2023-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the write logic in remote tier of Hybrid Shuffle</summary>
      <description>Currently, on the writer side in the remote tier, the flag file indicating the latest segment id is updated first, followed by the creation of the data file. This results in an incorrect order of file creation and we should fix it.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.remote.RemoteStorageScannerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.remote.RemoteStorageScanner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.file.SegmentPartitionFileWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="32709" opendate="2023-7-28 00:00:00" fixdate="2023-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the bug of low memory utilization for Hybrid Shuffle</summary>
      <description>Currently, each subpartition in Disk/Remote has a segment size of 8M. When writing segments to the Disk tier with a parallelism of 1000, only shuffle data exceeding 1000 * 8M can be written to the Memory tier again. However, for most shuffles, the data volume size falls below this limit, significantly impacting Memory tier utilization. For better performance, it is necessary to address this issue to improve the memory tier utilization.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.memory.MemoryTierProducerAgentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.TieredStorageSortBufferTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.memory.MemoryTierSubpartitionProducerAgent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.memory.MemoryTierProducerAgent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.memory.MemoryTierFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.TieredStorageSortBuffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.SortBufferAccumulator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.shuffle.TieredResultPartitionFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.common.TieredStorageConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="32710" opendate="2023-7-28 00:00:00" fixdate="2023-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The LeaderElection component IDs for running is only the JobID which might be confusing in the log output</summary>
      <description>I noticed that the leader log messages for the jobs use the plain job ID as the component ID. That might be confusing when reading the logs since it's a UUID with no additional context.We might want to add a prefix (e.g. job- to these component IDs.)</description>
      <version>1.18.0</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperLeaderElectionHaServices.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionHaServices.java</file>
    </fixedFiles>
  </bug>
  <bug id="32713" opendate="2023-7-30 00:00:00" fixdate="2023-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cascade deprecation to non-private methods that reference SourceFunction</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.connector.source.SourceFunctionProvider.java</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSourceContexts.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestJobFactory.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.archunit-violations.7602816f-5c01-4b7a-9e3e-235dfedec245</file>
    </fixedFiles>
  </bug>
  <bug id="32730" opendate="2023-8-2 00:00:00" fixdate="2023-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BatchExecMultipleInput may contain unsupported SourceTransformation</summary>
      <description>When testing 10T TPC-DS on cluster, Q6 always reports an error as shown in the figure during compile time.</description>
      <version>1.18.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.optimize.program.DynamicPartitionPruningProgramTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DynamicFilteringTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.optimize.program.DynamicPartitionPruningProgramTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.reuse.ScanReuserUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.reuse.ReusableScanVisitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="32731" opendate="2023-8-2 00:00:00" fixdate="2023-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SqlGatewayE2ECase.testHiveServer2ExecuteStatement failed due to MetaException</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51891&amp;view=logs&amp;j=fb37c667-81b7-5c22-dd91-846535e99a97&amp;t=011e961e-597c-5c96-04fe-7941c8b83f23&amp;l=10987Aug 02 02:14:04 02:14:04.957 [ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 198.658 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.table.gateway.SqlGatewayE2ECaseAug 02 02:14:04 02:14:04.966 [ERROR] org.apache.flink.table.gateway.SqlGatewayE2ECase.testHiveServer2ExecuteStatement Time elapsed: 31.437 s &lt;&lt;&lt; ERROR!Aug 02 02:14:04 java.util.concurrent.ExecutionException: Aug 02 02:14:04 java.sql.SQLException: org.apache.flink.table.gateway.service.utils.SqlExecutionException: Failed to execute the operation d440e6e7-0fed-49c9-933e-c7be5bbae50d.Aug 02 02:14:04 at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.processThrowable(OperationManager.java:414)Aug 02 02:14:04 at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:267)Aug 02 02:14:04 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)Aug 02 02:14:04 at java.util.concurrent.FutureTask.run(FutureTask.java:266)Aug 02 02:14:04 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)Aug 02 02:14:04 at java.util.concurrent.FutureTask.run(FutureTask.java:266)Aug 02 02:14:04 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)Aug 02 02:14:04 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)Aug 02 02:14:04 at java.lang.Thread.run(Thread.java:750)Aug 02 02:14:04 Caused by: org.apache.flink.table.api.TableException: Could not execute CreateTable in path `hive`.`default`.`CsvTable`Aug 02 02:14:04 at org.apache.flink.table.catalog.CatalogManager.execute(CatalogManager.java:1289)Aug 02 02:14:04 at org.apache.flink.table.catalog.CatalogManager.createTable(CatalogManager.java:939)Aug 02 02:14:04 at org.apache.flink.table.operations.ddl.CreateTableOperation.execute(CreateTableOperation.java:84)Aug 02 02:14:04 at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1080)Aug 02 02:14:04 at org.apache.flink.table.gateway.service.operation.OperationExecutor.callOperation(OperationExecutor.java:570)Aug 02 02:14:04 at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:458)Aug 02 02:14:04 at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:210)Aug 02 02:14:04 at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212)Aug 02 02:14:04 at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119)Aug 02 02:14:04 at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:258)Aug 02 02:14:04 ... 7 moreAug 02 02:14:04 Caused by: org.apache.flink.table.catalog.exceptions.CatalogException: Failed to create table default.CsvTableAug 02 02:14:04 at org.apache.flink.table.catalog.hive.HiveCatalog.createTable(HiveCatalog.java:547)Aug 02 02:14:04 at org.apache.flink.table.catalog.CatalogManager.lambda$createTable$16(CatalogManager.java:950)Aug 02 02:14:04 at org.apache.flink.table.catalog.CatalogManager.execute(CatalogManager.java:1283)Aug 02 02:14:04 ... 16 moreAug 02 02:14:04 Caused by: MetaException(message:Got exception: java.net.ConnectException Call From 70d5c7217fe8/172.17.0.2 to hadoop-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused)Aug 02 02:14:04 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42225)Aug 02 02:14:04 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42193)Aug 02 02:14:04 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result.read(ThriftHiveMetastore.java:42119)Aug 02 02:14:04 at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)Aug 02 02:14:04 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_create_table_with_environment_context(ThriftHiveMetastore.java:1203)Aug 02 02:14:04 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.create_table_with_environment_context(ThriftHiveMetastore.java:1189)Aug 02 02:14:04 at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)Aug 02 02:14:04 at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)Aug 02 02:14:04 at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)Aug 02 02:14:04 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)Aug 02 02:14:04 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)Aug 02 02:14:04 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)Aug 02 02:14:04 at java.lang.reflect.Method.invoke(Method.java:498)Aug 02 02:14:04 at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:169)Aug 02 02:14:04 at com.sun.proxy.$Proxy26.createTable(Unknown Source)Aug 02 02:14:04 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)Aug 02 02:14:04 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)Aug 02 02:14:04 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)Aug 02 02:14:04 at java.lang.reflect.Method.invoke(Method.java:498)Aug 02 02:14:04 at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2327)Aug 02 02:14:04 at com.sun.proxy.$Proxy26.createTable(Unknown Source)Aug 02 02:14:04 at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.createTable(HiveMetastoreClientWrapper.java:174)Aug 02 02:14:04 at org.apache.flink.table.catalog.hive.HiveCatalog.createTable(HiveCatalog.java:539)Aug 02 02:14:04 ... 18 moreAug 02 02:14:04 Aug 02 02:14:04 at java.util.concurrent.FutureTask.report(FutureTask.java:122)Aug 02 02:14:04 at java.util.concurrent.FutureTask.get(FutureTask.java:206)Aug 02 02:14:04 at org.apache.flink.tests.util.flink.FlinkDistribution.submitSQL(FlinkDistribution.java:341)Aug 02 02:14:04 at org.apache.flink.tests.util.flink.FlinkDistribution.submitSQLJob(FlinkDistribution.java:281)Aug 02 02:14:04 at org.apache.flink.tests.util.flink.LocalStandaloneFlinkResource$GatewayClusterControllerImpl.submitSQLJob(LocalStandaloneFlinkResource.java:220)Aug 02 02:14:04 at org.apache.flink.table.gateway.SqlGatewayE2ECase.executeStatement(SqlGatewayE2ECase.java:133)Aug 02 02:14:04 at org.apache.flink.table.gateway.SqlGatewayE2ECase.testHiveServer2ExecuteStatement(SqlGatewayE2ECase.java:107)Aug 02 02:14:04 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)Aug 02 02:14:04 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)Aug 02 02:14:04 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)Aug 02 02:14:04 at java.lang.reflect.Method.invoke(Method.java:498)Aug 02 02:14:04 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)Aug 02 02:14:04 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)Aug 02 02:14:04 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)Aug 02 02:14:04 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)Aug 02 02:14:04 at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)Aug 02 02:14:04 at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)Aug 02 02:14:04 at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)Aug 02 02:14:04 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)Aug 02 02:14:04 at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)Aug 02 02:14:04 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)Aug 02 02:14:04 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)Aug 02 02:14:04 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)Aug 02 02:14:04 at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)Aug 02 02:14:04 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)Aug 02 02:14:04 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)Aug 02 02:14:04 at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)Aug 02 02:14:04 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)Aug 02 02:14:04 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)Aug 02 02:14:04 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)Aug 02 02:14:04 at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)Aug 02 02:14:04 at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:29)Aug 02 02:14:04 at org.junit.rules.RunRules.evaluate(RunRules.java:20)Aug 02 02:14:04 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)Aug 02 02:14:04 at org.junit.runners.ParentRunner.run(ParentRunner.java:413)Aug 02 02:14:04 at org.junit.runner.JUnitCore.run(JUnitCore.java:137)Aug 02 02:14:04 at org.junit.runner.JUnitCore.run(JUnitCore.java:115)Aug 02 02:14:04 at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)Aug 02 02:14:04 at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)Aug 02 02:14:04 at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)Aug 02 02:14:04 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)Aug 02 02:14:04 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)Aug 02 02:14:04 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)Aug 02 02:14:04 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)Aug 02 02:14:04 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)Aug 02 02:14:04 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)Aug 02 02:14:04 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)Aug 02 02:14:04 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)Aug 02 02:14:04 at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)Aug 02 02:14:04 at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)Aug 02 02:14:04 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)Aug 02 02:14:04 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)Aug 02 02:14:04 at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:128)Aug 02 02:14:04 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)Aug 02 02:14:04 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)Aug 02 02:14:04 at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)Aug 02 02:14:04 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)Aug 02 02:14:04 Caused by: java.sql.SQLException: org.apache.flink.table.gateway.service.utils.SqlExecutionException: Failed to execute the operation d440e6e7-0fed-49c9-933e-c7be5bbae50d.Aug 02 02:14:04 at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.processThrowable(OperationManager.java:414)Aug 02 02:14:04 at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:267)Aug 02 02:14:04 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)Aug 02 02:14:04 at java.util.concurrent.FutureTask.run(FutureTask.java:266)Aug 02 02:14:04 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)Aug 02 02:14:04 at java.util.concurrent.FutureTask.run(FutureTask.java:266)Aug 02 02:14:04 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)Aug 02 02:14:04 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)Aug 02 02:14:04 at java.lang.Thread.run(Thread.java:750)Aug 02 02:14:04 Caused by: org.apache.flink.table.api.TableException: Could not execute CreateTable in path `hive`.`default`.`CsvTable`Aug 02 02:14:04 at org.apache.flink.table.catalog.CatalogManager.execute(CatalogManager.java:1289)Aug 02 02:14:04 at org.apache.flink.table.catalog.CatalogManager.createTable(CatalogManager.java:939)Aug 02 02:14:04 at org.apache.flink.table.operations.ddl.CreateTableOperation.execute(CreateTableOperation.java:84)Aug 02 02:14:04 at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1080)Aug 02 02:14:04 at org.apache.flink.table.gateway.service.operation.OperationExecutor.callOperation(OperationExecutor.java:570)Aug 02 02:14:04 at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:458)Aug 02 02:14:04 at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:210)Aug 02 02:14:04 at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212)Aug 02 02:14:04 at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119)Aug 02 02:14:04 at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:258)Aug 02 02:14:04 ... 7 moreAug 02 02:14:04 Caused by: org.apache.flink.table.catalog.exceptions.CatalogException: Failed to create table default.CsvTableAug 02 02:14:04 at org.apache.flink.table.catalog.hive.HiveCatalog.createTable(HiveCatalog.java:547)Aug 02 02:14:04 at org.apache.flink.table.catalog.CatalogManager.lambda$createTable$16(CatalogManager.java:950)Aug 02 02:14:04 at org.apache.flink.table.catalog.CatalogManager.execute(CatalogManager.java:1283)Aug 02 02:14:04 ... 16 moreAug 02 02:14:04 Caused by: MetaException(message:Got exception: java.net.ConnectException Call From 70d5c7217fe8/172.17.0.2 to hadoop-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused)Aug 02 02:14:04 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42225)Aug 02 02:14:04 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42193)Aug 02 02:14:04 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result.read(ThriftHiveMetastore.java:42119)Aug 02 02:14:04 at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)Aug 02 02:14:04 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_create_table_with_environment_context(ThriftHiveMetastore.java:1203)Aug 02 02:14:04 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.create_table_with_environment_context(ThriftHiveMetastore.java:1189)Aug 02 02:14:04 at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)Aug 02 02:14:04 at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)Aug 02 02:14:04 at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)Aug 02 02:14:04 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)Aug 02 02:14:04 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)Aug 02 02:14:04 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)Aug 02 02:14:04 at java.lang.reflect.Method.invoke(Method.java:498)Aug 02 02:14:04 at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:169)Aug 02 02:14:04 at com.sun.proxy.$Proxy26.createTable(Unknown Source)Aug 02 02:14:04 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)Aug 02 02:14:04 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)Aug 02 02:14:04 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)Aug 02 02:14:04 at java.lang.reflect.Method.invoke(Method.java:498)Aug 02 02:14:04 at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2327)Aug 02 02:14:04 at com.sun.proxy.$Proxy26.createTable(Unknown Source)Aug 02 02:14:04 at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.createTable(HiveMetastoreClientWrapper.java:174)Aug 02 02:14:04 at org.apache.flink.table.catalog.hive.HiveCatalog.createTable(HiveCatalog.java:539)Aug 02 02:14:04 ... 18 moreAug 02 02:14:04 Aug 02 02:14:04 at org.apache.hive.jdbc.HiveStatement.waitForOperationToComplete(HiveStatement.java:385)Aug 02 02:14:04 at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:254)Aug 02 02:14:04 at org.apache.flink.tests.util.flink.FlinkDistribution.lambda$submitSQLJob$6(FlinkDistribution.java:293)Aug 02 02:14:04 at org.apache.flink.util.function.FunctionUtils.lambda$asCallable$5(FunctionUtils.java:126)Aug 02 02:14:04 at java.util.concurrent.FutureTask.run(FutureTask.java:266)Aug 02 02:14:04 at java.lang.Thread.run(Thread.java:750)</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0,1.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-hive.src.test.java.org.apache.flink.tests.hive.HiveITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-sql-gateway-test.src.test.java.org.apache.flink.table.gateway.containers.HiveContainer.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-hive.src.test.java.org.apache.flink.tests.hive.containers.HiveContainer.java</file>
    </fixedFiles>
  </bug>
  <bug id="32751" opendate="2023-8-4 00:00:00" fixdate="2023-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DistinctAggregateITCaseBase.testMultiDistinctAggOnDifferentColumn got stuck on AZP</summary>
      <description>This build hangs https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51955&amp;view=logs&amp;j=ce3801ad-3bd5-5f06-d165-34d37e757d90&amp;t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&amp;l=14399Aug 04 03:03:47 "ForkJoinPool-1-worker-51" #28 daemon prio=5 os_prio=0 cpu=49342.66ms elapsed=3079.49s tid=0x00007f67ccdd0000 nid=0x5234 waiting on condition [0x00007f6791a19000]Aug 04 03:03:47 java.lang.Thread.State: WAITING (parking)Aug 04 03:03:47 at jdk.internal.misc.Unsafe.park(java.base@11.0.19/Native Method)Aug 04 03:03:47 - parking to wait for &lt;0x00000000ad3b1fb8&gt; (a java.util.concurrent.CompletableFuture$Signaller)Aug 04 03:03:47 at java.util.concurrent.locks.LockSupport.park(java.base@11.0.19/LockSupport.java:194)Aug 04 03:03:47 at java.util.concurrent.CompletableFuture$Signaller.block(java.base@11.0.19/CompletableFuture.java:1796)Aug 04 03:03:47 at java.util.concurrent.ForkJoinPool.managedBlock(java.base@11.0.19/ForkJoinPool.java:3118)Aug 04 03:03:47 at java.util.concurrent.CompletableFuture.waitingGet(java.base@11.0.19/CompletableFuture.java:1823)Aug 04 03:03:47 at java.util.concurrent.CompletableFuture.get(java.base@11.0.19/CompletableFuture.java:1998)Aug 04 03:03:47 at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sendRequest(CollectResultFetcher.java:171)Aug 04 03:03:47 at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:129)Aug 04 03:03:47 at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)Aug 04 03:03:47 at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)Aug 04 03:03:47 at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222)Aug 04 03:03:47 at java.util.Iterator.forEachRemaining(java.base@11.0.19/Iterator.java:132)Aug 04 03:03:47 at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:122)Aug 04 03:03:47 at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:309)Aug 04 03:03:47 at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:145)Aug 04 03:03:47 at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:109)Aug 04 03:03:47 at org.apache.flink.table.planner.runtime.batch.sql.agg.DistinctAggregateITCaseBase.testMultiDistinctAggOnDifferentColumn(DistinctAggregateITCaseBase.scala:97)~~it is very likely that it is an old issuethe similar case was mentioned for 1.11.0 and closed because of lack of occurrences FLINK-16923and another similar one FLINK-22100 which was marked as a duplicate of FLINK-21996</description>
      <version>1.18.0,1.16.3,1.17.2,1.19.0</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2,1.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="32768" opendate="2023-8-7 00:00:00" fixdate="2023-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SpeculativeSchedulerITCase.testSpeculativeExecutionOfInputFormatSource times out</summary>
      <description>SpeculativeSchedulerITCase.testSpeculativeExecutionOfInputFormatSource is timing out:https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52009&amp;view=logs&amp;j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&amp;t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&amp;l=90222023-08-06T05:34:27.1867230Z Aug 06 05:34:27 "ForkJoinPool-1-worker-1" #14 daemon prio=5 os_prio=0 tid=0x00007fb7d4e82800 nid=0x6dde7 waiting on condition [0x00007fb7834a4000]2023-08-06T05:34:27.1867541Z Aug 06 05:34:27 java.lang.Thread.State: WAITING (parking)2023-08-06T05:34:27.1867777Z Aug 06 05:34:27 at sun.misc.Unsafe.park(Native Method)2023-08-06T05:34:27.1868191Z Aug 06 05:34:27 - parking to wait for &lt;0x00000000a77360d8&gt; (a java.util.concurrent.CompletableFuture$Signaller)2023-08-06T05:34:27.1868571Z Aug 06 05:34:27 at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)2023-08-06T05:34:27.1868896Z Aug 06 05:34:27 at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)2023-08-06T05:34:27.1869240Z Aug 06 05:34:27 at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)2023-08-06T05:34:27.1869682Z Aug 06 05:34:27 at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)2023-08-06T05:34:27.1870022Z Aug 06 05:34:27 at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)2023-08-06T05:34:27.1870395Z Aug 06 05:34:27 at org.apache.flink.test.scheduling.SpeculativeSchedulerITCase.executeJob(SpeculativeSchedulerITCase.java:229)2023-08-06T05:34:27.1870858Z Aug 06 05:34:27 at org.apache.flink.test.scheduling.SpeculativeSchedulerITCase.testSpeculativeExecutionOfInputFormatSource(SpeculativeSchedulerITCase.java:165)2023-08-06T05:34:27.1871251Z Aug 06 05:34:27 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[...]</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.TestExecutionSlotAllocator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SimpleExecutionSlotAllocatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SimpleExecutionSlotAllocator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.ExecutionSlotAllocator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.java</file>
    </fixedFiles>
  </bug>
  <bug id="32770" opendate="2023-8-7 00:00:00" fixdate="2023-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the inaccurate backlog number of Hybrid Shuffle</summary>
      <description>The backlog is inaccurate in both memory and disk tier. We should fix it to prevent redundant memory usage in reader side.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.netty.TieredStorageResultSubpartitionViewTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.netty.NettyConnectionWriterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.netty.TieredStorageResultSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.netty.TieredStorageNettyServiceImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.netty.NettyConnectionWriterImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="3278" opendate="2016-1-22 00:00:00" fixdate="2016-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Partitioned State Backend Based on RocksDB</summary>
      <description>This would allow users to have state that can grow larger than the available memory. Once WindowOperator is based on the partitioned state abstraction (FLINK-3200) this will also allow windows to grow larger than the available memory.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase.java</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-contrib.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32788" opendate="2023-8-8 00:00:00" fixdate="2023-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SpeculativeScheduler do not handle allocate task errors when schedule speculative tasks may causes resource leakage.</summary>
      <description>When the SpeculativeScheduler allocates slots for speculative tasks, exceptions may occur, but there is currently no exception handling mechanism in place. This can lead to resource leakage (such as FLINK-32768) when errors occur. In such cases, a fatalError should be triggered.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.slowtaskdetector.ExecutionTimeBasedSlowTaskDetectorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.slowtaskdetector.ExecutionTimeBasedSlowTaskDetector.java</file>
    </fixedFiles>
  </bug>
  <bug id="32811" opendate="2023-8-8 00:00:00" fixdate="2023-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add port range support for taskmanager.data.bind-port</summary>
      <description>Adding this feature could be helpful for installation in a restrictive network setup. The "port range" support is already available for some other port config options anyway.Right now, it is possible to specify a taskmanager.data.port and taskmanager.data.bind-port to be able to support NAT-like setups, although taskmanager.data.port is not bound to anything itself, so supporting a port range there is not an option according to my understanding.Although, supporting a port range only for taskmanager.data.bind-port can be still helpful for anyone who does not require a NAT capability, because if taskmanager.data.bind-port is set and taskmanager.data.port is set to 0, then the bound port will be used everywhere.This change should keep the already possible setups working as is.</description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyTestUtil.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.NettyShuffleEnvironmentConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyServer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyConfig.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyClient.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.NetUtilsTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.NetUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.NettyShuffleEnvironmentOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.netty.shuffle.environment.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.common.host.port.section.html</file>
      <file type="M">docs.layouts.shortcodes.generated.all.taskmanager.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="32824" opendate="2023-8-10 00:00:00" fixdate="2023-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port Calcite&amp;#39;s fix for the sql like operator</summary>
      <description>we should port the bugfix of sql like operator https://issues.apache.org/jira/browse/CALCITE-1898The LIKE operator must match '.' (period) literally, not treat it as a wild-card. Currently it treats it the same as '_'.</description>
      <version>1.18.0,1.17.1</version>
      <fixedVersion>1.18.0,1.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.functions.SqlLikeUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="32827" opendate="2023-8-10 00:00:00" fixdate="2023-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Operator fusion codegen may not take effect when enable runtime filter</summary>
      <description>Currently, the RuntimeFilterOperator does not support operator fusion codegen(OFCG), which means the Runtime Filter and OFCG can not take affect together, we should fix it.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.RuntimeFilterITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.runtimefilter.BatchExecRuntimeFilter.java</file>
    </fixedFiles>
  </bug>
  <bug id="32831" opendate="2023-8-11 00:00:00" fixdate="2023-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RuntimeFilterProgram should aware join type when looking for the build side</summary>
      <description>Currently, runtime filter program will try to look for an Exchange as build side to avoid affecting MultiInput. It will try to push down the runtime filter builder if the original build side is not Exchange.Currenlty, the builder-push-down does not aware the join type, which may lead to incorrect results(For example, push down the builder to the right input of left-join).We should only support following cases:1. Inner join: builder can push to left + right input2. semi join: builder can push to left + right input3. left join: builder can only push to the left input4. right join: builder can only push to the right input</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0,1.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.optimize.program.FlinkRuntimeFilterProgramTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.optimize.program.FlinkRuntimeFilterProgramTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.optimize.program.FlinkRuntimeFilterProgram.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.RuntimeFilterITCase.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.OptimizerConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.optimizer.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="32844" opendate="2023-8-11 00:00:00" fixdate="2023-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Runtime Filter should not be applied if the field is already filtered by DPP</summary>
      <description>Currently, the runtime filter and DPP may take effect on the same key. In this case, the runtime filter may be redundant because the data may have been filtered out by the DPP. We should avoid this because redundant runtime filters can have negative effects.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.optimize.program.FlinkRuntimeFilterProgramTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.optimize.program.FlinkRuntimeFilterProgramTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalDynamicFilteringTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.optimize.program.FlinkRuntimeFilterProgram.java</file>
    </fixedFiles>
  </bug>
  <bug id="32845" opendate="2023-8-11 00:00:00" fixdate="2023-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] The leaderelection, leaderretrieval, mailbox, memory and messages packages of flink-runtime module</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.messages.webmonitor.MultipleJobsDetailsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.messages.WebMonitorMessagesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.messages.GenericMessageTester.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.messages.checkpoint.DeclineCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.messages.CheckpointMessagesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.memory.UnsafeMemoryBudgetTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.memory.SharedResourcesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.memory.OpaqueMemoryResourceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.memory.MemorySegmentSimpleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.memory.MemoryManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.memory.MemoryManagerSharedResourcesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.memory.MemoryManagerConcurrentModReleaseTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="32846" opendate="2023-8-11 00:00:00" fixdate="2023-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] The metrics, minicluster and net packages of flink-runtime module</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.util.MetricUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.utils.SystemResourcesCounterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.TimerGaugeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.ThresholdMeterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.ReporterSetupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.MetricRegistryImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.TaskMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.TaskManagerMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.TaskManagerJobGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.TaskManagerGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.TaskIOMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.MetricGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.MetricGroupRegistrationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.JobManagerOperatorGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.JobManagerJobGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.JobManagerGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.InternalOperatorGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.FrontMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.AbstractMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.filter.DefaultMetricFilterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.dump.MetricQueryServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.dump.MetricDumpTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.dump.MetricDumpSerializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.DescriptiveStatisticsHistogramTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.net.SSLUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.net.ConnectionUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.MiniClusterITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="32847" opendate="2023-8-11 00:00:00" fixdate="2023-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] The operators package of flink-runtime module</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.ReduceTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.CoordinatorEventsToStreamOperatorRecipientExactlyOnceITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.OutputEmitterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.HashVsSortMiniBenchmark.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.BloomFilterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.util.BitSetTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.UnaryOperatorTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.TaskTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.TaskCancelThread.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MatchRemovingJoiner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DriverTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.BinaryOperatorTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.ReusingSortMergeOuterJoinIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.ReusingSortMergeInnerJoinIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.ReusingSortMergeCoGroupIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.NormalizedKeySorterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.NonReusingSortMergeOuterJoinIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.NonReusingSortMergeInnerJoinIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.NonReusingSortMergeCoGroupIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.MergeIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.LargeRecordHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.LargeRecordHandlerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.FixedLengthRecordSorterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.ExternalSortLargeRecordsITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.ExternalSortITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.ExternalSorterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.CombiningExternalSorterITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.AbstractSortMergeOuterJoinIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.RightOuterJoinTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.RightOuterJoinTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.resettable.SpillingResettableMutableObjectIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.resettable.SpillingResettableIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.resettable.ReusingBlockResettableIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.resettable.NonReusingBlockResettableIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.resettable.BlockResettableMutableObjectIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.AbstractOuterJoinTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.AbstractOuterJoinTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CachedMatchTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.chaining.ChainedAllReduceDriverTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.chaining.ChainedOperatorsMetricTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.chaining.ChainTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CoGroupTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CoGroupTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CombinerOversizedRecordsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CombineTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CombineTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.ComponentClosingUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.EventReceivingTasks.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.SubtaskGatewayImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.util.IncompleteFuturesTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CrossTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.CrossTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.DataSinkTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.DataSourceTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.drivers.AllGroupReduceDriverTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.drivers.AllReduceDriverTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.drivers.DriverTestData.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.drivers.GroupReduceDriverTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.drivers.ReduceCombineDriverTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.drivers.ReduceDriverTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.FlatMapTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.FullOuterJoinTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.FullOuterJoinTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.CompactingHashTableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashTablePerformanceComparison.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashTableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.InPlaceMutableHashTableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.MutableHashTableTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.NonReusingHashJoinIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.NonReusingReOpenableHashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReOpenableHashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReOpenableHashTableTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReusingHashJoinIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReusingReOpenableHashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.InvokableClassConstructorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.JoinTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.JoinTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.LeftOuterJoinTaskExternalITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.LeftOuterJoinTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.ReduceTaskExternalITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="32848" opendate="2023-8-11 00:00:00" fixdate="2023-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] The persistence, query, registration, rpc and shuffle packages of flink-runtime module</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.RpcSSLAuthITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.RpcEndpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.RpcConnectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.FencedRpcEndpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.AsyncCallsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.shuffle.ShuffleServiceLoaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.shuffle.ShuffleMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.shuffle.NettyShuffleUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.query.KvStateRegistryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.query.KvStateLocationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.query.KvStateLocationRegistryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.registration.RetryingRegistrationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.registration.RetryingRegistrationConfigurationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.registration.RegisteredRpcConnectionTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="3285" opendate="2016-1-25 00:00:00" fixdate="2016-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip Maven deployment of flink-java8</summary>
      <description>flink-java8 has a Scala dependency due to its dependency on flink-streaming-java. It deploys some examples and a test jar. However, the deployed jars are not necessary to develop or test Java 8 Lambda code.I propose to disable deployment of the flink-java8 module and its test jar.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java8.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32851" opendate="2023-8-11 00:00:00" fixdate="2023-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] The rest package of flink-runtime module</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.router.RouterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.versioning.RuntimeRestAPIVersionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerSSLAuthITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointConfigurationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestExternalHandlersITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestClientTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestClientMultipartTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.YarnStopJobTerminationHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.YarnCancelJobTerminationHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.VertexBackPressureStatusTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.VertexBackPressureLevelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.ThreadDumpInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagersInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerIdPathParameterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerDetailsInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.taskmanager.LogListInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.SubtasksTimesInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.SubtaskIndexPathParameterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.SavepointHandlerRequestBodyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.RestResponseMarshallingTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.RestRequestMarshallingTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.ResourceProfileInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.MessageParametersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.LogUrlResponseTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.json.SerializedValueSerializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.json.SerializedThrowableSerializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.json.JobResultDeserializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.SubtasksAllAccumulatorsInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.SubtaskExecutionAttemptDetailsInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.SubtaskExecutionAttemptAccumulatorsInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.savepoints.StopWithSavepointTriggerRequestBodyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.savepoints.SavepointTriggerRequestBodyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.savepoints.SavepointInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.savepoints.SavepointInfoMarshallingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.savepoints.SavepointDisposalRequestTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.TaskManagerMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.SubtaskMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.MetricsFilterParameterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.MetricCollectionResponseBodyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.JobVertexMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.JobMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.JobManagerMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.AbstractMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.JobResourceRequirementsBodyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.JobExecutionResultResponseBodyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.JobDetailsInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobVertexTaskManagersInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobVertexDetailsInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobVertexBackPressureInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobVertexAccumulatorsInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobSubmitResponseBodyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobSubmitRequestBodyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobPlanInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobExceptionsInfoWithHistoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobExceptionsInfoWithHistoryNoRootTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobConfigInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobAccumulatorsInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.EnvironmentInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.dataset.ClusterDataSetListResponseBodyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.DashboardConfigurationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.ConfigurationInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.checkpoints.TaskCheckpointStatisticsWithSubtaskDetailsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.checkpoints.TaskCheckpointStatisticsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointingStatisticsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointConfigInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.AggregatedTaskDetailsInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.util.HandlerRequestUtilsTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarListInfoTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBodyTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunResponseBodyTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarUploadResponseBodyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.messages.webmonitor.JobIdsWithStatusOverviewTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.messages.webmonitor.JobStatusInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.AbstractHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.async.AbstractAsynchronousOperationHandlersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.async.AsynchronousOperationResultTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.async.CompletedOperationCacheTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.async.TriggerResponseTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.FileUploadsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.InFlightRequestTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.checkpoints.AbstractCheckpointStatsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointHandlersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.GeneratedLogUrlHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobCancellationHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobConfigHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobManagerJobConfigurationHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobStatusHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobSubmitHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.AbstractMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingMetricsHandlerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.MetricsHandlerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointTestUtilities.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.savepoints.StopWithSavepointHandlersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.checkpoints.CheckpointStatsCacheTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCacheTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.files.MimeTypesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.messages.ClusterOverviewWithVersionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.metrics.MetricFetcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.RestHandlerConfigurationTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="32854" opendate="2023-8-11 00:00:00" fixdate="2023-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] The state package of flink-runtime module</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.OperatorStateBackendTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImplTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.ttl.RocksDBTtlStateTestBase.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendMigrationTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackendMigrationTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogStateBackendMigrationTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogDelegateMemoryStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogDelegateHashMapTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogDelegateHashMapInMemoryTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogDelegateFileStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-changelog.src.test.java.org.apache.flink.state.changelog.ChangelogDelegateEmbeddedRocksDBStateBackendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlStateTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskStateManagerImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskLocalStateStoreImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskExecutorStateChangelogStoragesManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateUtilTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateSnapshotCompressionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateSerializerProviderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendMigrationTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendLoadingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.SnapshotResultTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.SnapshotDirectoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.SharedStateRegistryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.SerializedCompositeKeyBuilderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.SerializationProxiesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.OperatorStreamStateHandleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.OperatorStateOutputCheckpointStreamTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.AsyncSnapshotCallableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ChangelogTaskLocalStateStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.changelog.ChangelogStateBackendHandleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.changelog.inmemory.StateChangelogStorageLoaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.changelog.inmemory.StateChangelogStorageTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.changelog.LocalChangelogRegistryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ChannelPersistenceITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.CheckpointStorageLoaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.CheckpointStreamWithResultProviderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.CompositeKeySerializationUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.DuplicatingCheckpointOutputStreamTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.FileStateBackendMigrationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.FileStateBackendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.AbstractFileCheckpointStorageAccessTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.CheckpointStateOutputStreamTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.FileStateHandleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.FsCheckpointMetadataOutputStreamTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStateOutputStreamTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStorageAccessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.FsStateBackendEntropyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.FsStorageLocationReferenceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.HashMapStateBackendMigrationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.HashMapStateBackendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.CopyOnWriteStateMapTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.CopyOnWriteStateTableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.HeapStateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.InternalKeyContextImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.TestDuplicateSerializer.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.IncrementalRemoteKeyedStateHandleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.InternalPriorityQueueTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.KeyedStateCheckpointOutputStreamTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.KeyGroupPartitionerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.KeyGroupRangeOffsetTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.KeyGroupRangeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.KeyGroupsStateHandleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.LocalRecoveryDirectoryProviderImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.MemoryStateBackendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.memory.ByteStreamStateHandleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.memory.MemoryCheckpointOutputStreamTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.memory.MemoryCheckpointStorageAccessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.metainfo.StateMetaInfoSnapshotEnumConstantsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.metrics.LatencyTrackingAggregatingStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.metrics.LatencyTrackingListStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.metrics.LatencyTrackingMapStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.metrics.LatencyTrackingReducingStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.metrics.LatencyTrackingStateConfigTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.metrics.LatencyTrackingStateFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.metrics.LatencyTrackingStateTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.metrics.LatencyTrackingValueStateTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="32855" opendate="2023-8-11 00:00:00" fixdate="2023-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] The taskexecutor package of flink-runtime module</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunnerStartupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorSubmissionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorSlotLifetimeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorRecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorOperatorEventHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorMemoryConfigurationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorExecutionDeploymentReconciliationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.slot.TaskSlotTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.slot.FileSlotAllocationSnapshotPersistenceServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.slot.DefaultTimerServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.partition.PartitionTableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.NettyShuffleEnvironmentConfigurationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.DefaultJobTableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.DefaultJobLeaderServiceTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="32856" opendate="2023-8-11 00:00:00" fixdate="2023-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] The testtasks, testutils, throughput and throwable packages of flink-runtime module</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.throwable.ThrowableClassifierTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.throughput.ThroughputCalculatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.throughput.BufferSizeEMATest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.throughput.BufferDebloaterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.TestJvmProcess.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.statemigration.TestType.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.PseudoRandomValueSelectorTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="32857" opendate="2023-8-11 00:00:00" fixdate="2023-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] The webmonitor and zookeeper packages of flink-runtime module</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.threadinfo.VertexThreadInfoTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.threadinfo.ThreadInfoRequestCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.retriever.LeaderGatewayRetrieverTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.retriever.impl.RpcGatewayRetrieverTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.history.HistoryServerUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.history.ArchivedJsonTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="32858" opendate="2023-8-11 00:00:00" fixdate="2023-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] The util package of flink-runtime module</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ZooKeeperUtilsWriteLeaderInformationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ZooKeeperUtilsTreeCacheTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ZooKeeperUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.StateHandleStoreUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.SlotSelectionStrategyUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.SerializedThrowableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.RunnablesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ReusingKeyGroupedIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ResourceManagerUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ResourceCounterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.NonReusingKeyGroupedIteratorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.JvmExitOnFatalErrorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.HardwareTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.FlinkSecurityManagerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.event.TaskEventHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.EnvironmentInformationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.DualKeyLinkedMapTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.config.memory.TaskExecutorProcessSpecTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.config.memory.ProcessMemoryUtilsTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.config.memory.ManagedMemoryUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.config.memory.JobManagerProcessSpecTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.BoundedFIFOQueueTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.BlockingShutdownTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.bash.FlinkConfigLoaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.bash.BashJavaUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.AddressResolutionTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="32865" opendate="2023-8-14 00:00:00" fixdate="2023-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DynamicFilteringDataCollectorOperator can&amp;#39;t chain with the upstream operator when the parallelism is inconsistent</summary>
      <description> If the DynamicFilteringDataCollectorOperator parallelism is not consistent with the upstream operator, they can't chain together, this will the DynamicFilteringDataCollectorOperator to execute after the fact source, so the dpp won't work. Due to the operator parallelism being decided during runtime, so we should add scheduler dependency forcibly in compile phase.</description>
      <version>1.16.2,1.18.0,1.17.1</version>
      <fixedVersion>1.18.0,1.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.dynamicfiltering.ExecutionOrderEnforcerOperatorFactory.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.dynamicfiltering.ExecutionOrderEnforcerOperator.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.optimize.program.FlinkRuntimeFilterProgramTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.optimize.program.DynamicPartitionPruningProgramTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.operator.BatchOperatorNameTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DynamicFilteringTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.processor.ResetTransformationProcessorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.ResetTransformationProcessor.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.DynamicFilteringDependencyProcessor.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecMultipleInput.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.DynamicFilteringITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecExecutionOrderEnforcer.java</file>
    </fixedFiles>
  </bug>
  <bug id="32870" opendate="2023-8-15 00:00:00" fixdate="2023-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reading multiple small buffers by reading and slicing one large buffer for tiered storage</summary>
      <description>Currently, when the file reader of tiered storage loads data from the disk file, it reads data in buffer granularity. Before compression, each buffer is 32K by default. After compressed, the size will become smaller (may less than 5K), which is pretty small for the network buffer and the file IO. We should read multiple small buffers by reading and slicing one large buffer to decrease the buffer competition and the file IO, leading to better performance.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.file.TestingPartitionFileReader.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.file.SegmentPartitionFileReaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.file.ProducerMergedPartitionFileReaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.file.ProducerMergedPartitionFileIndexTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.file.DiskIOSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.index.TestingFileDataIndexRegion.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.index.FileRegionWriteReadUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.index.FileDataIndexCacheTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.HybridShuffleTestUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.remote.RemoteTierConsumerAgent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.disk.DiskIOScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.file.SegmentPartitionFileReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.file.ProducerMergedPartitionFileWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.file.ProducerMergedPartitionFileReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.file.ProducerMergedPartitionFileIndex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.file.PartitionFileReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.index.FileRegionWriteReadUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.index.FileDataIndexRegionHelper.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.HsFileDataIndexImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BufferReaderWriterUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="32879" opendate="2023-8-16 00:00:00" fixdate="2023-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>slotmanager.max-total-resource.cpu/memory should be ignored in standalone mode.</summary>
      <description>Just as slotmanager.number-of-slots.max, we should also ignore the cpu and memory limitation in standalone mode.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.TaskManagerDisconnectOnShutdownITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManagerFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="32905" opendate="2023-8-22 00:00:00" fixdate="2023-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the bug of broadcast hash join doesn&amp;#39;t support spill to disk when enable operator fusion codegn</summary>
      <description></description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0,1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.hashtable.LongHybridHashTable.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.hashtable.BinaryHashTable.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.hashtable.BaseHybridHashTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.OperatorFusionCodegenITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.fusion.spec.HashJoinFusionCodegenSpec.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.fusion.spec.CalcFusionCodegenSpec.scala</file>
    </fixedFiles>
  </bug>
  <bug id="32968" opendate="2023-8-28 00:00:00" fixdate="2023-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix doc for customized catalog listener</summary>
      <description>Refer to https://issues.apache.org/jira/browse/FLINK-32798 for more details</description>
      <version>1.18.0,1.19.0</version>
      <fixedVersion>1.18.0,1.19.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.catalogs.md</file>
      <file type="M">docs.content.zh.docs.dev.table.catalogs.md</file>
    </fixedFiles>
  </bug>
  <bug id="33026" opendate="2023-9-4 00:00:00" fixdate="2023-1-4 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>The chinese doc of sql &amp;#39;Performance Tuning&amp;#39; has a wrong title in the index page</summary>
      <description>The chinese doc of sql 'Performance Tuning' has a wrong title in the index page</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.dev.table.tuning.md</file>
    </fixedFiles>
  </bug>
  <bug id="33042" opendate="2023-9-6 00:00:00" fixdate="2023-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow trigger flamegraph when task is initializing</summary>
      <description>Currently, the flamegraph can be triggered when task is running.After FLINK-17012 and FLINK-22215, flink split the running to running and initializing. We should allow trigger flamegraph when task is initializing. For example, the initialization is very slow, we need to troubleshoot. Here is a stack example, task is rebuilding the rocksdb after the parallelism is changed.  </description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.threadinfo.VertexThreadInfoTrackerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.VertexThreadInfoTracker.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.flamegraph.job-overview-drawer-flamegraph.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.flamegraph.job-overview-drawer-flamegraph.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphRestartTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="33044" opendate="2023-9-6 00:00:00" fixdate="2023-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce the frequency of triggering flush for the disk tier of the tiered storage</summary>
      <description>The disk cache of tiered storage will flush at the end of each subpartition's segment, which is too frequent and is bad for performance. We should improve it with some better flushing methods, e.g. flushing buffers with batch.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0,1.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.disk.DiskTierProducerAgentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.disk.DiskCacheManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.disk.DiskTierProducerAgent.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.disk.DiskTierFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.disk.DiskCacheManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.hybrid.tiered.common.TieredStorageConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="33050" opendate="2023-9-6 00:00:00" fixdate="2023-1-6 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Prompts user to close when atomicity implementation is not supported</summary>
      <description>When atomicity is enabled, an exception may occur when creating a DynamicTableSink, and we need to prompt the user to disable atomicity. When we use InMemoryCatalog, RTAS drop table will only delete the metadata, not clean up the underlying data files, RTAS write data does not use overwrite semantics by default, so it looks like the data is duplicated, this problem needs to be clarified in the documentation.</description>
      <version>1.18.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.create.md</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="33053" opendate="2023-9-7 00:00:00" fixdate="2023-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Watcher leak in Zookeeper HA mode</summary>
      <description>We observe a watcher leak in our OLAP stress test when enabling Zookeeper HA mode. TM's watches on the leader of JobMaster has not been stopped after job finished.Here is how we re-produce this issue: Start a session cluster and enable Zookeeper HA mode. Continuously and concurrently submit short queries, e.g. WordCount to the cluster. echo -n wchp | nc {zk host} {zk port} to get current watches.We can see a lot of watches on /flink/{cluster_name}/leader/{job_id}/connection_info.</description>
      <version>1.17.0,1.18.0,1.17.1</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="33055" opendate="2023-9-7 00:00:00" fixdate="2023-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct the error value about &amp;#39;state.backend.type&amp;#39; in the document</summary>
      <description> state.backend.type: The state backend to use. This defines the data structure mechanism for taking snapshots. Common values are filesystem or rocksdbfilesystem should be replaced with hashmap after FLINK-16444.</description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.config.md</file>
      <file type="M">docs.content.zh.docs.deployment.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="33063" opendate="2023-9-8 00:00:00" fixdate="2023-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>udaf with user defined pojo object throw error while generate record equaliser</summary>
      <description>Udaf with user define pojo object throw error while generating record equaliser: When user create an udaf while recore contains user define complex pojo object (like List&lt;UserDefinedObject&gt; or Map&lt;UserDefinedObject&gt;). The codegen will throw error while generating record equaliser, the error is:A method named "compareTo" is not declared in any enclosing class nor any subtype, nor through a static import.</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0,1.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.typeutils.TypeCheckUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.utils.JavaUserDefinedAggFunctions.java</file>
    </fixedFiles>
  </bug>
  <bug id="33065" opendate="2023-9-8 00:00:00" fixdate="2023-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize the exception message when the program plan could not be fetched</summary>
      <description>When the program plan could not be fetched, the root cause may be: the main method doesn't call the `env.execute()`. We can optimize the message to help user find this root cause.</description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgramUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="33071" opendate="2023-9-11 00:00:00" fixdate="2023-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log checkpoint statistics</summary>
      <description>This is a stop gap solution until we have a proper way of solving FLINK-23411.The plan is to dump JSON serialised checkpoint statistics into Flink JM's log, with a DEBUG level. This could be used to analyse what has happened with a certain checkpoint in the past.</description>
      <version>1.18.0</version>
      <fixedVersion>1.19.0,1.18.1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointStatsTracker.java</file>
    </fixedFiles>
  </bug>
  <bug id="33093" opendate="2023-9-15 00:00:00" fixdate="2023-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SHOW FUNCTIONS throw exception with unset catalog</summary>
      <description>A test like this throw an exception. It should instead return only built-in functions @Test public void testUnsetCatalogWithShowFunctions() throws Exception { TableEnvironment tEnv = TableEnvironment.create(ENVIRONMENT_SETTINGS); tEnv.useCatalog(null); TableResult table = tEnv.executeSql("SHOW FUNCTIONS"); final List&lt;Row&gt; functions = CollectionUtil.iteratorToList(table.collect()); // check it has some built-in functions assertThat(functions).hasSizeGreaterThan(0); }</description>
      <version>1.18.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.catalog.UnknownCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
