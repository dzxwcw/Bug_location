<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="30878" opendate="2023-2-2 00:00:00" fixdate="2023-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KubernetesHighAvailabilityRecoverFromSavepointITCase fails due to a deadlock</summary>
      <description>We're seeing a test failure in KubernetesHighAvailabilityRecoverFromSavepointITCase due to a deadlock:2023-02-01T18:53:35.5540322Z "ForkJoinPool-1-worker-1" #14 daemon prio=5 os_prio=0 tid=0x00007f68ecb18000 nid=0x43dd1 waiting on condition [0x00007f68c1711000]2023-02-01T18:53:35.5540900Z java.lang.Thread.State: TIMED_WAITING (parking)2023-02-01T18:53:35.5541272Z at sun.misc.Unsafe.park(Native Method)2023-02-01T18:53:35.5541932Z - parking to wait for &lt;0x00000000d14d7b60&gt; (a java.util.concurrent.CompletableFuture$Signaller)2023-02-01T18:53:35.5542496Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2023-02-01T18:53:35.5543088Z at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1709)2023-02-01T18:53:35.5543672Z at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)2023-02-01T18:53:35.5544240Z at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1788)2023-02-01T18:53:35.5544801Z at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)2023-02-01T18:53:35.5545632Z at org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityRecoverFromSavepointITCase.testRecoverFromSavepoint(KubernetesHighAvailabilityRecoverFromSavepointITCase.java:113)2023-02-01T18:53:35.5546409Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45565&amp;view=logs&amp;j=bea52777-eaf8-5663-8482-18fbc3630e81&amp;t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&amp;l=61916The build failure happens on 1.16. I'm adding 1.17 and 1.15 as fixVersions as well because it might be due to some recent changes which were introduced with FLINK-30462 and/or FLINK-30474</description>
      <version>1.17.0,1.15.4,1.16.2</version>
      <fixedVersion>1.17.0,1.15.4,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.DefaultMultipleComponentLeaderElectionServiceTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.DefaultMultipleComponentLeaderElectionService.java</file>
    </fixedFiles>
  </bug>
  <bug id="30972" opendate="2023-2-9 00:00:00" fixdate="2023-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>E2e tests always fail in phase "Prepare E2E run"</summary>
      <description>Installing required softwareReading package lists...Building dependency tree...Reading state information...bc is already the newest version (1.07.1-2build1).bc set to manually installed.libapr1 is already the newest version (1.6.5-1ubuntu1).libapr1 set to manually installed.0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.--2023-02-09 04:38:47-- http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.10_amd64.debResolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 185.125.190.36, 185.125.190.39, ...Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.HTTP request sent, awaiting response... 404 Not Found2023-02-09 04:38:47 ERROR 404: Not Found.WARNING: apt does not have a stable CLI interface. Use with caution in scripts.Reading package lists...E: Unsupported file ./libssl1.0.0_1.0.2n-1ubuntu5.10_amd64.deb given on commandline##[error]Bash exited with code '100'.Finishing: Prepare E2E run</description>
      <version>1.17.0,1.15.4,1.16.2,1.18.0</version>
      <fixedVersion>1.17.0,1.15.4,1.16.2,1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="31962" opendate="2023-4-27 00:00:00" fixdate="2023-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>libssl not found when running CI</summary>
      <description>Installed Maven 3.2.5 to /home/vsts/maven_cache/apache-maven-3.2.5Installing required softwareReading package lists...Building dependency tree...Reading state information...bc is already the newest version (1.07.1-2build1).bc set to manually installed.libapr1 is already the newest version (1.6.5-1ubuntu1).libapr1 set to manually installed.0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.--2023-04-27 11:42:53-- http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.11_amd64.debResolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 185.125.190.36, 185.125.190.39, ...Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.HTTP request sent, awaiting response... 404 Not Found2023-04-27 11:42:53 ERROR 404: Not Found.</description>
      <version>1.16.2,1.18.0,1.17.1</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="32152" opendate="2023-5-22 00:00:00" fixdate="2023-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Consolidate mocking library usage</summary>
      <description>Use mockito instead of powermock wherever possible, with the goal of restricting powermock to specific modules, eventually dropping it entirely.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.window.WindowOperatorContractTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.codegen.agg.AggTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.FlinkCalciteCatalogReaderTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TestSpyWrapperStateBackend.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StateSnapshotContextSynchronousImplTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.OperatorSnapshotFuturesTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.BackendRestorerProcedureTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.IncrementalRemoteKeyedStateHandleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NettyShuffleEnvironmentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseMigrationTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="32153" opendate="2023-5-22 00:00:00" fixdate="2023-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Limit powermock to flink-core/-runtime</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32349" opendate="2023-6-15 00:00:00" fixdate="2023-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support atomic for CREATE TABLE AS SELECT(CTAS) statement</summary>
      <description>For detailed information, see FLIP-305https://cwiki.apache.org/confluence/display/FLINK/FLIP-305%3A+Support+atomic+for+CREATE+TABLE+AS+SELECT%28CTAS%29+statement</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.delegation.DefaultExecutor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.ExecutorMock.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.CreateTableASOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.delegation.Executor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.TableConfigOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.executor.python.ChainingOptimizingExecutor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.execution.JobStatusHook.java</file>
      <file type="M">docs.layouts.shortcodes.generated.table.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="32351" opendate="2023-6-15 00:00:00" fixdate="2023-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce base interfaces for call procedure</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.Catalog.java</file>
      <file type="M">flink-python.pyflink.table.catalog.py</file>
    </fixedFiles>
  </bug>
  <bug id="32354" opendate="2023-6-15 00:00:00" fixdate="2023-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to execute the call procedure operation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.ProcedureITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.PlannerCallProcedureOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.CallProcedureOperation.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlNodeConvertContext.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConverters.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.bridging.BridgingUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.calcite.SqlToRexConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="32358" opendate="2023-6-15 00:00:00" fixdate="2023-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CI may unintentionally use fallback akka loader</summary>
      <description>We have a fallback akka loader for developer convenience in the IDE, that is on the classpath of most modules. Depending on the order of jars on the classpath it can happen that the fallback loader appears first, which we dont want because it slows down the build and creates noisy logs.We can add a simple prioritization scheme to the rpc system loading to remedy that.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.RpcSystemLoader.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.RpcSystem.java</file>
      <file type="M">flink-rpc.flink-rpc-akka-loader.src.test.java.org.apache.flink.runtime.rpc.akka.FallbackAkkaRpcSystemLoader.java</file>
      <file type="M">flink-rpc.flink-rpc-akka-loader.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcSystemLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="32369" opendate="2023-6-16 00:00:00" fixdate="2023-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setup cron build</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.compile.sh</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
    </fixedFiles>
  </bug>
  <bug id="32419" opendate="2023-6-23 00:00:00" fixdate="2023-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unused/obsolete classes</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriverTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingLeaderElectionListener.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingLeaderElectionDriver.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderElectionEvent.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunnerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriverFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.MultipleComponentLeaderElectionDriverFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.MultipleComponentLeaderElectionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperMultipleComponentLeaderElectionHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.AbstractHaServices.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionAndRetrievalITCase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityTestBase.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderRetrievalDriverFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderElectionHaServices.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderElectionDriverFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderElectionDriver.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingLeaderBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.DefaultMultipleComponentLeaderElectionServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.DefaultLeaderElectionServiceTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.LeaderRetrievalDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriverFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.MultipleComponentLeaderElectionService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.MultipleComponentLeaderElectionDriverAdapterFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.MultipleComponentLeaderElectionDriverAdapter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.LeaderElectionException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.LeaderElectionEventHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.LeaderElectionDriverFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.LeaderElectionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.DefaultMultipleComponentLeaderElectionService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderElectionDriverTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionDriverTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderElectionHaServicesFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderRetrievalDriverFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderRetrievalDriver.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionDriverFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="32457" opendate="2023-6-28 00:00:00" fixdate="2023-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update current documentation of JSON_OBJECTAGG/JSON_ARRAYAGG to clarify the limitation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0,1.17.2</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.WrapJsonAggFunctionArgumentsRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.logical.WrapJsonAggFunctionArgumentsRuleTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="32469" opendate="2023-6-28 00:00:00" fixdate="2023-7-28 01:00:00" resolution="Done">
    <buginformation>
      <summary>Improve checkpoint REST APIs for programmatic access</summary>
      <description>WhyWe want to enable programmatic use of the checkpoints REST API, independent of the Flink dashboard.Currently, REST APIs that retrieve information relating to a given Flink job passes through the ExecutionGraphCache. This means that all these APIs will retrieve stale data depending on the web.refresh-interval, which defaults to 3s. For programmatic use of the REST API, we should be able to retrieve the latest / cached version depending on the client (Flink dashboard gets the cached version, other clients get the updated version).For example, a user might want to use the REST API to retrieve the latest completed checkpoint for a given Flink job. This might be useful when trying to use existing checkpoints as state store when migrating a Flink job from one cluster to another. See Appendix for example.WhatThis change is about separating out the cache used for the checkpoints REST APIs to a separate cache. This way, a user can set the timeout for the checkpoints cache to 0s (disable cache), without causing much effect on the user experience on the Flink dashboard.In addition, the checkpoint handlers first retrieve the ExecutionGraph, then retrieve the CheckpointStatsSnapshot from the graph. This is not needed, since the checkpoint handlers only need the CheckpointStatsSnapshot. This change will mean these handlers retrieve the minimal required information (CheckpointStatsSnapshot) to construct a reply. Example use caseWhen performing security patching / maintenance of the infrastructure supporting the Flink cluster, we might want to transfer a given Flink job to another cluster, whilst maintaining state. We can do this via the below steps: Old cluster - Select completed checkpoint on existing Flink job Old cluster - Stop the existing Flink job New cluster - Start a new Flink job with selected checkpointStep 1 requires us to query the checkpoints REST API for the latest completed checkpoint. With the status quo, we need to wait 3s (or whatever the ExecutionGraphCache expiry may be). This is undesirable because this means the Flink job will have to reprocess data equivalent to 3s / whatever the execution graph cache timeout is.</description>
      <version>1.16.2,1.17.1</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingRestfulGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingDispatcherGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.TestingSchedulerNG.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.RestHandlerConfigurationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.TestingJobMasterGatewayBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.TestingJobMasterGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.TestingJobManagerRunner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.RestfulGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.NonLeaderRetrievalRestfulGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerNG.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.RestHandlerConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.AbstractCheckpointHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.rest.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="32501" opendate="2023-6-30 00:00:00" fixdate="2023-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong execution plan of a proctime window aggregation generated due to incorrect cost evaluation</summary>
      <description>Currently when uses window aggregation referring a windowing tvf with a filter condition, may encounter wrong plan which may hang forever in runtime(the window aggregate operator never output)for such a case:insert into sink select window_start, window_end, b, COALESCE(sum(case when a = 11 then 1 end), 0) c from TABLE( TUMBLE(TABLE source, DESCRIPTOR(proctime), INTERVAL '10' SECONDS) ) where a in (1, 5, 7, 9, 11) GROUP BY window_start, window_end, bgenerate wrong plan which didn't combine the proctime WindowTableFunction into WindowAggregate (so when translate to execution plan the WindowAggregate will wrongly recognize the window as an event-time window, then the WindowAggregateOperator will not receive watermark nor setup timers to fire any windows in runtime)Sink(table=[default_catalog.default_database.sink], fields=[ws, we, b, c])+- Calc(select=[CAST(window_start AS TIMESTAMP(6)) AS ws, CAST(window_end AS TIMESTAMP(6)) AS we, b, CAST(COALESCE($f1, 0) AS BIGINT) AS c]) +- WindowAggregate(groupBy=[b], window=[TUMBLE(win_start=[window_start], win_end=[window_end], size=[10 s])], select=[b, SUM($f3) AS $f1, start('w$) AS window_start, end('w$) AS window_end]) +- Exchange(distribution=[hash[b]]) +- Calc(select=[window_start, window_end, b, CASE((a = 11), 1, null:INTEGER) AS $f3], where=[SEARCH(a, Sarg[1, 5, 7, 9, 11])]) +- WindowTableFunction(window=[TUMBLE(time_col=[proctime], size=[10 s])]) +- Calc(select=[a, b, PROCTIME() AS proctime]) +- TableSourceScan(table=[[default_catalog, default_database, source, project=[a, b], metadata=[]]], fields=[a, b])expected plan:Sink(table=[default_catalog.default_database.sink], fields=[ws, we, b, c])+- Calc(select=[CAST(window_start AS TIMESTAMP(6)) AS ws, CAST(window_end AS TIMESTAMP(6)) AS we, b, CAST(COALESCE($f1, 0) AS BIGINT) AS c]) +- WindowAggregate(groupBy=[b], window=[TUMBLE(time_col=[proctime], size=[10 s])], select=[b, SUM($f3) AS $f1, start('w$) AS window_start, end('w$) AS window_end]) +- Exchange(distribution=[hash[b]]) +- Calc(select=[b, CASE((a = 11), 1, null:INTEGER) AS $f3, PROCTIME() AS proctime], where=[SEARCH(a, Sarg[1, 5, 7, 9, 11])]) +- TableSourceScan(table=[[default_catalog, default_database, source, project=[a, b], metadata=[]]], fields=[a, b])</description>
      <version>1.16.2,1.17.1</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdSelectivityTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdSelectivity.scala</file>
    </fixedFiles>
  </bug>
  <bug id="32556" opendate="2023-7-7 00:00:00" fixdate="2023-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Renames contenderID into componentId</summary>
      <description>We introduced contenderID in a lot of places with FLINK-26522. The original multi-component leader election classes of FLINK-24038 used componentId.Revisiting that naming made me realize that it's actually wrong. A contender is a specific instance of a component that participates in the leader election. A component, in this sense, is the more abstract concept. contenderID refers to an ID for the specific contender instance but the IDs we're sharing are actually referring to a Flink component and therefore, are the same between different contenders which compete for leadership for the same component. This contradicts the definition of an identifier.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriverTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingLeaderElectionListener.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingLeaderElectionDriver.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderInformationRegisterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderElectionEvent.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.DefaultLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.DefaultLeaderElectionServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderServiceTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.LeaderInformationRegister.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.LeaderElectionService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.LeaderElectionDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.DefaultLeaderElection.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperLeaderElectionHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionDriverTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionAndRetrievalITCase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityTestBase.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderRetrievalDriverFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionHaServices.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesLeaderElectionDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="32680" opendate="2023-7-26 00:00:00" fixdate="2023-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Job vertex names get messed up once there is a source vertex chained with a MultipleInput vertex in job graph</summary>
      <description>Take the following test(put it to MultipleInputITCase) as example: @Test public void testMultipleInputDoesNotChainedWithSource() throws Exception { testJobVertexName(false); } @Test public void testMultipleInputChainedWithSource() throws Exception { testJobVertexName(true); } public void testJobVertexName(boolean chain) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); TestListResultSink&lt;Long&gt; resultSink = new TestListResultSink&lt;&gt;(); DataStream&lt;Long&gt; source1 = env.fromSequence(0L, 3L).name("source1"); DataStream&lt;Long&gt; source2 = env.fromElements(4L, 6L).name("source2"); DataStream&lt;Long&gt; source3 = env.fromElements(7L, 9L).name("source3"); KeyedMultipleInputTransformation&lt;Long&gt; transform = new KeyedMultipleInputTransformation&lt;&gt;( "MultipleInput", new KeyedSumMultipleInputOperatorFactory(), BasicTypeInfo.LONG_TYPE_INFO, 1, BasicTypeInfo.LONG_TYPE_INFO); if (chain) { transform.setChainingStrategy(ChainingStrategy.HEAD_WITH_SOURCES); } KeySelector&lt;Long, Long&gt; keySelector = (KeySelector&lt;Long, Long&gt;) value -&gt; value % 3; env.addOperator( transform .addInput(source1.getTransformation(), keySelector) .addInput(source2.getTransformation(), keySelector) .addInput(source3.getTransformation(), keySelector)); new MultipleConnectedStreams(env).transform(transform).rebalance().addSink(resultSink).name("sink"); env.execute(); } When we run testMultipleInputDoesNotChainedWithSource , all job vertex names are normal:When we run testMultipleInputChainedWithSource (the MultipleInput chained with source1), job vertex names get messed up (all job vertex names contain Source: source1): I think it's a bug.</description>
      <version>1.16.2,1.18.0,1.17.1</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="32703" opendate="2023-7-27 00:00:00" fixdate="2023-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hotfix] flink-python POM has a typo for protobuf-java in shading config</summary>
      <description>Fix typo. `inculde` -&gt; `include`                                  &lt;includes combine.children="append"&gt;                                    &lt;include&gt;net.razorvine:*&lt;/include&gt;                                    &lt;include&gt;net.sf.py4j:*&lt;/include&gt;                                    &lt;include&gt;org.apache.beam:*&lt;/include&gt;                                    &lt;include&gt;com.fasterxml.jackson.core:*&lt;/include&gt;                                    &lt;include&gt;joda-time:*&lt;/include&gt;                                    &lt;inculde&gt;com.google.protobuf:*&lt;/inculde&gt;                                    &lt;include&gt;org.apache.arrow:*&lt;/include&gt;                                    &lt;include&gt;io.netty:*&lt;/include&gt;                                    &lt;include&gt;com.google.flatbuffers:*&lt;/include&gt;                                    &lt;include&gt;com.alibaba:pemja&lt;/include&gt;                                &lt;/includes&gt;</description>
      <version>1.16.2,1.18.0,1.17.1</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32865" opendate="2023-8-14 00:00:00" fixdate="2023-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DynamicFilteringDataCollectorOperator can&amp;#39;t chain with the upstream operator when the parallelism is inconsistent</summary>
      <description> If the DynamicFilteringDataCollectorOperator parallelism is not consistent with the upstream operator, they can't chain together, this will the DynamicFilteringDataCollectorOperator to execute after the fact source, so the dpp won't work. Due to the operator parallelism being decided during runtime, so we should add scheduler dependency forcibly in compile phase.</description>
      <version>1.16.2,1.18.0,1.17.1</version>
      <fixedVersion>1.18.0,1.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.dynamicfiltering.ExecutionOrderEnforcerOperatorFactory.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.dynamicfiltering.ExecutionOrderEnforcerOperator.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.optimize.program.FlinkRuntimeFilterProgramTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.optimize.program.DynamicPartitionPruningProgramTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.operator.BatchOperatorNameTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DynamicFilteringTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.processor.ResetTransformationProcessorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.ResetTransformationProcessor.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.DynamicFilteringDependencyProcessor.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecMultipleInput.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.DynamicFilteringITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecExecutionOrderEnforcer.java</file>
    </fixedFiles>
  </bug>
  <bug id="33010" opendate="2023-8-31 00:00:00" fixdate="2023-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE when using GREATEST() in Flink SQL</summary>
      <description>Hi,I see NPEs in flink 1.14 and flink 1.16 when running queries with GREATEST() and timestamps. Below is an example to help in reproducing the issue.CREATE TEMPORARY VIEW Positions ASSELECTSecurityId,ccy1,CAST(publishTimestamp AS TIMESTAMP(3)) as publishTimestampFROM (VALUES(1, 'USD', '2022-01-01'),(2, 'GBP', '2022-02-02'),(3, 'GBX', '2022-03-03'),(4, 'GBX', '2022-04-4'))AS ccy(SecurityId, ccy1, publishTimestamp);CREATE TEMPORARY VIEW Benchmarks ASSELECTSecurityId,ccy1,CAST(publishTimestamp AS TIMESTAMP(3)) as publishTimestampFROM (VALUES(3, 'USD', '2023-01-01'),(4, 'GBP', '2023-02-02'),(5, 'GBX', '2023-03-03'),(6, 'GBX', '2023-04-4'))AS ccy(SecurityId, ccy1, publishTimestamp);SELECT *,GREATEST(IFNULL(Positions.publishTimestamp,CAST('1970-1-1' AS TIMESTAMP(3))),IFNULL(Benchmarks.publishTimestamp,CAST('1970-1-1' AS TIMESTAMP(3))))FROM PositionsFULL JOIN Benchmarks ON Positions.SecurityId = Benchmarks.SecurityId  Using "IF" is a workaround at the moment instead of using "GREATEST"  </description>
      <version>1.16.1,1.16.2</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2,1.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.GreatestLeastFunctionsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="33083" opendate="2023-9-13 00:00:00" fixdate="2023-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SupportsReadingMetadata is not applied when loading a CompiledPlan</summary>
      <description>If a few conditions are met, we can not apply ReadingMetadata interface: source overwrites: @Override public boolean supportsMetadataProjection() { return false; } source does not implement SupportsProjectionPushDown table has metadata columns e.g.CREATE TABLE src ( physical_name STRING, physical_sum INT, timestamp TIMESTAMP_LTZ(3) NOT NULL METADATA VIRTUAL) we query the table SELECT * FROM srcIt fails with:Caused by: java.lang.IllegalArgumentException: Row arity: 1, but serializer arity: 2 at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:124)The reason is SupportsReadingMetadataSpec is created only in the PushProjectIntoTableSourceScanRule, but the rule is not applied when 1 &amp; 2</description>
      <version>1.16.2,1.17.1</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.TableSourceJsonPlanITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSourceUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSinkUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.NonDeterministicDagTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.WatermarkAssignerChangelogNormalizeTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.PushLocalAggIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.optimize.ScanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testReuseSourceWithoutProjectionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.connector.file.table.FileSystemTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.spec.DynamicTableSourceSpec.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
