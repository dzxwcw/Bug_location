<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="7211" opendate="2017-7-17 00:00:00" fixdate="2017-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude Gelly javadoc jar from release</summary>
      <description></description>
      <version>1.3.2,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7220" opendate="2017-7-18 00:00:00" fixdate="2017-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update RocksDB dependency to 5.5.5</summary>
      <description>The latest release of RocksDB (5.5.5) fixes the issues from previous versions (slow merge performance, segfaults) in connection with Flink and seems stable for us to use. We can move away from our custom FRocksDB build, back to the latest release.</description>
      <version>1.3.2,1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.benchmark.RocksDBPerformanceTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.PredefinedOptions.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7234" opendate="2017-7-19 00:00:00" fixdate="2017-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix CombineHint documentation</summary>
      <description>The CombineHint documentation applies to DataSet#reduce not DataSet#reduceGroup and should also be note for DataSet#distinct. It is also set with .setCombineHint(CombineHint) rather than alongside the UDF parameter.</description>
      <version>1.2.2,1.3.2,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.ReduceOperatorBase.java</file>
      <file type="M">docs.dev.batch.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="7363" opendate="2017-8-3 00:00:00" fixdate="2017-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add hashes and signatures to the download page</summary>
      <description>As part of the releases, we also generate MD5 hashes and cryptographic signatures but neither link to those nor do we explain which keys are valid release-signing keys. This should be added.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualProducerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualConsumerProducerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.examples.ProduceIntoKinesis.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.ProducerConfigConstants.java</file>
      <file type="M">docs.dev.connectors.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug id="7366" opendate="2017-8-3 00:00:00" fixdate="2017-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade kinesis producer library in flink-connector-kinesis</summary>
      <description>We need to upgrade KPL and KCL to pick up the enhanced performance and stability for Flink to work better with Kinesis. Upgrading KPL is specially necessary, because the KPL version Flink uses is old, and doesn't have good retry and error handling logic.KPL:flink-connector-kinesis currently uses kinesis-producer-library 0.10.2, which is released in Nov 2015 by AWS. It's old. It's the fourth release, and thus problematic. It doesn't even have good retry logic, therefore Flink fails really frequently (about every 10 mins as we observed) when Flink writes too fast to Kinesis and receives RateLimitExceededException, Quotes from https://github.com/awslabs/amazon-kinesis-producer/issues/56, "With the newer version of the KPL it uses the AWS C++ SDK which should offer additional retries." on Oct 2016. 0.12.5, the version we are upgrading to, is released in May 2017 and should have the enhanced retry logic.</description>
      <version>1.3.2</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7386" opendate="2017-8-8 00:00:00" fixdate="2017-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink Elasticsearch 5 connector is not compatible with Elasticsearch 5.2+ client</summary>
      <description>In Elasticsearch 5.2.0 client the class BulkProcessor was refactored and has no longer the method add(ActionRequest).For more info see: https://github.com/elastic/elasticsearch/pull/20109</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.Elasticsearch1ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.Elasticsearch5ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.Elasticsearch2ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.BulkProcessorIndexer.java</file>
    </fixedFiles>
  </bug>
  <bug id="7388" opendate="2017-8-8 00:00:00" fixdate="2017-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ProcessFunction.onTimer() sets processing time as timestamp</summary>
      <description>The ProcessFunction.onTimer() method sets the current processing time as event-time timestamp when it is called from a processing time timer.I don't think this behavior is useful. Processing time timestamps won't be aligned with watermarks and are not deterministic. The only reason would be to have some value in the timestamp field. However, the behavior is very subtle and might not be noticed by users.IMO, it would be better to erase the timestamp. This will cause downstream operator that rely on timestamps to fail and notify the users that the logic they implemented was probably not what they intended to do.What do you think Aljoscha Krettek?</description>
      <version>1.3.2,1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.KeyedProcessOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.co.KeyedCoProcessOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.KeyedProcessOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.co.KeyedCoProcessOperator.java</file>
      <file type="M">docs.dev.stream.operators.process.function.md</file>
    </fixedFiles>
  </bug>
  <bug id="7422" opendate="2017-8-10 00:00:00" fixdate="2017-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Kinesis Client Library (KCL) and AWS SDK in flink-connector-kinesis</summary>
      <description>Upgrade KCL from 1.6.2 to 1.8.1 (https://mvnrepository.com/artifact/com.amazonaws/amazon-kinesis-client)Since FLINK-7366, we may also need to bump aws sdk version as well in this ticket.KCL:Upgrade KCL from 1.6.2 to 1.8.1AWS SDKfrom 1.10.71 to 1.11.171</description>
      <version>1.3.2</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7441" opendate="2017-8-14 00:00:00" fixdate="2017-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Double quote string literals is not supported in Table API and SQL</summary>
      <description>Code generation doesn't handle double quote string literals and some control characters which leads to compile error.Caused by: org.codehaus.commons.compiler.CompileException: Line 50, Column 48: Expression "hello" is not an rvalue</description>
      <version>1.3.2</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.UserDefinedScalarFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="7442" opendate="2017-8-14 00:00:00" fixdate="2017-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option for using a child-first classloader for loading user code</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerComponentsStartupShutdownTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.JobManagerLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerHARecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManagerTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.minicluster.FlinkMiniCluster.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobManagerServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.client.JobClient.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDbMultiClassLoaderTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.JobWithJars.java</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="7456" opendate="2017-8-16 00:00:00" fixdate="2017-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement Netty sender incoming pipeline for credit-based</summary>
      <description>This is a part of work for credit-based network flow control.On sender side, each subpartition view maintains an atomic integer currentCredit from receiver. Once receiving the messages of PartitionRequest and AddCredit, the currentCredit is added by deltas.Each view also maintains an atomic boolean field to mark it as registered available for transfer to make sure it is enqueued in handler only once. If the currentCredit increases from zero and there are available buffers in the subpartition, the corresponding view will be enqueued for transferring data.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.misc.SuccessAfterNetworkBuffersFailureITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkBenchmarkEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskCancelAsyncProducerConsumerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NetworkEnvironmentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.ServerTransportErrorHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestQueueTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.CancelPartitionRequestTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpilledSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionType.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NetworkEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.SequenceNumberingViewReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestServerHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyMessage.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.CreditBasedClientHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="7563" opendate="2017-8-31 00:00:00" fixdate="2017-3-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix watermark semantics in CEP operators</summary>
      <description>See https://lists.apache.org/thread.html/3541e72ba3842192e58a487e54c2817f6b2b9d12af5fee97af83e5df@%3Cdev.flink.apache.org%3E for reference.</description>
      <version>None</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="7564" opendate="2017-8-31 00:00:00" fixdate="2017-3-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Watermark semantics in Table API</summary>
      <description>For reference, see https://lists.apache.org/thread.html/3541e72ba3842192e58a487e54c2817f6b2b9d12af5fee97af83e5df@%3Cdev.flink.apache.org%3E.</description>
      <version>None</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.OverWindowHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.RowTimeUnboundedOver.scala</file>
    </fixedFiles>
  </bug>
  <bug id="7568" opendate="2017-9-1 00:00:00" fixdate="2017-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bring window documentation up-to-date with latest changes and improve</summary>
      <description>Off the top of my head: Make ProcessWindowFunction the primary window function, threat WindowFunction as legacy Document more specifically how windowing behaves: window boundaries, chaining of several windowed operations, timestamps of emitted data, what is "key" in a ProcessWindowFunction...</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.operators.windows.md</file>
    </fixedFiles>
  </bug>
  <bug id="7575" opendate="2017-9-4 00:00:00" fixdate="2017-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dashboard jobs/tasks metrics display 0 when metrics are not yet available</summary>
      <description>The web frontend is currently displaying "0" when a metric is not available yet (ex: records-in/out, bytes-in/out). 0 is misleading and it's preferable to display no value while the value is still unknown.</description>
      <version>1.3.2</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.MutableIOMetrics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.metrics.MetricStore.java</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.plan.node.taskmanagers.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.plan.node.subtasks.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.plan.node-list.subtasks.html</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.node.taskmanagers.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.node.subtasks.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.node-list.subtasks.jade</file>
    </fixedFiles>
  </bug>
  <bug id="7595" opendate="2017-9-6 00:00:00" fixdate="2017-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Removing stateless task from task chain breaks savepoint restore</summary>
      <description>When removing a stateless operator from a 2-task chain where the head operator is stateful breaks savepoint restore with Caused by: java.lang.IllegalStateException: Failed to rollback to savepoint /var/folders/py/s_1l8vln6f19ygc77m8c4zhr0000gn/T/junit1167397515334838028/junit8006766303945373008/savepoint-cb0bcf-3cfa67865ac0. Cannot map savepoint state...</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.AbstractNonKeyedOperatorRestoreTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.AbstractOperatorRestoreTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointLoaderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="7596" opendate="2017-9-7 00:00:00" fixdate="2017-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix bug when Set Operation handles ANY type</summary>
      <description>If two inputs with Any(GenericRelDataType), when they comes to Set Operation(UNION, MINUS,...), it will cause a TableException with info is "Type is not supported: ANY"Here is the test case:@Test def testUnion(): Unit = { val list = List((1, new NODE), (2, new NODE)) val list2 = List((3, new NODE), (4, new NODE)) val env = StreamExecutionEnvironment.getExecutionEnvironment val tEnv = TableEnvironment.getTableEnvironment(env) val s1 = tEnv.fromDataStream(env.fromCollection(list)) val s2 = tEnv.fromDataStream(env.fromCollection(list2)) val result = s1.unionAll(s2).toAppendStream[Row] result.addSink(new StreamITCase.StringSink[Row]) env.execute() } class NODE { val x = new util.HashMap[String, String]()}This bug happens because Flink doesn't handle createSqlType(ANY) and Calcite doesn't know the differences between ANY and ANY(GenericRelDataType), so the createSqlType(ANY) of Calcite will return a BasicSqlType instead.</description>
      <version>None</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
    </fixedFiles>
  </bug>
  <bug id="7600" opendate="2017-9-7 00:00:00" fixdate="2017-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>shorten delay of KinesisProducerConfiguration.setCredentialsRefreshDelay() to avoid updateCredentials Exception</summary>
      <description>we saw the following warning in Flink log:2017-08-11 02:33:24,473 WARN org.apache.flink.kinesis.shaded.com.amazonaws.services.kinesis.producer.Daemon - Exception during updateCredentialsjava.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at org.apache.flink.kinesis.shaded.com.amazonaws.services.kinesis.producer.Daemon$5.run(Daemon.java:316) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)According to discussion in https://github.com/awslabs/amazon-kinesis-producer/issues/10, setting the delay to 100 will fix this issue</description>
      <version>1.3.2</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="7603" opendate="2017-9-8 00:00:00" fixdate="2017-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support time constraint in MatchRecognize</summary>
      <description>It is a common use case to search for patterns within some time limit. This also allows more fine grained state control.One option is to support WITHIN clause provided by CALCITE, the downside of this approach is that it is not SQL standard compliant.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.match.PatternTranslatorTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.match.PatternTranslatorTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamMatch.scala</file>
      <file type="M">docs.dev.table.streaming.match.recognize.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug id="7608" opendate="2017-9-8 00:00:00" fixdate="2017-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LatencyGauge change to histogram metric</summary>
      <description>I used slf4jReporterhttps://issues.apache.org/jira/browse/FLINK-4831 to export metrics the log file.I found:-- Gauges ---------------------------------------------------------------------......zhouhai-mbp.taskmanager.f3fd3a269c8c3da4e8319c8f6a201a57.Flink Streaming Job.Map.0.latency: value={LatencySourceDescriptor{vertexID=1, subtaskIndex=-1}={p99=116.0, p50=59.5, min=11.0, max=116.0, p95=116.0, mean=61.833333333333336}}zhouhai-mbp.taskmanager.f3fd3a269c8c3da4e8319c8f6a201a57.Flink Streaming Job.Sink- Unnamed.0.latency: value={LatencySourceDescriptor{vertexID=1, subtaskIndex=0}={p99=195.0, p50=163.5, min=115.0, max=195.0, p95=195.0, mean=161.0}}......</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.StreamSourceOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.streamrecord.LatencyMarker.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="7623" opendate="2017-9-14 00:00:00" fixdate="2017-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Detecting whether an operator is restored doesn&amp;#39;t work with chained state</summary>
      <description>Originally reported on the ML: https://lists.apache.org/thread.html/22a2cf83de3107aa81a03a921325a191c29df8aa8676798fcd497199@%3Cuser.flink.apache.org%3EIf we have a chain of operators where multiple of them have operator state, detection of the context.isRestored() flag (of CheckpointedFunction) does not work correctly. It's best exemplified using this minimal example where both the source and the flatMap have state:final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env .addSource(new MaSource()).uid("source-1") .flatMap(new MaFlatMap()).uid("flatMap-1");env.execute("testing");If I do a savepoint with these UIDs, then change "source-1" to "source-2" and restore from the savepoint context.isRestored() still reports true for the source.</description>
      <version>1.3.2,1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="7632" opendate="2017-9-16 00:00:00" fixdate="2017-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better documentation and examples on C* sink usage for Pojo and Tuples data types</summary>
      <description>Cassandra sink supports Pojo and Java Tuple data types. We should improve documentation on its usage as well as some concrete / meaningful examples for both cases.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.cassandra.md</file>
    </fixedFiles>
  </bug>
  <bug id="7635" opendate="2017-9-18 00:00:00" fixdate="2017-9-18 01:00:00" resolution="Done">
    <buginformation>
      <summary>Support sideOutput in ProcessWindowFunciton</summary>
      <description>FLINK-4460 only implemented output to ProcessFunction Context. It would be nice to add support to ProcessWindow and ProcessAllWindow functions as well. email threadsAljoscha Krettek I thought this is good warm up task for ppl to learn how window function works in general. Otherwise feel free to assign back to me.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.SideOutputITCase.java</file>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.SideOutputITCase.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.function.util.ScalaProcessWindowFunctionWrapper.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.function.ProcessWindowFunction.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.functions.InternalWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.functions.InternalProcessWindowContext.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.ProcessOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.InternalProcessApplyWindowContext.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorContractTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="7649" opendate="2017-9-20 00:00:00" fixdate="2017-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port JobStoppingHandler to new REST endpoint</summary>
      <description>Port existing JobStoppingHandler to new REST endpoint</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobMessageParameters.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobCancellationHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.JobCancellationHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="7658" opendate="2017-9-20 00:00:00" fixdate="2017-2-20 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support COLLECT Aggregate function in Flink TABLE API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.AggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.stringexpr.AggregateStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.aggregations.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="7666" opendate="2017-9-21 00:00:00" fixdate="2017-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ContinuousFileReaderOperator swallows chained watermarks</summary>
      <description>I use event time and read from a (finite) file. I assign watermarks right after the ContinuousFileReaderOperator with parallelism 1.env .readFile(new TextInputFormat(...), ...) .setParallelism(1) .assignTimestampsAndWatermarks(...) .setParallelism(1) .map()...The watermarks I assign never progress through the pipeline.I can work around this by inserting a shuffle() after the file reader or starting a new chain at the assigner:env .readFile(new TextInputFormat(...), ...) .setParallelism(1) .shuffle() .assignTimestampsAndWatermarks(...) .setParallelism(1) .map()...</description>
      <version>1.3.2</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeServiceTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TestProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ProcessingTimeService.java</file>
    </fixedFiles>
  </bug>
  <bug id="7678" opendate="2017-9-23 00:00:00" fixdate="2017-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL UserDefineTableFunction does not take CompositeType input correctly</summary>
      <description>UDF is using FlinkTypeFactory to infer operand type while UDTF does not go through the same code path. This result in:Unable to find source-code formatter for language: console. Available languages are: actionscript, ada, applescript, bash, c, c#, c++, cpp, css, erlang, go, groovy, haskell, html, java, javascript, js, json, lua, none, nyan, objc, perl, php, python, r, rainbow, ruby, scala, sh, sql, swift, visualbasic, xml, yamlorg.apache.flink.table.api.ValidationException: SQL validation failed. From line 1, column 38 to line 1, column 44: No match found for function signature func(&lt;COMPOSITE(Row(f0: Integer, f1: String))&gt;) Caused by: org.apache.calcite.runtime.CalciteContextException: From line 1, column 38 to line 1, column 44: No match found for function signature func(&lt;COMPOSITE(Row(f0: Integer, f1: String))&gt;)Please see github code for more info:https://github.com/walterddr/flink/blob/bug_report/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/api/batch/sql/UDTFCompositeTypeTestFailure.scala</description>
      <version>1.3.2</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.UserDefinedTableFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.CorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.CorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.userDefinedScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.UserDefinedScalarFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.CorrelateTest.scala</file>
    </fixedFiles>
  </bug>
  <bug id="7679" opendate="2017-9-23 00:00:00" fixdate="2017-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade maven enforcer plugin to 3.0.0-M1</summary>
      <description>I got the following build error against Java 9:[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:1.4.1:enforce (enforce-maven) on project flink-parent: Execution enforce-maven of goal org.apache.maven.plugins:maven-enforcer-plugin:1.4.1:enforce failed: An API incompatibility was encountered while executing org.apache.maven.plugins:maven-enforcer-plugin:1.4.1:enforce: java.lang.ExceptionInInitializerError: null[ERROR] -----------------------------------------------------[ERROR] realm = plugin&gt;org.apache.maven.plugins:maven-enforcer-plugin:1.4.1[ERROR] strategy = org.codehaus.plexus.classworlds.strategy.SelfFirstStrategy[ERROR] urls[0] = file:/home/hbase/.m2/repository/org/apache/maven/plugins/maven-enforcer-plugin/1.4.1/maven-enforcer-plugin-1.4.1.jarUpgrading maven enforcer plugin to 3.0.0-M1 would get over the above error.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.force-shading.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7701" opendate="2017-9-27 00:00:00" fixdate="2017-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>IllegalArgumentException in Netty bootstrap with small memory state segment size</summary>
      <description>FLINK-7258 broke setting high and low watermarks for small segment sizes. We should tackle both use cases.</description>
      <version>1.3.2,1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyServerLowAndHighWatermarkTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyServer.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.AbstractServerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="7702" opendate="2017-9-27 00:00:00" fixdate="2017-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Javadocs are not being built</summary>
      <description>The "Javadocs" link in the left side menu of this page doesn't work:https://ci.apache.org/projects/flink/flink-docs-master/Note that it works in 1.3:https://ci.apache.org/projects/flink/flink-docs-release-1.3/</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-filesystem.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7738" opendate="2017-9-29 00:00:00" fixdate="2017-10-29 01:00:00" resolution="Abandoned">
    <buginformation>
      <summary>Create WebSocket handler (server)</summary>
      <description>An abstract handler is needed to support websocket communication.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.triggers.Trigger.java</file>
    </fixedFiles>
  </bug>
  <bug id="7739" opendate="2017-9-29 00:00:00" fixdate="2017-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Kafka*ITCase tests stability</summary>
      <description></description>
      <version>1.3.2</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.TimestampITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.IPv6HostnamesITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAJobGraphRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.misc.SuccessAfterNetworkBuffersFailureITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.misc.MiscellaneousIssuesITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.misc.CustomSerializationITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.manual.StreamingScalabilityAndLatency.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.manual.NotSoMiniClusterIterations.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.example.failing.JobSubmissionFailsITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.classloading.ClassLoaderITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.accumulators.AccumulatorErrorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskCancelAsyncProducerConsumerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerCleanupITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.client.JobClientActorRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CoordinatorShutdownTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.minicluster.FlinkMiniCluster.scala</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitorITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.NonHAAbstractQueryableStateTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaShortRetentionTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RescalingITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="7744" opendate="2017-10-1 00:00:00" fixdate="2017-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add missing top links to documentation</summary>
      <description>The links to the top of the page are missing on many pages. Those are very useful for reading the documentation on mobile.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.testing.md</file>
      <file type="M">docs.search-results.md</file>
      <file type="M">docs.quickstart.setup.quickstart.md</file>
      <file type="M">docs.quickstart.scala.api.quickstart.md</file>
      <file type="M">docs.quickstart.run.example.quickstart.md</file>
      <file type="M">docs.quickstart.java.api.quickstart.md</file>
      <file type="M">docs.ops.upgrading.md</file>
      <file type="M">docs.ops.state.state.backends.md</file>
      <file type="M">docs.ops.state.savepoints.md</file>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
      <file type="M">docs.ops.state.checkpoints.md</file>
      <file type="M">docs.ops.security-ssl.md</file>
      <file type="M">docs.ops.security-kerberos.md</file>
      <file type="M">docs.ops.production.ready.md</file>
      <file type="M">docs.ops.jobmanager.high.availability.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.md</file>
      <file type="M">docs.ops.deployment.mesos.md</file>
      <file type="M">docs.ops.deployment.mapr.setup.md</file>
      <file type="M">docs.ops.deployment.gce.setup.md</file>
      <file type="M">docs.ops.deployment.aws.md</file>
      <file type="M">docs.ops.config.md</file>
      <file type="M">docs.ops.cli.md</file>
      <file type="M">docs.monitoring.rest.api.md</file>
      <file type="M">docs.monitoring.logging.md</file>
      <file type="M">docs.monitoring.historyserver.md</file>
      <file type="M">docs.monitoring.debugging.event.time.md</file>
      <file type="M">docs.monitoring.debugging.classloading.md</file>
      <file type="M">docs.monitoring.checkpoint.monitoring.md</file>
      <file type="M">docs.monitoring.back.pressure.md</file>
      <file type="M">docs.monitoring.application.profiling.md</file>
      <file type="M">docs.internals.task.lifecycle.md</file>
      <file type="M">docs.internals.stream.checkpointing.md</file>
      <file type="M">docs.internals.job.scheduling.md</file>
      <file type="M">docs.internals.ide.setup.md</file>
      <file type="M">docs.internals.filesystems.md</file>
      <file type="M">docs.internals.components.md</file>
      <file type="M">docs.dev.types.serialization.md</file>
      <file type="M">docs.dev.batch.connectors.md</file>
      <file type="M">docs.dev.batch.dataset.transformations.md</file>
      <file type="M">docs.dev.batch.examples.md</file>
      <file type="M">docs.dev.batch.hadoop.compatibility.md</file>
      <file type="M">docs.dev.batch.iterations.md</file>
      <file type="M">docs.dev.batch.zip.elements.guide.md</file>
      <file type="M">docs.dev.best.practices.md</file>
      <file type="M">docs.dev.cluster.execution.md</file>
      <file type="M">docs.dev.connectors.cassandra.md</file>
      <file type="M">docs.dev.connectors.elasticsearch.md</file>
      <file type="M">docs.dev.connectors.filesystem.sink.md</file>
      <file type="M">docs.dev.connectors.kafka.md</file>
      <file type="M">docs.dev.connectors.kinesis.md</file>
      <file type="M">docs.dev.connectors.nifi.md</file>
      <file type="M">docs.dev.connectors.rabbitmq.md</file>
      <file type="M">docs.dev.connectors.twitter.md</file>
      <file type="M">docs.dev.custom.serializers.md</file>
      <file type="M">docs.dev.event.time.md</file>
      <file type="M">docs.dev.event.timestamps.watermarks.md</file>
      <file type="M">docs.dev.event.timestamp.extractors.md</file>
      <file type="M">docs.dev.java8.md</file>
      <file type="M">docs.dev.libs.cep.md</file>
      <file type="M">docs.dev.libs.gelly.bipartite.graph.md</file>
      <file type="M">docs.dev.libs.ml.als.md</file>
      <file type="M">docs.dev.libs.ml.contribution.guide.md</file>
      <file type="M">docs.dev.libs.ml.cross.validation.md</file>
      <file type="M">docs.dev.libs.ml.distance.metrics.md</file>
      <file type="M">docs.dev.libs.ml.knn.md</file>
      <file type="M">docs.dev.libs.ml.min.max.scaler.md</file>
      <file type="M">docs.dev.libs.ml.multiple.linear.regression.md</file>
      <file type="M">docs.dev.libs.ml.optimization.md</file>
      <file type="M">docs.dev.libs.ml.pipelines.md</file>
      <file type="M">docs.dev.libs.ml.polynomial.features.md</file>
      <file type="M">docs.dev.libs.ml.quickstart.md</file>
      <file type="M">docs.dev.libs.ml.sos.md</file>
      <file type="M">docs.dev.libs.ml.standard.scaler.md</file>
      <file type="M">docs.dev.libs.ml.svm.md</file>
      <file type="M">docs.dev.libs.storm.compatibility.md</file>
      <file type="M">docs.dev.linking.md</file>
      <file type="M">docs.dev.local.execution.md</file>
      <file type="M">docs.dev.migration.md</file>
      <file type="M">docs.dev.scala.api.extensions.md</file>
      <file type="M">docs.dev.scala.shell.md</file>
      <file type="M">docs.dev.stream.operators.asyncio.md</file>
      <file type="M">docs.dev.stream.operators.windows.md</file>
      <file type="M">docs.dev.stream.side.output.md</file>
      <file type="M">docs.dev.stream.state.custom.serialization.md</file>
      <file type="M">docs.dev.stream.state.queryable.state.md</file>
      <file type="M">docs.dev.stream.state.state.md</file>
      <file type="M">docs.dev.stream.state.state.backends.md</file>
    </fixedFiles>
  </bug>
  <bug id="7755" opendate="2017-10-2 00:00:00" fixdate="2017-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Null values are not correctly handled by batch inner and outer joins</summary>
      <description>Join predicates of batch joins are not correctly evaluated according to three-value logic.This affects inner as well as outer joins.The problem is that some equality predicates are only evaluated by the internal join algorithms of Flink which are based on TypeComparator. The field TypeComparator for Row are implemented such that null == null results in TRUE to ensure correct ordering and grouping. However, three-value logic requires that null == null results to UNKNOWN (or null). The code generator implements this logic correctly, but for equality predicates, no code is generated.For outer joins, the problem is a bit tricker because these do not support code-generated predicates yet (see FLINK-5520). FLINK-5498 proposes a solution for this issue.We also need to extend several of the existing tests and add null values to ensure that the join logic is correctly implemented.</description>
      <version>1.3.2,1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.validation.JoinValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.validation.JoinValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.FunctionCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
    </fixedFiles>
  </bug>
  <bug id="7765" opendate="2017-10-5 00:00:00" fixdate="2017-1-5 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Enable dependency convergence</summary>
      <description>For motivation check https://issues.apache.org/jira/browse/FLINK-7739SubTasks of this task depends on one another - to enable convergence in `flink-runtime` it has to be enabled for `flink-shaded-hadoop` first.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-libraries.pom.xml</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">flink-fs-tests.pom.xml</file>
      <file type="M">flink-filesystems.pom.xml</file>
      <file type="M">flink-examples.pom.xml</file>
      <file type="M">flink-connectors.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7766" opendate="2017-10-5 00:00:00" fixdate="2017-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove obsolete reflection for hflush on HDFS</summary>
      <description>This code originally existed for compatibility with Hadoop 1.Since Hadoop 1 support is dropped, this is no longer necessary.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-filesystem.src.main.java.org.apache.flink.streaming.connectors.fs.StreamWriterBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="7767" opendate="2017-10-5 00:00:00" fixdate="2017-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid loading Hadoop conf dynamically at runtime</summary>
      <description>The bucketing sink dynamically loads the Hadoop configuration in various places.The result of that configuration is not always predictable, as it tries to automagically discover the Hadoop config files.A better approach is to rely on the Flink configuration to find the Hadoop configuration, or to directly use the Hadoop configuration used by the Hadoop file systems.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.main.java.org.apache.flink.streaming.connectors.fs.SequenceFileWriter.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.main.java.org.apache.flink.streaming.connectors.fs.RollingSink.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.main.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="7768" opendate="2017-10-5 00:00:00" fixdate="2017-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Load File Systems via Java Service abstraction</summary>
      <description>We should change the discovery mechanism of file from static class name configurations to a service mechanism (META-INF/services). With this change, users can add new filesystem implementations and make them available by simply adding them to the class path. As part of that, factoring HDFS and MapR FS implementations into separate modules helps with a better and more fine grained dependency management, needing less explicit reflection logic.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.fs.hdfs.HadoopDataInputStreamTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.HadoopUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.fs.maprfs.MapRFileSystem.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.fs.hdfs.HadoopFsFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.fs.hdfs.HadoopFileStatus.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.fs.hdfs.HadoopDataOutputStream.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.fs.hdfs.HadoopDataInputStream.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.fs.hdfs.HadoopBlockLocation.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.testutils.TestFileSystem.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.io.DelimitedInputFormatSamplingTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.FileSystemFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.FileSystem.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.factories.UnsupportedSchemeFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.factories.MapRFsFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.factories.LocalFileSystemFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.factories.HadoopFileSystemFactoryLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="7844" opendate="2017-10-15 00:00:00" fixdate="2017-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fine Grained Recovery triggers checkpoint timeout failure</summary>
      <description>Context: We are using "individual" failover (fine-grained) recovery strategy for our embarrassingly parallel router use case. The topic has over 2000 partitions, and parallelism is set to ~180 that dispatched to over 20 task managers with around 180 slots.Observations:We've noticed after one task manager termination, even though the individual recovery happens correctly, that the workload was re-dispatched to a new available task manager instance. However, the checkpoint would take 10 mins to eventually timeout, causing all other task managers not able to commit checkpoints. In a worst-case scenario, if job got restarted for other reasons (i.e. job manager termination), that would cause more messages to be re-processed/duplicates compared to the job without fine-grained recovery enabled.I am suspecting that uber checkpoint was waiting for a previous checkpoint that initiated by the old task manager and thus taking a long time to time out.Two questions:1. Is there a configuration that controls this checkpoint timeout?2. Is there any reason that when Job Manager realizes that Task Manager is gone and workload is redispatched, it still need to wait for the checkpoint initiated by the old task manager?Checkpoint screenshot in attachments.</description>
      <version>1.3.2,1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.IndividualRestartsConcurrencyTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="7853" opendate="2017-10-17 00:00:00" fixdate="2017-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reject table function outer joins with predicates in Table API</summary>
      <description>Due to CALCITE-2004, the table function outer joins can not be normally executed. We should cover it up by rejecting join predicates temporarily, until the issue is fixed in Calcite.</description>
      <version>None</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.CorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.CorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.validation.CorrelateValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.CorrelateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.CorrelateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="7854" opendate="2017-10-17 00:00:00" fixdate="2017-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reject lateral table outer joins with predicates in SQL</summary>
      <description>Due to CALCITE-2004, lateral table outer joins can not be normally executed. We should cover it up by rejecting join predicates temporarily, until the issue is fixed in Calcite.</description>
      <version>None</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.CorrelateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.CorrelateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkCalciteSqlValidator.scala</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug id="7866" opendate="2017-10-18 00:00:00" fixdate="2017-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Weigh list of preferred locations for scheduling</summary>
      <description>Sihua Zhou proposed to not only use the list of preferred locations to decide where to schedule a task, but to also weigh the list according to how often a location appeared and then select the location based on the weight. That way, we would obtain better locality in some cases.Example:Preferred locations list: &amp;#91;location1, location2, location2&amp;#93;Weighted preferred locations list &amp;#91;(location2 , 2), (location1, 1)&amp;#93;</description>
      <version>1.3.2,1.4.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.SlotProfileTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.SlotProfile.java</file>
    </fixedFiles>
  </bug>
  <bug id="795" opendate="2014-6-9 00:00:00" fixdate="2014-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Possibly extend the cost model of the optimizer</summary>
      <description>I have started the task to integrate the AbstractCachedBuildSideMatchDriver into the optimizer. The driver caches one side of the join and thereby can accellerate iterations if there are joins with static (non-changing) datasets inside the iteration.The current way of calculating the cost of operators inside of iterations is basically to multiply them by the number of iterations. I would like to propose to extend this to have one static part of costs, that is counted only once for all iterations, and one dynamic part that is multiplied by the number of iterations.In my opinion that would be the cleanest way to intergrate the cached match, by assigning it a higher starting cost then the regular match and a cheaper dynamic part.One other approach would be to always use the cached match inside of iterations. For that I would probably have to add a new RequestedLocalProperty that tells the optimizer if the operator is used inside of a iteration.A simple hacked solution could also be to simply exchange all suitable regular matches inside of an iteration by the cached alternative.What do you think is the best approach?---------------- Imported from GitHub ----------------Url: https://github.com/stratosphere/stratosphere/issues/795Created by: markus-hLabels: Created at: Mon May 12 18:51:51 CEST 2014State: open</description>
      <version>None</version>
      <fixedVersion>0.6-incubating</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">stratosphere-compiler.src.test.java.eu.stratosphere.pact.compiler.UnionReplacementTest.java</file>
      <file type="M">stratosphere-compiler.src.test.java.eu.stratosphere.pact.compiler.CompilerTestBase.java</file>
      <file type="M">stratosphere-compiler.src.main.java.eu.stratosphere.compiler.dag.PactConnection.java</file>
      <file type="M">stratosphere-compiler.src.main.java.eu.stratosphere.compiler.dag.OptimizerNode.java</file>
    </fixedFiles>
  </bug>
  <bug id="7968" opendate="2017-11-2 00:00:00" fixdate="2017-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deduplicate serializer classes between runtime and queryable state</summary>
      <description>Some serializer classes where duplicated into flink-queryable-state to avoid a dependency on flink-runtime.The proper solution here is to move the classes to the shared flink-core project, because these classes are actually useful in a series of API utilities and they do not have any dependency on other flink classes at all.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializerTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.util.serialization.TypeInformationSerializationSchema.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.SerializedCheckpointData.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.DataInputOutputSerializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.DataInputDeserializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.serialization.types.LargeObjectType.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.serialization.LargeRecordsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.types.Util.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.types.UnsignedShortType.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.types.UnsignedByteType.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.types.ShortType.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.types.SerializationTestTypeFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.types.SerializationTestType.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.types.LongType.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.types.IntType.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.types.FloatType.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.types.DoubleType.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.types.CharType.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.types.ByteType.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.types.ByteSubArrayType.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.types.ByteArrayType.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.types.BooleanType.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.types.AsciiStringType.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.PagedViewsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.CheckpointBarrierTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.DataOutputSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.DataInputDeserializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.dump.MetricDumpSerialization.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.EventSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.AdaptiveSpanningRecordDeserializer.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.client.state.serialization.KvStateSerializer.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.client.state.serialization.DataOutputSerializer.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.client.state.serialization.DataInputDeserializer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.TypeInformationKeyValueSerializationSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="7973" opendate="2017-11-3 00:00:00" fixdate="2017-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix service shading relocation for S3 file systems</summary>
      <description>The shade plugin relocates services incorrectly currently, applying relocation patterns multiple times.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test-infra.end-to-end-test.common.sh</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.README.md</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.README.md</file>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8099" opendate="2017-11-17 00:00:00" fixdate="2017-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce default restart delay to 1 second</summary>
      <description>Currently, when a job fails Flink will wait for 10 seconds until restarting the job. Even zero delay is a reasonable setting but will result in "flooding" the logs and quickly increasing the restart counter because at zero delay you will always see failures when no standby resources are available.Reducing this to 1 second should make for a nicer out-of-box experience and not flood too much.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.restart.FixedDelayRestartStrategy.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="8199" opendate="2017-12-5 00:00:00" fixdate="2017-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Annotation for Elasticsearch connector</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.IndexRequestBuilderWrapperFunction.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.Elasticsearch1ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.Elasticsearch5ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.OldNewRequestIndexerBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.OldNewElasticsearchSinkFunctionBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.Elasticsearch2ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.util.RetryRejectedExecutionFailureHandler.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.util.NoOpFailureHandler.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.util.ElasticsearchUtils.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkFunction.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.BulkProcessorIndexer.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ActionRequestFailureHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="820" opendate="2014-6-9 00:00:00" fixdate="2014-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support for unconnected data flows</summary>
      <description>Currently the compiler fails when given a plan with separate data flows.```javathrow new CompilerException("The given Pact program contains multiple disconnected data flows.");```However, I do not see a good conceptual reason to not support such data flows. Are there any technical issues?To elaborate a bit on the issue: I have a higher level operator that may or may not produce separated data flows. In a current workaround I have to pipe the union of all data flows to a dummy sink to avoid such an exception.---------------- Imported from GitHub ----------------Url: https://github.com/stratosphere/stratosphere/issues/820Created by: AHeiseLabels: Created at: Thu May 15 16:51:46 CEST 2014State: open</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.BranchingPlansCompilerTest.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.SinkJoiner.java</file>
    </fixedFiles>
  </bug>
  <bug id="8201" opendate="2017-12-5 00:00:00" fixdate="2017-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>YarnResourceManagerTest causes license checking failure</summary>
      <description>YarnResourceManagerTest generates a temporary taskmanager config file in flink-yarn module root folder and never clear it, which makes license checking fail when we run mvn clean verify multiple times in the same source folder.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
    </fixedFiles>
  </bug>
  <bug id="8278" opendate="2017-12-18 00:00:00" fixdate="2017-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala examples in Metric documentation do not compile</summary>
      <description>The Scala examples in the Metrics documentation do not compile.The line @transient private var counter: Counterneeds to be extended to@transient private var counter: Counter = _</description>
      <version>1.3.2,1.4.0,1.5.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.metrics.md</file>
      <file type="M">docs.dev.stream.state.state.md</file>
    </fixedFiles>
  </bug>
  <bug id="8479" opendate="2018-1-22 00:00:00" fixdate="2018-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement time-bounded inner join of streams as a TwoInputStreamOperator</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.TestHarnessUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="8487" opendate="2018-1-23 00:00:00" fixdate="2018-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>State loss after multiple restart attempts</summary>
      <description>A user reported this issue on the user@f.a.o mailing list and analyzed the situation.Scenario: A program that reads from Kafka and computes counts in a keyed 15 minute tumbling window. StateBackend is RocksDB and checkpointing is enabled.keyBy(0) .timeWindow(Time.of(window_size, TimeUnit.MINUTES)) .allowedLateness(Time.of(late_by, TimeUnit.SECONDS)) .reduce(new ReduceFunction(), new WindowFunction()) At some point HDFS went into a safe mode due to NameNode issues The following exception was thrownorg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby. Visit https://s.apache.org/sbnn-error .................. at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.mkdirs(HadoopFileSystem.java:453) at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.mkdirs(SafetyNetWrapperFileSystem.java:111) at org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory.createBasePath(FsCheckpointStreamFactory.java:132) The pipeline came back after a few restarts and checkpoint failures, after the HDFS issues were resolved. It was evident that operator state was lost. Either it was the Kafka consumer that kept on advancing it's offset between a start and the next checkpoint failure (a minute's worth) or the the operator that had partial aggregates was lost.The user did some in-depth analysis (see mail thread) and might have (according to aljoscha) identified the problem.stefanrichter83@gmail.com, can you have a look at this issue and check if it is relevant?</description>
      <version>1.3.2</version>
      <fixedVersion>1.3.3,1.4.3,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="8847" opendate="2018-3-3 00:00:00" fixdate="2018-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modules containing package-info.java are always recompiled</summary>
      <description>All modules that contain a package-info.java file (that do not contain annotations which applies to all instances in Flink) will always be recompiled by the maven-compiler-plugin.To detect modified files the compiler compares timestamps of the source and .class file. In the case of package-info.java no .class file is created if it doesn't contain annotations, which the compiler interprets as a missing .class file.We can add -Xpkginfo:always to the compiler configuration to force the generation of these files to prevent this from happening.</description>
      <version>1.3.2,1.4.1,1.5.0</version>
      <fixedVersion>1.4.3,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8871" opendate="2018-3-5 00:00:00" fixdate="2018-5-5 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Checkpoint cancellation is not propagated to stop checkpointing threads on the task manager</summary>
      <description>Flink currently lacks any form of feedback mechanism from the job manager / checkpoint coordinator to the tasks when it comes to failing a checkpoint. This means that running snapshots on the tasks are also not stopped even if their owning checkpoint is already cancelled. Two examples for cases where this applies are checkpoint timeouts and local checkpoint failures on a task together with a configuration that does not fail tasks on checkpoint failure.Notice that thoserunning snapshots dono longer account for the maximum number of parallel checkpoints, because their owning checkpoint is considered as cancelled.Not stopping the task's snapshot thread can lead to a problematicsituation wherethe next checkpoints already started, while the abandoned checkpoint thread from a previous checkpoint is still lingering around running. This scenario can potentially cascade: many parallel checkpoints will slow down checkpointing and make timeouts even more likely.A possible solution is introducing a cancelCheckpointmethod as counterpart to the triggerCheckpointmethod in the task manager gateway, which is invoked by the checkpoint coordinator as part ofcancelling the checkpoint.</description>
      <version>1.3.2,1.4.1,1.5.0,1.6.0,1.7.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.utils.SimpleAckingTaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.RpcTaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.slots.TaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureReason.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.api.datastream.ReinterpretDataStreamAsKeyedStreamITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.classloading.jar.CheckpointingCustomKvStateProgram.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.classloading.jar.CheckpointedStreamingProgram.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.ZooKeeperHighAvailabilityITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.FailingSource.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.CancellingIntegerSource.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.AccumulatingIntegerSink.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StreamCheckpointNotifierITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StateCheckpointedITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.KeyedStateCheckpointingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.CoStreamCheckpointingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.ContinuousFileProcessingCheckpointITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.ExceptionallyDoneFuture.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.main.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.java</file>
      <file type="M">flink-connectors.flink-connector-gcp-pubsub.src.main.java.org.apache.flink.streaming.connectors.gcp.pubsub.common.AcknowledgeOnCheckpoint.java</file>
      <file type="M">flink-connectors.flink-connector-gcp-pubsub.src.main.java.org.apache.flink.streaming.connectors.gcp.pubsub.PubSubSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.IntegerSource.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.FailureMapper.java</file>
      <file type="M">flink-end-to-end-tests.flink-heavy-deployment-stress-test.src.main.java.org.apache.flink.deployment.HeavyDeploymentStressTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-local-recovery-and-allocation-test.src.main.java.org.apache.flink.streaming.tests.StickyAllocationAndLocalRecoveryTestJob.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointTaskStateManager.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.output.SnapshotUtilsTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.AbstractQueryableStateTestBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.CheckpointListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.NoOpTaskLocalStateStoreImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskLocalStateStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskLocalStateStoreImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskStateManagerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskLocalStateStoreImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestTaskLocalStateStore.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestTaskStateManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksFullSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.MessageAcknowledgingSourceBase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperatorV2.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectSinkFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperatorLifecycleTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.LocalStateForwardingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MockSubtaskCheckpointCoordinatorBuilder.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SynchronousCheckpointITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.FailingCollectionSource.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.scala</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.FiniteTestSource.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointIT.java</file>
    </fixedFiles>
  </bug>
  <bug id="8897" opendate="2018-3-8 00:00:00" fixdate="2018-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rowtime materialization causes "mismatched type" AssertionError</summary>
      <description>As raised in this thread, the query created by the following code will throw a calcite "mismatch type" (Timestamp(3) and TimeIndicator) exception.String sql1 = "select id, eventTs as t1, count(*) over (partition by id order by eventTs rows between 100 preceding and current row) as cnt1 from myTable1";String sql2 = "select distinct id as r_id, eventTs as t2, count(*) over (partition by id order by eventTs rows between 50 preceding and current row) as cnt2 from myTable2";Table left = tableEnv.sqlQuery(sql1);Table right = tableEnv.sqlQuery(sql2);left.join(right).where("id === r_id &amp;&amp; t1 === t2").select("id, t1").writeToSink(...)The logical plan is as follows.LogicalProject(id=[$0], t1=[$1]) LogicalFilter(condition=[AND(=($0, $3), =($1, $4))]) LogicalJoin(condition=[true], joinType=[inner]) LogicalAggregate(group=[{0, 1, 2}]) LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])]) LogicalProject(id=[$0], eventTs=[$3]) LogicalTableScan(table=[[_DataStreamTable_0]]) LogicalAggregate(group=[{0, 1, 2}]) LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])]) LogicalProject(id=[$0], eventTs=[$3]) LogicalTableScan(table=[[_DataStreamTable_0]])That is because the the rowtime field after an aggregation will be materialized while the RexInputRef type for the filter's operands (t1 === t2) is still TimeIndicator. We should make them unified.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.WindowJoinUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamWindowJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
    </fixedFiles>
  </bug>
  <bug id="9074" opendate="2018-3-26 00:00:00" fixdate="2018-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Resume from retained checkpoints</summary>
      <description>This tracks the implementation of an end-to-end test that resumes from a retained checkpoint.It should be possible to extend / re-use the "Resume from Savepoint" (FLINK-8975) tests for this.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestJobFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="9499" opendate="2018-6-1 00:00:00" fixdate="2018-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow REST API for running a job to provide job configuration as body of POST request</summary>
      <description>Based on thisdocumentation,the REST API provides a way to submit a request for running a Flink job. The POST request must include the job configuration information as query parameters using thedocumented parameter names ("program-args", "entry-class", "parallelism", etc.)Depending on the job parameters, the full URL for the POST request can reacha size that is over themaximum size (currently at 4096 bytes) of what is allowed by the configuration of Netty. To overcome this, it would be useful to allow users to provide the job configuration not only as query parameters but also as POST parameters.For the most part, it is the "program-args" parameter that can make the URL grow in size based on the needs of the developer and the job. All other attributes should be pretty constant.</description>
      <version>1.3.2</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.hs.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.submit.submit.svc.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.submit.submit.ctrl.coffee</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarSubmissionITCase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.pom.xml</file>
      <file type="M">docs..includes.generated.rest.dispatcher.html</file>
    </fixedFiles>
  </bug>
</bugrepository>
