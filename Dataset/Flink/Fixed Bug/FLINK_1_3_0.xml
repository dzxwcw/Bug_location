<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="4769" opendate="2016-10-7 00:00:00" fixdate="2016-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate Metrics configuration options</summary>
      <description></description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.MetricRegistryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.TaskMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.TaskManagerJobGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.TaskManagerGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.MetricGroupRegistrationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.JobManagerJobGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.JobManagerGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.AbstractMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.scope.ScopeFormats.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.scope.ScopeFormat.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricRegistryConfiguration.java</file>
      <file type="M">flink-metrics.flink-metrics-statsd.src.test.java.org.apache.flink.metrics.statsd.StatsDReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.runtime.jobmanager.JMXJobManagerMetricTest.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.metrics.jmx.JMXReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.test.java.org.apache.flink.dropwizard.ScheduledDropwizardReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.test.java.org.apache.flink.dropwizard.metrics.DropwizardFlinkHistogramWrapperTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="4770" opendate="2016-10-7 00:00:00" fixdate="2016-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate core options</summary>
      <description>The core options contain everything that is specific to job cross TaskManager / JobManager</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnClusterDescriptorTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNHighAvailabilityITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.classloading.ClassLoaderITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.SavepointMigrationTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RescalingITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.ZooKeeperTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.BootstrapToolsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.HDFSTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.RollingSinkSecuredITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="4850" opendate="2016-10-18 00:00:00" fixdate="2016-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FlinkML - SVM predict Operation for Vector and not LaveledVector</summary>
      <description>It seems that evaluate operation is defined for Vector and not LabeledVector.It impacts QuickStart guide for FlinkML when using SVM.We need to update the documentation as follows:val astroTest:DataSet&amp;#91;(Vector,Double)&amp;#93; = MLUtils .readLibSVM(env, "src/main/resources/svmguide1.t") .map(l =&gt; (l.vector, l.label))val predictionPairs = svm.evaluate(astroTest)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.libs.ml.quickstart.md</file>
    </fixedFiles>
  </bug>
  <bug id="5118" opendate="2016-11-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Inconsistent records sent/received metrics</summary>
      <description>In 1.2-SNAPSHOT running a large scale job you see that the counts for send/received records are inconsistent, e.g. in a simple word count job we see more received records/bytes than we see sent. This is a regression from 1.1 where everything works as expected.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.RecordWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="5119" opendate="2016-11-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Last taskmanager heartbeat not showing in web frontend</summary>
      <description>The web frontend does not list anything for the last heartbeat in the web frontend.</description>
      <version>1.1.3,1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.index.coffee</file>
    </fixedFiles>
  </bug>
  <bug id="5150" opendate="2016-11-23 00:00:00" fixdate="2016-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebUI metric-related resource leak</summary>
      <description>The WebUI maintains a list of selected metrics for all jobs and vertices. When a metric is selected in the metric screen it is added to this list, and removed if it is unselected.The contents of this list are stored in the browser's localStorage. This allows a user to setup a metric screen, move to another page, and return to the original screen completely intact.However, if the metrics are never unselected by the user they will remain in this list. They will also still be in this list if the WebUI can't even display the corresponding job page anymore, if for example the history size limit was exceeded. They will even survive a browser restart, since they are not stored in a session-based storage.Furthermore, the WebUI still tries to update these metricsd, adding additional overhead to the WebBackend and potentially network.In other words, if you ever checked out metrics tab for some job, chances are that the next time you start the WebInterface it will still try to update the metrics for it.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobs.metrics.svc.coffee</file>
    </fixedFiles>
  </bug>
  <bug id="5365" opendate="2016-12-19 00:00:00" fixdate="2016-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mesos AppMaster/TaskManager should obey sigterm</summary>
      <description>The AppMaster and TaskManager are ignoring the sigterm sent by Marathon/Mesos. The reason is simply that the shell scripts used to start them don't pass the signal to java.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-taskmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-appmaster.sh</file>
    </fixedFiles>
  </bug>
  <bug id="5366" opendate="2016-12-19 00:00:00" fixdate="2016-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add end-to-end tests for Savepoint Backwards Compatibility</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.UserFunctionStateJob.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.SavepointUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="5378" opendate="2016-12-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Scopt version to 3.5.0</summary>
      <description>Is it possible to increase the Scopt version to 3.5.0? This version does also support comma-separated lists of arguments.I'm using this in my project and indeed I can use Maven to use the latest Scopt version. But, once I want to deploy an uber-Jar to Flink, it obviously fails because of two different versions of Scopt in the classpath - one in my uber-Jar (Scopt 3.5.0) and the one shipped with Flink distribution (Scopt 3.2.0).I know that there is another open issue regarding refactoring the CLI parser (FLINK-1347), but as far as I can see there is no progress yet.</description>
      <version>1.1.3,1.2.0,1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5380" opendate="2016-12-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Number of outgoing records not reported in web interface</summary>
      <description>The web frontend does not report any outgoing records in the web frontend.The amount of data in MB is reported correctly.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="5381" opendate="2016-12-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scrolling in some web interface pages doesn&amp;#39;t work (taskmanager details, jobmanager config)</summary>
      <description>It seems that scrolling in the web interface doesn't work anymore on some pages in the 1.2 release branch.Example pages: When you click the "JobManager" tab The TaskManager logs page</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.css.index.css</file>
      <file type="M">flink-runtime-web.web-dashboard.app.styles.index.styl</file>
    </fixedFiles>
  </bug>
  <bug id="5414" opendate="2017-1-5 00:00:00" fixdate="2017-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump up Calcite version to 1.11</summary>
      <description>The upcoming Calcite release 1.11 has a lot of stability fixes and new features. We should update it for the Table API.E.g. we can hopefully merge FLINK-4864</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.FieldProjectionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.ProjectionTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.FlinkRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.ScalarSqlFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5417" opendate="2017-1-6 00:00:00" fixdate="2017-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the wrong config file name</summary>
      <description>As the config file name is conf/flink-conf.yaml, the usage "conf/flink-config.yaml" in document is wrong and easy to confuse user. We should correct them.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.yarn.setup.md</file>
      <file type="M">docs.fig.slots.parallelism.svg</file>
    </fixedFiles>
  </bug>
  <bug id="5434" opendate="2017-1-10 00:00:00" fixdate="2017-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unsupported project() transformation from Scala DataStream docs</summary>
      <description>The Scala DataStream does not have a project() transformation, yet the docs include it as a supported operation.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.datastream.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="5447" opendate="2017-1-11 00:00:00" fixdate="2017-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sync documentation of built-in functions for Table API with SQL</summary>
      <description>I will split up the documentation for the built-in functions similar to the SQL structure.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="5452" opendate="2017-1-11 00:00:00" fixdate="2017-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make table unit tests pass under cluster mode</summary>
      <description>Currently if we change the test execution mode to TestExecutionMode.CLUSTER in TableProgramsTestBase, some cases will fail. Need to figure out whether it's the case design problem or there are some bugs.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.SortITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5454" opendate="2017-1-11 00:00:00" fixdate="2017-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Documentation about how to tune Checkpointing for large state</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.rest.api.md</file>
      <file type="M">docs.monitoring.large.state.tuning.md</file>
    </fixedFiles>
  </bug>
  <bug id="5455" opendate="2017-1-11 00:00:00" fixdate="2017-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create documentation how to upgrade jobs and Flink framework versions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.upgrading.md</file>
    </fixedFiles>
  </bug>
  <bug id="5458" opendate="2017-1-11 00:00:00" fixdate="2017-1-11 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Add documentation how to migrate from Flink 1.1. to Flink 1.2</summary>
      <description>Docs should go to docs/dev/migration.md</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="5496" opendate="2017-1-15 00:00:00" fixdate="2017-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClassCastException when using Mesos HA mode</summary>
      <description>When using the Mesos' HA mode, one cannot start the Mesos appmaster, because the following class cast exception occurs:java.lang.ClassCastException: org.apache.flink.shaded.org.apache.curator.framework.imps.CuratorFrameworkImpl cannot be cast to org.apache.flink.mesos.shaded.org.apache.curator.framework.CuratorFramework at org.apache.flink.mesos.util.ZooKeeperUtils.startCuratorFramework(ZooKeeperUtils.java:38) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.createWorkerStore(MesosApplicationMasterRunner.java:510) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.runPrivileged(MesosApplicationMasterRunner.java:320) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner$1.call(MesosApplicationMasterRunner.java:178) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner$1.call(MesosApplicationMasterRunner.java:175) at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:29) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.run(MesosApplicationMasterRunner.java:175) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.main(MesosApplicationMasterRunner.java:135)It seems as if the flink-mesos module relocates the curator dependency in another namespace than flink-runtime. Not sure why this is done.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-mesos.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5497" opendate="2017-1-15 00:00:00" fixdate="2017-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove duplicated tests</summary>
      <description>Now we have test which run the same code 4 times, every run 17+ seconds.Need do small refactoring and remove duplicated code.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReusingReOpenableHashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.NonReusingReOpenableHashTableITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="5508" opendate="2017-1-16 00:00:00" fixdate="2017-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Mesos dynamic class loading</summary>
      <description>Mesos uses dynamic class loading in order to load the ZooKeeperStateHandleStore and the CuratorFramework class. This can be replaced by a compile time dependency.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosFlinkResourceManagerTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.store.ZooKeeperMesosWorkerStore.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="5512" opendate="2017-1-16 00:00:00" fixdate="2017-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RabbitMQ documentation should inform that exactly-once holds for RMQSource only when parallelism is 1</summary>
      <description>See here for the reasoning: FLINK-2624. We should add an informative warning about this limitation in the docs.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.rabbitmq.md</file>
    </fixedFiles>
  </bug>
  <bug id="5517" opendate="2017-1-17 00:00:00" fixdate="2017-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade hbase version to 1.3.0</summary>
      <description>In the thread 'Help using HBase with Flink 1.1.4', Giuliano reported seeing:java.lang.IllegalAccessError: tried to access method com.google.common.base.Stopwatch.&lt;init&gt;()V from class org.apache.hadoop.hbase.zookeeper.MetaTableLocatorThe above has been solved by HBASE-14963hbase 1.3.0 is being released.We should upgrade hbase dependency to 1.3.0</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-hbase.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5524" opendate="2017-1-17 00:00:00" fixdate="2017-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support early out for code generated conjunctive conditions</summary>
      <description>Currently, all nested conditions for a conjunctive predicate are evaluated before the conjunction is checked.A condition like (v1 == v2) &amp;&amp; (v3 &lt; 5) would be compiled intoboolean res1;if (v1 == v2) { res1 = true;} else { res1 = false;}boolean res2;if (v3 &lt; 5) { res2 = true;} else { res2 = false;}boolean res3;if (res1 &amp;&amp; res2) { res3 = true;} else { res3 = false;}if (res3) { // emit something}It would be better to leave the generated code as early as possible, e.g., with a return instead of res1 = false. The code generator needs a bit of context information for that.</description>
      <version>1.1.4,1.2.0,1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.UserDefinedScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5531" opendate="2017-1-17 00:00:00" fixdate="2017-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SSl code block formatting is broken</summary>
      <description>Most code blocks on the ssl page aren't rendered properly and are simply shown as text.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.security-ssl.md</file>
    </fixedFiles>
  </bug>
  <bug id="5575" opendate="2017-1-19 00:00:00" fixdate="2017-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>in old releases, warn users and guide them to the latest stable docs</summary>
      <description>Old versions of Flink (especially version 0.8) are being frequently studied, downloaded, and used by new users (because google leads them there). I propose to guide folks to the latest stable release by adding a link on every documentation page in the old docs that links to the home page of the latest stable docs. The redirect lives at flink.apache.org/q/stable-docs.html, and will need to be modified with each major release (e.g. when 1.2 is released).This problem affects all releases before 1.1, but the stats show that 0.8, 0.9, 0.10, and 1.0 are the most important to deal with.</description>
      <version>None</version>
      <fixedVersion>1.0.4,1.1.5,1.2.0,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..layouts.base.html</file>
      <file type="M">docs..config.yml</file>
    </fixedFiles>
  </bug>
  <bug id="5624" opendate="2017-1-24 00:00:00" fixdate="2017-2-24 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support tumbling window on streaming tables in the SQL API</summary>
      <description>This is a follow up of FLINK-4691.FLINK-4691 adds supports for group-windows for streaming tables. This jira proposes to expose the functionality in the SQL layer via the GROUP BY clauses, as described in http://calcite.apache.org/docs/stream.html#tumbling-windows.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5625" opendate="2017-1-24 00:00:00" fixdate="2017-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Let Date format for timestamp-based start position in Kinesis consumer be configurable.</summary>
      <description>Currently, the Kinesis consumer's Date format for timestamp-based start positions is fixed. It'll be nice to make this format configurable.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.ConsumerConfigConstants.java</file>
      <file type="M">docs.dev.connectors.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug id="5630" opendate="2017-1-24 00:00:00" fixdate="2017-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Followups to AggregationFunction</summary>
      <description>Various followup issues to the aggregation function, like Allowing different input/output types for the cases where an additional window apply function is specified Adding the aggregate() methods to the Scala API Adding the window translation tests</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.WindowTranslationTest.scala</file>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.AllWindowTranslationTest.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.WindowedStream.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.AllWindowedStream.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowTranslationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AllWindowTranslationTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.AggregateApplyWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.windowing.AggregateApplyAllWindowFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.WindowedStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.AllWindowedStream.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.AggregateFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="5644" opendate="2017-1-25 00:00:00" fixdate="2017-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Task#lastCheckpointSize metric broken</summary>
      <description>The lastCheckpointSIze metric was broken when we introduced the key-groups. I couldn't find an easy way to fix the metric, as such i propose to remove it.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="5652" opendate="2017-1-26 00:00:00" fixdate="2017-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Memory leak in AsyncDataStream</summary>
      <description>When async operation timeout is &gt; 0, the number of StreamRecordQueueEntry instances keeps growing.It can be easily reproduced with the following code: val src: DataStream[Int] = env.fromCollection((1 to Int.MaxValue).iterator) val asyncFunction = new AsyncFunction[Int, Int] with Serializable { override def asyncInvoke(input: Int, collector: AsyncCollector[Int]): Unit = { collector.collect(List(input)) } } AsyncDataStream.unorderedWait(src, asyncFunction, 1, TimeUnit.MINUTES, 1).print()</description>
      <version>1.3.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="5653" opendate="2017-1-26 00:00:00" fixdate="2017-3-26 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add processing time OVER ROWS BETWEEN x PRECEDING aggregation to SQL</summary>
      <description>The goal of this issue is to add support for OVER ROWS aggregations on processing time streams to the SQL interface.Queries similar to the following should be supported:SELECT a, SUM(b) OVER (PARTITION BY c ORDER BY procTime() ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS sumB, MIN(b) OVER (PARTITION BY c ORDER BY procTime() ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS minBFROM myStreamThe following restrictions should initially apply: All OVER clauses in the same SELECT clause must be exactly the same. The PARTITION BY clause is optional (no partitioning results in single threaded execution). The ORDER BY clause may only have procTime() as parameter. procTime() is a parameterless scalar function that just indicates processing time mode. UNBOUNDED PRECEDING is not supported (see FLINK-5656) FOLLOWING is not supported.The restrictions will be resolved in follow up issues. If we find that some of the restrictions are trivial to address, we can add the functionality in this issue as well.This issue includes: Design of the DataStream operator to compute OVER ROW aggregates Translation from Calcite's RelNode representation (LogicalProject with RexOver expression).</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamOverAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5654" opendate="2017-1-26 00:00:00" fixdate="2017-3-26 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add processing time OVER RANGE BETWEEN x PRECEDING aggregation to SQL</summary>
      <description>The goal of this issue is to add support for OVER RANGE aggregations on processing time streams to the SQL interface.Queries similar to the following should be supported:SELECT a, SUM(b) OVER (PARTITION BY c ORDER BY procTime() RANGE BETWEEN INTERVAL '1' HOUR PRECEDING AND CURRENT ROW) AS sumB, MIN(b) OVER (PARTITION BY c ORDER BY procTime() RANGE BETWEEN INTERVAL '1' HOUR PRECEDING AND CURRENT ROW) AS minBFROM myStreamThe following restrictions should initially apply: All OVER clauses in the same SELECT clause must be exactly the same. The PARTITION BY clause is optional (no partitioning results in single threaded execution). The ORDER BY clause may only have procTime() as parameter. procTime() is a parameterless scalar function that just indicates processing time mode. UNBOUNDED PRECEDING is not supported (see FLINK-5657) FOLLOWING is not supported.The restrictions will be resolved in follow up issues. If we find that some of the restrictions are trivial to address, we can add the functionality in this issue as well.This issue includes: Design of the DataStream operator to compute OVER ROW aggregates Translation from Calcite's RelNode representation (LogicalProject with RexOver expression).</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamOverAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5655" opendate="2017-1-26 00:00:00" fixdate="2017-3-26 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add event time OVER RANGE BETWEEN x PRECEDING aggregation to SQL</summary>
      <description>The goal of this issue is to add support for OVER RANGE aggregations on event time streams to the SQL interface.Queries similar to the following should be supported:SELECT a, SUM(b) OVER (PARTITION BY c ORDER BY rowTime() RANGE BETWEEN INTERVAL '1' HOUR PRECEDING AND CURRENT ROW) AS sumB, MIN(b) OVER (PARTITION BY c ORDER BY rowTime() RANGE BETWEEN INTERVAL '1' HOUR PRECEDING AND CURRENT ROW) AS minBFROM myStreamThe following restrictions should initially apply: All OVER clauses in the same SELECT clause must be exactly the same. The PARTITION BY clause is optional (no partitioning results in single threaded execution). The ORDER BY clause may only have rowTime() as parameter. rowTime() is a parameterless scalar function that just indicates processing time mode. UNBOUNDED PRECEDING is not supported (see FLINK-5658) FOLLOWING is not supported.The restrictions will be resolved in follow up issues. If we find that some of the restrictions are trivial to address, we can add the functionality in this issue as well.This issue includes: Design of the DataStream operator to compute OVER ROW aggregates Translation from Calcite's RelNode representation (LogicalProject with RexOver expression).</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamOverAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5658" opendate="2017-1-26 00:00:00" fixdate="2017-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add event time OVER ROWS BETWEEN UNBOUNDED PRECEDING aggregation to SQL</summary>
      <description>The goal of this issue is to add support for OVER RANGE aggregations on event time streams to the SQL interface.Queries similar to the following should be supported:SELECT a, SUM(b) OVER (PARTITION BY c ORDER BY rowTime() ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS sumB, MIN(b) OVER (PARTITION BY c ORDER BY rowTime() ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS minBFROM myStreamThe following restrictions should initially apply: All OVER clauses in the same SELECT clause must be exactly the same. The PARTITION BY clause is optional (no partitioning results in single threaded execution). The ORDER BY clause may only have rowTime() as parameter. rowTime() is a parameterless scalar function that just indicates processing time mode. bounded PRECEDING is not supported (see FLINK-5655) FOLLOWING is not supported.The restrictions will be resolved in follow up issues. If we find that some of the restrictions are trivial to address, we can add the functionality in this issue as well.An event-time OVER ROWS window will not be able to handle late data, because this would mean in insert a row into a sorted order shift all other computations. This would be too expensive to maintain. Therefore, we will throw an error if a user tries to use an event-time OVER ROWS window with late data handling.This issue includes: Design of the DataStream operator to compute OVER ROW aggregates Translation from Calcite's RelNode representation (LogicalProject with RexOver expression).</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamOverAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5659" opendate="2017-1-26 00:00:00" fixdate="2017-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FileBaseUtils#deleteFileOrDirectory not thread-safe on Windows</summary>
      <description>The FileBaseUtils#deleteFileOrDirectory is not thread-safe on Windows.First you will run into AccessDeniedExceptions since one thread tried to delete a file while another thread was already doing that, for which the file has to be opened.Once you resolve those exceptions (by catching them double checking whether the file still exists), you run into DirectoryNotEmptyExceptions since there is some wacky timing/visibility issue when deleting files concurrently.</description>
      <version>1.2.0,1.3.0,1.4.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.spotbugs-exclude.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.LambdaUtil.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.function.ThrowingConsumer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="5667" opendate="2017-1-26 00:00:00" fixdate="2017-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Possible state data loss when task fails while checkpointing</summary>
      <description>It is possible that Flink loses state data when a Task fails while a checkpoint is being drawn. The scenario is the following:Flink has finished the synchronous checkpointing part and starts the asynchronous part by creating and submitting a AsyncCheckpointRunnable to an Executor. This runnable is also registered at the closeable registry. If the Task now fails before the AsyncCheckpointRunnable has completed, it will be closed due to being registered in the closeable registry. The closing operation will discard all state handles and cancel all runnable state futures. However, it will not stop the runnable from sending an acknowledge message to the CheckpointCoordinator.If this message completes the pending checkpoint, then this checkpoint will be transformed into a CompletedCheckpoint which is faulty (some of the data has already been deleted). Depending on Flink's configuration, this will discard older completed checkpoints and thus we will have state data loss.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="5670" opendate="2017-1-27 00:00:00" fixdate="2017-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Local RocksDB directories not cleaned up</summary>
      <description>After cancelling a job with a RocksDB backend all files are properly cleaned up, but the parent directories still exist and are empty:859546fec3dac36bb9fcc8cbdd4e291e+- StreamFlatMap_3_0+- StreamFlatMap_3_3+- StreamFlatMap_3_4+- StreamFlatMap_3_5+- StreamFlatMap_3_6The number of empty folders varies between runs.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="5672" opendate="2017-1-27 00:00:00" fixdate="2017-5-27 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Job fails with java.lang.IllegalArgumentException: port out of range:-1</summary>
      <description>I started the JobManager with start-local.sh and started another TaskManager with taskmanager.sh start. My job is a Table API job with a orderBy (range partitioning with parallelism 2).The job fails with the following exception:java.lang.IllegalArgumentException: port out of range:-1 at java.net.InetSocketAddress.checkPort(InetSocketAddress.java:143) at java.net.InetSocketAddress.&lt;init&gt;(InetSocketAddress.java:188) at org.apache.flink.runtime.io.network.ConnectionID.&lt;init&gt;(ConnectionID.java:47) at org.apache.flink.runtime.deployment.InputChannelDeploymentDescriptor.fromEdges(InputChannelDeploymentDescriptor.java:124) at org.apache.flink.runtime.executiongraph.ExecutionVertex.createDeploymentDescriptor(ExecutionVertex.java:627) at org.apache.flink.runtime.executiongraph.Execution.deployToSlot(Execution.java:358) at org.apache.flink.runtime.executiongraph.Execution$1.apply(Execution.java:284) at org.apache.flink.runtime.executiongraph.Execution$1.apply(Execution.java:279) at org.apache.flink.runtime.concurrent.impl.FlinkFuture$5.onComplete(FlinkFuture.java:259) at akka.dispatch.OnComplete.internal(Future.scala:248) at akka.dispatch.OnComplete.internal(Future.scala:245) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:175) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:172) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) at org.apache.flink.runtime.concurrent.Executors$DirectExecutor.execute(Executors.java:56) at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:122) at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40) at scala.concurrent.impl.Promise$KeptPromise.onComplete(Promise.scala:333) at org.apache.flink.runtime.concurrent.impl.FlinkFuture.handleAsync(FlinkFuture.java:256) at org.apache.flink.runtime.concurrent.impl.FlinkFuture.handle(FlinkFuture.java:270) at org.apache.flink.runtime.executiongraph.Execution.scheduleForExecution(Execution.java:279) at org.apache.flink.runtime.executiongraph.ExecutionVertex.scheduleForExecution(ExecutionVertex.java:479) at org.apache.flink.runtime.executiongraph.Execution$5.call(Execution.java:525) at org.apache.flink.runtime.executiongraph.Execution$5.call(Execution.java:521) at akka.dispatch.Futures$$anonfun$future$1.apply(Future.scala:95) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.stop-cluster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-cluster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
    </fixedFiles>
  </bug>
  <bug id="5690" opendate="2017-1-31 00:00:00" fixdate="2017-10-31 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>protobuf is not shaded properly</summary>
      <description>Currently distributive contains com/google/protobuf package. Without proper shading client code could fail with:Caused by: java.lang.IllegalAccessError: tried to access method com.google.protobuf.XXXXSteps to reproduce: create job class "com.google.protobuf.TestClass" call com.google.protobuf.TextFormat.escapeText(String) method from this class deploy job to flink cluster (usign web console for example) run job. In logs IllegalAccessError.Issue in package protected method and different classloaders. TestClass loaded by FlinkUserCodeClassLoader, but TextFormat class loaded by sun.misc.Launcher$AppClassLoader</description>
      <version>1.1.4,1.3.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.debugging.classloading.md</file>
      <file type="M">docs.monitoring.best.practices.md</file>
    </fixedFiles>
  </bug>
  <bug id="5698" opendate="2017-2-2 00:00:00" fixdate="2017-3-2 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add NestedFieldsProjectableTableSource interface</summary>
      <description>Add a NestedFieldsProjectableTableSource interface for some TableSource implementation that support nesting projection push-down.The interface could look as followsdef trait NestedFieldsProjectableTableSource { def projectNestedFields(fields: Array[String]): NestedFieldsProjectableTableSource[T]}This interface works together with ProjectableTableSource</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.util.RexProgramExtractorTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.util.RexProgramExtractor.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.common.PushProjectIntoTableSourceScanRuleBase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5701" opendate="2017-2-3 00:00:00" fixdate="2017-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FlinkKafkaProducer should check asyncException on checkpoints</summary>
      <description>Reported in ML: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Fink-KafkaProducer-Data-Loss-td11413.htmlThe problem:The producer holds a pendingRecords value that is incremented on each invoke() and decremented on each callback, used to check if the producer needs to sync on pending callbacks on checkpoints.On each checkpoint, we should only consider the checkpoint succeeded iff after flushing the pendingRecords == 0 and asyncException == null (currently, were only checking pendingRecords).A quick fix for this is to check and rethrow async exceptions in the snapshotState method both before and after flushing and pendingRecords becomes 0.</description>
      <version>None</version>
      <fixedVersion>1.1.5,1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="5702" opendate="2017-2-3 00:00:00" fixdate="2017-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka Producer docs should warn if using setLogFailuresOnly, at-least-once is compromised</summary>
      <description>The documentation for FlinkKafkaProducer does not have any information about the setLogFailuresOnly. It should emphasize that if users choose to only log failures instead of failing the sink, at-least-once can not be guaranteed .</description>
      <version>None</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="5705" opendate="2017-2-3 00:00:00" fixdate="2017-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>webmonitor&amp;#39;s request/response use UTF-8 explicitly</summary>
      <description>QueryStringDecoder and HttpPostRequestDecoder use UTF-8 defined in flink.Response set content-encoding header with utf-8</description>
      <version>None</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.RuntimeMonitorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.HttpRequestHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="5706" opendate="2017-2-3 00:00:00" fixdate="2017-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement Flink&amp;#39;s own S3 filesystem</summary>
      <description>As part of the effort to make Flink completely independent from Hadoop, Flink needs its own S3 filesystem implementation. Currently Flink relies on Hadoop's S3a and S3n file systems.An own S3 file system can be implemented using the AWS SDK. As the basis of the implementation, the Hadoop File System can be used (Apache Licensed, should be okay to reuse some code as long as we do a proper attribution).</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">flink-filesystems.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5709" opendate="2017-2-3 00:00:00" fixdate="2017-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Max Parallelism to Parallel Execution Doc</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.parallel.md</file>
    </fixedFiles>
  </bug>
  <bug id="5710" opendate="2017-2-3 00:00:00" fixdate="2017-2-3 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add ProcTime() function to indicate StreamSQL</summary>
      <description>procTime() is a parameterless scalar function that just indicates processing time mode</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.LogicalWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.TimeModeIndicatorFunctions.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5722" opendate="2017-2-6 00:00:00" fixdate="2017-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement DISTINCT as dedicated operator</summary>
      <description>DISTINCT is currently implemented for batch Table API / SQL as an aggregate which groups on all fields. Grouped aggregates are implemented as GroupReduce with sort-based combiner.This operator can be more efficiently implemented by using ReduceFunction and hinting a HashCombine strategy. The same ReduceFunction can be used for all DISTINCT operations and can be assigned with appropriate forward field annotations.We would need a custom conversion rule which translates distinct aggregations (grouping on all fields and returning all fields) into a custom DataSetRelNode.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.FieldProjectionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.QueryDecorrelationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.DistinctAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5723" opendate="2017-2-6 00:00:00" fixdate="2017-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use "Used" instead of "Initial" to make taskmanager tag more readable</summary>
      <description>Now in JobManager web fronted, the used memory of task managers is presented as "Initial" in table header, which actually means "memory used", from codes.I'd like change it to be more readable, even it is trivial one.</description>
      <version>None</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.taskmanager.taskmanager.metrics.html</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanager.taskmanager.metrics.jade</file>
    </fixedFiles>
  </bug>
  <bug id="5728" opendate="2017-2-7 00:00:00" fixdate="2017-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FlinkKafkaProducer should flush on checkpoint by default</summary>
      <description>As discussed in FLINK-5702, it might be a good idea to let the FlinkKafkaProducer flush on checkpoints by default. Currently, it is disabled by default.It's a very simple change, but we should think about whether or not we want to break user behaviour, or have proper usage migration.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.java</file>
      <file type="M">docs.dev.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="5731" opendate="2017-2-7 00:00:00" fixdate="2017-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split up CI builds</summary>
      <description>Test builds regularly time out because we are hitting the Travis 50 min limit. Previously, we worked around this by splitting up the tests into groups. I think we have to split them further.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="5762" opendate="2017-2-9 00:00:00" fixdate="2017-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Protect initializeState() and open() by the same lock.</summary>
      <description>Currently the initializeState() of all operators in a task is called without the checkpoint lock, and before the open(). This may lead to problematic situations as the following:In the case that we retrieve timers from a checkpoint, e.g. WindowOperator and (future) CEP, if we re-register them in the initializeState(), then if they fire before the open() of the downstream operators is called, we will have a task failure, as the downstream channels are not open.To avoid this, we can put the initializeState() in the same lock as the open(), and the two operations will happen while being protected by the same lock, which also keeps timers from firing.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="5798" opendate="2017-2-14 00:00:00" fixdate="2017-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Let the RPCService provide a ScheduledExecutorService</summary>
      <description>Currently the RPCService interface provides a scheduleRunnable method to schedule Runnables. I would like to generalize this functionality by letting the RPCService provide a ScheduledExecutorService to the user. That way other components which require such an executor service could simply use the one provided by the RPCService.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.TestingSerialRpcService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcService.java</file>
    </fixedFiles>
  </bug>
  <bug id="5836" opendate="2017-2-17 00:00:00" fixdate="2017-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Race condition between slot offering and task deployment</summary>
      <description>The Flip-6 code has a race condition when offering slots to a JobManager which directly deploys tasks to the offered slots. In such a situation it is possible that the deploy call overtakes the acknowledge message for the slot offering. As a result, the slots are not marked yet as active and the deployment will fail.I propose to fix this problem by first activating all offered slots before sending the slot offer message to the JobManager. Consequently, we'll deactivate and free slots which haven't been accepted by the JobManager once we've received the offering acknowledge message.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="5842" opendate="2017-2-19 00:00:00" fixdate="2017-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong &amp;#39;since&amp;#39; version for ElasticSearch 5.x connector</summary>
      <description>The documentation claims that ElasticSearch 5.x is supported since Flink 1.2.0 which is not true, as the support was merged after 1.2.0.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.elasticsearch.md</file>
    </fixedFiles>
  </bug>
  <bug id="5846" opendate="2017-2-20 00:00:00" fixdate="2017-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CEP: make the operators backwards compatible.</summary>
      <description>This targets making the new CEP operators compatible with their previous versions from Flink 1.1 and Flink 1.2.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPRescalingTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.TimeoutKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.KeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.CEPOperatorUtils.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.NFA.java</file>
    </fixedFiles>
  </bug>
  <bug id="5852" opendate="2017-2-20 00:00:00" fixdate="2017-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move JSON generation code into static methods</summary>
      <description>In order to implement the HistoryServer we need a way to generate the JSON responses independent of the REST API. As such i suggest to move the main parts of the generation code for job-specific handlers into static methods.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.IOMetrics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionJobVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ArchivedExecution.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.SubtasksTimesHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.SubtasksAllAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexTaskManagersHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobExceptionsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobConfigHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.DashboardConfigHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.CurrentJobsOverviewHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtasksTimesHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtasksAllAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexTaskManagersHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobExceptionsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobConfigHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.DashboardConfigHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.CurrentJobsOverviewHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsDetailsSubtasksHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointConfigHandler.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ArchivedExecutionConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="5864" opendate="2017-2-21 00:00:00" fixdate="2017-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CEP: fix duplicate output patterns problem.</summary>
      <description>Currently when searching for a pattern a,b,c and we have input elements a -&gt; b1 -&gt; b2 -&gt;c where b1 and b2 are both valid elements for the position b, then instead of having an output of 2 matched patterns: a, b1, c and a, b2, c, we have 4, with 2 copies of each valid pattern.The problem is with the creation of Dewey number, cause it is not increased on graph branching.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFAITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.NFA.java</file>
    </fixedFiles>
  </bug>
  <bug id="5912" opendate="2017-2-24 00:00:00" fixdate="2017-3-24 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Inputs for CSV and graph generators</summary>
      <description>Create Input classes for reading graphs from CSV as well as for each of the graph generators.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.parameter.ParameterizedBase.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.parameter.Parameterized.java</file>
    </fixedFiles>
  </bug>
  <bug id="5913" opendate="2017-2-24 00:00:00" fixdate="2017-3-24 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Example drivers</summary>
      <description>Replace existing and create new algorithm Driver implementations for each of the library methods.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.utils.NullValueEdgeMapper.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.utils.GraphUtils.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.similarity.JaccardIndex.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.similarity.AdamicAdar.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.link.analysis.PageRank.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.LabelPropagation.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.GSAConnectedComponents.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.ConnectedComponents.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.clustering.undirected.LocalClusteringCoefficient.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.library.clustering.directed.TriangleListing.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.Graph.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.test.java.org.apache.flink.graph.test.examples.ConnectedComponentsITCase.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.Usage.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.examples.GSASingleSourceShortestPaths.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.examples.ConnectedComponents.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.TriangleListing.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.JaccardIndex.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.HITS.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.GraphMetrics.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.Graph500.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.ClusteringCoefficient.java</file>
    </fixedFiles>
  </bug>
  <bug id="5915" opendate="2017-2-25 00:00:00" fixdate="2017-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for the aggregate on multi fields</summary>
      <description>some UDAGGs have multi-fields as input. For instance,table.window(Tumble over 10.minutes on 'rowtime as 'w ).groupBy('key, 'w).select('key, weightedAvg('value, 'weight))This task will add the support for the aggregate on multi fields.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.UnboundedProcessingOverProcessFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.UnboundedNonPartitionedProcessingOverProcessFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.UnboundedEventTimeOverProcessFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.RowsClauseBoundedOverProcessFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.RangeClauseBoundedOverProcessFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetWindowAggMapFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetPreAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.BoundedProcessingOverRowProcessFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateAggFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5916" opendate="2017-2-25 00:00:00" fixdate="2017-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>make env.java.opts.jobmanager and env.java.opts.taskmanager working in YARN mode</summary>
      <description>Now only env.java.opts works in YARN mode, and it applies both to JM and TM. I'd like to make env.java.opts.jobmanager and env.java.opts.taskmanager working in YARN mode in addition, to support fine grained params setting.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnClusterDescriptorTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.BootstrapToolsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="592" opendate="2014-6-9 00:00:00" fixdate="2014-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for secure YARN clusters with Kerberos Auth</summary>
      <description>The current YARN client will throw an exception (as of https://github.com/stratosphere/stratosphere/pull/591) if it detects a secure environment.---------------- Imported from GitHub ----------------Url: https://github.com/stratosphere/stratosphere/issues/592Created by: rmetzgerLabels: enhancement, YARN, Created at: Sun Mar 16 11:05:07 CET 2014State: open</description>
      <version>None</version>
      <fixedVersion>pre-apache</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-dist.src.main.assemblies.yarn.xml</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMaster.scala</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClient.java</file>
      <file type="M">flink-dist.src.main.flink-bin.yarn-bin.yarn-session.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">docs.yarn.setup.md</file>
    </fixedFiles>
  </bug>
  <bug id="5920" opendate="2017-2-25 00:00:00" fixdate="2017-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make port (range) of queryable state server configurable</summary>
      <description>we should support to set port range for config query.server.port</description>
      <version>1.3.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServicesConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.QueryableStateConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.query.QueryableStateUtils.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.test.java.org.apache.flink.queryablestate.network.KvStateServerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.test.java.org.apache.flink.queryablestate.network.KvStateServerHandlerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.test.java.org.apache.flink.queryablestate.network.ClientTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.test.java.org.apache.flink.queryablestate.itcases.NonHAAbstractQueryableStateITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.test.java.org.apache.flink.queryablestate.itcases.HAAbstractQueryableStateITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.main.java.org.apache.flink.queryablestate.server.KvStateServerImpl.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.main.java.org.apache.flink.queryablestate.network.AbstractServerBase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.main.java.org.apache.flink.queryablestate.client.proxy.KvStateClientProxyImpl.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.QueryableStateOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="5921" opendate="2017-2-25 00:00:00" fixdate="2017-2-25 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Adapt time mode indicator functions return custom data types</summary>
      <description>The functions that indicate event time (rowtime()) and processing time (proctime()) are defined to return TIMESTAMP.These functions should be updated to return custom types in order to ease the identification of the time semantics during optimization.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.LogicalWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.TimeModeIndicatorFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="5928" opendate="2017-2-27 00:00:00" fixdate="2017-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Externalized checkpoints overwritting each other</summary>
      <description>I noticed that PR #3346 accidentally broke externalized checkpoints by using a fixed meta data file name. We should restore the old behaviour with creating random files and double check why no test caught this.This will likely superseded by upcoming changes from StephanEwen to use metadata streams on the JobManager.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="5941" opendate="2017-3-1 00:00:00" fixdate="2017-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Let handlers take part in job archiving</summary>
      <description>The key idea behind the HistoryServer is to pre-compute all JSON responses which the WebFrontend could request and store them as files in a directory structure resembling the REST-API.For this require a mechanism to generate the responses and their corresponding REST URL.FLINK-5852 made it easier to re-use the JSON generation code, while FLINK-5870 made handlers aware of the REST URLs that they are registered one.The aim of this JIRA is to extend job-related handlers, building on the above JIRAs, enabling them to generate a number of (Path, Json) pairs for a given ExecutionGraph, containing all responses that they could generate for the given graph and their respective REST URL..</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.utils.ArchivedJobGenerationUtils.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.SubtasksTimesHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.SubtasksAllAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexTaskManagersHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobPlanHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobExceptionsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobConfigHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.CurrentJobsOverviewHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsSubtaskDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointConfigHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtasksTimesHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtasksAllAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexTaskManagersHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobPlanHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobExceptionsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobConfigHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.CurrentJobsOverviewHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsDetailsSubtasksHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointConfigHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="5972" opendate="2017-3-6 00:00:00" fixdate="2017-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t allow shrinking merging windows</summary>
      <description>A misbehaving MergingWindowAssigner can cause a merge that results in a window that is smaller than the span of all the merged windows. This, in itself is not problematic. It becomes problematic when the end timestamp of a window that was not late before merging is now earlier than the watermark (the timestamp is smaller than the watermark).There are two choices: immediately process the window drop the windowprocessing the window will lead to late data downstream.The current behaviour is to silently drop the window but that logic has a bug: we only remove the dropped window from the MergingWindowSet but we don't properly clean up state and timers that the window still (possibly) has. We should fix this bug in the process of resolving this issue.We should either just fix the bug and still silently drop windows or add a check and throw an exception when the end timestamp falls below the watermark.</description>
      <version>1.1.0,1.2.0,1.3.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorContractTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.EvictingWindowOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="6005" opendate="2017-3-9 00:00:00" fixdate="2017-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>unit test ArrayList initializations without initial size</summary>
      <description>I found some ArrayList initializations without a sensible initial size although it is possible to select one. The following PR will show some cases that I'd like to fix.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.serialization.LargeRecordsTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.generator.TestUtils.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.test.java.org.apache.flink.graph.library.SummarizationITCase.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.ListSerializerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="6032" opendate="2017-3-13 00:00:00" fixdate="2017-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CEP-Clean up the operator state when not needed.</summary>
      <description></description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.SubEvent.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.CEPITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.NFA.java</file>
    </fixedFiles>
  </bug>
  <bug id="6033" opendate="2017-3-13 00:00:00" fixdate="2017-5-13 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support UNNEST query in the stream SQL API</summary>
      <description>It would be nice to support the UNNEST keyword in the stream SQL API. The keyword is widely used in queries that relate to nested fields.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.typeutils.TypeCheckUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.FlinkRelNode.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="6048" opendate="2017-3-14 00:00:00" fixdate="2017-5-14 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Asynchronous snapshots for heap-based operator state backends</summary>
      <description>The synchronous checkpointing mechanism of heap-based operator state backends blocks element processing for the duration of the checkpoint.We could implement an heap-based operator state backend that allows for asynchronous checkpoints.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.StateBackendITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.BlockingCheckpointsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.OperatorStateBackendTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.OperatorStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.MemoryStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.DefaultOperatorStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="6050" opendate="2017-3-14 00:00:00" fixdate="2017-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve failure reporting when using Future.thenAccept</summary>
      <description>When applying Future.thenAccept(Async) onto a Future, then we should register the exception handler on the returned Future&lt;Void&gt; and not on the original future. This has the advantage that we also catch exceptions which are thrown in the AcceptFunction and not only those originating from the original Future. This improve Flink's behaviour, because exceptions are not swallowed in the returned Future.</description>
      <version>1.3.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.instance.SlotPoolTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.concurrent.FlinkFutureTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.registration.RetryingRegistration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.registration.RegisteredRpcConnection.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.SlotPool.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.heartbeat.HeartbeatManagerSenderImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.heartbeat.HeartbeatManagerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="6059" opendate="2017-3-15 00:00:00" fixdate="2017-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reject DataSet&lt;Row&gt; and DataStream&lt;Row&gt; without RowTypeInformation</summary>
      <description>It is not possible to automatically extract proper type information for Row because it is not typed with generics and holds values in an Object[].Consequently is handled as GenericType&lt;Row&gt; unless a RowTypeInfo is explicitly specified.This can lead to unexpected behavior when converting a DataSet&lt;Row&gt; or DataStream&lt;Row&gt; into a Table. If the data set or data stream has a GenericType&lt;Row&gt;, the rows are treated as atomic type and converted into a single field.I think we should reject input types of GenericType&lt;Row&gt; when converting data sets and data streams and request a proper RowTypeInfo.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.2,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.TableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.TableEnvironmentITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.TableEnvironmentITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6089" opendate="2017-3-17 00:00:00" fixdate="2017-3-17 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement decoration phase for rewriting predicated logical plan after volcano optimization phase</summary>
      <description>At present, there is no chance to modify the DataStreamRel tree after the volcano optimization. We consider to add a decoration phase after volcano optimization phase. Decoration phase is dedicated for rewriting predicated logical plan and is independent of cost module. After decoration phase is added, we get the chance to apply retraction rules at this phase.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.CalciteConfig.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6090" opendate="2017-3-17 00:00:00" fixdate="2017-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add RetractionRule at the stage of decoration</summary>
      <description>Implement optimizer for retraction: 1.Add RetractionRule at the stage of decorationwhich can derive the replace table/append table, NeedRetraction property. 2.Match the NeedRetraction and replace table, mark the accumulating modeWhen this task is finished, we can turn on retraction for different operators according to accumulating mode.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamOverAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupWindowAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamCorrelate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6094" opendate="2017-3-17 00:00:00" fixdate="2017-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement stream-stream non-window inner join</summary>
      <description>This includes:1.Implement stream-stream non-window inner join</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.NonWindowHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.JoinHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.RetractionRulesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.validation.JoinValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamWindowJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.typeutils.TypeCheckUtilsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.UpdatingPlanCheckerTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.validation.JoinValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.typeutils.TypeCheckUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.NonWindowInnerJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.CRowKeySelector.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.util.UpdatingPlanChecker.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupWindowAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="610" opendate="2014-6-9 00:00:00" fixdate="2014-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Avro serialization by Kryo</summary>
      <description>---------------- Imported from GitHub ----------------Url: https://github.com/stratosphere/stratosphere/issues/610Created by: rmetzgerLabels: java api, Milestone: Release 0.6 (unplanned)Created at: Tue Mar 18 17:29:28 CET 2014State: open</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.util.CollectionDataSets.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.GroupReduceITCase.java</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.type.extractor.PojoTypeExtractionTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.KryoGenericTypeSerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.KryoGenericTypeComparatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.AbstractGenericTypeSerializerTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.runtime.KryoSerializer.java</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.SerializerTestInstance.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.SerializerTestBase.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleSerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.GenericTypeSerializerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.GenericTypeComparatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.GenericArraySerializerTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.GenericTypeInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="6111" opendate="2017-3-18 00:00:00" fixdate="2017-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove sleep after python process generation</summary>
      <description>The python api contains some unnecessary (2 second!) sleeps after the python process created. These are now plain unnecessary with the recent refactorings in FLINK-5650.There are furthermore some sleeps after process shutdown, which can be reworked as well.Preliminary tests show that this will have off roughly 40 seconds from the test execution :/</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.plan.PythonPlanStreamer.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.data.PythonStreamer.java</file>
    </fixedFiles>
  </bug>
  <bug id="6112" opendate="2017-3-18 00:00:00" fixdate="2017-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Calcite 1.12&amp;#39;s new numerical functions</summary>
      <description>CALCITE-1557 introduces the support of some missing numerical functions.We should add the functions.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.mathExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ConstantCallGen.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="6124" opendate="2017-3-20 00:00:00" fixdate="2017-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support max/min aggregations for string type</summary>
      <description>Recently when I port some query to Flink SQL, I found currently min/max aggregations on string type is not supported and should be added.When min/max aggregations are used on string column, return min/max value by lexicographically order.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.MinWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.MaxWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.AggFunctionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunctionWithRetract.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunctionWithRetract.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6128" opendate="2017-3-20 00:00:00" fixdate="2017-3-20 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Optimize JVM options for improve test performance</summary>
      <description>Tune JVM options for run tests by maven-surefire-plugin at travis</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6134" opendate="2017-3-20 00:00:00" fixdate="2017-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set UUID(0L, 0L) as default leader session id</summary>
      <description>The leader election/retrieval services use the null value as the default leader id in the standalone case but also as no active leader in the ZooKeeper case. This is ambiguous and therefore I propose to change the default leader id to UUID(0L, 0L). Consequently, a null leader id value can then indicate that there is no active leader available.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.UtilsTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.minicluster.LocalFlinkMiniClusterITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAJobGraphRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingUtils.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.ScalaTestingUtils.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.JobManagerRegistrationTest.scala</file>
      <file type="M">flink-runtime.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerRegistrationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerJobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.JobLeaderIdServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.query.AkkaKvStateLocationLookupServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.TaskManagerMetricsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingLeaderRetrievalService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.StandaloneLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderElectionRetrievalTestingCluster.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerHARecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.client.JobClientActorTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.messages.JobManagerMessages.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.messages.JobClientMessages.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.LeaderSessionMessageFilter.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.StandaloneLeaderRetrievalService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.StandaloneLeaderElectionService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.AkkaActorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.FlinkResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.client.JobClientActor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.akka.FlinkUntypedActor.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.StackTraceSampleCoordinatorITCase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.BackPressureStatsTrackerITCase.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosFlinkResourceManagerTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ClientTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="6137" opendate="2017-3-20 00:00:00" fixdate="2017-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate strict checkstyle for flink-cep</summary>
      <description>Add a custom checkstyle.xml for `flink-cep` library as in FLINK-6107</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.SubEvent.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.pattern.PatternTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPRescalingTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPMigration11to13Test.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPFrom12MigrationTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.SharedBufferTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFATest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFAITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.DeweyNumberTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.compiler.NFACompilerTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.Event.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.CEPITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.SubtypeFilterFunction.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.Quantifier.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.Pattern.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.OrFilterFunction.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.conditions.IterativeCondition.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.conditions.BooleanConditions.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.AndFilterFunction.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.PatternStream.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.StreamRecordComparator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.CEPOperatorUtils.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.NonDuplicatingTypeSerializer.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.StateTransition.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.State.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.SharedBuffer.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.NFA.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.DeweyNumber.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.compiler.NFACompiler.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.CEP.java</file>
      <file type="M">flink-libraries.flink-cep.pom.xml</file>
      <file type="M">flink-libraries.flink-cep-scala.src.main.scala.org.apache.flink.cep.scala.pattern.Pattern.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6139" opendate="2017-3-21 00:00:00" fixdate="2017-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation for building / preparing Flink for MapR</summary>
      <description>MapR users frequently bump into problems with trying to run Flink on YARN in MapR environments. We should have a document for users to reference that answers all these problems once and for all.</description>
      <version>None</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2.pom.xml</file>
      <file type="M">docs.setup.yarn.setup.md</file>
      <file type="M">docs.setup.gce.setup.md</file>
      <file type="M">docs.setup.cluster.setup.md</file>
      <file type="M">docs.setup.aws.md</file>
    </fixedFiles>
  </bug>
  <bug id="6144" opendate="2017-3-21 00:00:00" fixdate="2017-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port job manager related configuration options to ConfigOption</summary>
      <description>As part of the introduction of the ConfigOption we migrate old configuration options to the new ConfigOption abstraction. A subset of the configuration options are the JobManager related configuration options.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.JobManagerOptions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionJobVertex.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="6163" opendate="2017-3-22 00:00:00" fixdate="2017-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document per-window state in ProcessWindowFunction</summary>
      <description>The current windowing documentation mostly describes WindowFunction and treats ProcessWindowFunction as an afterthought. We should reverse that and also document the new per-key state that is only available to ProcessWindowFunction.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.operators.windows.md</file>
    </fixedFiles>
  </bug>
  <bug id="6165" opendate="2017-3-22 00:00:00" fixdate="2017-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement internal continuity for looping states.</summary>
      <description>We should be able to specify an internal continuity for a looping state. The API could look like: zeroOrMore().consecutive(). So that we have a continuity up to the first element of a loop and between elements in the loop.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFAITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.Quantifier.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.Pattern.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.State.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.compiler.NFACompiler.java</file>
      <file type="M">flink-libraries.flink-cep-scala.src.main.scala.org.apache.flink.cep.scala.pattern.Pattern.scala</file>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug id="6173" opendate="2017-3-23 00:00:00" fixdate="2017-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-table not pack-in com.fasterxml.jackson.* in after #FLINK-5414</summary>
      <description>Currently, flink-table will pack-in com.fasterxml.jackson.* and rename them to org.apache.flink.shaded.calcite.com.fasterxml.jackson.*If a project depends on flink-table, and uses fasterxml as follows(function explain uses fasterxml indirectly):WordCount.scalaobject WordCountWithTable { def main(args: Array[String]): Unit = { // set up execution environment val env = ExecutionEnvironment.getExecutionEnvironment val tEnv = TableEnvironment.getTableEnvironment(env) val input = env.fromElements(WC("hello", 1), WC("hello", 1), WC("ciao", 1)) val expr = input.toTable(tEnv) val result = expr .groupBy('word) .select('word, 'frequency.sum as 'frequency) .filter('frequency === 2) println(tEnv.explain(result)) result.toDataSet[WC].print() } case class WC(word: String, frequency: Long)}It actually uses org.apache.flink.shaded.calcite.com.fasterxml.jackson.*I found after FLINK-5414, flink-table didn't pack-in com.fasterxml.jackson.* and the project would throw class not found exception.Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/flink/shaded/calcite/com/fasterxml/jackson/databind/ObjectMapper at org.apache.flink.table.explain.PlanJsonParser.getSqlExecutionPlan(PlanJsonParser.java:32) at org.apache.flink.table.api.BatchTableEnvironment.explain(BatchTableEnvironment.scala:143) at org.apache.flink.table.api.BatchTableEnvironment.explain(BatchTableEnvironment.scala:164) at org.apache.flink.quickstart.WordCountWithTable$.main(WordCountWithTable.scala:34) at org.apache.flink.quickstart.WordCountWithTable.main(WordCountWithTable.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)Caused by: java.lang.ClassNotFoundException: org.apache.flink.shaded.calcite.com.fasterxml.jackson.databind.ObjectMapper at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 10 more</description>
      <version>None</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6181" opendate="2017-3-24 00:00:00" fixdate="2017-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Zookeeper scripts use invalid regex</summary>
      <description>This issue has been reported by a user: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/unable-to-add-more-servers-in-zookeeper-quorum-peers-in-flink-1-2-td12321.html</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.stop-zookeeper-quorum.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-zookeeper-quorum.sh</file>
    </fixedFiles>
  </bug>
  <bug id="6198" opendate="2017-3-27 00:00:00" fixdate="2017-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the documentation of the CEP library to include all the new features.</summary>
      <description>New features to include: Iterative Functions Quantifiers Time handling Migration from FilterFunction to IterativeCondition</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug id="6200" opendate="2017-3-28 00:00:00" fixdate="2017-3-28 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add event time OVER RANGE BETWEEN UNBOUNDED PRECEDING aggregation to SQL</summary>
      <description>The goal of this issue is to add support for OVER RANGE aggregations on event time streams to the SQL interface.Queries similar to the following should be supported:SELECT a, SUM(b) OVER (PARTITION BY c ORDER BY rowTime() RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS sumB, MIN(b) OVER (PARTITION BY c ORDER BY rowTime() RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS minBFROM myStreamThe following restrictions should initially apply:All OVER clauses in the same SELECT clause must be exactly the same.The PARTITION BY clause is optional (no partitioning results in single threaded execution).The ORDER BY clause may only have rowTime() as parameter. rowTime() is a parameterless scalar function that just indicates processing time mode.bounded PRECEDING is not supported (see FLINK-5655)FOLLOWING is not supported.The restrictions will be resolved in follow up issues. If we find that some of the restrictions are trivial to address, we can add the functionality in this issue as well.This issue includes:Design of the DataStream operator to compute OVER ROW aggregatesTranslation from Calcite's RelNode representation (LogicalProject with RexOver expression).</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.UnboundedEventTimeOverProcessFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamOverAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6201" opendate="2017-3-28 00:00:00" fixdate="2017-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>move python example files from resources to the examples</summary>
      <description>Python example in the resource dir is not suitable. Move them to the examples/python dir.```&lt;fileSet&gt; &lt;directory&gt;../flink-libraries/flink-python/src/main/python/org/apache/flink/python/api&lt;/directory&gt; &lt;outputDirectory&gt;resources/python&lt;/outputDirectory&gt; &lt;fileMode&gt;0755&lt;/fileMode&gt;&lt;/fileSet&gt;```</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6203" opendate="2017-3-28 00:00:00" fixdate="2017-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DataSet Transformations</summary>
      <description>the example of GroupReduce on sorted groups can't remove duplicate Strings in a DataSet.need to add "prev=t"such as:val output = input.groupBy(0).sortGroup(1, Order.ASCENDING).reduceGroup { (in, out: Collector[(Int, String)]) =&gt; var prev: (Int, String) = null for (t &lt;- in) { if (prev == null || prev != t) out.collect(t) prev=t // this line is missing in the example } }</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.batch.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  <bug id="6212" opendate="2017-3-29 00:00:00" fixdate="2017-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing reference to flink-avro dependency</summary>
      <description>In the Connectors page of the Batch (DataSet API) there is a section called "Avro support in Flink"This section mentions the use of certain classes that are part of the flink-avro dependency but this fact is mentioned nowhere. This explanation should be added as well as an xml snippet with the maven dependency as in other parts of the documentation.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.batch.connectors.md</file>
    </fixedFiles>
  </bug>
  <bug id="6213" opendate="2017-3-29 00:00:00" fixdate="2017-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When number of failed containers exceeds maximum failed containers and application is stopped, the AM container will be released 10 minutes later</summary>
      <description>When number of failed containers exceeds maximum failed containers and application is stopped, the AM container will be released 10 minutes later. I checked yarn log and found out after invoking unregisterApplicationMaster, the AM container is not released. After 10 minutes, the release is triggered by RM ping check timeout.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnFlinkResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="6244" opendate="2017-4-3 00:00:00" fixdate="2017-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Emit timeouted Patterns as Side Output</summary>
      <description>Now that we have SideOuputs I think timeouted patterns should be emitted into them rather than producing a stream of `Either`</description>
      <version>1.3.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPRescalingTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPMigrationTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPMigration11to13Test.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.PatternStream.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.TimeoutKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.KeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.CEPOperatorUtils.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.NFA.java</file>
      <file type="M">flink-libraries.flink-cep-scala.src.test.scala.org.apache.flink.cep.scala.PatternStreamScalaJavaAPIInteroperabilityTest.scala</file>
      <file type="M">flink-libraries.flink-cep-scala.src.main.scala.org.apache.flink.cep.scala.PatternStream.scala</file>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug id="6245" opendate="2017-4-3 00:00:00" fixdate="2017-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix late side output documentation in Window documents.</summary>
      <description>There are two things that need to be done:1) in the syntax description in the beginning of the page, we should also include the getSideOutput()2) in the "Getting late data as a side output" section and for the Java example, it should not be a DataStream&lt;T&gt; result ... but a SingleOutputStreamOperator, if we want to get the late event side output.</description>
      <version>1.3.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.operators.windows.md</file>
    </fixedFiles>
  </bug>
  <bug id="6261" opendate="2017-4-4 00:00:00" fixdate="2017-4-4 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add support for TUMBLE, HOP, SESSION to batch SQL</summary>
      <description>Add support for the TUMBLE, HOP, SESSION keywords for batch SQL.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.LogicalWindowAggregateRule.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="6265" opendate="2017-4-4 00:00:00" fixdate="2017-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix consecutive() for times() pattern.</summary>
      <description>When using next() with times() and times() is not consecutive(), the library ignores that relaxed continuity within the pattern.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFAITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.compiler.NFACompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="6274" opendate="2017-4-6 00:00:00" fixdate="2017-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace usages of org.codehaus.jackson</summary>
      <description>We have a few places left that use org.codehaus.jackson instead of com.fasterxml.jackson.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.optimizer.jsonplan.PreviewPlanDumpTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.optimizer.jsonplan.DumpCompiledPlanTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractDeserializationSchemaTest.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.twitter.TwitterExample.java</file>
    </fixedFiles>
  </bug>
  <bug id="6281" opendate="2017-4-7 00:00:00" fixdate="2017-8-7 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create TableSink for JDBC</summary>
      <description>It would be nice to integrate the table APIs with the JDBC connectors so that the rows in the tables can be directly pushed into JDBC.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCTestBase.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCOutputFormat.java</file>
      <file type="M">flink-connectors.flink-jdbc.pom.xml</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
    </fixedFiles>
  </bug>
  <bug id="6282" opendate="2017-4-8 00:00:00" fixdate="2017-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some words was spelled wrong</summary>
      <description>I find some words are spelled wrong.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.Pattern.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.compiler.NFACompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="6299" opendate="2017-4-12 00:00:00" fixdate="2017-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>make all IT cases extend from TestLogger</summary>
      <description>Not all of the integration tests extend from TestLogger but this is a very helpful tool so the currently running tests are written to the logs as well as their failures, especially for those tests where errors are often burried in the logs.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.manual.MassiveCaseClassSortingITCase.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.NetworkStackThroughputITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.minicluster.LocalFlinkMiniClusterITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.TaskManagerFailureRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.misc.SuccessAfterNetworkBuffersFailureITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.misc.MiscellaneousIssuesITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.misc.CustomSerializationITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.misc.AutoParallelismITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.RemoteEnvironmentITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.ExecutionEnvironmentITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.failingPrograms.JobSubmissionFailsITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.clients.examples.LocalExecutorITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.accumulators.AccumulatorLiveITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.accumulators.AccumulatorErrorITCase.java</file>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.WindowFunctionITCase.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.environment.LocalStreamEnvironmentITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.ReusingSortMergeInnerJoinIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.NonReusingSortMergeInnerJoinIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.LargeRecordHandlerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.ExternalSortLargeRecordsITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.ExternalSortITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.CombiningUnilateralSortMergerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.AbstractSortMergeOuterJoinIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReusingHashJoinIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReOpenableHashTableTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReOpenableHashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.NonReusingHashJoinIteratorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.iomanager.IOManagerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.FileChannelStreamsITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobRecoveryITCase.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.ReduceOnNeighborsWithExceptionITCase.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.ReduceOnEdgesWithExceptionITCase.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.DegreesWithExceptionITCase.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.test.CollectionModeSuperstepITCase.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.types.RecordITCase.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.test.java.org.apache.flink.contrib.streaming.CollectITCase.java</file>
      <file type="M">flink-connectors.flink-hbase.src.test.java.org.apache.flink.addons.hbase.HBaseTestingClusterAutostarter.java</file>
      <file type="M">flink-connectors.flink-avro.src.test.java.org.apache.flink.api.avro.AvroExternalJarProgramITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="6307" opendate="2017-4-15 00:00:00" fixdate="2017-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor JDBC tests</summary>
      <description>While glancing over the JDBC related tests I've found a lot of odds things that accumulated over time.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCTestBase.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCInputFormatTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCFullTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="6312" opendate="2017-4-17 00:00:00" fixdate="2017-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update curator version to 2.12.0</summary>
      <description>As there's a Major bug(https://issues.apache.org/jira/browse/CURATOR-344) in curator release used by flink, we need to update the release to 2.12.0 to avoid potential block in flink. (flink use recipes in checkpoint coordinator and we have already occurred problem in zookeeper failover when we're trying to fix https://issues.apache.org/jira/browse/FLINK-6174)</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-contrib.flink-storm-examples.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6313" opendate="2017-4-17 00:00:00" fixdate="2017-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some words was spelled wrong and incorrect LOG.error without print</summary>
      <description>I find some words are spelled wrong and log.error without print information.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnFlinkApplicationMasterRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.query.QueryableStateClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="6332" opendate="2017-4-19 00:00:00" fixdate="2017-5-19 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Upgrade Scala version to 2.11.11</summary>
      <description>Currently scala-2.11 profile uses Scala 2.11.72.11.11 is the most recent version.This issue is to upgrade to Scala 2.11.11</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6356" opendate="2017-4-21 00:00:00" fixdate="2017-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make times() eager and enable allowing combinations.</summary>
      <description>This is the PR that addresses it https://github.com/apache/flink/pull/3761</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFAITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.Quantifier.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.Pattern.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.compiler.NFACompiler.java</file>
      <file type="M">flink-libraries.flink-cep-scala.src.main.scala.org.apache.flink.cep.scala.pattern.Pattern.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6358" opendate="2017-4-22 00:00:00" fixdate="2017-7-22 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Write job details for Gelly examples</summary>
      <description>Add an option to write job details to a file in JSON format. Job details include: job ID, runtime, parameters with values, and accumulators with values.</description>
      <version>1.3.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.Runner.java</file>
    </fixedFiles>
  </bug>
  <bug id="6371" opendate="2017-4-24 00:00:00" fixdate="2017-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Return matched patterns as Map&lt;String, List&lt;T&gt;&gt; instead of Map&lt;String, T&gt;</summary>
      <description>As reported in the last comments of this PR https://github.com/apache/flink/pull/3477#issuecomment-294306997 , currently the select()/flatSelect() functions receive the matched patterns as maps of the form Map&lt;String, T&gt;, where String is the name of thepattern and T is the actual event.With the introduction of quantifiers, we now may have multiple events for a given pattern (e.g. with oneOrMore()). To accommodate this addition, this issue proposes to transform the output format of the matched patterns to Map&lt;String, List&lt;T&gt;&gt;.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPRescalingTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPMigration11to13Test.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPFrom12MigrationTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.SharedBufferTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFATest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFAITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.compiler.NFACompilerTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.CEPITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.PatternTimeoutFunction.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.PatternStream.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.PatternSelectFunction.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.PatternFlatTimeoutFunction.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.PatternFlatSelectFunction.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.TimeoutKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.KeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.CEPOperatorUtils.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.SharedBuffer.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.NFA.java</file>
      <file type="M">flink-libraries.flink-cep-scala.src.test.scala.org.apache.flink.cep.scala.PatternStreamScalaJavaAPIInteroperabilityTest.scala</file>
      <file type="M">flink-libraries.flink-cep-scala.src.main.scala.org.apache.flink.cep.scala.PatternStream.scala</file>
      <file type="M">flink-java8.src.test.java.org.apache.flink.cep.CEPLambdaTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="6389" opendate="2017-4-26 00:00:00" fixdate="2017-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade hbase dependency to 1.3.1</summary>
      <description>hbase 1.3.1 has been released.It fixes compatibility issue in 1.3.0 release, among other bug fixes.We should upgrade to hbase 1.3.1</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-hbase.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6390" opendate="2017-4-26 00:00:00" fixdate="2017-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Trigger Hooks to the Checkpoint Coordinator</summary>
      <description>Some source systems require to be notified prior to starting a checkpoint, in order to do preparatory work for the checkpoint.I propose to add an interface to allow sources to register hooks that are called by the checkpoint coordinator when triggering / restoring a checkpoint.These hooks may produce state that is stores with the checkpoint metadata.Envisioned interface for the hooks/** * The interface for hooks that can be called by the checkpoint coordinator when triggering or * restoring a checkpoint. Such a hook is useful for example when preparing external systems for * taking or restoring checkpoints. * * &lt;p&gt;The {@link #triggerCheckpoint(long, long, Executor)} method (called when triggering a checkpoint) * can return a result (via a future) that will be stored as part of the checkpoint metadata. * When restoring a checkpoint, that stored result will be given to the {@link #restoreCheckpoint(long, Object)} * method. The hook's {@link #getIdentifier() identifier} is used to map data to hook in the presence * of multiple hooks, and when resuming a savepoint that was potentially created by a different job. * The identifier has a similar role as for example the operator UID in the streaming API. * * &lt;p&gt;The MasterTriggerRestoreHook is defined when creating the streaming dataflow graph. It is attached * to the job graph, which gets sent to the cluster for execution. To avoid having to make the hook * itself serializable, these hooks are attached to the job graph via a {@link MasterTriggerRestoreHook.Factory}. * * @param &lt;T&gt; The type of the data produced by the hook and stored as part of the checkpoint metadata. * If the hook never stores any data, this can be typed to {@code Void}. */public interface MasterTriggerRestoreHook&lt;T&gt; { /** * Gets the identifier of this hook. The identifier is used to identify a specific hook in the * presence of multiple hooks and to give it the correct checkpointed data upon checkpoint restoration. * * &lt;p&gt;The identifier should be unique between different hooks of a job, but deterministic/constant * so that upon resuming a savepoint, the hook will get the correct data. * For example, if the hook calls into another storage system and persists namespace/schema specific * information, then the name of the storage system, together with the namespace/schema name could * be an appropriate identifier. * * &lt;p&gt;When multiple hooks of the same name are created and attached to a job graph, only the first * one is actually used. This can be exploited to deduplicate hooks that would do the same thing. * * @return The identifier of the hook. */ String getIdentifier(); /** * This method is called by the checkpoint coordinator prior when triggering a checkpoint, prior * to sending the "trigger checkpoint" messages to the source tasks. * * &lt;p&gt;If the hook implementation wants to store data as part of the checkpoint, it may return * that data via a future, otherwise it should return null. The data is stored as part of * the checkpoint metadata under the hooks identifier (see {@link #getIdentifier()}). * * &lt;p&gt;If the action by this hook needs to be executed synchronously, then this method should * directly execute the action synchronously and block until it is complete. The returned future * (if any) would typically be a completed future. * * &lt;p&gt;If the action should be executed asynchronously and only needs to complete before the * checkpoint is considered completed, then the method may use the given executor to execute the * actual action and would signal its completion by completing the future. For hooks that do not * need to store data, the future would be completed with null. * * @param checkpointId The ID (logical timestamp, monotonously increasing) of the checkpoint * @param timestamp The wall clock timestamp when the checkpoint was triggered, for * info/logging purposes. * @param executor The executor for asynchronous actions * * @return Optionally, a future that signals when the hook has completed and that contains * data to be stored with the checkpoint. * * @throws Exception Exceptions encountered when calling the hook will cause the checkpoint to abort. */ @Nullable Future&lt;T&gt; triggerCheckpoint(long checkpointId, long timestamp, Executor executor) throws Exception; /** * This method is called by the checkpoint coordinator prior to restoring the state of a checkpoint. * If the checkpoint did store data from this hook, that data will be passed to this method. * * @param checkpointId The The ID (logical timestamp) of the restored checkpoint * @param checkpointData The data originally stored in the checkpoint by this hook, possibly null. * * @throws Exception Exceptions thrown while restoring the checkpoint will cause the restore * operation to fail and to possibly fall back to another checkpoint. */ void restoreCheckpoint(long checkpointId, @Nullable T checkpointData) throws Exception; /** * Creates a the serializer to (de)serializes the data stored by this hook. The serializer * serializes the result of the Future returned by the {@link #triggerCheckpoint(long, long, Executor)} * method, and deserializes the data stored in the checkpoint into the object passed to the * {@link #restoreCheckpoint(long, Object)} method. * * &lt;p&gt;If the hook never returns any data to be stored, then this method may return null as the * serializer. * * @return The serializer to (de)serializes the data stored by this hook */ @Nullable SimpleVersionedSerializer&lt;T&gt; createCheckpointDataSerializer();</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.StreamRecordWriterTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointV1Test.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointV1SerializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointLoaderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ExecutionGraphCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.JobCheckpointingSettings.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointV1Serializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointV1.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointSerializers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointLoader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.Savepoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointDeclineReason.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.migration.runtime.checkpoint.savepoint.SavepointV0Serializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.migration.runtime.checkpoint.savepoint.SavepointV0.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.StringUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="6392" opendate="2017-4-26 00:00:00" fixdate="2017-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change the alias of Window from optional to essential.</summary>
      <description>Currently, The window clause use case looks like as following:tab //Table('a,'b,'c) .window( Slide over 10.milli every 5.milli as 'w) .groupBy('w,'a,'b) .select('a, 'b, 'c.sum, 'w.start, 'w.end)As we see the alias of window is essential. But the current implementation of the TableAPI does not have the constraint for the alias,So we must refactoring the API definition using TYPE SYSTEM lead to constraint for the alias.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.FieldProjectionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.common.WindowStartEndPropertiesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.LogicalWindow.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.groupWindows.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.windows.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.windows.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.java.windows.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6393" opendate="2017-4-27 00:00:00" fixdate="2017-5-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add Evenly Graph Generator to Flink Gelly</summary>
      <description>Evenly graph means every vertex in the graph has the same degree, so the graph can be treated as evenly due to all the edges in the graph are distributed evenly. when vertex degree is 0, an empty graph will be generated. when vertex degree is vertex count - 1, complete graph will be generated.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.generator.GridGraph.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.generator.CompleteGraph.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.test.java.org.apache.flink.graph.drivers.EdgeListITCase.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.Runner.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.input.GridGraph.java</file>
      <file type="M">docs.dev.libs.gelly.graph.generators.md</file>
    </fixedFiles>
  </bug>
  <bug id="6411" opendate="2017-4-28 00:00:00" fixdate="2017-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>YarnApplicationMasterRunner should not interfere with RunningJobsRegistry</summary>
      <description>The YarnApplicationMasterRunner removes the running job from the RunningJobsRegistry when it is shut down. This should not be its responsibility and rather be delegated to the JobManagerRunner.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnFlinkApplicationMasterRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="6429" opendate="2017-5-2 00:00:00" fixdate="2017-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump up Calcite version to 1.13</summary>
      <description>This is an umbrella issue for all tasks that need to be done once Apache Calcite 1.13 is released.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.CorrelateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.CorrelateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.CorrelateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.CorrelateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.util.RexProgramExtractor.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.stats.FlinkStatistic.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.AggSqlFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.call.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.catalog.ExternalCatalogSchema.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkTypeSystem.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.calcite.sql.validate.SqlUserDefinedAggFunction.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.calcite.sql.validate.AggChecker.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.calcite.sql.fun.SqlStdOperatorTable.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.calcite.sql.fun.SqlGroupFunction.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.calcite.sql2rel.SqlToRelConverter.java</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
      <file type="M">tools.maven.spotbugs-exclude.xml</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.validation.ScalarFunctionsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.time.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.calcite.sql.fun.SqlTimestampAddFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="6431" opendate="2017-5-2 00:00:00" fixdate="2017-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate strict checkstyle for flink-metrics</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-statsd.src.test.java.org.apache.flink.metrics.statsd.StatsDReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-statsd.src.main.java.org.apache.flink.metrics.statsd.StatsDReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.runtime.jobmanager.JMXJobManagerMetricTest.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.metrics.jmx.JMXReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.main.java.org.apache.flink.metrics.jmx.JMXReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-graphite.src.main.java.org.apache.flink.metrics.graphite.GraphiteReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-ganglia.src.main.java.org.apache.flink.metrics.ganglia.GangliaReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.test.java.org.apache.flink.dropwizard.ScheduledDropwizardReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.test.java.org.apache.flink.dropwizard.metrics.FlinkMeterWrapperTest.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.test.java.org.apache.flink.dropwizard.metrics.DropwizardMeterWrapperTest.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.test.java.org.apache.flink.dropwizard.metrics.DropwizardFlinkHistogramWrapperTest.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.main.java.org.apache.flink.dropwizard.ScheduledDropwizardReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.main.java.org.apache.flink.dropwizard.metrics.HistogramStatisticsWrapper.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.main.java.org.apache.flink.dropwizard.metrics.FlinkMeterWrapper.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.main.java.org.apache.flink.dropwizard.metrics.FlinkHistogramWrapper.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.main.java.org.apache.flink.dropwizard.metrics.FlinkGaugeWrapper.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.main.java.org.apache.flink.dropwizard.metrics.FlinkCounterWrapper.java</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.main.java.org.apache.flink.dropwizard.metrics.DropwizardHistogramStatistics.java</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.test.java.org.apache.flink.metrics.datadog.DatadogHttpClientTest.java</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.MetricType.java</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DSeries.java</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DMetric.java</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DMeter.java</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DGauge.java</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DCounter.java</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DatadogHttpReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DatadogHttpClient.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.test.java.org.apache.flink.metrics.MeterViewTest.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.View.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.util.TestMeter.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.SimpleCounter.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.reporter.MetricReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.reporter.AbstractReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.MetricGroup.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.MetricConfig.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.MeterView.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.HistogramStatistics.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.Histogram.java</file>
      <file type="M">flink-metrics.flink-metrics-core.src.main.java.org.apache.flink.metrics.CharacterFilter.java</file>
    </fixedFiles>
  </bug>
  <bug id="6432" opendate="2017-5-2 00:00:00" fixdate="2017-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate strict checkstyle for flink-python</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.test.java.org.apache.flink.python.api.streaming.data.SingleElementPushBackIteratorTest.java</file>
      <file type="M">flink-libraries.flink-python.src.test.java.org.apache.flink.python.api.PythonPlanBinderTest.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.util.SetCache.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.types.CustomTypeWrapper.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.util.StreamPrinter.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.util.SerializationUtils.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.plan.PythonPlanStreamer.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.plan.PythonPlanSender.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.plan.PythonPlanReceiver.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.data.SingleElementPushBackIterator.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.data.PythonStreamer.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.data.PythonSingleInputStreamer.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.data.PythonSingleInputSender.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.data.PythonSender.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.data.PythonReceiver.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.data.PythonDualInputStreamer.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.data.PythonDualInputSender.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonPlanBinder.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonOptions.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonOperationInfo.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.functions.util.StringTupleDeserializerMap.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.functions.util.StringDeserializerMap.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.functions.util.SerializerMap.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.functions.util.NestedKeyDiscarder.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.functions.util.KeyDiscarder.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.functions.util.IdentityGroupReduce.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.functions.PythonMapPartition.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.functions.PythonCoGroup.java</file>
      <file type="M">flink-libraries.flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6435" opendate="2017-5-3 00:00:00" fixdate="2017-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AsyncWaitOperator does not handle exceptions properly</summary>
      <description>A user reported that the AsyncWaitOperator does not handle exceptions properly. The following code snipped does not make the job fail.public void test() throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream&lt;Integer&gt; withTimestamps = env.fromCollection(Arrays.asList(1,2,3,4,5)); AsyncDataStream.unorderedWait(withTimestamps, (AsyncFunction&lt;Integer, String&gt;) (input, collector) -&gt; { if (input == 3){ collector.collect(new RuntimeException("Test")); return; } collector.collect(Collections.singleton("Ok")); }, 10, TimeUnit.MILLISECONDS) .returns(String.class) .print(); env.execute("unit-test");}</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.queue.StreamElementQueueEntry.java</file>
    </fixedFiles>
  </bug>
  <bug id="6438" opendate="2017-5-3 00:00:00" fixdate="2017-5-3 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Expand docs home page a little</summary>
      <description>The idea is to improve the documentation home page by adding a few links to valuable items that are too easily overlooked.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="6443" opendate="2017-5-4 00:00:00" fixdate="2017-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more doc links in concepts sections</summary>
      <description>Some sections in the high-level concepts docs don't have any pointers to help you learn more. It can be useful to point people to these concept sections when answering questions on stackoverflow and the mailing list, but that doesn't work well if the writeup there is a dead-end.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.concepts.runtime.md</file>
      <file type="M">docs.concepts.programming-model.md</file>
    </fixedFiles>
  </bug>
  <bug id="6445" opendate="2017-5-4 00:00:00" fixdate="2017-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix NullPointerException in CEP pattern without condition</summary>
      <description></description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFAITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.Pattern.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.conditions.SubtypeCondition.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.conditions.OrCondition.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.conditions.NotCondition.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.conditions.AndCondition.java</file>
    </fixedFiles>
  </bug>
  <bug id="6447" opendate="2017-5-4 00:00:00" fixdate="2017-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AWS/EMR docs are out-of-date</summary>
      <description>EMR now has explicit Flink support, so there's no need to install Flink by hand.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.aws.md</file>
      <file type="M">docs.fig.flink-on-emr.png</file>
    </fixedFiles>
  </bug>
  <bug id="6448" opendate="2017-5-4 00:00:00" fixdate="2017-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web UI TaskManager view: Rename &amp;#39;Free Memory&amp;#39; to &amp;#39;JVM Heap&amp;#39;</summary>
      <description>In the TaskManager view, the laben 'Free Memory' is wrong / misleading and should be 'JVM Heap Size' instead.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanager.taskmanager.metrics.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanager.index.jade</file>
    </fixedFiles>
  </bug>
  <bug id="6450" opendate="2017-5-4 00:00:00" fixdate="2017-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web UI Subtasks view for TaskManagers has a misleading name</summary>
      <description>The register for the subtasks grouped by TaskManager is simply called TaskManager, which is confusing users. I suggest to rename it to Subtasks by TaskManager.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.jade</file>
    </fixedFiles>
  </bug>
  <bug id="6451" opendate="2017-5-4 00:00:00" fixdate="2017-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web UI: Rename &amp;#39;Metrics&amp;#39; view to &amp;#39;Task Metrics&amp;#39;</summary>
      <description>In the UI, under the Overview of a specific job, the tab Metrics shows metrics for tasks only, and not all available metrics.We should rename that to Task Metrics. That also differentiates the view clearly from the job-level metrics view proposed in FLINK-6449</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.jade</file>
    </fixedFiles>
  </bug>
  <bug id="6463" opendate="2017-5-5 00:00:00" fixdate="2017-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Throw exception when NOT-NEXT is after OPTIONAL</summary>
      <description></description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFAITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.Pattern.java</file>
    </fixedFiles>
  </bug>
  <bug id="6475" opendate="2017-5-7 00:00:00" fixdate="2017-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incremental snapshots in RocksDB hold lock during async file upload</summary>
      <description>The implementation of incremental checkpoints in RocksDB mistakenly holds the asyncSnapshotLock during the whole async part, effectively blocking all asynchronous processing. Holding the lock is only required in the synchronous part, while the backup to local FS is running.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.DefaultOperatorStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackend.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.AbstractCloseableRegistry.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="6483" opendate="2017-5-8 00:00:00" fixdate="2017-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support time materialization</summary>
      <description>FLINK-5884 added support for time indicators. However, there are still some features missing i.e. materialization of metadata timestamp.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.sources.DefinedTimeAttributes.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.CRowFlatMapRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.CRowCorrelateFlatMapRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.schema.StreamTableSourceTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.schema.RowSchema.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalTableSourceScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.StreamTableSourceScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.CommonCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.CommonCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6494" opendate="2017-5-8 00:00:00" fixdate="2017-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate ResourceManager configuration options</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnClusterDescriptorTest.java</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.YarnJobManager.scala</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnFlinkResourceManager.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnApplicationMasterRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnEntrypointUtils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.UtilsTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.minicluster.LocalFlinkMiniCluster.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.minicluster.FlinkMiniCluster.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.ContaineredTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosFlinkResourceManagerTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosArtifactServer.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosFlinkResourceManager.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ResourceManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="6496" opendate="2017-5-8 00:00:00" fixdate="2017-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate SSL configuration options</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.akka.AkkaSslITCase.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.net.SSLUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.overlays.SSLStoreOverlayTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobClientSslTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.SSLUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.overlays.SSLStoreOverlay.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HistoryServerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="6498" opendate="2017-5-8 00:00:00" fixdate="2017-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate Zookeeper configuration options</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAJobGraphRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.services.MesosServicesUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HighAvailabilityOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.RollingSinkSecuredITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="6499" opendate="2017-5-8 00:00:00" fixdate="2017-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate state configuration options</summary>
      <description>/** The default directory for savepoints. */ @PublicEvolving public static final String SAVEPOINT_DIRECTORY_KEY = "state.savepoints.dir"; /** The default directory used for persistent checkpoints. */ @PublicEvolving public static final String CHECKPOINTS_DIRECTORY_KEY = "state.checkpoints.dir";</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.NonKeyedJob.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.keyed.KeyedJob.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHACheckpointRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.classloading.ClassLoaderITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RescalingITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.CheckpointConfig.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobCancellationWithSavepointHandlersTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobCancellationWithSavepointHandlers.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="6501" opendate="2017-5-8 00:00:00" fixdate="2017-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make sure NOTICE files are bundled into shaded JAR files</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6504" opendate="2017-5-9 00:00:00" fixdate="2017-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lack of synchronization on materializedSstFiles in RocksDBKEyedStateBackend</summary>
      <description>Concurrent checkpoints could access `materializedSstFiles` in the `RocksDBStateBackend` concurrently. This should be avoided.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="6506" opendate="2017-5-9 00:00:00" fixdate="2017-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some tests in flink-tests exceed the memory resources in containerized Travis builds</summary>
      <description>The tests in flink-tests are currently executed on Travis with fork parallelism of 2. It seems that the memory requirements for certain tests have become so high, that certain combinations for two tests in parallel can exceed the available resources on the containerized Travis build infrastructure, triggering the OOM Killer.I think that we can reduce the fork parallelism to 1 for our Travis builds. Tests seem to execute at comparable speed as before without running into resource problems.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">flink-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6508" opendate="2017-5-9 00:00:00" fixdate="2017-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include license files of packaged dependencies</summary>
      <description>The Maven artifact for flink-table bundles its (non-Flink) dependencies to have a self-contained JAR file that can be moved to the ./lib folder without adding additional dependencies.Currently, we include Apache Calcite, Guava (relocates and required by Calcite), Janino, and Reflections.Janino and Reflections are not under Apache license, so we need to include their license files into the JAR file.</description>
      <version>1.2.1,1.3.0,1.4.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6512" opendate="2017-5-9 00:00:00" fixdate="2017-5-9 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>some code examples are poorly formatted</summary>
      <description>Some code examples in the docs are hard to read, mostly because the code highlighting plugin was overlooked.</description>
      <version>1.2.1,1.3.0</version>
      <fixedVersion>1.2.2,1.3.0,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.migration.md</file>
      <file type="M">docs.dev.best.practices.md</file>
    </fixedFiles>
  </bug>
  <bug id="6513" opendate="2017-5-9 00:00:00" fixdate="2017-5-9 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>various typos and grammatical flaws</summary>
      <description>I want to propose small changes to several pages to fix some typos and grammatical flaws.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.internals.stream.checkpointing.md</file>
      <file type="M">docs.dev.stream.side.output.md</file>
      <file type="M">docs.dev.stream.process.function.md</file>
      <file type="M">docs.dev.stream.checkpointing.md</file>
      <file type="M">docs.dev.best.practices.md</file>
    </fixedFiles>
  </bug>
  <bug id="6517" opendate="2017-5-10 00:00:00" fixdate="2017-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support multiple consecutive windows</summary>
      <description>FLINK-5884 changed the way how windows can be defined, however, it is not possible to define multiple consecutive windows right now. It should be possible to refine the end property of a window as a new time attribute.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverterTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.typeutils.TimeIndicatorTypeInfo.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupWindowAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.LogicalWindow.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.windowProperties.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.fieldExpression.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6531" opendate="2017-5-10 00:00:00" fixdate="2017-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deserialize checkpoint hooks with user classloader</summary>
      <description>The checkpoint hooks introduced in FLINK-6390 aren't being deserialized with the user classloader, breaking remote execution.Remote execution produces a `ClassNotFoundException` as the job graph is transferred from the client to the JobManager.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.graph.WithMasterCheckpointHookConfigTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.JobCheckpointingSettings.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="6537" opendate="2017-5-11 00:00:00" fixdate="2017-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Umbrella issue for fixes to incremental snapshots</summary>
      <description>This issue tracks ongoing fixes in the incremental checkpointing feature for the 1.3 release.</description>
      <version>1.3.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.SharedStateRegistryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.SharedStateRegistry.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBIncrementalKeyedStateHandle.java</file>
    </fixedFiles>
  </bug>
  <bug id="6539" opendate="2017-5-11 00:00:00" fixdate="2017-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add automated end-to-end tests</summary>
      <description>We should add simple tests that exercise all the paths that a user would use when starting a cluster and submitting a program. Preferably with a simple batch program and a streaming program that uses Kafka.This would have catched some of the bugs that we now discovered right before the release.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.kafka.WriteIntoKafka.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.kafka.ReadFromKafka.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.kafka.WriteIntoKafka.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.kafka.ReadFromKafka.java</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">test-infra.end-to-end-test.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="6541" opendate="2017-5-11 00:00:00" fixdate="2017-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jar upload directory not created</summary>
      <description>Steps to reproduce: setup configuration property: jobmanager.web.tmpdir = /mnt/flink/web this directory should not exist Run flink job manager. in logs:2017-05-11 12:07:58,397 ERROR org.apache.flink.runtime.webmonitor.WebMonitorUtils - WebServer could not be created [main]java.io.IOException: Jar upload directory /mnt/flink/web/flink-web-3f2733c3-6f4c-4311-b617-1e93d9535421 cannot be created or is not writable.Expected: create parent directories if they do not exit. i.e. use "uploadDir.mkdirs()" instead of "uploadDir.mkdir()"Note: BlobServer create parent directories (See BlobUtils storageDir.mkdirs())</description>
      <version>1.2.0,1.3.0,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServices.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="6543" opendate="2017-5-11 00:00:00" fixdate="2017-5-11 01:00:00" resolution="Done">
    <buginformation>
      <summary>Deprecate toDataStream</summary>
      <description>With retraction support, we should deprecate toDataStream and introduce a new toAppendStream to clearly differentiate between retraction and non-retraction.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.DataStreamUserDefinedFunctionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.DataStreamCalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.DataStreamAggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.UnionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupWindowAggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.stream.sql.SqlITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.TableConversions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.package.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.java.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.table.api.java.package-info.java</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.StreamTableExample.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.StreamSQLExample.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6549" opendate="2017-5-11 00:00:00" fixdate="2017-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve error message for type mismatches with side outputs</summary>
      <description>A type mismatch when using side outputs causes a ClassCastException to be thrown. It would be neat to include the name of the OutputTags in the exception message.This can occur when multiple {{OutputTag]}s with different types but identical names are being used.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
    </fixedFiles>
  </bug>
  <bug id="6550" opendate="2017-5-11 00:00:00" fixdate="2017-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Calling ctx.collect with a null OutputTag should log a warning or throw an exception</summary>
      <description>When using side outputs it is currently possible to call {Context#collect(OutputTag&lt;X&gt;, X record)} with null for the OutputTag. Effectively this causes the data to be swallowed. It is unlikely that a user actually intends this, so i propose to either log a warning or throw an exception.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.ProcessOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.ProcessOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.KeyedProcessOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="6552" opendate="2017-5-11 00:00:00" fixdate="2017-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Side outputs don&amp;#39;t allow differing output types</summary>
      <description>When calling {SingleOutputStreamOperator#getSideOutput(OutputTag&lt;X&gt;} multiple times with the output tags having different types you get the following exception: "Trying to add a side input for the same id with a different type. This is not allowed." This error message is ambiguous, as it could either mean that you cannot add 2 side outputs with the same name but different types or that 2 side outputs with different types cannot be retrieved from a single operator.Furthermore, the error message contains the concept of node id's (i guess?) which users aren't exposed to. This is confusing and should be reworded to work with operators.Lastly, i find this limitation rather odd. It is possible for an operator to have multiple side outputs. It is also possible to have a side output with a different type than the main output. Yet, it is not possible to have multiple side outputs with different types.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.util.TestListResultSink.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.SideOutputITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
    </fixedFiles>
  </bug>
  <bug id="6560" opendate="2017-5-11 00:00:00" fixdate="2017-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Restore maven parallelism in flink-tests</summary>
      <description>FLINK-6506 added the maven variable flink.forkCountTestPackage which is used by the TravisCI script but no default value is set.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6562" opendate="2017-5-11 00:00:00" fixdate="2017-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support implicit table references for nested fields in SQL</summary>
      <description>Currently nested fields can only be accessed through fully qualified identifiers. For example, users need to specify the following query for the table f that has a nested field foo.barSELECT f.foo.bar FROM fOther query engines like Hive / Presto supports implicit table references. For example:SELECT foo.bar FROM fThis jira proposes to support the latter syntax in the SQL API.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.CompositeAccessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.schema.CompositeRelDataType.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6565" opendate="2017-5-12 00:00:00" fixdate="2017-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve error messages for state restore failures</summary>
      <description>The error messages thrown when state restore fails needs to be more explicit and clear of the actual reason.At least 2 cases we've seen so far:1.For example, currently, when restoring an operator state or memory-backed keyed state, the previous serializer must exist. If it doesn't exist, currently only a vague NPE is thrown, without a clear message of the actual reason.2.If the restore failure was due to an incompatible version of a serializer's config snapshot, then it should report something more informative then: "Incompatible version: found 1, required 1."</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.OperatorStateBackendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.MemoryStateBackendTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.OperatorBackendStateMetaInfoSnapshotReaderWriters.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyedBackendStateMetaInfoSnapshotReaderWriters.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.DefaultOperatorStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="6575" opendate="2017-5-13 00:00:00" fixdate="2017-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable all tests on Windows that use HDFS</summary>
      <description>Similar reasoning as FLINK-6558.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.HDFSTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.FsNegativeRunningJobsRegistryTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.FileStateBackendTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.ContinuousFileProcessingTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.ContinuousFileProcessingMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.bucketing.RollingToBucketingMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.bucketing.RollingSinkMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSinkMigrationTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="658" opendate="2014-6-9 00:00:00" fixdate="2014-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Group Sorting to CoGroup</summary>
      <description>Add group sorting to CoGroup to allow the user to work on sorted input groups in a CoGroup function.This is analogous to group sorting for Reduce.---------------- Imported from GitHub ----------------Url: https://github.com/stratosphere/stratosphere/issues/658Created by: fhueskeLabels: enhancement, java api, user satisfaction, Milestone: Release 0.6 (unplanned)Created at: Thu Apr 03 10:09:35 CEST 2014State: open</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.joinDataSet.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.coGroupDataSet.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.CoGroupOperator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.CoGroupOperatorBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="6580" opendate="2017-5-15 00:00:00" fixdate="2017-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink on YARN doesnt start with default parameters</summary>
      <description>Just doing ./bin/yarn-session.sh -n 1 fails with Error while deploying YARN cluster: Couldn't deploy Yarn clusterjava.lang.RuntimeException: Couldn't deploy Yarn cluster at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deploy(AbstractYarnClusterDescriptor.java:436) at org.apache.flink.yarn.cli.FlinkYarnSessionCli.run(FlinkYarnSessionCli.java:626) at org.apache.flink.yarn.cli.FlinkYarnSessionCli$1.call(FlinkYarnSessionCli.java:482) at org.apache.flink.yarn.cli.FlinkYarnSessionCli$1.call(FlinkYarnSessionCli.java:479) at org.apache.flink.runtime.security.HadoopSecurityContext$1.run(HadoopSecurityContext.java:43) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:40) at org.apache.flink.yarn.cli.FlinkYarnSessionCli.main(FlinkYarnSessionCli.java:479)Caused by: java.lang.IllegalArgumentException: The configuration value 'containerized.heap-cutoff-min' is higher (600) than the requested amount of memory 256 at org.apache.flink.yarn.Utils.calculateHeapSize(Utils.java:100) at org.apache.flink.yarn.AbstractYarnClusterDescriptor.setupApplicationMasterContainer(AbstractYarnClusterDescriptor.java:1263) at org.apache.flink.yarn.AbstractYarnClusterDescriptor.startAppMaster(AbstractYarnClusterDescriptor.java:803) at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deployInternal(AbstractYarnClusterDescriptor.java:568) at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deploy(AbstractYarnClusterDescriptor.java:434) ... 9 moreI think this issue has been introduced in FLINK-5904.Flink on YARN is now using the configuration parameters from the configuration file.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
    </fixedFiles>
  </bug>
  <bug id="6582" opendate="2017-5-15 00:00:00" fixdate="2017-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Project from maven archetype is not buildable by default due to ${scala.binary.version}</summary>
      <description>When creating a java project from maven-archetype dependencies to flink are unresolvable due to ${scala.binary.version} placeholder.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6583" opendate="2017-5-15 00:00:00" fixdate="2017-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable QueryConfig in count base GroupWindow</summary>
      <description>Enable QueryConfig in count base GroupWindow by Add a custom Trigger `CountTriggerWithCleanupState`. See more in FLINK-6491.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupWindowAggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupWindowAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6584" opendate="2017-5-15 00:00:00" fixdate="2017-10-15 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support multiple consecutive windows in SQL</summary>
      <description>Right now, the Table API supports multiple consecutive windows as follows:val table = stream.toTable(tEnv, 'rowtime.rowtime, 'int, 'double, 'float, 'bigdec, 'string)val t = table .window(Tumble over 2.millis on 'rowtime as 'w) .groupBy('w) .select('w.rowtime as 'rowtime, 'int.count as 'int) .window(Tumble over 4.millis on 'rowtime as 'w2) .groupBy('w2) .select('w2.rowtime, 'w2.end, 'int.count)Similar behavior should be supported by the SQL API as well. We need to introduce a new auxiliary group function, but this should happen in sync with Apache Calcite.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.validation.GroupWindowValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.common.WindowStartEndPropertiesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.common.LogicalWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupWindowAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalWindowAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug id="6585" opendate="2017-5-15 00:00:00" fixdate="2017-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table examples are not runnable in IDE</summary>
      <description>Running Table API examples in flink-examples-table fails with:Caused by: java.lang.ClassNotFoundException: org.apache.flink.table.api.TableEnvironmentSeems to be a Maven issue.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6590" opendate="2017-5-15 00:00:00" fixdate="2017-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate generated tables into documentation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">flink-libraries.flink-python.pom.xml</file>
      <file type="M">flink-docs.README.md</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.ConfigOptionsDocGeneratorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.ConfigDocsCompletenessChecker.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.QueryableStateOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.MetricOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HistoryServerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">docs.page.css.flink.css</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="6602" opendate="2017-5-16 00:00:00" fixdate="2017-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table source with defined time attributes allows empty string</summary>
      <description>DefinedRowtimeAttribute and DefinedProctimeAttribute are not checked for empty strings.</description>
      <version>None</version>
      <fixedVersion>1.3.1,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.schema.StreamTableSourceTable.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6614" opendate="2017-5-17 00:00:00" fixdate="2017-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Applying function on window auxiliary function fails</summary>
      <description>SQL queries that apply a function or expression on a window auxiliary function (TUMBLE_START, TUMBLE_END, HOP_START, etc). cannot be translated and fail with a CodeGenException:Exception in thread "main" org.apache.flink.table.codegen.CodeGenException: Unsupported call: TUMBLE_ENDExample query:SELECT a, toLong(TUMBLE_END(rowtime, INTERVAL '10' MINUTE)) AS t, COUNT(b) AS cntBFROM myTableGROUP BY a, TUMBLE(rowtime, INTERVAL '10' MINUTE)</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.common.WindowStartEndPropertiesRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6616" opendate="2017-5-17 00:00:00" fixdate="2017-5-17 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Clarify provenance of official Docker images</summary>
      <description>Note that the official Docker images for Flink are community supported and not an official release of the Apache Flink PMC.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.docker.md</file>
    </fixedFiles>
  </bug>
  <bug id="6617" opendate="2017-5-18 00:00:00" fixdate="2017-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve JAVA and SCALA logical plans consistent test</summary>
      <description>Currently,we need some `StringExpression` test,for all JAVA and SCALA API.Such as:`GroupAggregations`,`GroupWindowAggregaton`(Session,Tumble),`Calc` etc.</description>
      <version>1.3.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.OverWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.UserDefinedFunctionTestUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.CommonTestData.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sources.validation.TableSourceBalidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sources.TableSourceTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sources.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sources.stream.table.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sources.stream.sql.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sources.batch.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sinks.validation.TableSinksValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.table.UserDefinedFunctionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.table.UnionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.table.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.table.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.table.TableSinksITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.table.RetractionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.table.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.table.GroupWindowAggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.table.GroupAggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.StreamITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.StreamingWithStateTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.TableSinkITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.TableEnvironmentITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.DataSetWindowAggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.DataSetUserDefinedFunctionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.DataSetCalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.CastingITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.sql.TableWithSQLITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.sql.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.sql.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.sql.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggregate.TimeSortProcessFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.util.RexProgramTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.util.RexProgramRewriterTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.util.RexProgramExtractorTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.SumWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.SumAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.Sum0WithRetractAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.Sum0AggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.MinWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.MaxWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.CountAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.AvgFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.AggFunctionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.validation.ScalarOperatorsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.validation.ScalarFunctionsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.validation.CompositeAccessValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.UserDefinedScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.ScalarOperatorsTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.ScalarFunctionsTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.MapTypeTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.CompositeAccessTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.UserDefinedScalarFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.stream.table.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.stream.sql.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.NonDeterministicTests.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.batch.table.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.batch.sql.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.catalog.utils.ExternalCatalogTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.catalog.stream.table.ExternalCatalogTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.catalog.stream.sql.ExternalCatalogTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.catalog.ExternalCatalogSchemaTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.catalog.batch.table.ExternalCatalogTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.catalog.batch.sql.ExternalCatalogTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.calcite.table.RelTimeIndicatorConverterTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.calcite.sql.RelTimeIndicatorConverterTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.calcite.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.validation.TableSchemaValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.validation.TableEnvironmentValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.utils.StreamTestData.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.validation.UserDefinedTableFunctionValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.validation.UnsupportedOpsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.validation.UnionValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.validation.TimeAttributesValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.validation.TableSourceValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.validation.TableSinksValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.validation.OverWindowValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.validation.GroupWindowAggregationsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.validation.GroupAggregationsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.stringexpr.UserDefinedTableFunctionStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.stringexpr.UnionStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.stringexpr.GroupAggregationsStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.stringexpr.CalcStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupWindowAggregationsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.FieldProjectionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.ExplainTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.TableSchemaTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.TableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.validation.WindowAggregateValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.validation.OverWindowValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.validation.AggregationsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.TimeTestUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SortTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.OverWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.validation.TableEnvironmentValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.utils.TableProgramsTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.utils.TableProgramsCollectionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.utils.SortTestUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.utils.LogicalPlanFormatUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.validation.GroupWindowValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.validation.CompositeFlatteningValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.stringexpr.UserDefinedTableFunctionStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.ExplainTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.CompositeFlatteningTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.AggregationsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.TableSchemaTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.TableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.validation.WindowAggregateValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.validation.SortValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.validation.JoinValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.validation.CalcValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.validation.AggregationsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.GroupingSetsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.DistinctAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.DataSetSingleRowJoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.CompositeFlatteningTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.utils.UserDefinedTableFunctions.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.utils.UserDefinedScalarFunctions.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.utils.UserDefinedAggFunctions.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.utils.Pojos.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.stream.utils.StreamTestData.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.TableSourceITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.TableEnvironmentITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.sql.SqlITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.sql.GroupingSetsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.TableSchemaTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.TableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sinks.StreamTableSinksITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.DataStreamUserDefinedFunctionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.DataStreamCalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.DataStreamAggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.DataSetWindowAggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.DataSetUserDefinedFunctionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.DataSetCalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.rules.RetractionRulesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.rules.NormalizationRulesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.ExternalCatalogTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.MapTypeTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.CompositeAccessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ArrayTypeTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.CompositeFlatteningTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverterTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.utils.StreamITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.utils.StreamingWithStateTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.UnsupportedOpsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.UnionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.stringexpr.OverWindowStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.stringexpr.GroupWindowStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.stream.sql.SqlITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.AggregationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.ExplainTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.AggregationsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.QueryDecorrelationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.TableWithSQLITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.TableEnvironmentITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.TableSinkITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.CastingITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.FieldProjectionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.stringexpr.AggregationsStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.stringexpr.CalcStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.stringexpr.CastingStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.stringexpr.JoinStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.validation.AggregationsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.validation.JoinValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.validation.SetOperatorsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.validation.SortValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.utils.TableProgramsClusterTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.ExplainStreamTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.RetractionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.AggregationsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.TableSinkITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupAggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupAggregationsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupWindowAggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.OverWindowITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6630" opendate="2017-5-19 00:00:00" fixdate="2017-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement FLIP-6 MesosAppMasterRunner</summary>
      <description>A new runner must be developed for the FLIP-6 RM. Target the "single job" scenario.Take some time to consider a general solution or a base implementation that is shared with the old implementation.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.SessionClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.JobClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerTest.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosFlinkResourceManagerTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.services.ZooKeeperMesosServices.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.services.StandaloneMesosServices.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.services.MesosServicesUtils.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.services.MesosServices.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.LaunchableMesosWorker.java</file>
    </fixedFiles>
  </bug>
  <bug id="6632" opendate="2017-5-19 00:00:00" fixdate="2017-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix parameter case sensitive error for test passing/rejecting filter API</summary>
      <description>TableAPI testAllPassingFilter: val t = util.addTable[(Int, Long, String)]('int, 'long, 'string) val resScala = t.filter(Literal(true)).select('int as 'myInt, 'string) val resJava = t.filter("TrUe").select("int as myInt, string")We got error:org.apache.flink.table.api.ValidationException: Cannot resolve [TrUe] given input [int, long, string].The error is caused by : lazy val boolLiteral: PackratParser[Expression] = ("true" | "false") ^^ { str =&gt; Literal(str.toBoolean) }I want improve the method as follow: lazy val boolLiteral: PackratParser[Expression] = ("(t|T)(r|R)(u|U)(e|E)".r | "(f|F)(a|A)(l|L)(s|S)(e|E)".r) ^^ { str =&gt; Literal(str.toBoolean)}Is there any drawback to this improvement? Welcome anyone feedback ?</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.stringexpr.CalcStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.ExpressionParser.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6640" opendate="2017-5-19 00:00:00" fixdate="2017-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure registration of shared state happens before externalizing a checkpoint</summary>
      <description>Currently, a checkpoint is externalized before its shared state is registered. As a consequence, placeholder state handles become part of an externalized checkpoint, which they should not.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHACheckpointRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.RecoverableCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.SharedStateRegistry.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.PendingCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CompletedCheckpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.AbstractCompletedCheckpointStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="6656" opendate="2017-5-22 00:00:00" fixdate="2017-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate CEP PriorityQueue to MapState</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.resources.cep-non-keyed-1.1-snapshot</file>
      <file type="M">flink-libraries.flink-cep.src.test.resources.cep-keyed-1.1-snapshot</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPMigration11to13Test.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="6660" opendate="2017-5-22 00:00:00" fixdate="2017-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>expand the streaming connectors overview page</summary>
      <description>The overview page for streaming connectors is too lean &amp;#8211; it should provide more context and also guide the reader toward related topics.Note that FLINK-6038 will add links to the Bahir connectors.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.twitter.md</file>
      <file type="M">docs.dev.connectors.index.md</file>
      <file type="M">docs.dev.connectors.filesystem.sink.md</file>
    </fixedFiles>
  </bug>
  <bug id="6669" opendate="2017-5-23 00:00:00" fixdate="2017-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Build] Scala style check errror on Windows</summary>
      <description>When build the source code on Windows, a scala style check error happend.Here is the error messages.&amp;#91;INFO&amp;#93;&amp;#91;INFO&amp;#93; &amp;#8212; scalastyle-maven-plugin:0.8.0:check (default) @ flink-scala_2.10 &amp;#8212;error file=E:\github\flink\flink-scala\src\main\scala\org\apache\flink\api\scala\utils\package.scala message=Input length = 2Saving to outputFile=E:\github\flink\flink-scala\target\scalastyle-output.xmlProcessed 78 file(s)Found 1 errorsFound 0 warningsFound 0 infosFinished in 1189 ms&amp;#91;INFO&amp;#93; ------------------------------------------------------------------------&amp;#91;INFO&amp;#93; Reactor Summary:&amp;#91;INFO&amp;#93;&amp;#91;INFO&amp;#93; force-shading ...................................... SUCCESS [ 37.206 s]&amp;#91;INFO&amp;#93; flink .............................................. SUCCESS &amp;#91;03:27 min&amp;#93;&amp;#91;INFO&amp;#93; flink-annotations .................................. SUCCESS [ 3.020 s]&amp;#91;INFO&amp;#93; flink-shaded-hadoop ................................ SUCCESS [ 0.928 s]&amp;#91;INFO&amp;#93; flink-shaded-hadoop2 ............................... SUCCESS [ 15.314 s]&amp;#91;INFO&amp;#93; flink-shaded-hadoop2-uber .......................... SUCCESS [ 13.085 s]&amp;#91;INFO&amp;#93; flink-shaded-curator ............................... SUCCESS [ 0.234 s]&amp;#91;INFO&amp;#93; flink-shaded-curator-recipes ....................... SUCCESS [ 3.336 s]&amp;#91;INFO&amp;#93; flink-shaded-curator-test .......................... SUCCESS [ 2.948 s]&amp;#91;INFO&amp;#93; flink-metrics ...................................... SUCCESS [ 0.286 s]&amp;#91;INFO&amp;#93; flink-metrics-core ................................. SUCCESS [ 9.065 s]&amp;#91;INFO&amp;#93; flink-test-utils-parent ............................ SUCCESS [ 0.327 s]&amp;#91;INFO&amp;#93; flink-test-utils-junit ............................. SUCCESS [ 1.452 s]&amp;#91;INFO&amp;#93; flink-core ......................................... SUCCESS [ 54.277 s][INFO] flink-java ......................................... SUCCESS [ 25.244 s]&amp;#91;INFO&amp;#93; flink-runtime ...................................... SUCCESS &amp;#91;03:08 min&amp;#93;&amp;#91;INFO&amp;#93; flink-optimizer .................................... SUCCESS [ 14.540 s]&amp;#91;INFO&amp;#93; flink-clients ...................................... SUCCESS [ 14.457 s]&amp;#91;INFO&amp;#93; flink-streaming-java ............................... SUCCESS [ 58.130 s]&amp;#91;INFO&amp;#93; flink-test-utils ................................... SUCCESS [ 19.906 s]&amp;#91;INFO&amp;#93; flink-scala ........................................ FAILURE [ 56.634 s]&amp;#91;INFO&amp;#93; flink-runtime-web .................................. SKIPPEDI think this is caused by the Windows default encoding. When I set the inputEncoding to UTF-8 in scalastyle-maven-plugin, the error don't happen.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.1,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6674" opendate="2017-5-23 00:00:00" fixdate="2017-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update release 1.3 docs</summary>
      <description>Umbrella issue to track required updates to the documentation for the 1.3 release.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.migration.md</file>
    </fixedFiles>
  </bug>
  <bug id="6675" opendate="2017-5-23 00:00:00" fixdate="2017-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate strict checkstyle for flink-annotations</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-annotations.src.main.java.org.apache.flink.annotation.VisibleForTesting.java</file>
      <file type="M">flink-annotations.src.main.java.org.apache.flink.annotation.PublicEvolving.java</file>
      <file type="M">flink-annotations.src.main.java.org.apache.flink.annotation.Public.java</file>
      <file type="M">flink-annotations.src.main.java.org.apache.flink.annotation.Internal.java</file>
      <file type="M">flink-annotations.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6687" opendate="2017-5-23 00:00:00" fixdate="2017-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate strict checkstyle for flink-runtime-web</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandler.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitorITCase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.WebMonitorUtilsTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.utils.ArchivedJobGenerationUtils.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.utils.ArchivedExecutionVertexBuilder.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.utils.ArchivedExecutionJobVertexBuilder.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.utils.ArchivedExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.utils.ArchivedExecutionConfigBuilder.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.utils.ArchivedExecutionBuilder.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.testutils.HttpTestClient.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.StackTraceSampleCoordinatorTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.StackTraceSampleCoordinatorITCase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.metrics.TaskManagerMetricsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.metrics.MetricStoreTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.metrics.MetricFetcherTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.metrics.JobVertexMetricsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.metrics.JobMetricsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.metrics.JobManagerMetricsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.metrics.AbstractMetricsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.history.HistoryServerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.history.FsJobArchivistTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.TaskManagersHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.TaskManagerLogHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.SubtasksTimesHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.SubtasksAllAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskCurrentAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexTaskManagersHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexBackPressureHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobStoppingHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobPlanHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobManagerConfigHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobExceptionsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobConfigHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobCancellationWithSavepointHandlersTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobCancellationHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarUploadHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarListHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarActionHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarAccessDeniedHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.HandlerRedirectUtilsTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.DashboardConfigHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.CurrentJobsOverviewHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.CurrentJobIdsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.ClusterOverviewHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsSubtaskDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsCacheTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointConfigHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.files.MimeTypesTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.BackPressureStatsTrackerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.BackPressureStatsTrackerITCase.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorConfig.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.utils.WebFrontendBootstrap.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.utils.MutableIOMetrics.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.StackTraceSampleCoordinator.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.StackTraceSample.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.RuntimeMonitorHandlerBase.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.RuntimeMonitorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.PipelineErrorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.metrics.TaskManagerMetricsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.metrics.MetricStore.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.metrics.MetricFetcher.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.metrics.JobVertexMetricsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.metrics.JobMetricsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.metrics.JobManagerMetricsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.metrics.AbstractMetricsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.JobManagerRetriever.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.HttpRequestHandler.java</file>
      <file type="M">flink-runtime-web.pom.xml</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.BackPressureStatsTracker.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.ExecutionGraphHolder.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.files.MimeTypes.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.files.StaticFileServerHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.AbstractExecutionGraphRequestHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.AbstractJobVertexRequestHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.AbstractJsonRequestHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.AbstractSubtaskAttemptRequestHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.AbstractSubtaskRequestHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointConfigHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsCache.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsDetailsSubtasksHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.ClusterOverviewHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.ConstantTextHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.CurrentJobIdsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.CurrentJobsOverviewHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.DashboardConfigHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.HandlerRedirectUtils.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarAccessDeniedHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarActionHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarListHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobCancellationWithSavepointHandlers.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobConfigHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobExceptionsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobManagerConfigHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobPlanHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexBackPressureHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JobVertexTaskManagersHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JsonFactory.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.RequestHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskCurrentAttemptDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtaskExecutionAttemptDetailsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtasksAllAccumulatorsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.SubtasksTimesHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.TaskManagerLogHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.TaskManagersHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServer.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServerArchiveFetcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="6688" opendate="2017-5-23 00:00:00" fixdate="2017-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate strict checkstyle in flink-test-utils</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.MapPartitionITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.aggregators.AggregatorConvergenceITCase.java</file>
      <file type="M">flink-test-utils-parent.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.TestingSecurityContext.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.TestEnvironment.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.SecureTestEnvironment.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.MultipleProgramsTestBase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.JavaProgramTestBase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.CollectionTestEnvironment.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.AbstractTestBase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.testdata.WordCountData.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.testdata.WebLogAnalysisData.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.testdata.TransitiveClosureData.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.testdata.PageRankData.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.testdata.KMeansData.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.testdata.EnumTriangleData.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.testdata.ConnectedComponentsData.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.TestStreamEnvironment.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.StreamingProgramTestBase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.StreamingMultipleProgramsTestBase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.test.java.org.apache.flink.testutils.junit.RetryOnFailureTest.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.test.java.org.apache.flink.testutils.junit.RetryOnExceptionTest.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.test.java.org.apache.flink.core.testutils.OneShotLatchTest.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.TestLogger.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.testutils.junit.RetryRule.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.testutils.junit.RetryOnException.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.OneShotLatch.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.ManuallyTriggeredDirectExecutor.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.CommonTestUtils.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.CheckedThread.java</file>
    </fixedFiles>
  </bug>
  <bug id="6691" opendate="2017-5-23 00:00:00" fixdate="2017-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add checkstyle import block rule for scala imports</summary>
      <description>Similar to java and javax imports we should give scala imports a separate import block.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.strict-checkstyle.xml</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.util.typeutils.FieldAccessor.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.runtime.jobmanager.JMXJobManagerMetricTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="6695" opendate="2017-5-24 00:00:00" fixdate="2017-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate strict checkstyle in flink-contrib</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.StormFieldsGroupingITCase.java</file>
      <file type="M">flink-contrib.flink-connector-wikiedits.src.test.java.org.apache.flink.streaming.connectors.wikiedits.WikipediaEditsSourceTest.java</file>
      <file type="M">flink-contrib.flink-connector-wikiedits.src.main.java.org.apache.flink.streaming.connectors.wikiedits.WikipediaEditsSource.java</file>
      <file type="M">flink-contrib.flink-connector-wikiedits.src.main.java.org.apache.flink.streaming.connectors.wikiedits.WikipediaEditEvent.java</file>
      <file type="M">flink-contrib.flink-connector-wikiedits.pom.xml</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactoryTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBReducingStateTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDbMultiClassLoaderTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBMergeIteratorTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBListStateTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBInitResetTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBAsyncSnapshotTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBAggregatingStateTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.benchmark.RocksDBPerformanceTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.migration.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBValueState.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBReducingState.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMapState.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBListState.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBFoldingState.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBAggregatingState.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.PredefinedOptions.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.OptionsFactory.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.AbstractRocksDBState.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.pom.xml</file>
      <file type="M">flink-contrib.flink-storm.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.WrapperSetupInLocalClusterTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.WrapperSetupHelperTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.StormTupleTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.SpoutWrapperTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.SpoutCollectorTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.SetupOutputFieldsDeclarerTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.FlinkTopologyContextTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.BoltWrapperTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.BoltCollectorTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.util.TestSink.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.util.TestDummySpout.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.util.TestDummyBolt.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.util.StormStreamSelectorTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.util.SpoutOutputCollectorObserverTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.util.NullTerminatingSpoutTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.util.FiniteTestSpout.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.util.AbstractTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.api.TestSpout.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.api.TestBolt.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.api.FlinkTopologyTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.api.FlinkOutputFieldsDeclarerTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.WrapperSetupHelper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.StormTuple.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.SpoutWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.SpoutCollector.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.MergedInputsBoltWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.FlinkTopologyContext.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.BoltWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.BoltCollector.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.AbstractStormCollector.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.util.StormStreamSelector.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.util.StormConfig.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.util.SpoutOutputCollectorObserver.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.util.SplitStreamType.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.util.SplitStreamMapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.util.NullTerminatingSpout.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.util.FiniteSpout.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.TwoFlinkStreamsMerger.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.StormFlinkStreamMerger.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkTopology.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkSubmitter.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkOutputFieldsDeclarer.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkLocalCluster.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkClient.java</file>
      <file type="M">flink-contrib.flink-storm.pom.xml</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.test.java.org.apache.flink.contrib.streaming.SocketStreamIteratorTest.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.test.java.org.apache.flink.contrib.streaming.CollectITCase.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.main.java.org.apache.flink.contrib.streaming.SocketStreamIterator.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.main.java.org.apache.flink.contrib.streaming.DataStreamUtils.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.main.java.org.apache.flink.contrib.streaming.CollectSink.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.pom.xml</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.wordcount.WordCountLocalNamedITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.wordcount.WordCountLocalITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.wordcount.SpoutSourceWordCountITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.wordcount.BoltTokenizerWordCountWithNamesITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.wordcount.BoltTokenizerWordCountPojoITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.wordcount.BoltTokenizerWordCountITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.StormUnionITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.StormMetaDataITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.pom.xml</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.exclamation.ExclamationLocal.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.exclamation.ExclamationTopology.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.exclamation.ExclamationWithBolt.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.exclamation.ExclamationWithSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.exclamation.operators.ExclamationBolt.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.join.SingleJoinExample.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.print.PrintSampleStream.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.split.operators.RandomSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.split.operators.VerifyAndEnrichBolt.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.split.SpoutSplitExample.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.AbstractBoltSink.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.AbstractLineSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.FileSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.FiniteFileSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.OutputFormatter.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.SimpleOutputFormatter.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.util.TupleOutputFormatter.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.BoltTokenizerWordCount.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.BoltTokenizerWordCountPojo.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.BoltTokenizerWordCountWithNames.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.operators.BoltCounter.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.operators.BoltCounterByName.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.operators.BoltTokenizer.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.operators.BoltTokenizerByName.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.operators.WordCountDataPojos.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.operators.WordCountDataTuple.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.operators.WordCountInMemorySpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.SpoutSourceWordCount.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountLocal.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountLocalByName.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountRemoteByClient.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountRemoteBySubmitter.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.main.java.org.apache.flink.storm.wordcount.WordCountTopology.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.exclamation.ExclamationWithBoltITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.exclamation.ExclamationWithSpoutITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.exclamation.StormExclamationLocalITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.exclamation.util.ExclamationData.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.join.SingleJoinITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SplitBolt.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SplitBoltTopology.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SplitITCase.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SplitSpoutTopology.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SplitStreamBoltLocal.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.split.SplitStreamSpoutLocal.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.operators.FiniteRandomSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.operators.MergerBolt.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.operators.MetaDataSpout.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.operators.TaskIdBolt.java</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.tests.operators.VerifyMetaDataBolt.java</file>
    </fixedFiles>
  </bug>
  <bug id="6697" opendate="2017-5-24 00:00:00" fixdate="2017-10-24 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add batch multi-window support</summary>
      <description>Multiple consecutive windows on batch are not tested yet and I think they are also not supported, because the syntax is not defined for batch yet.The following should be supported:val t = table .window(Tumble over 2.millis on 'rowtime as 'w) .groupBy('w) .select('w.rowtime as 'rowtime, 'int.count as 'int) .window(Tumble over 4.millis on 'rowtime as 'w2) .groupBy('w2) .select('w2.rowtime, 'w2.end, 'int.count)</description>
      <version>1.3.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.GroupWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.GroupWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.sql.AggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.validation.GroupWindowValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.TimeWindowPropertyCollector.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceCombineFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSlideWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSlideWindowAggReduceCombineFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSessionWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.common.WindowPropertiesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.fieldExpression.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.calcite.sql.fun.SqlGroupFunction.java</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug id="6699" opendate="2017-5-24 00:00:00" fixdate="2017-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate strict-checkstyle in flink-yarn-tests</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-yarn-tests.src.test.scala.org.apache.flink.yarn.TestingYarnTaskManager.scala</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOSecuredITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNHighAvailabilityITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnClusterDescriptorTest.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.UtilsTest.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.TestingYarnClusterDescriptor.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.FlinkYarnSessionCliTest.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.CliFrontendYarnAddressConfigurationTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="6703" opendate="2017-5-24 00:00:00" fixdate="2017-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document how to take a savepoint on YARN</summary>
      <description>The documentation should have a separate entry for savepoint related CLI commands in combination with YARN. It is currently not documented that you have to supply the application id, nor how you can pass it../bin/flink savepoint &lt;jobID&gt; -m yarn-cluster (-yid|-yarnapplicationId) &lt;appID&gt;</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.savepoints.md</file>
      <file type="M">docs.ops.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="6704" opendate="2017-5-24 00:00:00" fixdate="2017-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot disable YARN user jar inclusion</summary>
      <description></description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="6736" opendate="2017-5-27 00:00:00" fixdate="2017-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix UDTF codegen bug when window follow by join( UDTF)</summary>
      <description>When we run the tableAPI as follows:val table = stream.toTable(tEnv, 'long.rowtime, 'int, 'double, 'float, 'bigdec, 'date,'pojo, 'string) val windowedTable = table .join(udtf2('string) as ('a, 'b)) .window(Slide over 5.milli every 2.milli on 'long as 'w) .groupBy('w) .select('int.count, agg1('pojo, 'bigdec, 'date, 'int), 'w.start, 'w.end)We will get the error message:org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply$mcV$sp(JobManager.scala:933) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply(JobManager.scala:876) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply(JobManager.scala:876) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.codegen.Compiler$class.compile(Compiler.scala:36) at org.apache.flink.table.runtime.CRowCorrelateProcessRunner.compile(CRowCorrelateProcessRunner.scala:35) at org.apache.flink.table.runtime.CRowCorrelateProcessRunner.open(CRowCorrelateProcessRunner.scala:59) at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36) at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:111) at org.apache.flink.streaming.api.operators.ProcessOperator.open(ProcessOperator.java:56) at org.apache.flink.streaming.runtime.tasks.StreamTask.openAllOperators(StreamTask.java:377) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:254) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:702) at java.lang.Thread.run(Thread.java:745)Caused by: org.codehaus.commons.compiler.CompileException: Line 77, Column 62: Unknown variable or type "in2" at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:11523) at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6292) at org.codehaus.janino.UnitCompiler.access$12900(UnitCompiler.java:209) at org.codehaus.janino.UnitCompiler$18.visitPackage(UnitCompiler.java:5904) at org.codehaus.janino.UnitCompiler$18.visitPackage(UnitCompiler.java:5901) at org.codehaus.janino.Java$Package.accept(Java.java:4074) at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:5901) at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6287) at org.codehaus.janino.UnitCompiler.access$13500(UnitCompiler.java:209)The reason is val generator = new CodeGenerator(config, false, inputSchema.physicalTypeInfo) `physicalTypeInfo` will remove the TimeIndicator.I think we should fix this. What do you think Fabian Hueske Timo Walther , And hope your suggestions.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverterTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.CommonCorrelate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6737" opendate="2017-5-27 00:00:00" fixdate="2017-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix over expression parse String error.</summary>
      <description>When we run the TableAPI as follows:val windowedTable = table .window(Over partitionBy 'c orderBy 'proctime preceding UNBOUNDED_ROW as 'w) .select('c, "countFun(b)" over 'w as 'mycount, weightAvgFun('a, 'b) over 'w as 'wAvg)We get the error:org.apache.flink.table.api.TableException: The over method can only using with aggregation expression. at org.apache.flink.table.api.scala.ImplicitExpressionOperations$class.over(expressionDsl.scala:469) at org.apache.flink.table.api.scala.ImplicitExpressionConversions$LiteralStringExpression.over(expressionDsl.scala:756)The reason is, the `over` method of `expressionDsl` not parse the String case.I think we should fix this before 1.3 release.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6753" opendate="2017-5-28 00:00:00" fixdate="2017-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flaky SqlITCase</summary>
      <description>Tests run: 11, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 6.674 sec &lt;&lt;&lt; FAILURE! - in org.apache.flink.table.api.scala.stream.sql.SqlITCasetestUnnestArrayOfArrayFromTable(org.apache.flink.table.api.scala.stream.sql.SqlITCase) Time elapsed: 0.289 sec &lt;&lt;&lt; ERROR!org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$6.apply$mcV$sp(JobManager.scala:933) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$6.apply(JobManager.scala:876) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$6.apply(JobManager.scala:876) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.codegen.Compiler$class.compile(Compiler.scala:36) at org.apache.flink.table.runtime.CRowCorrelateProcessRunner.compile(CRowCorrelateProcessRunner.scala:35) at org.apache.flink.table.runtime.CRowCorrelateProcessRunner.open(CRowCorrelateProcessRunner.scala:59) at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36) at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:111) at org.apache.flink.streaming.api.operators.ProcessOperator.open(ProcessOperator.java:56) at org.apache.flink.streaming.runtime.tasks.StreamTask.openAllOperators(StreamTask.java:377) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:254) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:702) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.AssertionError: null at org.codehaus.janino.IClass.isAssignableFrom(IClass.java:652) at org.codehaus.janino.UnitCompiler.isWideningReferenceConvertible(UnitCompiler.java:10844) at org.codehaus.janino.UnitCompiler.isMethodInvocationConvertible(UnitCompiler.java:9010) at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:8799) at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:8657) at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:8539) at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:8441) at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4609) at org.codehaus.janino.UnitCompiler.access$8200(UnitCompiler.java:209) at org.codehaus.janino.UnitCompiler$12.visitMethodInvocation(UnitCompiler.java:3969) at org.codehaus.janino.UnitCompiler$12.visitMethodInvocation(UnitCompiler.java:3942) at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:4874) at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3942) at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5125) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3343) at org.codehaus.janino.UnitCompiler.access$5000(UnitCompiler.java:209) at org.codehaus.janino.UnitCompiler$9.visitMethodInvocation(UnitCompiler.java:3322) at org.codehaus.janino.UnitCompiler$9.visitMethodInvocation(UnitCompiler.java:3294) at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:4874) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3294) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2214) at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:209) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1445) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1438) at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2848) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1438) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1518) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2950) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1308) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1281) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:780) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:431) at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:209) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:385) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:380) at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1405) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:380) at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:354) at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:413) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:209) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:200) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:76) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:71) at org.apache.flink.table.codegen.Compiler$class.compile(Compiler.scala:33) at org.apache.flink.table.runtime.CRowCorrelateProcessRunner.compile(CRowCorrelateProcessRunner.scala:35) at org.apache.flink.table.runtime.CRowCorrelateProcessRunner.open(CRowCorrelateProcessRunner.scala:59) at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36) at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:111) at org.apache.flink.streaming.api.operators.ProcessOperator.open(ProcessOperator.java:56) at org.apache.flink.streaming.runtime.tasks.StreamTask.openAllOperators(StreamTask.java:377) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:254) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:702) at java.lang.Thread.run(Thread.java:745)</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6756" opendate="2017-5-29 00:00:00" fixdate="2017-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide RichAsyncFunction to Scala API suite</summary>
      <description>I can't find any tracking info about the chance to have RichAsyncFunction in the Scala API suite. I think it'd be nice to have this function in order to access open/close methods and the RuntimeContext.I was able to retrieve http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/There-is-no-Open-and-Close-method-in-Async-I-O-API-of-Scala-td11591.html#a11593 only, so my question is if there are some blocking issues avoiding this feature. &amp;#91;~till.rohrmann&amp;#93;If it's possible and nobody already have done it, I can assign the issue to myself in order to implement it.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.AsyncDataStreamITCase.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.async.JavaResultFutureWrapper.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.AsyncDataStream.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6772" opendate="2017-5-30 00:00:00" fixdate="2017-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect ordering of matched state events in Flink CEP</summary>
      <description>I've stumbled across an unexepected ordering of the matched state events. Pattern:Pattern&lt;String, ?&gt; pattern = Pattern .&lt;String&gt;begin("start") .where(new IterativeCondition&lt;String&gt;() { @Override public boolean filter(String s, Context&lt;String&gt; context) throws Exception { return s.startsWith("a-"); } }).times(4).allowCombinations() .followedByAny("end") .where(new IterativeCondition&lt;String&gt;() { public boolean filter(String s, Context&lt;String&gt; context) throws Exception { return s.startsWith("b-"); } }).times(3).consecutive();Input event sequence:a-1, a-2, a-3, a-4, b-1, b-2, b-3On b-3 a matched pattern would be triggered.Now, in the Map&lt;String, List&lt;IN&gt;&gt; map passed via select in PatternSelectFunction, the list for the "end" state is:b-3, b-1, b-2.Based on the timestamp of the events (simply using processing time), the correct order should be b-1, b-2, b-3.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.SharedBufferTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFAITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.compiler.NFACompilerTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.SharedBuffer.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.NFA.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.compiler.NFACompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="6773" opendate="2017-5-30 00:00:00" fixdate="2017-7-30 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Use compression (e.g. snappy) for full check/savepoints</summary>
      <description>We could use compression (e.g. snappy stream compression) to decrease the size of our full checkpoints and savepoints. From some initial experiments, I think there is great potential to achieve compression rates around 30-50%. Given those numbers, I think this is very low hanging fruit to implement.One point to consider in the implementation is that compression blocks should respect key-groups, i.e. typically it should make sense to compress per key-group.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RescalingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.AbstractEventTimeWindowCheckpointingITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.SerializationProxiesTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.NonClosingStreamDecorator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StatePartitionStreamProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyedBackendStateMetaInfoSnapshotReaderWriters.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyedBackendSerializationProxy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.StateTableByKeyGroupReaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackend.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateSnapshotCompressionTest.java</file>
      <file type="M">docs.monitoring.large.state.tuning.md</file>
    </fixedFiles>
  </bug>
  <bug id="6776" opendate="2017-5-30 00:00:00" fixdate="2017-6-30 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Use skip instead of seek for small forward repositioning in DFS streams</summary>
      <description>Reading checkpoint meta data and finding key-groups in restores sometimes require to seek in input streams. Currently, we always use a seek, even for small position changes. As small true seeks are far more expensive than small reads/skips, we should just skip over small gaps instead of performing the seek.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.fs.hdfs.HadoopDataInputStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="6782" opendate="2017-5-31 00:00:00" fixdate="2017-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update savepoint documentation</summary>
      <description>Savepoint documentation is a bit outdated regarding full data being stored in the savepoint path, not just a metadata file</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.savepoints.md</file>
    </fixedFiles>
  </bug>
  <bug id="6805" opendate="2017-6-1 00:00:00" fixdate="2017-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink Cassandra connector dependency on Netty disagrees with Flink</summary>
      <description>The Flink Cassandra connector has a dependency on Netty libraries (via promotion of transitive dependencies by the Maven shade plugin) at version 4.0.33.Final, which disagrees with the version included in Flink of 4.0.27.Final which is included &amp; managed by the parent POM via dependency on netty-all.Due to use of netty-all, the dependency management doesn't take effect on the individual libraries such as netty-handler, netty-codec, etc.I suggest that dependency management of Netty should be added for all Netty libraries individually (netty-handler, etc.) so that all Flink modules use the same version, and similarly I suggest that exclusions be added to the quickstart example POM for the individual Netty libraries so that fat JARs don't include conflicting versions of Netty.It seems like this problem started when FLINK-6084 was implemented: transitive dependencies of the flink-connector-cassandra were previously omitted, and now that they are included we must make sure that they agree with the Flink distribution.</description>
      <version>1.2.1,1.3.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6812" opendate="2017-6-2 00:00:00" fixdate="2017-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elasticsearch 5 release artifacts not published to Maven central</summary>
      <description>Release artifacts for the Elasticsearch 5 connector is not published to the Maven Central. Elasticsearch 5 requires Java 8 at minimum, so for the release we need to build with Java 8 for this.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.1,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6831" opendate="2017-6-2 00:00:00" fixdate="2017-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate checkstyle for runtime/*</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.JobException.java</file>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6898" opendate="2017-6-12 00:00:00" fixdate="2017-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Limit size of operator component in metric name</summary>
      <description>The operator name for some operators (specifically windows) can be very, very long (250+) characters.I propose to limit the total space that the operator component can take up in a metric name to 60 characters.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.TaskMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.AbstractMetricGroup.java</file>
    </fixedFiles>
  </bug>
  <bug id="6937" opendate="2017-6-16 00:00:00" fixdate="2017-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix link markdown in Production Readiness Checklist doc</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.production.ready.md</file>
    </fixedFiles>
  </bug>
  <bug id="6940" opendate="2017-6-18 00:00:00" fixdate="2017-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clarify the effect of configuring per-job state backend</summary>
      <description>The documentation of having different options configuring flink state backend is confusing. We should add explicit doc explaining configuring a per-job flink state backend in code will overwrite any default state backend configured in flink-conf.yaml</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.backends.md</file>
    </fixedFiles>
  </bug>
  <bug id="6951" opendate="2017-6-20 00:00:00" fixdate="2017-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incompatible versions of httpcomponents jars for Flink kinesis connector</summary>
      <description>In the following thread, Bowen reported incompatible versions of httpcomponents jars for Flink kinesis connector :http://search-hadoop.com/m/Flink/VkLeQN2m5EySpb1?subj=Re+Incompatible+Apache+Http+lib+in+Flink+kinesis+connectorWe should find a solution such that users don't have to change dependency version(s) themselves when building Flink kinesis connector.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.4,1.4.1,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6952" opendate="2017-6-20 00:00:00" fixdate="2017-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add link to Javadocs</summary>
      <description>The project webpage and the docs are missing links to the Javadocs.I think we should add them as part of the external links at the bottom of the doc navigation (above "Project Page").In the same manner we could add a link to the Scaladocs, but if I remember correctly there was a problem with the build of the Scaladocs. Correct, Aljoscha Krettek?</description>
      <version>None</version>
      <fixedVersion>1.2.2,1.3.1,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.create.release.files.sh</file>
      <file type="M">docs..layouts.base.html</file>
      <file type="M">docs..includes.sidenav.html</file>
      <file type="M">docs..config.yml</file>
      <file type="M">docs.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="6965" opendate="2017-6-21 00:00:00" fixdate="2017-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avro is missing snappy dependency</summary>
      <description>The shading rework made before 1.3 removed a snappy dependency that was accidentally pulled in through hadoop. This is technically alright, until class-loaders rear their ugly heads.Our kafka connector can read avro records, which may or may not require snappy. Usually this should be solvable by including the snappy dependency in the user-jar if necessary, however since the kafka connector loads classes that it requires using the system class loader this doesn't work.As such we have to add a separate snappy dependency to flink-core.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6985" opendate="2017-6-22 00:00:00" fixdate="2017-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove bugfix version from docs title</summary>
      <description>The docs HTML title contains the minor version of the corresponding release. This can be confusing as we build the docs nightly from the respective release branch.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..layouts.base.html</file>
      <file type="M">docs..includes.sidenav.html</file>
      <file type="M">docs..config.yml</file>
      <file type="M">docs.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="6994" opendate="2017-6-23 00:00:00" fixdate="2017-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong base url in master docs</summary>
      <description>The base url of the master docs point to 1.3 instead of 1.4. At the moment the menu items point to the latest stable release docs instead of the nightly master docs.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..config.yml</file>
    </fixedFiles>
  </bug>
  <bug id="7" opendate="2014-6-9 00:00:00" fixdate="2014-12-9 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>[GitHub] Enable Range Partitioner</summary>
      <description>The range partitioner is currently disabled. We need to implement the following aspects:1) Distribution information, if available, must be propagated back together with the ordering property.2) A generic bucket lookup structure (currently specific to PactRecord).Tests to re-enable after fixing this issue: TeraSortITCase GlobalSortingITCase GlobalSortingMixedOrderITCase---------------- Imported from GitHub ----------------Url: https://github.com/stratosphere/stratosphere/issues/7Created by: StephanEwenLabels: core, enhancement, optimizer, Milestone: Release 0.4Assignee: fhueskeCreated at: Fri Apr 26 13:48:24 CEST 2013State: open</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.PartitionITCase.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.DataSet.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.shipping.OutputEmitter.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.java.PartitionOperatorTest.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.util.Utils.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plan.Channel.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plantranslate.JobGraphGenerator.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.Optimizer.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.operators.GroupReduceWithCombineProperties.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.operators.CoGroupDescriptor.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.operators.AbstractJoinDescriptor.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dag.PartitionNode.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.PartitionOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.DataSet.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.distributions.DataDistribution.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ExecutionPlanAfterExecutionTest.java</file>
      <file type="M">docs.apis.programming.guide.md</file>
      <file type="M">docs.apis.dataset.transformations.md</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.PartitionITCase.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.traversals.RangePartitionRewriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="7004" opendate="2017-6-26 00:00:00" fixdate="2017-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Switch to Travis Trusty image</summary>
      <description>As shown in this PR https://github.com/apache/flink/pull/4167 switching to the Trusty image on Travis seems to stabilize the build times.We should switch for 1.2, 1.3 and 1.4.</description>
      <version>1.2.0,1.3.0,1.4.0</version>
      <fixedVersion>1.2.2,1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="7005" opendate="2017-6-26 00:00:00" fixdate="2017-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimization steps are missing for nested registered tables</summary>
      <description>Tables that are registered (implicitly or explicitly) do not pass the first three optimization steps: decorrelate convert time indicators normalize the logical planE.g. this has the wrong plan right now:val table = stream.toTable(tEnv, 'rowtime.rowtime, 'int, 'double, 'float, 'bigdec, 'string)val table1 = tEnv.sql(s"""SELECT 1 + 1 FROM $table""") // not optimizedval table2 = tEnv.sql(s"""SELECT myrt FROM $table1""")val results = table2.toAppendStream[Row]</description>
      <version>1.3.0,1.3.1</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.rules.NormalizationRulesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug id="7034" opendate="2017-6-28 00:00:00" fixdate="2017-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>GraphiteReporter cannot recover from lost connection</summary>
      <description>Now Flink uses metric version 1.3.0 in which there is a Bug. I think you should use version 1.3.1 or higher</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7044" opendate="2017-6-29 00:00:00" fixdate="2017-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add methods to the client API that take the stateDescriptor.</summary>
      <description></description>
      <version>1.3.0,1.3.1</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.query.AbstractQueryableStateITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.query.QueryableStateClientTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.query.QueryableStateClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="7058" opendate="2017-6-30 00:00:00" fixdate="2017-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-scala-shell unintended dependencies for scala 2.11</summary>
      <description>Activation of profile scala-2.10 in `flink-scala-shell` and `flink-scala` do not work as intended. &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;scala-2.10&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;!scala-2.11&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scalamacros&lt;/groupId&gt; &lt;artifactId&gt;quasiquotes_2.10&lt;/artifactId&gt; &lt;version&gt;${scala.macros.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;jline&lt;/artifactId&gt; &lt;version&gt;2.10.4&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/profile&gt; &lt;/profiles&gt;&lt;activation&gt;&lt;/activation&gt;This activation IMO have nothing to do with `-Pscala-2.11` profile switch used in our build. "properties" are defined by `-Dproperty` switches. As far as I understand that, those additional dependencies would be added only if nobody defined property named `scala-2.11`, which means, they would be added only if switch `-Dscala-2.11` was not used, so it seems like those dependencies were basically added always. This quick test proves that I'm correct:$ mvn dependency:tree -pl flink-scala | grep quasi[INFO] +- org.scalamacros:quasiquotes_2.10:jar:2.1.0:compile$ mvn dependency:tree -pl flink-scala -Pscala-2.11 | grep quasi[INFO] +- org.scalamacros:quasiquotes_2.10:jar:2.1.0:compile$ mvn dependency:tree -pl flink-scala -Pscala-2.10 | grep quasi[INFO] +- org.scalamacros:quasiquotes_2.10:jar:2.1.0:compileregardless of the selected profile those dependencies are always there.</description>
      <version>1.3.0,1.3.1</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-scala-shell.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7061" opendate="2017-7-2 00:00:00" fixdate="2017-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix quantifier range starting from 0</summary>
      <description>Currently, there is a bug in quantifier range implementation that for times(0, m), it will match m+1 events at most.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.pattern.PatternTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.TimesRangeITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.Quantifier.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.Pattern.java</file>
    </fixedFiles>
  </bug>
  <bug id="7062" opendate="2017-7-3 00:00:00" fixdate="2017-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support the basic functionality of MATCH_RECOGNIZE</summary>
      <description>In this JIRA, we will support the basic functionality of MATCH_RECOGNIZE in Flink SQL API which includes the support of syntax MEASURES, PATTERN and DEFINE. This would allow users write basic cep use cases with SQL like the following example:SELECT T.aid, T.bid, T.cidFROM MyTableMATCH_RECOGNIZE ( MEASURES A.id AS aid, B.id AS bid, C.id AS cid PATTERN (A B C) DEFINE A AS A.name = 'a', B AS B.name = 'b', C AS C.name = 'c') AS T</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.CepITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.match.PatternSelectFunctionRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.match.PatternFlatSelectFunctionRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.match.MatchUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.match.IterativeConditionRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.match.ConvertToRow.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.SortUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamMatchRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.FlinkRelNode.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamMatch.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetSort.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.CommonSort.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.MatchCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.Indenter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.generated.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
    </fixedFiles>
  </bug>
  <bug id="7133" opendate="2017-7-7 00:00:00" fixdate="2017-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Elasticsearch version interference</summary>
      <description>At least two users have encountered problems with shading in the Elasticsearch connector: https://lists.apache.org/thread.html/b5bc1f690dc894ea9a8b69e82c89eb89ba6dfc2fec2588d2ccacee2c@%3Cuser.flink.apache.org%3E https://lists.apache.org/thread.html/2356670d168f61c20e34611e3c4aeb9c9b3f959f23a9833f631da1ba@%3Cuser.flink.apache.org%3EThe problem seems to be (quote from the second mail):I've found out the source of the problem when I build flink locally.elastic-search base depends on (by default) ES version 1.7.1 that depends onasm 4.1 and that version is shaded to elasticsearch-base-jar. I tried to setelasticsearch.version property in Maven to 5.1.2 (the same as elasticsearch5connector) but then elasticsearch-base does not compile:[ERROR] Failed to execute goalorg.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile(default-testCompile) on project flink-connector-elasticsearch-base_2.11:Compilation failure[ERROR]/home/adebski/Downloads/flink-release-1.3.1/flink-connectors/flink-connector-elasticsearch-base/src/test/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBaseTest.java:[491,92]no suitable constructor found forBulkItemResponse(int,java.lang.String,org.elasticsearch.action.ActionResponse)[ERROR] constructororg.elasticsearch.action.bulk.BulkItemResponse.BulkItemResponse(int,java.lang.String,org.elasticsearch.action.DocWriteResponse)is not applicable[ERROR] (argument mismatch; org.elasticsearch.action.ActionResponse cannotbe converted to org.elasticsearch.action.DocWriteResponse)[ERROR] constructororg.elasticsearch.action.bulk.BulkItemResponse.BulkItemResponse(int,java.lang.String,org.elasticsearch.action.bulk.BulkItemResponse.Failure)is not applicable[ERROR] (argument mismatch; org.elasticsearch.action.ActionResponse cannotbe converted to org.elasticsearch.action.bulk.BulkItemResponse.Failure)To me, it seems like we have to get rid of the "base" package and have two completely separate packages.</description>
      <version>1.3.0,1.3.1</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7176" opendate="2017-7-13 00:00:00" fixdate="2017-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failed builds (due to compilation) don&amp;#39;t upload logs</summary>
      <description>If the compile phase fails on travis flink-dist may not be created. This causes the check for the inclusion of snappy in flink-dist to fail.The function doing this check calls exit 1 on error, which exits the entire shell, thus skipping subsequent actions like the upload of logs.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
    </fixedFiles>
  </bug>
  <bug id="7370" opendate="2017-8-4 00:00:00" fixdate="2017-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>rework operator documentation</summary>
      <description>The structure of the operator documentation could be improved the following way: Create category Streaming/Operators. Move Streaming/Overview/DataStream Transformations to Streaming/Operators/Overview. Move ProcessFunction, Windows, and Async IO to Streaming/Operators create any necessary redirects for old URLs</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.windows.md</file>
      <file type="M">docs.redirects.windows.2.md</file>
      <file type="M">docs.redirects.windows.md</file>
      <file type="M">docs.ops.state.checkpoints.md</file>
      <file type="M">docs.dev.stream.windows.md</file>
      <file type="M">docs.dev.stream.state.checkpointing.md</file>
      <file type="M">docs.dev.stream.side.output.md</file>
      <file type="M">docs.dev.stream.process.function.md</file>
      <file type="M">docs.dev.stream.operators.md</file>
      <file type="M">docs.dev.stream.asyncio.md</file>
      <file type="M">docs.dev.event.timestamp.extractors.md</file>
      <file type="M">docs.dev.event.time.md</file>
      <file type="M">docs.dev.datastream.api.md</file>
      <file type="M">docs.dev.connectors.index.md</file>
      <file type="M">docs.concepts.programming-model.md</file>
    </fixedFiles>
  </bug>
  <bug id="7683" opendate="2017-9-25 00:00:00" fixdate="2017-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add method to iterate over all of the existing keys in a statebackend</summary>
      <description>This is required to make possible preserving backward compatibility while changing state definition of a keyed state operator (to do so operator must iterate over all of the existing keys and rewrites them into a new state variable).</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.HeapStateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.StateTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.NestedMapsStateTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="7962" opendate="2017-11-2 00:00:00" fixdate="2017-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add built-in support for min/max aggregation for Timestamp</summary>
      <description>This JIRA adds the built-in support for min/max aggregation for Timestamp.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MinWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MinAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MaxWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MaxAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunctionWithRetract.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunctionWithRetract.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8458" opendate="2018-1-19 00:00:00" fixdate="2018-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the switch for keeping both the old mode and the new credit-based mode</summary>
      <description>After the whole feature of credit-based flow control is done, we should add a config parameter to switch on/off the new credit-based mode. To do so, we can roll back to the old network mode for any expected risks.The parameter is defined as taskmanager.network.credit-based-flow-control.enabledand the default value is true. This switch may be removed after next release.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs..includes.generated.netty.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="8475" opendate="2018-1-22 00:00:00" fixdate="2018-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move remaining sections to generated tables</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HeartbeatManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.AkkaOptions.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.io.DelimitedInputFormatSamplingTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.DelimitedInputFormat.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.configuration.MesosOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HighAvailabilityOptions.java</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.jobmanager.JobManagerFailsITCase.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerStartupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerRegistrationTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.minicluster.LocalFlinkMiniCluster.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServicesConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyConfig.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ResourceManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CheckpointingOptions.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.cancelling.CancelingTestBase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.FileOutputFormat.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.LocalExecutor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.BlobServerOptions.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.JoinDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AbstractCachedBuildSideJoinDriver.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plantranslate.JobGraphGenerator.java</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="8897" opendate="2018-3-8 00:00:00" fixdate="2018-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rowtime materialization causes "mismatched type" AssertionError</summary>
      <description>As raised in this thread, the query created by the following code will throw a calcite "mismatch type" (Timestamp(3) and TimeIndicator) exception.String sql1 = "select id, eventTs as t1, count(*) over (partition by id order by eventTs rows between 100 preceding and current row) as cnt1 from myTable1";String sql2 = "select distinct id as r_id, eventTs as t2, count(*) over (partition by id order by eventTs rows between 50 preceding and current row) as cnt2 from myTable2";Table left = tableEnv.sqlQuery(sql1);Table right = tableEnv.sqlQuery(sql2);left.join(right).where("id === r_id &amp;&amp; t1 === t2").select("id, t1").writeToSink(...)The logical plan is as follows.LogicalProject(id=[$0], t1=[$1]) LogicalFilter(condition=[AND(=($0, $3), =($1, $4))]) LogicalJoin(condition=[true], joinType=[inner]) LogicalAggregate(group=[{0, 1, 2}]) LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])]) LogicalProject(id=[$0], eventTs=[$3]) LogicalTableScan(table=[[_DataStreamTable_0]]) LogicalAggregate(group=[{0, 1, 2}]) LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])]) LogicalProject(id=[$0], eventTs=[$3]) LogicalTableScan(table=[[_DataStreamTable_0]])That is because the the rowtime field after an aggregation will be materialized while the RexInputRef type for the filter's operands (t1 === t2) is still TimeIndicator. We should make them unified.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.WindowJoinUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamWindowJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
    </fixedFiles>
  </bug>
  <bug id="9107" opendate="2018-3-29 00:00:00" fixdate="2018-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document timer coalescing for ProcessFunctions</summary>
      <description>In a ProcessFunction, registering timers for each event via ctx.timerService().registerEventTimeTimer() using times like ctx.timestamp() + timeout will get a millisecond accuracy and may thus create one timer per millisecond which may lead to some overhead in the TimerService.This problem can be mitigated by using timer coalescing if the desired accuracy of the timer can be larger than 1ms. A timer firing at full seconds only, for example, can be realised like this:coalescedTime = ((ctx.timestamp() + timeout) / 1000) * 1000;ctx.timerService().registerEventTimeTimer(coalescedTime);As a result, only a single timer may exist for every second since we do not add timers for timestamps that are already there.This should be documented in the ProcessFunction docs.</description>
      <version>1.3.0,1.4.0,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.operators.process.function.md</file>
    </fixedFiles>
  </bug>
  <bug id="9108" opendate="2018-3-29 00:00:00" fixdate="2018-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>invalid ProcessWindowFunction link in Document</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.side.output.md</file>
    </fixedFiles>
  </bug>
</bugrepository>
