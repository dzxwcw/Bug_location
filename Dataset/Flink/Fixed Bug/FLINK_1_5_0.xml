<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="10016" opendate="2018-8-1 00:00:00" fixdate="2018-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make YARN/Kerberos end-to-end test stricter</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.yarn.kerberos.docker.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.config.yarn-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10020" opendate="2018-8-1 00:00:00" fixdate="2018-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kinesis Consumer listShards should support more recoverable exceptions</summary>
      <description>Currently transient errors in listShards make the consumer fail and cause the entire job to reset. That is unnecessary for certain exceptions (like status 503 errors). It should be possible to control the exceptions that qualify for retry, similar to getRecords/isRecoverableSdkClientException.</description>
      <version>None</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.ConsumerConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="10101" opendate="2018-8-8 00:00:00" fixdate="2018-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mesos web ui url is missing.</summary>
      <description>Mesos web ui url is missing in new deploy mode.</description>
      <version>1.5.0,1.5.1,1.5.2</version>
      <fixedVersion>1.5.4,1.6.1,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosSessionClusterEntrypoint.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosJobClusterEntrypoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="10135" opendate="2018-8-13 00:00:00" fixdate="2018-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Certain cluster-level metrics are no longer exposed</summary>
      <description>In the documentation for metrics in the Flink 1.5.0 release, it says that the following metrics are reported by the JobManager:numRegisteredTaskManagersnumRunningJobstaskSlotsAvailabletaskSlotsTotalIn the job manager REST endpoint (http://&lt;job-manager&gt;:8081/jobmanager/metrics), those metrics don't appear.</description>
      <version>1.5.0,1.6.0,1.7.0</version>
      <fixedVersion>1.5.5,1.6.2,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingRestfulGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.TestingResourceManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerJobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerHATest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricNames.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerFactory.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="10142" opendate="2018-8-14 00:00:00" fixdate="2018-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce synchronization overhead for credit notifications</summary>
      <description>When credit-based flow control was introduced, we also added some checks and optimisations for uncommon code paths that make common code paths unnecessarily more expensive, e.g. checking whether a channel was released before forwarding a credit notification to Netty. Such checks would have to be confirmed by the Netty thread anyway and thus only add additional load for something that happens only once (per channel).</description>
      <version>1.5.0,1.5.1,1.5.2,1.5.3,1.6.0,1.7.0</version>
      <fixedVersion>1.5.4,1.6.1,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="10150" opendate="2018-8-15 00:00:00" fixdate="2018-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Chained batch operators interfere with each other other</summary>
      <description>The flink web ui displays an inconsistent number of "Records received" / "Records sent” in the job overview "Subtasks" view.When I run the example wordcount batch job with a small input file on flink 1.3.2 I get 3 records sent by the first subtask and 3 records received by the second subtaskThis is the result I would expect.If I run the same job on flink 1.4.0 / 1.5.2 / 1.6.0 I get 13 records sent by the first subtask and 3 records received by the second subtaskIn real life jobs the numbers are much more strange.</description>
      <version>1.4.0,1.5.0,1.6.0,1.7.0</version>
      <fixedVersion>1.5.4,1.6.1,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironmentBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskMetricGroup.java</file>
    </fixedFiles>
  </bug>
  <bug id="10195" opendate="2018-8-22 00:00:00" fixdate="2018-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RabbitMQ Source With Checkpointing Doesn&amp;#39;t Backpressure Correctly</summary>
      <description>The connection between the RabbitMQ server and the client does not appropriately back pressure when auto acking is disabled. This becomes very problematic when a downstream process throttles the data processing to slower then RabbitMQ sends the data to the client.The difference in records ends up being stored in the flink's heap space, which grows indefinitely (or technically to "Integer Max" Deliveries). Looking at RabbitMQ's metrics the number of unacked messages looks like steadily rising saw tooth shape.Upon further invesitgation it looks like this is due to how the QueueingConsumer works, messages are added to the BlockingQueue faster then they are being removed and processed, resulting in the previously described behavior.This may be intended behavior, however this isn't explicitly obvious in the documentation or any of the examples I have seen.</description>
      <version>1.4.0,1.5.0,1.5.1,1.6.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.test.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.test.java.org.apache.flink.streaming.connectors.rabbitmq.common.RMQConnectionConfigTest.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSource.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.common.RMQConnectionConfig.java</file>
      <file type="M">docs.dev.connectors.rabbitmq.md</file>
    </fixedFiles>
  </bug>
  <bug id="10295" opendate="2018-9-6 00:00:00" fixdate="2018-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tokenisation of Program Args resulting in unexpected results</summary>
      <description>We were upgrading from Flink 1.4 to 1.6. At present we have a jar which takes all the details to run the job as program args against a jarid, including sql query and kafka details. In version 1.5 the program args are tokenised as a result single quote (') and double quote(") are stripped from the arguments. This results in malformed args.Attached a sample request for reference.</description>
      <version>1.5.0,1.6.0,1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerRequestUtils.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarSubmissionITCase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBodyTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.ProgramArgsQueryParameter.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBody.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunMessageParameters.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanMessageParameters.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler.java</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="10331" opendate="2018-9-13 00:00:00" fixdate="2018-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce number of flush requests to the network stack</summary>
      <description>With the re-design of the record writer interaction with the result(sub)partitions, flush requests can currently pile up in these scenarios: a previous flush request has not been completely handled yet and/or is still enqueued or the network stack is still polling from this subpartition and doesn't need a new notificationThese lead to increased notifications in low latency settings (low output flusher intervals) which can be avoided.</description>
      <version>1.5.0,1.5.1,1.5.2,1.5.3,1.6.0,1.7.0</version>
      <fixedVersion>1.5.5,1.6.2,1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
    </fixedFiles>
  </bug>
  <bug id="5750" opendate="2017-2-9 00:00:00" fixdate="2017-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect translation of n-ary Union</summary>
      <description>Calcite's union operator is supports more than two input relations. However, Flink's translation rules only consider the first two relations because we assumed that Calcite's union is binary. This problem exists for batch and streaming queries.It seems that Calcite only generates non-binary Unions in rare cases ((SELECT * FROM t) UNION ALL (SELECT * FROM t) UNION ALL (SELECT * FROM t) results in two binary union operators) but the problem definitely needs to be fixed.The following query can be used to validate the problem. @Test public void testValuesWithCast() throws Exception { ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config()); String sqlQuery = "VALUES (1, cast(1 as BIGINT) )," + "(2, cast(2 as BIGINT))," + "(3, cast(3 as BIGINT))"; String sqlQuery2 = "VALUES (1,1)," + "(2, 2)," + "(3, 3)"; Table result = tableEnv.sql(sqlQuery); DataSet&lt;Row&gt; resultSet = tableEnv.toDataSet(result, Row.class); List&lt;Row&gt; results = resultSet.collect(); Table result2 = tableEnv.sql(sqlQuery2); DataSet&lt;Row&gt; resultSet2 = tableEnv.toDataSet(result2, Row.class); List&lt;Row&gt; results2 = resultSet2.collect(); String expected = "1,1\n2,2\n3,3"; compareResultAsText(results2, expected); compareResultAsText(results, expected); }AR for results variablejava.lang.AssertionError: Different elements in arrays: expected 3 elements and received 2 expected: [1,1, 2,2, 3,3] received: [1,1, 2,2] Expected :3Actual :2</description>
      <version>1.2.0,1.3.4,1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.3,1.6.0,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testUnionStream0.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testUnion1.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testUnion0.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.UnionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.ExternalCatalogTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.GroupingSetsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamUnionRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetUnionRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
    </fixedFiles>
  </bug>
  <bug id="6719" opendate="2017-5-25 00:00:00" fixdate="2017-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add details about fault-tolerance of timers to ProcessFunction docs</summary>
      <description>The fault-tolerance of timers is a frequently asked questions on the mailing lists. We should add details about the topic in the ProcessFunction docs.</description>
      <version>1.5.0</version>
      <fixedVersion>1.4.3,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.testjar.WordCount.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.testjar.JobWithExternalDependency.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.RemoteExecutorHostnameResolutionTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.PackagedProgramTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.LeaderRetrievalServiceHostnameResolutionTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ExecutionPlanCreationTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ExecutionPlanAfterExecutionTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ClusterClientTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ClientTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ClientConnectionTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.CliFrontendTestUtils.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.CliFrontendStopTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.CliFrontendSavepointTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.CliFrontendRunTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.CliFrontendPackageProgramTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.CliFrontendListCancelTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.CliFrontendInfoTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.CliFrontendAddressConfigurationTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.RemoteExecutor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.StandaloneClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ProgramInvocationException.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PreviewPlanEnvironment.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgram.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.OptimizerPlanEnvironment.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.JobWithJars.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.DetachedEnvironment.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ContextEnvironmentFactory.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ContextEnvironment.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.LocalExecutor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.StandaloneClusterDescriptor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.ClusterDescriptor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.StopOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.SavepointOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.RunOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ListOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.InfoOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.DefaultCLI.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CustomCommandLine.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CommandLineOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliArgsException.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CancelOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.ClientUtils.java</file>
      <file type="M">docs.dev.stream.operators.process.function.md</file>
    </fixedFiles>
  </bug>
  <bug id="6720" opendate="2017-5-25 00:00:00" fixdate="2017-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate strict checkstyle for flink-java8</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java8.src.test.java.org.apache.flink.runtime.util.jartestprogram.WordFilter.java</file>
      <file type="M">flink-java8.src.test.java.org.apache.flink.runtime.util.jartestprogram.UtilFunctionWrapper.java</file>
      <file type="M">flink-java8.src.test.java.org.apache.flink.runtime.util.jartestprogram.UtilFunction.java</file>
      <file type="M">flink-java8.src.test.java.org.apache.flink.runtime.util.jartestprogram.FilterLambda4.java</file>
      <file type="M">flink-java8.src.test.java.org.apache.flink.runtime.util.jartestprogram.FilterLambda3.java</file>
      <file type="M">flink-java8.src.test.java.org.apache.flink.runtime.util.jartestprogram.FilterLambda2.java</file>
      <file type="M">flink-java8.src.test.java.org.apache.flink.runtime.util.jartestprogram.FilterLambda1.java</file>
      <file type="M">flink-java8.src.test.java.org.apache.flink.runtime.util.JarFileCreatorLambdaTest.java</file>
      <file type="M">flink-java8.src.test.java.org.apache.flink.cep.CEPLambdaTest.java</file>
      <file type="M">flink-java8.src.test.java.org.apache.flink.api.java.type.lambdas.LambdaExtractionTest.java</file>
      <file type="M">flink-java8.src.main.java.org.apache.flink.streaming.examples.java8.wordcount.WordCount.java</file>
      <file type="M">flink-java8.src.main.java.org.apache.flink.examples.java8.wordcount.WordCount.java</file>
      <file type="M">flink-java8.src.main.java.org.apache.flink.examples.java8.relational.TPCHQuery10.java</file>
      <file type="M">flink-java8.pom.xml</file>
      <file type="M">flink-java8.src.test.java.org.apache.flink.test.javaApiOperators.lambdas.ReduceITCase.java</file>
      <file type="M">flink-java8.src.test.java.org.apache.flink.test.javaApiOperators.lambdas.MapITCase.java</file>
      <file type="M">flink-java8.src.test.java.org.apache.flink.test.javaApiOperators.lambdas.JoinITCase.java</file>
      <file type="M">flink-java8.src.test.java.org.apache.flink.test.javaApiOperators.lambdas.GroupReduceITCase.java</file>
      <file type="M">flink-java8.src.test.java.org.apache.flink.test.javaApiOperators.lambdas.FlatMapITCase.java</file>
      <file type="M">flink-java8.src.test.java.org.apache.flink.test.javaApiOperators.lambdas.FlatJoinITCase.java</file>
      <file type="M">flink-java8.src.test.java.org.apache.flink.test.javaApiOperators.lambdas.FilterITCase.java</file>
      <file type="M">flink-java8.src.test.java.org.apache.flink.test.javaApiOperators.lambdas.CrossITCase.java</file>
      <file type="M">flink-java8.src.test.java.org.apache.flink.test.javaApiOperators.lambdas.CoGroupITCase.java</file>
      <file type="M">flink-java8.src.test.java.org.apache.flink.test.javaApiOperators.lambdas.AllGroupReduceITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="6721" opendate="2017-5-25 00:00:00" fixdate="2017-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate strict checkstyle for flink-fs-tests</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-fs-tests.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.HDFSTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.FsNegativeRunningJobsRegistryTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.FileStateBackendTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.ContinuousFileProcessingTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.ContinuousFileProcessingITCase.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.ContinuousFileProcessingFrom12MigrationTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.ContinuousFileProcessingFrom11MigrationTest.java</file>
      <file type="M">flink-fs-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6722" opendate="2017-5-25 00:00:00" fixdate="2017-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate strict checkstyle for flink-table</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.ExplainTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.utils.UserDefinedTableFunctions.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.utils.UserDefinedScalarFunctions.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.utils.UserDefinedAggFunctions.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.stream.utils.StreamTestData.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.stream.sql.SqlITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.TableSourceITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.TableEnvironmentITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.sql.SqlITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.sql.GroupingSetsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.resources.tableSourceConverter.properties</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.table.explain.PlanJsonParser.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.table.explain.Node.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.table.api.java.package-info.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.table.annotation.TableType.java</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7" opendate="2014-6-9 00:00:00" fixdate="2014-12-9 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>[GitHub] Enable Range Partitioner</summary>
      <description>The range partitioner is currently disabled. We need to implement the following aspects:1) Distribution information, if available, must be propagated back together with the ordering property.2) A generic bucket lookup structure (currently specific to PactRecord).Tests to re-enable after fixing this issue: TeraSortITCase GlobalSortingITCase GlobalSortingMixedOrderITCase---------------- Imported from GitHub ----------------Url: https://github.com/stratosphere/stratosphere/issues/7Created by: StephanEwenLabels: core, enhancement, optimizer, Milestone: Release 0.4Assignee: fhueskeCreated at: Fri Apr 26 13:48:24 CEST 2013State: open</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.PartitionITCase.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.DataSet.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.shipping.OutputEmitter.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.java.PartitionOperatorTest.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.util.Utils.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plan.Channel.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plantranslate.JobGraphGenerator.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.Optimizer.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.operators.GroupReduceWithCombineProperties.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.operators.CoGroupDescriptor.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.operators.AbstractJoinDescriptor.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dag.PartitionNode.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.PartitionOperator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.DataSet.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.distributions.DataDistribution.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ExecutionPlanAfterExecutionTest.java</file>
      <file type="M">docs.apis.programming.guide.md</file>
      <file type="M">docs.apis.dataset.transformations.md</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.PartitionITCase.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.traversals.RangePartitionRewriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="7521" opendate="2017-8-25 00:00:00" fixdate="2017-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the 10MB limit from the current REST implementation.</summary>
      <description>In the current AbstractRestServer we impose an upper bound of 10MB in the states we can transfer. This is in the line .addLast(new HttpObjectAggregator(1024 * 1024 * 10)) of the server implementation. This limit is restrictive for some of the usecases planned to use this implementation (e.g. the job submission client which has to send full jars, or the queryable state client which may have to receive states bigger than that).This issue proposes the elimination of this limit.</description>
      <version>1.4.0,1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.RouterHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.RestHandlerConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.PipelineErrorHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.JobSubmitRequestBody.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpointConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClientConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClient.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="7777" opendate="2017-10-9 00:00:00" fixdate="2017-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump japicmp to 0.11.0</summary>
      <description>Currently, flink used japicmp-maven-plugin version is 0.7.0, I'm getting these warnings from the maven plugin during a mvn clean verify:[INFO] Written file '.../target/japicmp/japicmp.diff'.[INFO] Written file '.../target/japicmp/japicmp.xml'.[INFO] Written file '.../target/japicmp/japicmp.html'.Warning: org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.Compiler warnings: WARNING: 'org.apache.xerces.jaxp.SAXParserImpl: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.'Warning: org.apache.xerces.parsers.SAXParser: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.Warning: org.apache.xerces.parsers.SAXParser: Property 'http://javax.xml.XMLConstants/property/accessExternalDTD' is not recognized.Warning: org.apache.xerces.parsers.SAXParser: Property 'http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit' is not recognized.japicmp fixed in version 0.7.1 : _Excluded xerces vom maven-reporting dependency in order to prevent warnings from SAXParserImpl. _The current stable version is 0.11.0, we can consider upgrading to this version.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7857" opendate="2017-10-17 00:00:00" fixdate="2017-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port JobVertexDetails to REST endpoint</summary>
      <description>Port JobVertexDetails to REST endpoint</description>
      <version>1.5.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="7992" opendate="2017-11-6 00:00:00" fixdate="2017-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>extend PR template with S3 question</summary>
      <description>S3 file system tests are only run if AWS credentials are specified, i.e. ARTIFACTS_AWS_BUCKET, ARTIFACTS_AWS_ACCESS_KEY, and ARTIFACTS_AWS_SECRET_KEY. Since these must remain secret, they are only set in Apache Flink's Travis CI configuration and not available in the Travis runs on pull requests (PR) to not leak them in any way. This however means that if a contributor changes something S3-related, the PR's test results will not reflect the actual changes and if something breaks there, we will only see it once merged.Therefore, I propose to add one more question to the PR template so that the committer is aware of this fact and the need to run the tests in his own Travis CI configuration first with proper AWS credentials set up.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.github.PULL.REQUEST.TEMPLATE.md</file>
    </fixedFiles>
  </bug>
  <bug id="8010" opendate="2017-11-7 00:00:00" fixdate="2017-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump remaining flink-shaded dependencies</summary>
      <description></description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8027" opendate="2017-11-8 00:00:00" fixdate="2017-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generalize existing rest handlers to work with arbitrary RestfulGateway</summary>
      <description>In order to reuse the existing AbstractRestHandler we should refactor them such that they work with arbitrary RestfulGateway.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.metrics.AbstractMetricsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobIdsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="8079" opendate="2017-11-15 00:00:00" fixdate="2017-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip remaining E2E tests if one failed</summary>
      <description>I propose that if one end-to-end tests fails the remaining tests are skipped.aljoscha What do you think?</description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
    </fixedFiles>
  </bug>
  <bug id="8082" opendate="2017-11-15 00:00:00" fixdate="2017-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump version compatibility check to 1.4</summary>
      <description>Similar to FLINK-7977, we must bump the version of the compatibility check to compare 1.5 against 1.4, once it is released.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8088" opendate="2017-11-15 00:00:00" fixdate="2017-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bind logical slots to their request id instead of the slot allocation id</summary>
      <description>Since allocated slots can be reused to fulfil multiple slot requests, we should bind the resulting logical slots to their slot request id instead of the allocation id of the underlying allocated slot.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.slots.TestingSlotOwner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.slots.DummySlotOwner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.instance.TestingLogicalSlot.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.instance.SlotPoolTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.instance.SlotPoolRpcTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.instance.AllocatedSlotsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.utils.SimpleSlotProvider.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionVertexLocalityTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionVertexDeploymentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphSchedulingTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.slots.SlotOwner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.slots.SlotContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.slots.SimpleSlotContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.SlotPoolGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.SlotPool.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.Slot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.SimpleSlot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.SharedSlot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.LogicalSlot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.Instance.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.AllocatedSlot.java</file>
    </fixedFiles>
  </bug>
  <bug id="8095" opendate="2017-11-17 00:00:00" fixdate="2017-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce ProjectSetOpTransposeRule to Flink</summary>
      <description>ProjectSetOpTransposeRule is similar to FilterSetOpTransposeRule, adding ProjectSetOpTransposeRule is necessary.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8097" opendate="2017-11-17 00:00:00" fixdate="2017-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add built-in support for min/max aggregation for Date/Time</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MinWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MinAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MaxWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MaxAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.Ordering.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunctionWithRetract.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunctionWithRetract.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8130" opendate="2017-11-22 00:00:00" fixdate="2017-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Javadocs link for snapshot release is not correct</summary>
      <description>See last comments on FLINK-7702.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..config.yml</file>
    </fixedFiles>
  </bug>
  <bug id="8131" opendate="2017-11-22 00:00:00" fixdate="2017-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update to Kafka 0.11.0.2</summary>
      <description>This update fixes some critical bugs, for example: KAFKA-6119 KAFKA-6131</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8133" opendate="2017-11-22 00:00:00" fixdate="2017-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate documentation for new REST API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">docs.monitoring.rest.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="8141" opendate="2017-11-23 00:00:00" fixdate="2017-1-23 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Add AppendStreamTableSink for bucketed ORC files</summary>
      <description>It would be good to have an AppendStreamTableSink that writes to bucketed ORC files.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.JobDetailsInfoTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.JobDetailsInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobDetailsHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="8151" opendate="2017-11-26 00:00:00" fixdate="2017-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Table] Map equality check to use entrySet equality</summary>
      <description>Following up with FLINK-8038. The equality check currently is broken. Plan to support element-wise equality check by always using the base class: "java.util.Map.equals" method.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.validation.MapTypeValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.MapTypeTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.MapTypeTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.collection.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8161" opendate="2017-11-27 00:00:00" fixdate="2017-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flakey YARNSessionCapacitySchedulerITCase on Travis</summary>
      <description>The YARNSessionCapacitySchedulerITCase spuriously fails on Travis because it now contains 2017-11-25 22:49:49,204 WARN akka.remote.transport.netty.NettyTransport - Remote connection to &amp;#91;null&amp;#93; failed with java.nio.channels.NotYetConnectedException from time to time in the logs. I suspect that this is due to switching from Flakka to Akka 2.4.0. In order to solve this problem I propose to add this log statement to the whitelisted log statements.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="8175" opendate="2017-11-30 00:00:00" fixdate="2017-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove flink-streaming-contrib and migrate its classes to flink-streaming-java/scala</summary>
      <description>I propose removing flink-streaming-contrib from flink-contrib, and migrating its classes to flink-streaming-java/scala for the following reasons: flink-streaming-contrib is so small that it only has 4 classes (3 java and 1 scala), and they don't need a dedicated jar for Flink to distribute and maintain it and for users to deal with the overhead of dependency management the 4 classes in flink-streaming-contrib are logically more tied to flink-streaming-java/scala, and thus can be easily migrated flink-contrib is already too crowded and noisy. It contains lots of sub modules with different purposes which confuse developers and users, and they lack a proper project hierarchyTo take a step even forward, I would argue that even flink-contrib should be removed and all its submodules should be migrated to other top-level modules for the following reasons: 1) Apache Flink the whole project itself is a result of contributions from many developers, there's no reason to highlight some contributions in a dedicated module named 'contrib' 2) flink-contrib inherently doesn't have a good hierarchy to hold submodules</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.pom.xml</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.test.java.org.apache.flink.contrib.streaming.SocketStreamIteratorTest.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.test.java.org.apache.flink.contrib.streaming.CollectITCase.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.main.scala.org.apache.flink.contrib.streaming.scala.utils.package.scala</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.main.java.org.apache.flink.contrib.streaming.SocketStreamIterator.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.main.java.org.apache.flink.contrib.streaming.DataStreamUtils.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.src.main.java.org.apache.flink.contrib.streaming.CollectSink.java</file>
      <file type="M">flink-contrib.flink-streaming-contrib.pom.xml</file>
      <file type="M">docs.dev.datastream.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="8190" opendate="2017-12-4 00:00:00" fixdate="2017-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add extra FlinkKafkaConsumer constructors to expose pattern-based topic subscription functionality</summary>
      <description>The required internals for pattern-based topic discovery was implemented as part of FLINK-4022 (along with partition discovery). However, the functionality for pattern-based topic discovery was not yet exposed via any visible user API on the version-specific subclasses of FlinkKafkaConsumerBase.I propose to add two more constructors for this:public FlinkKafkaConsumerXX(java.util.regex.Pattern subscriptionPattern, DeserializationSchema&lt;T&gt; schema, Properties props);public FlinkKafkaConsumerXX(java.util.regex.Pattern subscriptionPattern, KeyedDeserializationSchema&lt;T&gt; schema, Properties props);This allows the consumer to pick up all matching topics on startup.To continuously pick up matching topics on the fly when they are created after the job has already started running, users should additionally set the KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS property, as they would do for partition discovery.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer08.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer010.java</file>
      <file type="M">docs.dev.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug id="8194" opendate="2017-12-4 00:00:00" fixdate="2017-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable akka.actor.warn-about-java-serializer-usage to suppress akka warnings when using the Java serializer</summary>
      <description>With Akka 2.4, Akka is logging warnings when using the Java serializer for message serialization. We should turn this off via akka.actor.warn-about-java-serializer-usage since we used Java serialization before and it is only cluttering the logs making the users worry.</description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8196" opendate="2017-12-4 00:00:00" fixdate="2017-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Hadoop Servlet Dependency Exclusion</summary>
      <description>We currently exclude the `javax.servlet` API dependency, which is unfortunately needed as a core dependency by Hadoop 2.7.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8203" opendate="2017-12-5 00:00:00" fixdate="2017-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make schema definition of DataStream/DataSet to Table conversion more flexible</summary>
      <description>When converting or registering a DataStream or DataSet as Table, the schema of the table can be defined (by default it is extracted from the TypeInformation.The schema needs to be manually specified to select (project) fields, rename fields, or define time attributes. Right now, there are several limitations how the fields can be defined that also depend on the type of the DataStream / DataSet. Types with explicit field ordering (e.g., tuples, case classes, Row) require schema definition based on the position of fields. Pojo types which have no fixed order of fields, require to refer to fields by name. Moreover, there are several restrictions on how time attributes can be defined, e.g., event time attribute must replace an existing field or be appended and proctime attributes must be appended.I think we can make the schema definition more flexible and provide two modes:1. Reference input fields by name: All fields in the schema definition are referenced by name (and possibly renamed using an alias (as). In this mode, fields can be reordered and projected out. Moreover, we can define proctime and eventtime attributes at arbitrary positions using arbitrary names (except those that existing the result schema). This mode can be used for any input type, including POJOs. This mode is used if all field references exist in the input type.2. Reference input fields by position: Field references might not refer to existing fields in the input type. In this mode, fields are simply renamed. Event-time attributes can replace the field on their position in the input data (if it is of correct type) or be appended at the end. Proctime attributes must be appended at the end. This mode can only be used if the input type has a defined field order (tuple, case class, Row).We need to add more tests the check for all combinations of input types and schema definition modes.</description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.TableEnvironmentITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.validation.TableEnvironmentValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.StreamTableEnvironmentValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.runtime.batch.table.JavaTableEnvironmentITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.sources.TableSourceUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.SortUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.schema.InlineTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.schema.FlinkTableFunctionImpl.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8215" opendate="2017-12-7 00:00:00" fixdate="2017-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support implicit type widening for array/map constructors in SQL</summary>
      <description>TableAPI goes through `LogicalNode.validate()`, which brings up the collection validation and rejects inconsistent type, this will throw `ValidationExcpetion` for something like `array(1.0, 2.0f)`.SqlAPI uses `FlinkPlannerImpl.validator(SqlNode)`, which uses calcite SqlNode validation, which supports resolving leastRestrictive type. `ARRAY&amp;#91;CAST(1 AS DOUBLE), CAST(2 AS FLOAT)&amp;#93;` throws codegen exception.Root cause is the CodeGeneration for these collection value constructors does not cast or resolve leastRestrictive type correctly. I see 2 options:1. Strengthen validation to not allow resolving leastRestrictive type on SQL.2. Making codegen support leastRestrictive type cast, such as using `generateCast` instead of direct casting like `(ClassType) element`.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.validation.MapTypeValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.validation.ArrayTypeValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.MapTypeTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ArrayTypeTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8216" opendate="2017-12-7 00:00:00" fixdate="2017-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unify test utils in flink-connector-kinesis</summary>
      <description>currently there are a few ways to get a Properties object with required fields (aws access key, aws secret key, aws region) for KinesisConsumer and KinesisProducer in unit tests. We should unify them and provide a single util to get that Properties object</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisProducerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerMigrationTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="8217" opendate="2017-12-7 00:00:00" fixdate="2017-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Properly annotate APIs of flink-connector-kinesis</summary>
      <description></description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.serialization.KinesisSerializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.serialization.KinesisDeserializationSchemaWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.serialization.KinesisDeserializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyInterface.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.GetShardListResult.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.model.StreamShardMetadata.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.model.StreamShardHandle.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.model.SequenceNumber.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.model.SentinelSequenceNumber.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.model.KinesisStreamShardState.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.model.KinesisStreamShard.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.KinesisPartitioner.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisProducer.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.ConsumerConfigConstants.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.AWSConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="8218" opendate="2017-12-7 00:00:00" fixdate="2017-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>move flink-connector-kinesis examples from /src to /test</summary>
      <description>I just saw there are runnable examples in src tree and traced back to this PR. I feel it's really bad to have runnable examples (with java main method) in a lib jar that Flink distributes.chatted with tzulitai , we agreed that we should move those examples from /src to /test</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.examples.ProduceIntoKinesis.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.examples.ConsumeFromKinesis.java</file>
    </fixedFiles>
  </bug>
  <bug id="8224" opendate="2017-12-8 00:00:00" fixdate="2017-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should shudownApplication when job terminated in job mode</summary>
      <description>For job mode， one job is an application. When job finished, it should tell the resource manager to shutdown the application, otherwise the resource manager can not set the application status. For example, if yarn resource manager don't set application as finished to yarn master, the yarn will consider the application as still running.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.TestingResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.StandaloneResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.JobClusterEntrypoint.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="8235" opendate="2017-12-11 00:00:00" fixdate="2017-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot run spotbugs for single module</summary>
      <description>When running the spotbugs plugin (-Dspotbugs) in a sub-module of Flink the build will fail because it cannot find the exclusion file.[ERROR] Could not find resource 'tools/maven/spotbugs-exclude.xml'. -&gt; [Help 1]The problem is that the configured relative path is resolved against the sub-module directory, and not the parent one.</description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8241" opendate="2017-12-11 00:00:00" fixdate="2017-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove ResultPartitionWriter related PrepareForTest annotations</summary>
      <description>With the latest refactorings around the ResultPartitionWriter we no longer have to use @PrepareForTest annotations for it.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.StreamRecordWriterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.chaining.ChainTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.chaining.ChainedAllReduceDriverTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.RecordWriterTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="8252" opendate="2017-12-13 00:00:00" fixdate="2017-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Convert network benchmarks to streaming benchmarks</summary>
      <description>We'd like to run the network benchmarks in a streaming fashion including the OutputFlusher thread.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkPointToPointBenchmark.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkBenchmarkEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.benchmark.SerializingLongReceiver.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.benchmark.ReceiverThread.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.benchmark.NetworkThroughputBenchmarkTests.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.benchmark.NetworkThroughputBenchmark.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.benchmark.NetworkBenchmarkEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.benchmark.LongRecordWriterThread.java</file>
    </fixedFiles>
  </bug>
  <bug id="8254" opendate="2017-12-13 00:00:00" fixdate="2017-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REST API documentation wonky due to shading</summary>
      <description>The REST API documentation isn't quite correct as all jackson annotations are being ignored. Our annotations come from flink-shaded-jackson, but the tool we use (jackson-module-jsonSchema) checks against vanilla jackson.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.generated.rest.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="8255" opendate="2017-12-13 00:00:00" fixdate="2017-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Key expressions on named row types do not work</summary>
      <description>The following program fails with a ClassCastException. It seems that key expressions and rows are not tested well. We should add more tests for them.final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();TypeInformation[] types = new TypeInformation[] {Types.INT, Types.INT};String[] fieldNames = new String[]{"id", "value"};RowTypeInfo rowTypeInfo = new RowTypeInfo(types, fieldNames);env.fromCollection(Collections.singleton(new Row(2)), rowTypeInfo).keyBy("id").sum("value").print();env.execute("Streaming WordCount");</description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.typeutils.FieldAccessorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.util.typeutils.FieldAccessorFactory.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.MinByOperatorTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operator.MaxByOperatorTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.UnsortedGrouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.DataSet.java</file>
    </fixedFiles>
  </bug>
  <bug id="8274" opendate="2017-12-18 00:00:00" fixdate="2017-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Java 64K method compiling limitation for CommonCalc</summary>
      <description>For complex SQL Queries, the generated code for DataStreamCalc, DataSetCalc may exceed Java's method length limitation 64kb.This issue will split long method to several sub method calls.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.utils.StreamTestData.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.utils.StreamingWithStateTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.CorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.InputFormatCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.FunctionCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CollectorCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableConfig.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8278" opendate="2017-12-18 00:00:00" fixdate="2017-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala examples in Metric documentation do not compile</summary>
      <description>The Scala examples in the Metrics documentation do not compile.The line @transient private var counter: Counterneeds to be extended to@transient private var counter: Counter = _</description>
      <version>1.3.2,1.4.0,1.5.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.metrics.md</file>
      <file type="M">docs.dev.stream.state.state.md</file>
    </fixedFiles>
  </bug>
  <bug id="8283" opendate="2017-12-18 00:00:00" fixdate="2017-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FlinkKafkaConsumerBase failing on Travis with no output in 10min</summary>
      <description>Since a few days, Travis builds with the connectors profile keep failing more often with no new output being received within 10 minutes. It seems to start with the Travis build for https://github.com/apache/flink/commit/840cbfbf0845b60dbf02dd2f37f696f1db21b1e9 but may have been introduced earlier. The printed offsets look strange though.16:33:12,508 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}16:33:12,520 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 2 will start reading 66 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=851}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=716}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=461}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=206}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=971}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=836}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=581}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=326}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=71}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=956}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=701}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=446}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=191}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=56}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=821}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=566}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=311}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=881}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=626}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=371}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=236}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=746}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=491}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=356}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=101}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=866}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=611}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=476}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=221}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=986}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=731}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=596}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=341}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=86}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=656}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=401}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=146}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=911}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=776}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=521}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=266}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=11}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=896}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=641}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=386}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=131}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=761}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=506}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=251}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=116}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=176}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=941}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=686}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=431}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=296}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=41}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=806}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=551}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=416}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=161}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=926}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=671}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=536}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=281}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=26}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=791}=-915623761775}16:33:12,580 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}16:33:12,592 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 3 will start reading 66 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=972}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=717}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=462}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=207}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=72}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=837}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=582}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=327}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=192}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=957}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=702}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=447}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=312}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=57}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=822}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=567}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=882}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=627}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=492}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=237}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=747}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=612}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=357}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=102}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=867}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=732}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=477}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=222}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=987}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=852}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=597}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=342}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=87}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=912}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=657}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=402}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=147}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=12}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=777}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=522}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=267}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=132}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=897}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=642}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=387}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=252}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=762}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=507}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=372}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=117}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=432}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=177}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=942}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=687}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=552}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=297}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=42}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=807}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=672}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=417}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=162}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=927}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=792}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=537}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=282}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=27}=-915623761775}16:33:12,653 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}16:33:12,676 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 4 will start reading 66 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=208}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=973}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=718}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=463}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=328}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=73}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=838}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=583}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=448}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=193}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=958}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=703}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=568}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=313}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=58}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=823}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=883}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=748}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=493}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=238}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=868}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=613}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=358}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=103}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=988}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=733}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=478}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=223}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=88}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=853}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=598}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=343}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=913}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=658}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=403}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=268}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=13}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=778}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=523}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=388}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=133}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=898}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=643}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=508}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=253}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=763}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=628}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=373}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=118}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=688}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=433}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=178}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=943}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=808}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=553}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=298}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=43}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=928}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=673}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=418}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=163}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=28}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=793}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=538}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=283}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=148}=-915623761775}16:33:12,769 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}16:33:12,775 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 5 will start reading 66 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=464}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=209}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=974}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=719}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=584}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=329}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=74}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=839}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=704}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=449}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=194}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=959}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=824}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=569}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=314}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=59}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=749}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=494}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=239}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=104}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=869}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=614}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=359}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=224}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=989}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=734}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=479}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=344}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=89}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=854}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=599}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=914}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=659}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=524}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=269}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=14}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=779}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=644}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=389}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=134}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=899}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=764}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=509}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=254}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=884}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=629}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=374}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=119}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=944}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=689}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=434}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=179}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=44}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=809}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=554}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=299}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=164}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=929}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=674}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=419}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=284}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=29}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=794}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=539}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=404}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=149}=-915623761775}16:33:12,852 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}16:33:12,855 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 6 will start reading 67 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=720}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=465}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=210}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=975}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=840}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=585}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=330}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=75}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=960}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=705}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=450}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=195}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=60}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=825}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=570}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=315}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=180}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=240}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=750}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=495}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=360}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=105}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=870}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=615}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=480}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=225}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=990}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=735}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=600}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=345}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=90}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=855}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=915}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=780}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=525}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=270}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=15}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=900}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=645}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=390}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=135}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=765}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=510}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=255}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=120}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=885}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=630}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=375}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=945}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=690}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=435}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=300}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=45}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=810}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=555}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=420}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=165}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=930}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=675}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=540}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=285}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=30}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=795}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=660}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=405}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=150}=-915623761775}16:33:12,968 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}16:33:12,973 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 7 will start reading 67 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=976}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=721}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=466}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=211}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=76}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=841}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=586}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=331}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=196}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=961}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=706}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=451}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=316}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=61}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=826}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=571}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=436}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=181}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=496}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=241}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=751}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=616}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=361}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=106}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=871}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=736}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=481}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=226}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=991}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=856}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=601}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=346}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=91}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=16}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=781}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=526}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=271}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=136}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=901}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=646}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=391}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=256}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=766}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=511}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=376}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=121}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=886}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=631}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=946}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=691}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=556}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=301}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=46}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=811}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=676}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=421}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=166}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=931}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=796}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=541}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=286}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=31}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=916}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=661}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=406}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=151}=-915623761775}16:33:13,037 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}16:33:13,038 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 8 will start reading 67 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=977}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=722}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=467}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=332}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=77}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=842}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=587}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=452}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=197}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=962}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=707}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=572}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=317}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=62}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=827}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=692}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=437}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=182}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=752}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=497}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=242}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=872}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=617}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=362}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=107}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=992}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=737}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=482}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=227}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=92}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=857}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=602}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=347}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=212}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=272}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=17}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=782}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=527}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=392}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=137}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=902}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=647}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=512}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=257}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=2}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=767}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=632}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=377}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=122}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=887}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=947}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=812}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=557}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=302}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=47}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=932}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=677}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=422}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=167}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=32}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=797}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=542}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=287}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=152}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=917}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=662}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=407}=-915623761775}16:33:13,110 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}16:33:13,115 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 9 will start reading 67 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=978}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=723}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=588}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=333}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=78}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=843}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=708}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=453}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=198}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=963}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=828}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=573}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=318}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=63}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=948}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=693}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=438}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=183}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=753}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=498}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=243}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=108}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=873}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=618}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=363}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=228}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=993}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=738}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=483}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=348}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=93}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=858}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=603}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=468}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=213}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=528}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=273}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=18}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=783}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=648}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=393}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=138}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=903}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=768}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=513}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=258}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=3}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=888}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=633}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=378}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=123}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=48}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=813}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=558}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=303}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=168}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=933}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=678}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=423}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=288}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=33}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=798}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=543}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=408}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=153}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=918}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=663}=-915623761775}16:33:13,186 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}16:33:13,190 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 10 will start reading 67 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=979}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=844}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=589}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=334}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=79}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=964}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=709}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=454}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=199}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=64}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=829}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=574}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=319}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=184}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=949}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=694}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=439}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=754}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=499}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=364}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=109}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=874}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=619}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=484}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=229}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=994}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=739}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=604}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=349}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=94}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=859}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=724}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=469}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=214}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=784}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=529}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=274}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=19}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=904}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=649}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=394}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=139}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=4}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=769}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=514}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=259}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=124}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=889}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=634}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=379}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=244}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=304}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=49}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=814}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=559}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=424}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=169}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=934}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=679}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=544}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=289}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=34}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=799}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=664}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=409}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=154}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=919}=-915623761775}16:33:13,255 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}16:33:13,261 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 11 will start reading 67 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=80}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=845}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=590}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=335}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=200}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=965}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=710}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=455}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=320}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=65}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=830}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=575}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=440}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=185}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=950}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=695}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=755}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=620}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=365}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=110}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=875}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=740}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=485}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=230}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=995}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=860}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=605}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=350}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=95}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=980}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=725}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=470}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=215}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=785}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=530}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=275}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=140}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=905}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=650}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=395}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=260}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=5}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=770}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=515}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=380}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=125}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=890}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=635}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=500}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=245}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=560}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=305}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=50}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=815}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=680}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=425}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=170}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=935}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=800}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=545}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=290}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=35}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=920}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=665}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=410}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=155}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=20}=-915623761775}16:33:13,316 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}16:33:13,321 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 12 will start reading 67 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=336}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=81}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=846}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=591}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=456}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=201}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=966}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=711}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=576}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=321}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=66}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=831}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=696}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=441}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=186}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=951}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=876}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=621}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=366}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=111}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=996}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=741}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=486}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=231}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=96}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=861}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=606}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=351}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=216}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=981}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=726}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=471}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=786}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=531}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=396}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=141}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=906}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=651}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=516}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=261}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=6}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=771}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=636}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=381}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=126}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=891}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=756}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=501}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=246}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=816}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=561}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=306}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=51}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=936}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=681}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=426}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=171}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=36}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=801}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=546}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=291}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=156}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=921}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=666}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=411}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=276}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=21}=-915623761775}16:33:13,385 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}16:33:13,388 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 13 will start reading 67 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=592}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=337}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=82}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=847}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=712}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=457}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=202}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=967}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=832}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=577}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=322}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=67}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=952}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=697}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=442}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=187}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=52}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=112}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=877}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=622}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=367}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=232}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=997}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=742}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=487}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=352}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=97}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=862}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=607}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=472}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=217}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=982}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=727}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=787}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=652}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=397}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=142}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=907}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=772}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=517}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=262}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=7}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=892}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=637}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=382}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=127}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=757}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=502}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=247}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=817}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=562}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=307}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=172}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=937}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=682}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=427}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=292}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=37}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=802}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=547}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=412}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=157}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=922}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=667}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=532}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=277}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=22}=-915623761775}16:33:13,449 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}16:33:13,453 INFO org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 14 will start reading 67 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=848}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=593}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=338}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=83}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=968}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=713}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=458}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=203}=-915623761775, KafkaTopicPartition{topic='tee: standard output: Resource temporarily unavailableNo output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#Build-times-out-because-no-output-was-receivedThe build has been terminatedhttps://travis-ci.org/apache/flink/jobs/316399980Some more instances: https://travis-ci.org/apache/flink/jobs/316414133 https://travis-ci.org/apache/flink/jobs/316545759 https://travis-ci.org/apache/flink/jobs/316804998</description>
      <version>1.5.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="8288" opendate="2017-12-19 00:00:00" fixdate="2017-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Register the web interface url to yarn for yarn job mode</summary>
      <description>For flip-6 job mode， the resource manager is created before the web monitor, so the web interface url is not set to resource manager, and the resource manager can not register the url to yarn.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.JobClusterEntrypoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="8301" opendate="2017-12-21 00:00:00" fixdate="2017-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Unicode in codegen for SQL &amp;&amp; TableAPI</summary>
      <description>The current code generation do not support Unicode, "\u0001" will be generated to "u0001", function call like concat(str, "\u0001") will lead to wrong result.This issue intend to handle char/varchar literal correctly, some examples followed as below.literal: '\u0001abc' -&gt; codegen: "\u0001abc"literal: '\u0022\' -&gt; codegen: "\""</description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.userDefinedScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.literals.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8323" opendate="2017-12-27 00:00:00" fixdate="2017-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Mod scala function bug</summary>
      <description>As we know mod(1514356320000,60000)=0, but currently we get `-15488` when call `MOD(1514356320000,60000)`.</description>
      <version>1.5.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.ScalarOperatorsTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8325" opendate="2017-12-28 00:00:00" fixdate="2017-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add COUNT AGG support constant parameter, i.e. COUNT(*), COUNT(1)</summary>
      <description>COUNT(1) with Group Window, always output 0. e.g.DATA:val data = List( (1L, 1, "Hi"), (2L, 2, "Hello"), (4L, 2, "Hello"), (8L, 3, "Hello world"), (16L, 3, "Hello world"))SQL:SELECT b, COUNT(1) FROM MyTable GROUP BY Hop(proctime, interval '0.001' SECOND, interval '0.002' SECOND),bOUTPUT:1,0,1, 1,0,1, 2,0,1,2,0,1, 2,0,2, 3,0,1,3,0,1</description>
      <version>1.5.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.CountAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamOverAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.CountAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8345" opendate="2018-1-2 00:00:00" fixdate="2018-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Iterate over keyed state on broadcast side of connect with broadcast.</summary>
      <description></description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.co.CoBroadcastWithKeyedOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.DataStreamTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.co.CoBroadcastWithKeyedOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.co.KeyedBroadcastProcessFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.BroadcastConnectedStream.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="8360" opendate="2018-1-4 00:00:00" fixdate="2018-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement task-local state recovery</summary>
      <description>This issue tracks the development of recovery from task-local state. The main idea is to have a secondary, local copy of the checkpointed state, while there is still a primary copy in DFS that we report to the checkpoint coordinator.Recovery can attempt to restore from the secondary local copy, if available, to save network bandwidth. This requires that the assignment from tasks to slots is as sticky is possible.For starters, we will implement this feature for all managed keyed states and can easily enhance it to all other state types (e.g. operator state) later.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamOperatorSnapshotRestoreTest.java</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.YarnTaskManager.scala</file>
      <file type="M">flink-yarn-tests.src.test.scala.org.apache.flink.yarn.TestingYarnTaskManager.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.typeserializerupgrade.PojoSerializerUpgradeTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.ManualWindowSpeedITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RescalingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.AbstractEventTimeWindowCheckpointingITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.OperatorSnapshotUtil.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TaskCheckpointingBehaviourTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTerminationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamMockEnvironment.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.RestoreStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.InterruptSensitiveRestoreTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImplTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StateInitializationContextImplTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.OperatorSnapshotFuturesTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractStreamOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperatorStateContext.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.OperatorSnapshotFutures.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBAsyncSnapshotTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingTaskManager.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.TestByteStreamStateHandleDeepCompare.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.JvmExitOnFatalErrorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.BlockerCheckpointStreamFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerStartupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerComponentsStartupShutdownTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerServicesBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.NetworkBufferCalculationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.testutils.BackendForTestStream.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestTaskStateManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestMemoryCheckpointOutputStream.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskStateManagerImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateSnapshotCompressionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.OperatorStateHandleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.OperatorStateBackendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.MultiStreamStateHandleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.MemoryStateBackendTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.HeapStateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackendSnapshotMigrationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.FsCheckpointMetadataOutputStreamTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.MockEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.testutils.DummyEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.TaskManagerMetricsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.ZooKeeperSubmittedJobGraphsStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.SchedulerTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerHARecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.savepoint.CheckpointTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointMetadataLoadingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBRocksIteratorWrapperTest.java</file>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011ITCase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CheckpointingOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigurationUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.local.LocalFileSystem.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">flink-mesos.src.main.scala.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManager.scala</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KVStateRequestSerializerRocksDBTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KvStateRequestSerializerTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServerArchiveFetcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.JobManagerTaskRestore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.OperatorSubtaskState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.RoundRobinOperatorStateRepartitioner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.Savepoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointV1Serializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointV2.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointV2Serializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.SubtaskState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.TaskStateSnapshot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.InMemorySorter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.CheckpointStreamFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.DefaultOperatorStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.DoneFuture.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.MemCheckpointStreamFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.MemoryStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.MultiStreamStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.OperatorStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.OperatorStateCheckpointOutputStream.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.OperatorStateHandle.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.Snapshotable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateSnapshotContextSynchronousImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskLocalStateStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskStateManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskStateManagerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServicesConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.minicluster.LocalFlinkMiniCluster.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8383" opendate="2018-1-6 00:00:00" fixdate="2018-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-mesos build failing: duplicate Jackson relocation in shaded jar</summary>
      <description>Example: https://travis-ci.org/apache/flink/jobs/325604587The build for flink-mesos is failing with:[ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.0.0:shade (shade-flink) on project flink-mesos_2.11: Error creating shaded jar: duplicate entry: META-INF/services/org.apache.flink.mesos.shaded.com.fasterxml.jackson.core.JsonFactory -&gt; [Help 1]Seems to be caused by https://github.com/apache/flink/commit/9ae4c5447a2f5aae2b65d5860f822d452a9d5af1.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8384" opendate="2018-1-8 00:00:00" fixdate="2018-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Session Window Assigner with Dynamic Gaps</summary>
      <description>Reason for ImprovementCurrently both Session Window assigners only allow a static inactivity gap. Given the following scenario, this is too restrictive: Given a stream of IoT events from many device types Assume each device type could have a different inactivity gap Assume each device type gap could change while sessions are in flightBy allowing dynamic inactivity gaps, the correct gap can be determined in the assignWindows function by passing the element currently under consideration, the timestamp, and the context to a user defined function. This eliminates the need to create unwieldy work arounds if you only have static gaps.Dynamic Session Window gaps should be available for both Event Time and Processing Time streams.(short preliminary discussion: https://lists.apache.org/thread.html/08a011c0039e826343e9be0b1bb4e0000cfc2e12976bc65f8a43ee88@%3Cdev.flink.apache.org%3E)</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.ProcessingTimeSessionWindowsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.EventTimeSessionWindowsTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.assigners.ProcessingTimeSessionWindows.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.assigners.EventTimeSessionWindows.java</file>
      <file type="M">docs.dev.stream.operators.windows.md</file>
    </fixedFiles>
  </bug>
  <bug id="8392" opendate="2018-1-9 00:00:00" fixdate="2018-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Simplify termination future completion</summary>
      <description>With FLINK-7754, we tried to complete the termination future after an Actor has been completely stopped and has been removed from the ActorSystem. This, however, is not possible. Furthermore, this change made it impossible that a RpcEndpoint waits for the termination of another RpcEndpoint in its RpcEndpoint#postStop method. Therefore, I propose to revert the changes done by FLINK-7754.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.FencedAkkaInvocationHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaBasedEndpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="8401" opendate="2018-1-10 00:00:00" fixdate="2018-2-10 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Allow subclass to override write-failure behavior in CassandraOutputFormat</summary>
      <description>Currently it will throw an exception and fail the entire job, we would like to keep the current default behavior, but refactor the code to allow subclass to override and customize the failure handling.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.batch.connectors.cassandra.CassandraOutputFormatBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="8404" opendate="2018-1-10 00:00:00" fixdate="2018-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mark Flip-6 tests with Flip6 annotation</summary>
      <description>After introducing the Flip6 marker interface, we should update all existing Flip-6 tests with this annotation. That way they will only be executed if the Flip-6 profile is active.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.NetworkBufferCalculationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerIdPathParameterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.TaskManagerMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.SubtaskMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.MetricsFilterParameterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.JobVertexMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.JobMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.JobManagerMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.AbstractMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.MetricsHandlerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.AbstractMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolSchedulingTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolRpcTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.DualKeyMapTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.AvailableSlotsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.AllocatedSlotsTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="8407" opendate="2018-1-10 00:00:00" fixdate="2018-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setting the parallelism after a partitioning operation should be forbidden</summary>
      <description>Partitioning operations (shuffle, rescale, etc.) for a DataStream create new DataStreams, which allow the users to set parallelisms for them. However, the PartitionTransformations in these returned DataStreams will only add virtual nodes, whose parallelisms could not be specified, in the execution graph. We should forbid users to set the parallelism after a partitioning operation since they won't actually work. Also the corresponding documents should be updated.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.DataStreamTest.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="8429" opendate="2018-1-13 00:00:00" fixdate="2018-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement stream-stream non-window right outer join</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.JoinHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.RetractionRulesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamJoin.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug id="8430" opendate="2018-1-13 00:00:00" fixdate="2018-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement stream-stream non-window full outer join</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.JoinHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.RetractionRulesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.NonWindowOuterJoinWithNonEquiPredicates.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.NonWindowOuterJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamJoin.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug id="8439" opendate="2018-1-16 00:00:00" fixdate="2018-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Flink shading to AWS credential provider s3 hadoop config</summary>
      <description>This came up when using the s3 for the file system backend and running under ECS.With no credentials in the container, hadoop-aws will default to EC2 instance level credentials when accessing S3. However when running under ECS, you will generally want to default to the task definition's IAM role.In this case you need to set the hadoop propertyfs.s3a.aws.credentials.providerto a fully qualified class name(s). see hadoop-aws docsThis works as expected when you add this setting to flink-conf.yaml but there is a further 'gotcha.'  Because the AWS sdk is shaded, the actual full class name for, in this case, the ContainerCredentialsProvider isorg.apache.flink.fs.s3hadoop.shaded.com.amazonaws.auth.ContainerCredentialsProvider meaning the full setting is:fs.s3a.aws.credentials.provider: org.apache.flink.fs.s3hadoop.shaded.com.amazonaws.auth.ContainerCredentialsProviderIf you instead set it to the unshaded class name you will see a very confusing error stating that the ContainerCredentialsProvider doesn't implement AWSCredentialsProvider (which it most certainly does.)Adding this information (how to specify alternate Credential Providers, and the name space gotcha) to the AWS deployment docs would be useful to anyone else using S3.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.java.org.apache.flink.fs.s3presto.PrestoS3FileSystemTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.java.org.apache.flink.fs.s3presto.S3FileSystemFactory.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.java.org.apache.flink.fs.s3hadoop.S3FileSystemFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="8451" opendate="2018-1-18 00:00:00" fixdate="2018-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CaseClassSerializer is not backwards compatible in 1.4</summary>
      <description>There seems to be problems with the updated Scala version and the CaseClassSerializer that make it impossible to restore from a Flink 1.3 savepoint.http://mail-archives.apache.org/mod_mbox/flink-user/201801.mbox/%3CCACk7FThV5itjSj_1fG9oaWS86z8WTKWs7abHvok6FnHzq9XT-A%40mail.gmail.com%3Ehttp://mail-archives.apache.org/mod_mbox/flink-user/201801.mbox/%3C7CABB00B-D52F-4878-B04F-201415CEB658%40mediamath.com%3E</description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>1.4.2,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.InstantiationUtil.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.runtime.TupleSerializerConfigSnapshot.java</file>
    </fixedFiles>
  </bug>
  <bug id="8456" opendate="2018-1-18 00:00:00" fixdate="2018-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Scala API for Connected Streams with Broadcast State.</summary>
      <description></description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.package.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="8475" opendate="2018-1-22 00:00:00" fixdate="2018-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move remaining sections to generated tables</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HeartbeatManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.AkkaOptions.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.io.DelimitedInputFormatSamplingTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.DelimitedInputFormat.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.configuration.MesosOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HighAvailabilityOptions.java</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.jobmanager.JobManagerFailsITCase.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerStartupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerRegistrationTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.minicluster.LocalFlinkMiniCluster.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServicesConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyConfig.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.configuration.YarnConfigOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ResourceManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CheckpointingOptions.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.cancelling.CancelingTestBase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.FileOutputFormat.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.LocalExecutor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.BlobServerOptions.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.JoinDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.AbstractCachedBuildSideJoinDriver.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plantranslate.JobGraphGenerator.java</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="8489" opendate="2018-1-23 00:00:00" fixdate="2018-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Data is not emitted by second ElasticSearch connector</summary>
      <description>A user reported this issue on the user@f.a.o mailing list.Setup: A program with two pipelines that write to ElasticSearch. The pipelines can be connected or completely separate. ElasticSearch 5.6.4, connector flink-connector-elasticsearch5_2.11Problem: Only one of the ES connectors correctly emits data. The other connector writes a single record and then stops emitting data (or does not write any data at all). The problem does not exist, if the second ES connector is replaced by a different connector (for example Cassandra).Below is a program to reproduce the issue:public class ElasticSearchTest1 { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // set elasticsearch connection details Map&lt;String, String&gt; config = new HashMap&lt;&gt;(); config.put("bulk.flush.max.actions", "1"); config.put("cluster.name", "&lt;cluster name&gt;"); List&lt;InetSocketAddress&gt; transports = new ArrayList&lt;&gt;(); transports.add(new InetSocketAddress(InetAddress.getByName("&lt;host ip&gt;"), 9300)); //Set properties for Kafka Streaming Properties properties = new Properties(); properties.setProperty("bootstrap.servers", "&lt;host ip&gt;"+":9092"); properties.setProperty("group.id", "testGroup"); properties.setProperty("auto.offset.reset", "latest"); //Create consumer for log records FlinkKafkaConsumer011 inputConsumer1 = new FlinkKafkaConsumer011&lt;&gt;("elastic_test1", new JSONDeserializationSchema(), properties); DataStream&lt;RecordOne&gt; firstStream = env .addSource(inputConsumer1) .flatMap(new CreateRecordOne()); firstStream .addSink(new ElasticsearchSink&lt;RecordOne&gt;(config, transports, new ElasticSearchOutputRecord("elastic_test_index1","elastic_test_index1"))); FlinkKafkaConsumer011 inputConsumer2 = new FlinkKafkaConsumer011&lt;&gt;("elastic_test2", new JSONDeserializationSchema(), properties); DataStream&lt;RecordTwo&gt; secondStream = env .addSource(inputConsumer2) .flatMap(new CreateRecordTwo()); secondStream .addSink(new ElasticsearchSink&lt;RecordTwo&gt;(config, transports, new ElasticSearchOutputRecord2("elastic_test_index2","elastic_test_index2"))); env.execute("Elastic Search Test"); }}public class ElasticSearchOutputRecord implements ElasticsearchSinkFunction&lt;RecordOne&gt; { String index; String type; // Initialize filter function public ElasticSearchOutputRecord(String index, String type) { this.index = index; this.type = type; } // construct index request @Override public void process( RecordOne record, RuntimeContext ctx, RequestIndexer indexer) { // construct JSON document to index Map&lt;String, String&gt; json = new HashMap&lt;&gt;(); json.put("item_one", record.item1); json.put("item_two", record.item2); IndexRequest rqst = Requests.indexRequest() .index(index) // index name .type(type) // mapping name .source(json); indexer.add(rqst); }}public class ElasticSearchOutputRecord2 implements ElasticsearchSinkFunction&lt;RecordTwo&gt; { String index; String type; // Initialize filter function public ElasticSearchOutputRecord2(String index, String type) { this.index = index; this.type = type; } // construct index request @Override public void process( RecordTwo record, RuntimeContext ctx, RequestIndexer indexer) { // construct JSON document to index Map&lt;String, String&gt; json = new HashMap&lt;&gt;(); json.put("item_three", record.item3); json.put("item_four", record.item4); IndexRequest rqst = Requests.indexRequest() .index(index) // index name .type(type) // mapping name .source(json); indexer.add(rqst); }}public class CreateRecordOne implements FlatMapFunction&lt;ObjectNode,RecordOne&gt; { static final Logger log = LoggerFactory.getLogger(CreateRecordOne.class); @Override public void flatMap(ObjectNode value, Collector&lt;RecordOne&gt; out) throws Exception { try { out.collect(new RecordOne(value.get("item1").asText(),value.get("item2").asText())); } catch(Exception e) { log.error("error while creating RecordOne", e); } }}public class CreateRecordTwo implements FlatMapFunction&lt;ObjectNode,RecordTwo&gt; { static final Logger log = LoggerFactory.getLogger(CreateRecordTwo.class); @Override public void flatMap(ObjectNode value, Collector&lt;RecordTwo&gt; out) throws Exception { try { out.collect(new RecordTwo(value.get("item1").asText(),value.get("item2").asText())); } catch(Exception e) { log.error("error while creating RecordTwo", e); } }}public class RecordOne { public String item1; public String item2; public RecordOne() {}; public RecordOne ( String item1, String item2 ) { this.item1 = item1; this.item2 = item2; } }public class RecordTwo { public String item3; public String item4; public RecordTwo() {}; public RecordTwo ( String item3, String item4 ) { this.item3 = item3; this.item4 = item4; } }</description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="8492" opendate="2018-1-23 00:00:00" fixdate="2018-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve cost estimation for Calcs</summary>
      <description>Considering the following test, unsupported exception will be thrown due to multi calc existing between correlate and TableFunctionScan.// code placeholder@Testdef testCrossJoinWithMultiFilter(): Unit = { val t = testData(env).toTable(tEnv).as('a, 'b, 'c) val func0 = new TableFunc0 val result = t .join(func0('c) as('d, 'e)) .select('c, 'd, 'e) .where('e &gt; 10) .where('e &gt; 20) .select('c, 'd) .toAppendStream[Row] result.addSink(new StreamITCase.StringSink[Row]) env.execute() val expected = mutable.MutableList("Jack#22,Jack,22", "Anna#44,Anna,44") assertEquals(expected.sorted, StreamITCase.testResults.sorted)}I can see two options to fix this problem: Adapt calcite OptRule to merge the continuous calc. Merge multi calc in correlate convert rule.I prefer the second one, not only it is easy to implement but also i think with or without an optimize rule should not influence flink functionality. </description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.CorrelateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.CalcTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.CorrelateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.CalcTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamCorrelateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetCorrelateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.CommonCalc.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8496" opendate="2018-1-23 00:00:00" fixdate="2018-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebUI does not display TM MemorySegment metrics</summary>
      <description></description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.util.MetricUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="8507" opendate="2018-1-25 00:00:00" fixdate="2018-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Calcite dependency to 1.16</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalWindowAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8518" opendate="2018-1-26 00:00:00" fixdate="2018-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support DOW, EPOCH, DECADE for EXTRACT</summary>
      <description>We upgraded Calcite to version 1.15 in FLINK-7934. The EXTRACT method supports more conversion targets. The targets DOW, EPOCH, DECADE should be implemented and tested for different datatypes.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.validation.ScalarFunctionsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ExtractCallGen.scala</file>
      <file type="M">docs.dev.table.sql.md</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.validation.ScalarFunctionsValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.expressions.symbols.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ExtractCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.typeutils.SymbolUtil.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.DateTimeUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.TimePointUnit.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.TimeIntervalUnit.java</file>
      <file type="M">docs.content.docs.dev.table.functions.systemFunctions.md</file>
      <file type="M">docs.content.zh.docs.dev.table.functions.systemFunctions.md</file>
    </fixedFiles>
  </bug>
  <bug id="8520" opendate="2018-1-29 00:00:00" fixdate="2018-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CassandraConnectorITCase.testCassandraTableSink unstable on Travis</summary>
      <description>The CassandraConnectorITCase.testCassandraTableSink fails on Travis with a timeout. https://travis-ci.org/tillrohrmann/flink/jobs/333711342</description>
      <version>1.5.0</version>
      <fixedVersion>1.4.2,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.streaming.connectors.cassandra.CassandraSinkBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="8529" opendate="2018-1-30 00:00:00" fixdate="2018-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Let Yarn entry points use YarnConfigOptions#APPLICATION_MASTER_PORT</summary>
      <description>The Yarn cluster entry points should use `YarnConfigOptions#APPLICATION_MASTER_PORT` in order to select the common RpcService port.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnSessionClusterEntrypoint.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="8548" opendate="2018-2-2 00:00:00" fixdate="2018-2-2 01:00:00" resolution="Done">
    <buginformation>
      <summary>Add Streaming State Machine Example</summary>
      <description>Add the example from https://github.com/StephanEwen/flink-demos/tree/master/streaming-state-machine to the Flink examples.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8557" opendate="2018-2-5 00:00:00" fixdate="2018-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>OperatorSubtaskDescriptionText causes failures on Windows</summary>
      <description>File-based state backends that use the description provided by OperatorSubtaskDescriptionText for file names will categorically fail on Windows as they contain characters that aren't allowed in paths "(", ")" and "/".</description>
      <version>1.5.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="8559" opendate="2018-2-5 00:00:00" fixdate="2018-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exceptions in RocksDBIncrementalSnapshotOperation#takeSnapshot cause job to get stuck</summary>
      <description>In the RocksDBKeyedStatebackend#snapshotIncrementally we can find this code final RocksDBIncrementalSnapshotOperation&lt;K&gt; snapshotOperation = new RocksDBIncrementalSnapshotOperation&lt;&gt;( this, checkpointStreamFactory, checkpointId, checkpointTimestamp);snapshotOperation.takeSnapshot();return new FutureTask&lt;KeyedStateHandle&gt;( new Callable&lt;KeyedStateHandle&gt;() { @Override public KeyedStateHandle call() throws Exception { return snapshotOperation.materializeSnapshot(); } }) { @Override public boolean cancel(boolean mayInterruptIfRunning) { snapshotOperation.stop(); return super.cancel(mayInterruptIfRunning); } @Override protected void done() { snapshotOperation.releaseResources(isCancelled()); }};In the constructor of RocksDBIncrementalSnapshotOperation we call aquireResource() on the RocksDB ResourceGuard. If snapshotOperation.takeSnapshot() fails with an exception these resources are never released. When the task is shutdown due to the exception it will get stuck on releasing RocksDB.</description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="8563" opendate="2018-2-6 00:00:00" fixdate="2018-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support consecutive DOT operators</summary>
      <description>We added support for accessing fields of arrays of composite types in FLINK-7923. However, accessing another nested subfield is not supported by Calcite. See CALCITE-2162. We should fix this once we upgrade to Calcite 1.16.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.CompositeTypeTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.CompositeAccessTest.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8573" opendate="2018-2-7 00:00:00" fixdate="2018-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Print JobID for failed jobs</summary>
      <description>When a job is successfully run the client prints a something along the lines of "Job with &lt;JobID&gt; successfully run". If the job fails however we only print the exception but not the JobID.</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.RemoteStreamEnvironment.java</file>
      <file type="M">flink-scala-shell.src.main.java.org.apache.flink.api.java.ScalaShellRemoteStreamEnvironment.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.legacy.JarActionHandler.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ProgramInvocationException.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgramUtils.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgram.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.MiniClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ClusterClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="8576" opendate="2018-2-7 00:00:00" fixdate="2018-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log message for QueryableState loading failure too verbose</summary>
      <description>Whenever a job- or taskmanager is started it attempts to load the queryable state via reflection. If this fails due to the classes not being in the classpath (which is common and the default path) we log the full stacktrace as DEBUG.We should reduce this to a single line as it get's really verbose when sifting through debug logs.</description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>1.4.2,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.query.QueryableStateUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="8591" opendate="2018-2-7 00:00:00" fixdate="2018-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle unfinished BufferConsumers in subpartitions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.StreamRecordWriterTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.StreamTestSingleInputGate.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamRecordWriter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.RecordWriterOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.util.TestSubpartitionConsumer.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SubpartitionTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartialConsumePipelinedResultTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.TestInputChannel.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.IteratorWrappingTestSingleInputGate.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.ServerTransportErrorHandlingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestQueueTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.CancelPartitionRequestTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.BufferBuilderTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.RecordWriterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.AbstractCollectingResultPartitionWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.shipping.OutputCollector.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpilledSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.UnknownInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.InputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BufferAvailabilityListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.SequenceNumberingViewReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.CreditBasedSequenceNumberingViewReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.RecordWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="8593" opendate="2018-2-7 00:00:00" fixdate="2018-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Latency metric docs are outdated</summary>
      <description>I missed to update the latency metric documentation while working on FLINK-7608. The docs should be updated to contain the new naming scheme and that it is a job-level metric.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="8595" opendate="2018-2-7 00:00:00" fixdate="2018-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include table source factory services in flink-table jar</summary>
      <description>The flink-table jar does not include the table source factory services.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8634" opendate="2018-2-11 00:00:00" fixdate="2018-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement asynchronous rescaling REST handlers</summary>
      <description>In order to trigger the job rescaling we should offer REST handlers which expose the rescaling functionality. Since this is an asynchronous operation, the handlers should be implemented by extending AbstractAsynchronousOperationHandlers.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers.java</file>
    </fixedFiles>
  </bug>
  <bug id="8640" opendate="2018-2-12 00:00:00" fixdate="2018-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Japicmp fails on java 9</summary>
      <description>The japicmp plugin does not work out-of-the-box with java 9 as per https://github.com/siom79/japicmp/issues/177.We have to add the following to the plugins dependency section: &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;javax.xml.bind&lt;/groupId&gt; &lt;artifactId&gt;jaxb-api&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.sun.xml.bind&lt;/groupId&gt; &lt;artifactId&gt;jaxb-core&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.sun.xml.bind&lt;/groupId&gt; &lt;artifactId&gt;jaxb-impl&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.activation&lt;/groupId&gt; &lt;artifactId&gt;javax.activation-api&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;</description>
      <version>1.5.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.travis.yml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8642" opendate="2018-2-13 00:00:00" fixdate="2018-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Initialize descriptors before use at getBroadcastState().</summary>
      <description></description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.BroadcastStateITCase.java</file>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.BroadcastStateITCase.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.co.CoBroadcastWithNonKeyedOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.co.CoBroadcastWithKeyedOperator.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.broadcast.BroadcastExample.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8644" opendate="2018-2-13 00:00:00" fixdate="2018-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shut down AkkaRpcActors with PoisonPill</summary>
      <description>In order to ensure that all messages get processed before shutting down an AkkaRpcActor we should stop it by sending a PoisonPill. Otherwise we risk that we have some dangling futures which will time out.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcService.java</file>
    </fixedFiles>
  </bug>
  <bug id="8648" opendate="2018-2-13 00:00:00" fixdate="2018-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow for customization of emitRecordAndUpdateState in Kinesis connector</summary>
      <description>It should be possible to override the method to intercept the emit behavior, in this case for the purpose of custom watermark support. </description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="8659" opendate="2018-2-14 00:00:00" fixdate="2018-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add migration tests for Broadcast state.</summary>
      <description></description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.StatefulJobSavepointMigrationITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.SavepointMigrationTestBase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.migration.MigrationVersion.java</file>
    </fixedFiles>
  </bug>
  <bug id="8661" opendate="2018-2-15 00:00:00" fixdate="2018-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Collections.EMPTY_MAP with Collections.emptyMap()</summary>
      <description>The use of Collections.EMPTY_SET and Collections.EMPTY_MAP often causes unchecked assignment. It should be replaced with Collections.emptySet() and Collections.emptyMap() .</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.UtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobAccumulatorsInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.ResourceProfileTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.ResourceProfile.java</file>
    </fixedFiles>
  </bug>
  <bug id="8674" opendate="2018-2-16 00:00:00" fixdate="2018-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Efficiently handle alwaysFlush case (0ms flushTimeout)</summary>
      <description>Changes in data notifications introduced alongside BufferConsumer significantly degraded performance of 0ms flushTimeout. Previously flushing after writing only record, effectively triggered only one data notification for the sub-partition/channel to which this record was written. With low latency improvements, this changed and flush is now triggering data notifications for all of the partitions. This can (and should) be easily fixed.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.NetworkStackThroughputITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkPointToPointBenchmark.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.benchmark.LongRecordWriterThread.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamRecordWriter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.RecordWriterOutput.java</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.Tasks.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskCancelAsyncProducerConsumerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.SlotCountExceedingParallelismTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.ScheduleOrUpdateConsumersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.RecordWriterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.writer.AbstractCollectingResultPartitionWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.shipping.OutputCollector.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.RecordWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="8679" opendate="2018-2-16 00:00:00" fixdate="2018-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RocksDBKeyedBackend.getKeys(stateName, namespace) doesn&amp;#39;t filter data with namespace</summary>
      <description>Currently, `RocksDBKeyedBackend.getKeys(stateName, namespace)` is odds. It doesn't use the namespace to filter data. And `HeapKeyedBackend.getKeys(stateName, namespace)` has done that, I think they should be consistent at least.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.AbstractRocksDBState.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="8689" opendate="2018-2-18 00:00:00" fixdate="2018-4-18 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add runtime support of distinct filter using MapView</summary>
      <description>This ticket should cover distinct aggregate function support to codegen for AggregateCall, where isDistinct fields is set to true.This can be verified using the following SQL, which is not currently producing correct results.SELECT a, SUM(b) OVER (PARTITION BY a ORDER BY proctime ROWS BETWEEN 5 PRECEDING AND CURRENT ROW)FROM MyTable  </description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.OverWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.OverAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8692" opendate="2018-2-18 00:00:00" fixdate="2018-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mistake in MyMapFunction code snippet</summary>
      <description>The MyMapFunction code snippet on the Basic API Concepts page has an extra parenthesis. Just remove the last parenthesis. See the attached screenshot. Thanks. </description>
      <version>1.4.1,1.5.0</version>
      <fixedVersion>1.4.2,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.api.concepts.md</file>
    </fixedFiles>
  </bug>
  <bug id="8695" opendate="2018-2-19 00:00:00" fixdate="2018-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move RocksDB State Backend from &amp;#39;flink-contrib&amp;#39; to &amp;#39;flink-state-backends&amp;#39;</summary>
      <description>Having the RocksDB State Backend in flink-contrib is a bit of an understatement...</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-contrib.pom.xml</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactoryTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDbMultiClassLoaderTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBMergeIteratorTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBInitResetTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBAsyncSnapshotTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.benchmark.RocksDBPerformanceTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.benchmark.RocksDBListStatePerformanceTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBValueState.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBReducingState.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMapState.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBListState.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBFoldingState.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBAggregatingState.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.PredefinedOptions.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.OptionsFactory.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.AbstractRocksDBState.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8696" opendate="2018-2-19 00:00:00" fixdate="2018-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove JobManager local mode from the Unix Shell Scripts</summary>
      <description>In order to work towards removing the local JobManager mode, the shell scripts need to be changed to not use/assume that mode any more</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">test-infra.end-to-end-test.common.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.stop-local.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-local.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-cluster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.jobmanager.sh</file>
    </fixedFiles>
  </bug>
  <bug id="8699" opendate="2018-2-19 00:00:00" fixdate="2018-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix concurrency problem in rocksdb full checkpoint</summary>
      <description>In full checkpoint, `kvStateInformation` is not a copied object and it can be changed when writeKVStateMetaData() is invoking ... This can lead to problematic, which is serious.private void writeKVStateMetaData() throws IOException { // ... for (Map.Entry&lt;String, Tuple2&lt;ColumnFamilyHandle, RegisteredKeyedBackendStateMetaInfo&lt;?, ?&gt;&gt;&gt; column : stateBackend.kvStateInformation.entrySet()) { } //...}</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="8704" opendate="2018-2-19 00:00:00" fixdate="2018-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate tests from TestingCluster to MiniClusterResource</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartialConsumePipelinedResultTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.ScheduleOrUpdateConsumersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.SlotCountExceedingParallelismTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.classloading.ClassLoaderITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="8705" opendate="2018-2-19 00:00:00" fixdate="2018-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate Remote(Stream)Environment with Flip-6 cluster</summary>
      <description>Allow the Remote(Stream)Environment to submit jobs to a Flip-6 cluster. This entails that we create the correct ClusterClient to communicate with the respective cluster.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.TaskManagerProcessFailureStreamingRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.TaskManagerProcessFailureBatchRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAProcessFailureBatchRecoveryITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.RemoteStreamEnvironment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.operators.RemoteEnvironmentITCase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.MiniClusterResource.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.Flip6LocalStreamEnvironment.java</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellLocalStartupITCase.scala</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.MiniClusterITCase.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.minicluster.FlinkMiniCluster.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.StandaloneMiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterJobDispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.JobExecutorService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobResult.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.RemoteExecutor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.LocalExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="8723" opendate="2018-2-21 00:00:00" fixdate="2018-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove existing BroadcastState examples.</summary>
      <description></description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.broadcast.BroadcastExample.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.broadcast.BroadcastExample.java</file>
    </fixedFiles>
  </bug>
  <bug id="8731" opendate="2018-2-21 00:00:00" fixdate="2018-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TwoInputStreamTaskTest flaky on travis</summary>
      <description>https://travis-ci.org/zentol/flink/builds/344307861Tests run: 5, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.479 sec &lt;&lt;&lt; FAILURE! - in org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTesttestOpenCloseAndTimestamps(org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest) Time elapsed: 0.05 sec &lt;&lt;&lt; ERROR!java.lang.Exception: error in task at org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.waitForTaskCompletion(StreamTaskTestHarness.java:250) at org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.waitForTaskCompletion(StreamTaskTestHarness.java:233) at org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.testOpenCloseAndTimestamps(TwoInputStreamTaskTest.java:99)Caused by: org.mockito.exceptions.misusing.WrongTypeOfReturnValue: Boolean cannot be returned by getChannelIndex()getChannelIndex() should return int***If you're unsure why you're getting above error read on.Due to the nature of the syntax above problem might occur because:1. This exception *might* occur in wrongly written multi-threaded tests. Please refer to Mockito FAQ on limitations of concurrency testing.2. A spy is stubbed using when(spy.foo()).then() syntax. It is safer to stub spies - - with doReturn|Throw() family of methods. More in javadocs for Mockito.spy() method. at org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.waitAndGetNextInputGate(UnionInputGate.java:212) at org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.getNextBufferOrEvent(UnionInputGate.java:158) at org.apache.flink.streaming.runtime.io.BarrierBuffer.getNextNonBlocked(BarrierBuffer.java:164) at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processInput(StreamTwoInputProcessor.java:292) at org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.run(TwoInputStreamTask.java:115) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:308) at org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness$TaskThread.run(StreamTaskTestHarness.java:437)</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.StreamTestSingleInputGate.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.UnionInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.TestSingleInputGate.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.TestInputChannel.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.IteratorWrappingTestSingleInputGate.java</file>
    </fixedFiles>
  </bug>
  <bug id="8736" opendate="2018-2-21 00:00:00" fixdate="2018-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Memory segment offsets for slices of slices are wrong</summary>
      <description>FLINK-8588 introduced memory segment offsets but the offsets of slices of slices do not account for their parent's slice offset.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.ReadOnlySlicedBufferTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.ReadOnlySlicedNetworkBuffer.java</file>
    </fixedFiles>
  </bug>
  <bug id="8738" opendate="2018-2-21 00:00:00" fixdate="2018-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Converge runtime dependency versions for &amp;#39;scala-lang&amp;#39; and for &amp;#39;com.typesafe:config&amp;#39;</summary>
      <description>These dependencies are currently diverged:Dependency convergence error for com.typesafe:config:1.3.0 paths to dependency are:+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-actor_2.11:2.4.20 +-com.typesafe:config:1.3.0and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-stream_2.11:2.4.20 +-com.typesafe:ssl-config-core_2.11:0.2.1 +-com.typesafe:config:1.2.0andDependency convergence error for org.scala-lang:scala-library:2.11.12 paths to dependency are:+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-org.scala-lang:scala-library:2.11.12and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-actor_2.11:2.4.20 +-org.scala-lang:scala-library:2.11.11and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-actor_2.11:2.4.20 +-org.scala-lang.modules:scala-java8-compat_2.11:0.7.0 +-org.scala-lang:scala-library:2.11.7and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-stream_2.11:2.4.20 +-org.scala-lang:scala-library:2.11.11and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-stream_2.11:2.4.20 +-com.typesafe:ssl-config-core_2.11:0.2.1 +-org.scala-lang:scala-library:2.11.8and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-stream_2.11:2.4.20 +-com.typesafe:ssl-config-core_2.11:0.2.1 +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 +-org.scala-lang:scala-library:2.11.6and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-protobuf_2.11:2.4.20 +-org.scala-lang:scala-library:2.11.11and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-slf4j_2.11:2.4.20 +-org.scala-lang:scala-library:2.11.11and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-org.clapper:grizzled-slf4j_2.11:1.0.2 +-org.scala-lang:scala-library:2.11.0and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.twitter:chill_2.11:0.7.4 +-org.scala-lang:scala-library:2.11.7</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8739" opendate="2018-2-21 00:00:00" fixdate="2018-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize runtime support for distinct filter</summary>
      <description>Possible optimizaitons:1. Decouple distinct map and actual accumulator so that they can separately be created in codegen.2. Reuse same distinct accumulator for filtering, e.g. `SELECT COUNT(DISTINCT(a)), SUM(DISTINCT(a))` should reuse the same distinct map.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.GroupAggregateHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.runtime.utils.JavaUserDefinedAggFunctions.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.DistinctAccumulator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.MatchCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8741" opendate="2018-2-22 00:00:00" fixdate="2018-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaFetcher09/010/011 uses wrong user code classloader</summary>
      <description>This commit https://github.com/apache/flink/commit/0a1ce0060ef3af29b196ab6ad58f97e49a40a4cf#diff-51fb939365cf758a89794a2599344702R98 caused the wrong classloader to be used.The user code classloader should be used directly, not its parent. That change seems to be irrelevant to the issue, and seems to have been changed by accident.</description>
      <version>1.4.1,1.5.0</version>
      <fixedVersion>1.4.2,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test-infra.end-to-end-test.test.streaming.kafka010.sh</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.kafka.Kafka010Example.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="8746" opendate="2018-2-22 00:00:00" fixdate="2018-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support rescaling of jobs which are not fully running</summary>
      <description>We should support the rescaling of jobs which are only partially running. Currently, this fails because rescaling requires to take a savepoint. We can solve the problem by falling back to the latest rescaling savepoint.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.JobManagerITCase.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="8750" opendate="2018-2-22 00:00:00" fixdate="2018-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>InputGate may contain data after an EndOfPartitionEvent</summary>
      <description>The travis run at https://travis-ci.org/apache/flink/jobs/344425772 indicates that there was still some data after an EndOfPartitionEvent or that BufferOrEvent#moreAvailable contained the wrong value:testOutputWithoutPk(org.apache.flink.table.runtime.stream.table.JoinITCase) Time elapsed: 4.611 sec &lt;&lt;&lt; ERROR!org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$6.apply$mcV$sp(JobManager.scala:891) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$6.apply(JobManager.scala:834) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$6.apply(JobManager.scala:834) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.IllegalStateException: null at org.apache.flink.util.Preconditions.checkState(Preconditions.java:179) at org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.getNextBufferOrEvent(UnionInputGate.java:173) at org.apache.flink.streaming.runtime.io.BarrierTracker.getNextNonBlocked(BarrierTracker.java:94) at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processInput(StreamTwoInputProcessor.java:292) at org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.run(TwoInputStreamTask.java:115) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:308) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:703) at java.lang.Thread.run(Thread.java:748)</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
    </fixedFiles>
  </bug>
  <bug id="8759" opendate="2018-2-23 00:00:00" fixdate="2018-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prepare LengthFieldBasedFrameDecoder for Netty upgrade</summary>
      <description>For a bug in Netty's shutdown sequence and overall improvements in Netty, I'd like to bump the version (and stay within the 4.0 series for now). The problem we faced in the past should not be relevant for credit-based flow control anymore and can be worked around (for the fallback code path) by restoring LengthFieldBasedFrameDecoder's old behaviour of copying contents to new buffers instead of slicing the existing one (please refer to FLINK-7428 for the inverse direction).</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyMessageSerializationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyProtocol.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyMessage.java</file>
    </fixedFiles>
  </bug>
  <bug id="8765" opendate="2018-2-23 00:00:00" fixdate="2018-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Simplify quickstart properties</summary>
      <description>This does not pull out the slf4j and log4j version into properties any more, making the quickstarts a bit simpler.Given that both versions are used only once, and only for the feature to have convenience logging in the IDE, the versions might as well be defined directly in the dependencies.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8766" opendate="2018-2-23 00:00:00" fixdate="2018-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pin scala runtime version for Java Quickstart</summary>
      <description>Followup to FLINK-7414, which pinned the scala version for the Scala Quickstart</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8767" opendate="2018-2-23 00:00:00" fixdate="2018-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set the maven.compiler.source and .target properties for Java Quickstart</summary>
      <description>Setting these properties helps properly pinning the Java version in IntelliJ.Without these properties, Java version keeps switching back to 1.5 in some setups.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8771" opendate="2018-2-23 00:00:00" fixdate="2018-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade scalastyle to 1.0.0</summary>
      <description>scalastyle 1.0.0 fixes issue with import order, explicit type for public methods, line length limitation and comment validation.Also a few scala class header is not correctly formatted before. scalastyle 1.0.0 detected that. We should upgrade to scalastyle 1.0.0</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.taskmanager.TaskManagerFailsITCase.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.TaskManagerFailsWithSlotSharingITCase.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.SlotSharingITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8778" opendate="2018-2-26 00:00:00" fixdate="2018-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate queryable state ITCases to use MiniClusterResource</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.NonHAQueryableStateRocksDBBackendITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.NonHAQueryableStateFsBackendITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.NonHAAbstractQueryableStateTestBase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.HAQueryableStateRocksDBBackendITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.HAQueryableStateFsBackendITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.HAAbstractQueryableStateTestBase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.AbstractQueryableStateTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="8780" opendate="2018-2-26 00:00:00" fixdate="2018-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Broadcast State documentation.</summary>
      <description></description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.state.queryable.state.md</file>
      <file type="M">docs.dev.stream.state.index.md</file>
      <file type="M">docs.dev.stream.state.custom.serialization.md</file>
      <file type="M">docs.dev.stream.state.checkpointing.md</file>
    </fixedFiles>
  </bug>
  <bug id="8781" opendate="2018-2-26 00:00:00" fixdate="2018-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Try to reschedule failed tasks to previous allocation</summary>
      <description>For local recovery, we need to enhance the scheduler so that it always tries to reschedule tasks to their previous location under failures. In particular, tasks that lost their allocation should not take away the previous allocations from other tasks.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolSlotSharingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolRpcTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolCoLocationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.AvailableSlotsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.ScheduleWithCoLocationHintTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.SchedulerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.SchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.SchedulerSlotSharingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.SchedulerIsolatedTasksTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.utils.SimpleSlotProvider.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ProgrammedSlotProvider.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionVertexSchedulingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphMetricsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPool.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.slots.SlotAndLocality.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.scheduler.Scheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
    </fixedFiles>
  </bug>
  <bug id="8787" opendate="2018-2-26 00:00:00" fixdate="2018-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deploying FLIP-6 YARN session with HA fails</summary>
      <description>Starting a YARN session with HA in FLIP-6 mode fails with an exception.Commit: 5e3fa4403f518dd6d3fe9970fe8ca55871add7c9Command to start YARN session:export HADOOP_CLASSPATH=`hadoop classpath`HADOOP_CONF_DIR=/etc/hadoop/conf bin/yarn-session.sh -d -n 1 -s 1 -jm 2048 -tm 2048Stacktrace:java.lang.reflect.UndeclaredThrowableException at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1854) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.yarn.cli.FlinkYarnSessionCli.main(FlinkYarnSessionCli.java:790)Caused by: org.apache.flink.util.FlinkException: Could not write the Yarn connection information. at org.apache.flink.yarn.cli.FlinkYarnSessionCli.run(FlinkYarnSessionCli.java:612) at org.apache.flink.yarn.cli.FlinkYarnSessionCli.lambda$main$2(FlinkYarnSessionCli.java:790) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) ... 2 moreCaused by: org.apache.flink.runtime.leaderretrieval.LeaderRetrievalException: Could not retrieve the leader address and leader session ID. at org.apache.flink.runtime.util.LeaderRetrievalUtils.retrieveLeaderConnectionInfo(LeaderRetrievalUtils.java:116) at org.apache.flink.client.program.rest.RestClusterClient.getClusterConnectionInfo(RestClusterClient.java:405) at org.apache.flink.yarn.cli.FlinkYarnSessionCli.run(FlinkYarnSessionCli.java:589) ... 6 moreCaused by: java.util.concurrent.TimeoutException: Futures timed out after [60000 milliseconds] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:223) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:227) at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190) at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53) at scala.concurrent.Await$.result(package.scala:190) at scala.concurrent.Await.result(package.scala) at org.apache.flink.runtime.util.LeaderRetrievalUtils.retrieveLeaderConnectionInfo(LeaderRetrievalUtils.java:114) ... 8 more------------------------------------------------------------ The program finished with the following exception:java.lang.reflect.UndeclaredThrowableException at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1854) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.yarn.cli.FlinkYarnSessionCli.main(FlinkYarnSessionCli.java:790)Caused by: org.apache.flink.util.FlinkException: Could not write the Yarn connection information. at org.apache.flink.yarn.cli.FlinkYarnSessionCli.run(FlinkYarnSessionCli.java:612) at org.apache.flink.yarn.cli.FlinkYarnSessionCli.lambda$main$2(FlinkYarnSessionCli.java:790) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) ... 2 moreCaused by: org.apache.flink.runtime.leaderretrieval.LeaderRetrievalException: Could not retrieve the leader address and leader session ID. at org.apache.flink.runtime.util.LeaderRetrievalUtils.retrieveLeaderConnectionInfo(LeaderRetrievalUtils.java:116) at org.apache.flink.client.program.rest.RestClusterClient.getClusterConnectionInfo(RestClusterClient.java:405) at org.apache.flink.yarn.cli.FlinkYarnSessionCli.run(FlinkYarnSessionCli.java:589) ... 6 moreCaused by: java.util.concurrent.TimeoutException: Futures timed out after [60000 milliseconds] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:223) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:227) at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190) at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53) at scala.concurrent.Await$.result(package.scala:190) at scala.concurrent.Await.result(package.scala) at org.apache.flink.runtime.util.LeaderRetrievalUtils.retrieveLeaderConnectionInfo(LeaderRetrievalUtils.java:114) ... 8 more</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="8790" opendate="2018-2-26 00:00:00" fixdate="2018-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve performance for recovery from incremental checkpoint</summary>
      <description>When there are multi state handle to be restored, we can improve the performance as follow:1. Choose the best state handle to init the target db2. Use the other state handles to create temp db, and clip the db according to the target key group range (via rocksdb.deleteRange()), this can help use get rid of the `key group check` in `data insertion loop` and also help us get rid of traversing the useless record.</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBWriteBatchWrapper.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBReducingState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBListState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeySerializationUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBAggregatingState.java</file>
    </fixedFiles>
  </bug>
  <bug id="8791" opendate="2018-2-26 00:00:00" fixdate="2018-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix documentation on how to link dependencies</summary>
      <description>The documentation in "Linking with Flink" and "Linking with Optional Dependencies" is very outdated and gives wrong advise to users.</description>
      <version>None</version>
      <fixedVersion>1.4.2,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.linking.with.flink.md</file>
      <file type="M">docs.dev.linking.md</file>
    </fixedFiles>
  </bug>
  <bug id="8795" opendate="2018-2-27 00:00:00" fixdate="2018-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala shell broken for Flip6</summary>
      <description>I am trying to run the simple code below after building everything from Flink's github master branch for various reasons. I get an exception below and I wonder what runs on port 9065? and How to fix this exception?I followed the instructions from the Flink master branch so I did the following.git clone https://github.com/apache/flink.git cd flink mvn clean package -DskipTests cd build-target ./bin/start-scala-shell.sh localAnd Here is the code I ranval dataStream = senv.fromElements(1, 2, 3, 4)dataStream.countWindowAll(2).sum(0).print()senv.execute("My streaming program")And I finally get this exceptionCaused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph. at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$18(RestClusterClient.java:306) at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870) at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:852) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$222(RestClient.java:196) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:603) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:563) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:268) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:284) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137) at java.lang.Thread.run(Thread.java:745) Caused by: java.util.concurrent.CompletionException: java.net.ConnectException: Connection refused: localhost/127.0.0.1:9065 at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:943) at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926) ... 16 more Caused by: java.net.ConnectException: Connection refused: localhost/127.0.0.1:9065 at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:281) </description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellLocalStartupITCase.scala</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8800" opendate="2018-2-27 00:00:00" fixdate="2018-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set Logging to TRACE for org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler</summary>
      <description>When setting the log level to DEBUG, the logs are swamped with statements as below, making it hard to read the debug logs.2018-02-22 13:41:04,016 DEBUG org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler - Received request /jobs/ec1c9d7a3c413a9523656efa58735009/vertices/ded95c643b42f31cf882a8986207fd30/metrics?get=0.currentLowWatermark.2018-02-22 13:41:04,048 DEBUG org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler - Received request /jobs/ec1c9d7a3c413a9523656efa58735009/vertices/eec5890dac9c38f66954443809beb5b0/metrics?get=0.currentLowWatermark.2018-02-22 13:41:04,052 DEBUG org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler - Received request /jobs/ec1c9d7a3c413a9523656efa58735009/vertices/2a964ee72788c82cb7d15e352d9a94f6/metrics?get=0.currentLowWatermark.2018-02-22 13:41:04,079 DEBUG org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler - Received request /jobs/ec1c9d7a3c413a9523656efa58735009/vertices/1d9c83f6e1879fdbe461aafac16eb8a5/metrics?get=0.currentLowWatermark.2018-02-22 13:41:04,085 DEBUG org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler - Received request /jobs/ec1c9d7a3c413a9523656efa58735009/vertices/4063620891a151092c5bcedb218870a6/metrics?get=0.currentLowWatermark.2018-02-22 13:41:04,094 DEBUG org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler - Received request /jobs/ec1c9d7a3c413a9523656efa58735009/vertices/2a751c66e0e32aee2cd8120a1a72a4d6/metrics?get=0.currentLowWatermark.2018-02-22 13:41:04,142 DEBUG org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler - Received request /jobs/ec1c9d7a3c413a9523656efa58735009/vertices/37ecc85b429bd08d0fd539532055e117/metrics?get=0.currentLowWatermark.2018-02-22 13:41:04,173 DEBUG org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler - Received request /jobs/ec1c9d7a3c413a9523656efa58735009/vertices/20e20298680571979f690d36d1a6db36/metrics?get=0.currentLowWatermark.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.AbstractHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="8802" opendate="2018-2-28 00:00:00" fixdate="2018-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Concurrent serialization without duplicating serializers in state server.</summary>
      <description>The `getSerializedValue()` may be called by multiple threads but serializers are not duplicated, which may lead to exceptions thrown when a serializer is stateful.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.TriggerTestHarness.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.EvictingWindowOperator.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBValueState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBReducingState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMapState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBListState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBFoldingState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBAggregatingState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.AbstractRocksDBState.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateSnapshotCompressionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackendSnapshotMigrationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.query.KvStateRegistryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.internal.InternalValueState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.internal.InternalReducingState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.internal.InternalMergingState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.internal.InternalMapState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.internal.InternalListState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.internal.InternalKvState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.internal.InternalFoldingState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.internal.InternalAppendingState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.internal.InternalAggregatingState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapValueState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapReducingState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapMapState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapListState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapFoldingState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapAggregatingState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.AbstractHeapState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.AbstractHeapMergingState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.query.TaskKvStateRegistry.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.query.KvStateRegistry.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KvStateServerHandlerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KvStateRequestSerializerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KVStateRequestSerializerRocksDBTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.ClientTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.main.java.org.apache.flink.queryablestate.server.KvStateServerHandler.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.Client.java</file>
    </fixedFiles>
  </bug>
  <bug id="8804" opendate="2018-2-28 00:00:00" fixdate="2018-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump flink-shaded-jackson dependency to 3.0</summary>
      <description></description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.ConfigUtil.java</file>
      <file type="M">flink-libraries.flink-sql-client.pom.xml</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8810" opendate="2018-2-28 00:00:00" fixdate="2018-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move end-to-end test scripts to end-to-end module</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">test-infra.end-to-end-test.test.streaming.python.wordcount.sh</file>
      <file type="M">test-infra.end-to-end-test.test.streaming.kafka010.sh</file>
      <file type="M">test-infra.end-to-end-test.test.streaming.classloader.sh</file>
      <file type="M">test-infra.end-to-end-test.test.shaded.presto.s3.sh</file>
      <file type="M">test-infra.end-to-end-test.test.shaded.hadoop.s3a.sh</file>
      <file type="M">test-infra.end-to-end-test.test.hadoop.free.sh</file>
      <file type="M">test-infra.end-to-end-test.test.batch.wordcount.sh</file>
      <file type="M">test-infra.end-to-end-test.test-data.words</file>
      <file type="M">test-infra.end-to-end-test.common.sh</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8812" opendate="2018-2-28 00:00:00" fixdate="2018-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Possible resource leak in Flip6</summary>
      <description>In this build (https://travis-ci.org/zentol/flink/builds/347373839) I set the codebase to flip6 for half the profiles to find failing tests.The "libraries" job (https://travis-ci.org/zentol/flink/jobs/347373851) failed with an OutOfMemoryError.This could mean that there is a memory-leak somewhere.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.MiniClusterResource.java</file>
    </fixedFiles>
  </bug>
  <bug id="8824" opendate="2018-3-1 00:00:00" fixdate="2018-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>In Kafka Consumers, replace &amp;#39;getCanonicalName()&amp;#39; with &amp;#39;getClassName()&amp;#39;</summary>
      <description>The connector uses getCanonicalName() in all places, gather than getClassName().getCanonicalName()'s intention is to normalize class names for arrays, etc, but is problematic when instantiating classes from class names.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011.java</file>
    </fixedFiles>
  </bug>
  <bug id="8826" opendate="2018-3-1 00:00:00" fixdate="2018-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>In Flip6 mode, when starting yarn cluster, configured taskmanager.heap.mb is ignored</summary>
      <description>When I tried running some job on the cluster, despite setting taskmanager.heap.mb = 3072taskmanager.network.memory.fraction: 0.4and reported in the consoleCluster specification: ClusterSpecification{masterMemoryMB=768, taskManagerMemoryMB=3072, numberTaskManagers=92, slotsPerTaskManager=1}The actual settings were: 2018-03-01 14:53:18,918 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                  - --------------------------------------------------------------------------------2018-03-01 14:53:18,921 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                  -  Starting YARN TaskExecutor runner (Version: 1.5-SNAPSHOT, Rev:e92eb39, Date:28.02.2018 @ 17:43:39 UTC)2018-03-01 14:53:18,921 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                  -  OS current user: yarn2018-03-01 14:53:19,780 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                  -  Current Hadoop/Kerberos user: hadoop2018-03-01 14:53:19,781 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                  -  JVM: OpenJDK 64-Bit Server VM - Oracle Corporation - 1.8/25.161-b142018-03-01 14:53:19,781 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                  -  Maximum heap size: 245 MiBytes2018-03-01 14:53:19,781 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                  -  JAVA_HOME: /usr/lib/jvm/java-openjdk2018-03-01 14:53:19,783 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                  -  Hadoop version: 2.4.12018-03-01 14:53:19,783 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                  -  JVM Options:2018-03-01 14:53:19,783 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                  -     -Xms255m2018-03-01 14:53:19,784 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                  -     -Xmx255m2018-03-01 14:53:19,784 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                  -     -XX:MaxDirectMemorySize=769m2018-03-01 14:53:19,784 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                  -     -Dlog.file=/var/log/hadoop-yarn/containers/application_1516373731080_1150/container_1516373731080_1150_01_000105/taskmanager.log2018-03-01 14:53:19,784 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                  -     -Dlogback.configurationFile=file:./logback.xml2018-03-01 14:53:19,784 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                  -     -Dlog4j.configuration=file:./log4j.properties2018-03-01 14:53:19,784 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                  -  Program Arguments:2018-03-01 14:53:19,784 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                  -     --configDirHeap was set to 255, while with default cuts of it should be 1383. 255MB seems like coming from default taskmanager.heap.mb value of 1024.When starting in non flip6 everything works as expected: 2018-03-01 14:04:49,650 INFO  org.apache.flink.yarn.YarnTaskManagerRunnerFactory            - --------------------------------------------------------------------------------2018-03-01 14:04:49,700 INFO  org.apache.flink.yarn.YarnTaskManagerRunnerFactory            -  Starting YARN TaskManager (Version: 1.5-SNAPSHOT, Rev:e92eb39, Date:28.02.2018 @ 17:43:39 UTC)2018-03-01 14:04:49,700 INFO  org.apache.flink.yarn.YarnTaskManagerRunnerFactory            -  OS current user: yarn2018-03-01 14:04:53,277 INFO  org.apache.flink.yarn.YarnTaskManagerRunnerFactory            -  Current Hadoop/Kerberos user: hadoop2018-03-01 14:04:53,278 INFO  org.apache.flink.yarn.YarnTaskManagerRunnerFactory            -  JVM: OpenJDK 64-Bit Server VM - Oracle Corporation - 1.8/25.161-b142018-03-01 14:04:53,279 INFO  org.apache.flink.yarn.YarnTaskManagerRunnerFactory            -  Maximum heap size: 1326 MiBytes2018-03-01 14:04:53,279 INFO  org.apache.flink.yarn.YarnTaskManagerRunnerFactory            -  JAVA_HOME: /usr/lib/jvm/java-openjdk2018-03-01 14:04:53,282 INFO  org.apache.flink.yarn.YarnTaskManagerRunnerFactory            -  Hadoop version: 2.4.12018-03-01 14:04:53,284 INFO  org.apache.flink.yarn.YarnTaskManagerRunnerFactory            -  JVM Options:2018-03-01 14:04:53,284 INFO  org.apache.flink.yarn.YarnTaskManagerRunnerFactory            -     -Xms1383m2018-03-01 14:04:53,284 INFO  org.apache.flink.yarn.YarnTaskManagerRunnerFactory            -     -Xmx1383m2018-03-01 14:04:53,284 INFO  org.apache.flink.yarn.YarnTaskManagerRunnerFactory            -     -XX:MaxDirectMemorySize=1689m2018-03-01 14:04:53,284 INFO  org.apache.flink.yarn.YarnTaskManagerRunnerFactory            -     -Dlog.file=/var/log/hadoop-yarn/containers/application_1516373731080_1138/container_1516373731080_1138_01_000063/taskmanager.log2018-03-01 14:04:53,285 INFO  org.apache.flink.yarn.YarnTaskManagerRunnerFactory            -     -Dlogback.configurationFile=file:./logback.xml2018-03-01 14:04:53,286 INFO  org.apache.flink.yarn.YarnTaskManagerRunnerFactory            -     -Dlog4j.configuration=file:./log4j.properties2018-03-01 14:04:53,287 INFO  org.apache.flink.yarn.YarnTaskManagerRunnerFactory            -  Program Arguments:2018-03-01 14:04:53,287 INFO  org.apache.flink.yarn.YarnTaskManagerRunnerFactory            -     --configDir2018-03-01 14:04:53,287 INFO  org.apache.flink.yarn.YarnTaskManagerRunnerFactory            -     . CC till.rohrmann</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.FlinkYarnSessionCliTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="8832" opendate="2018-3-2 00:00:00" fixdate="2018-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a SQL Client Kafka fat-jar</summary>
      <description>Create fat-jars for Apache Kafka.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8833" opendate="2018-3-2 00:00:00" fixdate="2018-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a SQL Client JSON format fat-jar</summary>
      <description>Create a fat-jar for flink-json.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8838" opendate="2018-3-2 00:00:00" fixdate="2018-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Support for UNNEST a MultiSet type field</summary>
      <description>MultiSetTypeInfo was introduced by  FLINK-7491, and UNNEST support Array type only,  so it would be nice to support UNNEST a MultiSet type field.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.sql.AggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.util.ExplodeFunctionUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.logical.LogicalUnnestRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8839" opendate="2018-3-2 00:00:00" fixdate="2018-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table source factory discovery is broken in SQL Client</summary>
      <description>Table source factories cannot not be discovered if they were added using a jar file.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.descriptors.RowtimeTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.sources.TableSourceFactoryService.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.Rowtime.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.SessionContext.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-libraries.flink-sql-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8842" opendate="2018-3-2 00:00:00" fixdate="2018-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default REST port to 8081</summary>
      <description>In order to avoid confusion, we should set the default REST port to 8081.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterConfiguration.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="8845" opendate="2018-3-3 00:00:00" fixdate="2018-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use WriteBatch to improve performance for recovery in RocksDB backend</summary>
      <description>Base on WriteBatch we could get 30% ~ 50% performance lift when loading data into RocksDB.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMapState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="8847" opendate="2018-3-3 00:00:00" fixdate="2018-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modules containing package-info.java are always recompiled</summary>
      <description>All modules that contain a package-info.java file (that do not contain annotations which applies to all instances in Flink) will always be recompiled by the maven-compiler-plugin.To detect modified files the compiler compares timestamps of the source and .class file. In the case of package-info.java no .class file is created if it doesn't contain annotations, which the compiler interprets as a missing .class file.We can add -Xpkginfo:always to the compiler configuration to force the generation of these files to prevent this from happening.</description>
      <version>1.3.2,1.4.1,1.5.0</version>
      <fixedVersion>1.4.3,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8850" opendate="2018-3-4 00:00:00" fixdate="2018-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client does not support Event-time</summary>
      <description>The SQL client fails with an exception if a table includes a rowtime attribute.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableSchema.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-factory.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.TestTableSourceFactory.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ResultStore.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.PropertyStrings.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Execution.java</file>
      <file type="M">flink-libraries.flink-sql-client.conf.sql-client-defaults.yaml</file>
    </fixedFiles>
  </bug>
  <bug id="8852" opendate="2018-3-4 00:00:00" fixdate="2018-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client does not work with new FLIP-6 mode</summary>
      <description>The SQL client does not submit queries to local Flink cluster that runs in FLIP-6 mode. It doesn't throw an exception either.Job submission works if the legacy Flink cluster mode is used (`mode: old`)</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-factory.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.SqlClientException.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ResultStore.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.MaterializedResult.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.MaterializedCollectStreamResult.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.DynamicResult.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.CollectStreamResult.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ChangelogResult.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ChangelogCollectStreamResult.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Deployment.java</file>
      <file type="M">flink-libraries.flink-sql-client.pom.xml</file>
      <file type="M">flink-libraries.flink-sql-client.conf.sql-client-defaults.yaml</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">flink-clients.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8859" opendate="2018-3-5 00:00:00" fixdate="2018-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RocksDB backend should pass WriteOption to Rocks.put() when restoring</summary>
      <description>We should pass `WriteOption` to Rocks.put() when restoring from handle (Both in full &amp; incremental checkpoint). Because of `WriteOption.setDisableWAL(true)`, the performance can be increased by about 2 times.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="8861" opendate="2018-3-5 00:00:00" fixdate="2018-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for batch queries in SQL Client</summary>
      <description>This issue is a subtask of part two "Full Embedded SQL Client" of the implementation plan mentioned in FLIP-24.Similar to streaming queries, it should be possible to execute batch queries in the SQL Client and collect the results using DataSet.collect() for debugging purposes.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ResultStore.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.DynamicResult.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.CollectStreamTableSink.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.CollectStreamResult.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Execution.java</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="8871" opendate="2018-3-5 00:00:00" fixdate="2018-5-5 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Checkpoint cancellation is not propagated to stop checkpointing threads on the task manager</summary>
      <description>Flink currently lacks any form of feedback mechanism from the job manager / checkpoint coordinator to the tasks when it comes to failing a checkpoint. This means that running snapshots on the tasks are also not stopped even if their owning checkpoint is already cancelled. Two examples for cases where this applies are checkpoint timeouts and local checkpoint failures on a task together with a configuration that does not fail tasks on checkpoint failure. Notice that those running snapshots do no longer account for the maximum number of parallel checkpoints, because their owning checkpoint is considered as cancelled.Not stopping the task's snapshot thread can lead to a problematic situation where the next checkpoints already started, while the abandoned checkpoint thread from a previous checkpoint is still lingering around running. This scenario can potentially cascade: many parallel checkpoints will slow down checkpointing and make timeouts even more likely. A possible solution is introducing a cancelCheckpoint method  as counterpart to the triggerCheckpoint method in the task manager gateway, which is invoked by the checkpoint coordinator as part of cancelling the checkpoint.</description>
      <version>1.3.2,1.4.1,1.5.0,1.6.0,1.7.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.utils.SimpleAckingTaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.RpcTaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.slots.TaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureReason.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.api.datastream.ReinterpretDataStreamAsKeyedStreamITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.classloading.jar.CheckpointingCustomKvStateProgram.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.classloading.jar.CheckpointedStreamingProgram.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.ZooKeeperHighAvailabilityITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.FailingSource.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.CancellingIntegerSource.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.AccumulatingIntegerSink.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StreamCheckpointNotifierITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.StateCheckpointedITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.KeyedStateCheckpointingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.CoStreamCheckpointingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.ContinuousFileProcessingCheckpointITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.ExceptionallyDoneFuture.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.main.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.java</file>
      <file type="M">flink-connectors.flink-connector-gcp-pubsub.src.main.java.org.apache.flink.streaming.connectors.gcp.pubsub.common.AcknowledgeOnCheckpoint.java</file>
      <file type="M">flink-connectors.flink-connector-gcp-pubsub.src.main.java.org.apache.flink.streaming.connectors.gcp.pubsub.PubSubSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.IntegerSource.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.FailureMapper.java</file>
      <file type="M">flink-end-to-end-tests.flink-heavy-deployment-stress-test.src.main.java.org.apache.flink.deployment.HeavyDeploymentStressTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-local-recovery-and-allocation-test.src.main.java.org.apache.flink.streaming.tests.StickyAllocationAndLocalRecoveryTestJob.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointTaskStateManager.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.output.SnapshotUtilsTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.AbstractQueryableStateTestBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.CheckpointListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.NoOpTaskLocalStateStoreImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskLocalStateStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskLocalStateStoreImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskStateManagerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskLocalStateStoreImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestTaskLocalStateStore.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestTaskStateManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksFullSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.MessageAcknowledgingSourceBase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperatorV2.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectSinkFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperatorLifecycleTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.LocalStateForwardingTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MockSubtaskCheckpointCoordinatorBuilder.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SynchronousCheckpointITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.FailingCollectionSource.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.scala</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.streaming.util.FiniteTestSource.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointIT.java</file>
    </fixedFiles>
  </bug>
  <bug id="8872" opendate="2018-3-5 00:00:00" fixdate="2018-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn detached mode via -yd does not detach</summary>
      <description>Running yarn per-job cluster in detached mode currently does not work and waits for the job to finish.Example:./bin/flink run -m yarn-cluster -yn 10 -yjm 768 -ytm 3072 -ys 2 -yd -p 20 -c org.apache.flink.streaming.examples.wordcount.WordCount ./examples/streaming/WordCount.jar --inputOutput in case of an infinite program would then end with something like this:2018-03-05 13:41:23,311 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - Waiting for the cluster to be allocated2018-03-05 13:41:23,313 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - Deploying cluster, current state ACCEPTED2018-03-05 13:41:28,342 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - YARN application has been deployed successfully.2018-03-05 13:41:28,343 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - The Flink YARN client has been started in detached mode. In order to stop Flink on YARN, use the following command or a YARN web interface to stop it:yarn application -kill application_1519984124671_0006Please also note that the temporary files of the YARN session in the home directoy will not be removed.Starting execution of program</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.FlinkYarnSessionCliTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendRunTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.DefaultCLI.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">flink-clients.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8877" opendate="2018-3-6 00:00:00" fixdate="2018-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configure Kryo&amp;#39;s log level based on Flink&amp;#39;s log level</summary>
      <description>Kryo uses its embedded MinLog for logging.When Flink is set to trace, Kryo should be set to trace as well. Other log levels should not be uses, as even debug logging in Kryo results in excessive logging.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="8888" opendate="2018-3-6 00:00:00" fixdate="2018-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade AWS SDK in flink-connector-kinesis</summary>
      <description>Bump up the java aws sdk version to 1.11.272. Evaluate also the impact of this version upgrade for KCL and KPL versions.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8897" opendate="2018-3-8 00:00:00" fixdate="2018-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rowtime materialization causes "mismatched type" AssertionError</summary>
      <description>As raised in this thread, the query created by the following code will throw a calcite "mismatch type" (Timestamp(3) and TimeIndicator) exception.String sql1 = "select id, eventTs as t1, count(*) over (partition by id order by eventTs rows between 100 preceding and current row) as cnt1 from myTable1";String sql2 = "select distinct id as r_id, eventTs as t2, count(*) over (partition by id order by eventTs rows between 50 preceding and current row) as cnt2 from myTable2";Table left = tableEnv.sqlQuery(sql1);Table right = tableEnv.sqlQuery(sql2);left.join(right).where("id === r_id &amp;&amp; t1 === t2").select("id, t1").writeToSink(...)The logical plan is as follows.LogicalProject(id=[$0], t1=[$1]) LogicalFilter(condition=[AND(=($0, $3), =($1, $4))]) LogicalJoin(condition=[true], joinType=[inner]) LogicalAggregate(group=[{0, 1, 2}]) LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])]) LogicalProject(id=[$0], eventTs=[$3]) LogicalTableScan(table=[[_DataStreamTable_0]]) LogicalAggregate(group=[{0, 1, 2}]) LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])]) LogicalProject(id=[$0], eventTs=[$3]) LogicalTableScan(table=[[_DataStreamTable_0]])That is because the the rowtime field after an aggregation will be materialized while the RexInputRef type for the filter's operands (t1 === t2) is still TimeIndicator. We should make them unified.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.WindowJoinUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamWindowJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
    </fixedFiles>
  </bug>
  <bug id="8901" opendate="2018-3-8 00:00:00" fixdate="2018-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>YARN application name for Flink (per-job) submissions claims it is using only 1 TaskManager</summary>
      <description>If (with FLIP-6) a per-job YARN session is created without specifying the number of nodes, it will show up as "Flink session with 1 TaskManagers", e.g. this job:./bin/flink run -m yarn-cluster -yjm 768 -ytm 3072 -ys 2 -p 20 -c org.apache.flink.streaming.examples.wordcount.WordCount ./examples/streaming/WordCount.jar --input /usr/share/doc/rsync-3.0.6/COPYING</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Flip6YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="8906" opendate="2018-3-9 00:00:00" fixdate="2018-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flip6DefaultCLI is not tested in org.apache.flink.client.cli tests</summary>
      <description>Various tests in org.apache.flink.client.cli only test with the DefaultCLI but should also test Flip6DefaultCLI.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.DefaultCLITest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendStopTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendSavepointTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendRunTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendModifyTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendListTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendInfoTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendCancelTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="8910" opendate="2018-3-9 00:00:00" fixdate="2018-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce automated end-to-end test for local recovery (including sticky scheduling)</summary>
      <description>We should have an automated end-to-end test that can run nightly to check that sticky allocation and local recovery work as expected.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0,1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.BackendRestorerProcedure.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8912" opendate="2018-3-9 00:00:00" fixdate="2018-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web UI does not render error messages correctly in FLIP-6 mode</summary>
      <description>DescriptionThe Web UI renders error messages returned by the REST API incorrectly, e.g., on the job submission page. The JSON returned by the REST API is rendered as a whole. However, the UI should only render the contents of the errors field.Steps to reproduceSubmit examples/streaming/SocketWindowWordCount.jar without specifying program arguments. Error message will be rendered as follows:{"errors":["org.apache.flink.client.program.ProgramInvocationException: The program plan could not be fetched - the program aborted pre-maturely.\n\nSystem.err: (none)\n\nSystem.out: No port specified. Please run 'SocketWindowWordCount --hostname &lt;hostname&gt; --port &lt;port&gt;', where hostname (localhost by default) and port is the address of the text server\nTo start a simple text server, run 'netcat -l &lt;port&gt;' and type the input text into the command line\n"]}Note that flip6 mode must be enabled.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.submit.submit.svc.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.submit.submit.ctrl.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
    </fixedFiles>
  </bug>
  <bug id="8916" opendate="2018-3-11 00:00:00" fixdate="2018-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkpointing Mode is always shown to be "At Least Once" in Web UI</summary>
      <description>This only happens in flip6 mode. The CheckpointConfigHandler returns the checkpoint mode uppercased. For example:{"mode":"EXACTLY_ONCE","interval":5000,"timeout":600000,"min_pause":0,"max_concurrent":1,"externalization":{"enabled":false,"delete_on_cancellation":true}}However, the Web UI expects the value to be lower cased: &lt;tr&gt; &lt;td&gt;Checkpointing Mode&lt;/td&gt; &lt;td ng-if="checkpointConfig['mode'] == 'exactly_once'"&gt;Exactly Once&lt;/td&gt; &lt;td ng-if="checkpointConfig['mode'] != 'exactly_once'"&gt;At Least Once&lt;/td&gt; &lt;/tr&gt;</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointConfigInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="8922" opendate="2018-3-12 00:00:00" fixdate="2018-5-12 01:00:00" resolution="Feedback Received">
    <buginformation>
      <summary>Revert FLINK-8859 because it causes segfaults in testing</summary>
      <description>We need to revert FLINK-8859 because it causes problems with RocksDB that make our automated tests fail on Travis. The change looks actually good and it is currently unclear why this can introduce such a problem. This might also be a Rocks in RocksDB. Nevertheless, for the sake of a proper release testing, we should revert the change for now.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="8926" opendate="2018-3-12 00:00:00" fixdate="2018-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shutdown client proxy on test end.</summary>
      <description></description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.client.proxy.KvStateClientProxyImplTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="8927" opendate="2018-3-12 00:00:00" fixdate="2018-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Eagerly release the checkpoint object created from RocksDB</summary>
      <description>We should eagerly release the checkpoint object that is created from RocksDB, because it's a RocksObject (a native resource).</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="8928" opendate="2018-3-12 00:00:00" fixdate="2018-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve error message on server binding error.</summary>
      <description></description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.AbstractServerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="8934" opendate="2018-3-13 00:00:00" fixdate="2018-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cancel slot requests for otherwisely fulfilled requests</summary>
      <description>If a slot request is fulfilled with a different allocation id, then we should cancel the other slot request at the ResourceManager. Otherwise we might have some stale slot requests which first need to time out before slots are available again.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="8944" opendate="2018-3-14 00:00:00" fixdate="2018-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use ListShards for shard discovery in the flink kinesis connector</summary>
      <description>Currently the DescribeStream AWS API used to get list of shards is has a restricted rate limits on AWS. (5 requests per sec per account). This is problematic when running multiple flink jobs all on same account since each subtasks calls the Describe Stream. Changing this to ListShards will provide more flexibility on rate limits as ListShards has a 100 requests per second per data stream limits.More details on the mailing list. https://goo.gl/mRXjKh</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.ConsumerConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="8949" opendate="2018-3-15 00:00:00" fixdate="2018-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rest API failure with long URL</summary>
      <description>When you have jobs with high parallelism, the URL for a REST request can get very long. When the URL is longer than 4096 bytes, the  REST API will return errorFailure: 404 Not Found This can easily be seen in the Web UI, when Flink queries for the watermark using the REST API:GET /jobs/:jobId/vertices/:vertexId/metrics?get=0.currentLowWatermark,1.currentLowWatermark,2.currentLo...The request will fail with more than 170 subtasks and the watermark will not be displayed in the Web UI.</description>
      <version>1.4.2,1.5.0,1.6.4,1.7.2,1.8.2</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.metrics.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.watermarks.job-overview-drawer-watermarks.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.component.ts</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="8968" opendate="2018-3-15 00:00:00" fixdate="2018-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix native resource leak caused by ReadOptions</summary>
      <description>We should pull the creation of ReadOptions out of the loop in RocksDBFullSnapshotOperation.writeKVStateMetaData().</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="8971" opendate="2018-3-16 00:00:00" fixdate="2018-4-16 01:00:00" resolution="Done">
    <buginformation>
      <summary>Create general purpose testing job</summary>
      <description>In order to write better end-to-end tests we need a general purpose testing job which comprises as many Flink aspects as possible. These include different types for records and state, user defined components, state types and operators.The job should allow to activate a certain misbehavior, such as slowing certain paths down or throwing exceptions to simulate failures.The job should come with a data generator which generates input data such that the job can verify it's own behavior. This includes the state as well as the input/output records.We already have the heavily misbehaved job which simulates some misbehavior. There is also the state machine job which can verify itself for invalid state changes which indicate data loss. We should incorporate their characteristics into the new general purpose job.Additionally, the general purpose job should contain the following aspects: Job containing a sliding window aggregation At least one generic Kryo type At least one generic Avro type At least one Avro specific record type At least one input type for which we register a Kryo serializer At least one input type for which we provide a user defined serializer At least one state type for which we provide a user defined serializer At least one state type which uses the AvroSerializer Include an operator with ValueState Value state changes should be verified (e.g. predictable series of values) Include an operator with operator state Include an operator with broadcast state Broadcast state changes should be verified (e.g. predictable series of values) Include union state User defined watermark assignerThe job should be made available in the flink-end-to-end-tests module.This issue is intended to serve as an umbrella issue for developing and extending this job.</description>
      <version>1.5.0</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.savepoint.sh</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestJobFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="8972" opendate="2018-3-16 00:00:00" fixdate="2018-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create general purpose DataSet testing job</summary>
      <description>Likewise to FLINK-8971, we should have a general purpose testing job for the DataSet API. The job should contain at least the following aspects: Co-group operation Join operation IterationThe input should generate a deterministic output which can be verified for correctness.The job should be made available in the flink-end-to-end-tests module.The job should also include some misbehavior such as failures which can be activated.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.batch.allround.sh</file>
      <file type="M">flink-end-to-end-tests.flink-dataset-allround-test.src.main.java.org.apache.flink.batch.tests.DataSetAllroundTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-dataset-allround-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8973" opendate="2018-3-16 00:00:00" fixdate="2018-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Run general purpose job with failures in standalone mode</summary>
      <description>We should set up an end-to-end test which runs the general purpose job (FLINK-8971) in a standalone setting with HA enabled (ZooKeeper). When running the job, the job failures should be activated. Additionally, we should randomly kill Flink processes (cluster entrypoint and TaskExecutors). When killing them, we should also spawn new processes to make up for the loss.This end-to-end test case should run with all different state backend settings: RocksDB (full/incremental, async/sync), FsStateBackend (sync/async)We should then verify that the general purpose job is successfully recovered without data loss or other failures.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.statemachine.StateMachineExample.java</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="8974" opendate="2018-3-16 00:00:00" fixdate="2018-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Run general purpose DataSet job with failures in standalone mode</summary>
      <description>Similar to FLINK-8973, we should setup an end-to-end test where we run the general purpose DataSet job from FLINK-8972 in a HA standalone setting with failures and process kills.</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.ha.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.flink-dataset-allround-test.src.main.java.org.apache.flink.batch.tests.DataSetAllroundTestProgram.java</file>
    </fixedFiles>
  </bug>
  <bug id="8975" opendate="2018-3-16 00:00:00" fixdate="2018-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Resume from savepoint</summary>
      <description>User usually take a savepoint and want to resume from it. In order to verify that Flink supports this feature, we should add an end-to-end test which scripts this behavior. We should use the general purpose testing job FLINK-8971 with failures disabled for that.The end-to-end test should do the following: Submit FLINK-8971 job Verify that the savepoint is there Cancel job and resume from savepoint Verify that job could be resumed Use different StateBackends: RocksDB incremental async/sync, RocksDB full async/sync, FsStateBackend aysnc/sync</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.statemachine.StateMachineExample.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-pre-commit-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="8977" opendate="2018-3-16 00:00:00" fixdate="2018-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Manually resume job after terminal failure</summary>
      <description>We should add an end-to-end test which verifies that a job can be resumed manually after a terminal job failure if there is a checkpoint. In order to do that we should run the general purpose testing job FLINK-8971 wait for the completion of a checkpoint Trigger a failure which leads to a terminal failure Resume the job from the retained checkpointThis end-to-end test should run with all state backend combinations: RocksDB (incremental/full, async/sync), FsStateBackend (async/sync).</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestJobFactory.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.externalized.checkpoints.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="8979" opendate="2018-3-16 00:00:00" fixdate="2018-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend Kafka end-to-end tests to run with different versions</summary>
      <description>The current Kafka end-to-end test only runs with Kafka 0.10. We should extend the test to also run with Kafka 0.8 Kafka 0.9 Kafka 0.11Additionally we should change the test job to not be embarrassingly parallel by introducing a shuffle.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.kafka.Kafka010Example.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.kafka010.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.savepoint.sh</file>
    </fixedFiles>
  </bug>
  <bug id="8980" opendate="2018-3-16 00:00:00" fixdate="2018-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: BucketingSink</summary>
      <description>In order to verify the BucketingSink, we should add an end-to-end test which verifies that the BucketingSink does not lose data under failures.An idea would be to have a CountUp job which simply counts up a counter which is persisted. The emitted values will be written to disk by the BucketingSink. Now we should kill randomly Flink processes (cluster entrypoint and TaskExecutors) to simulate failures. Even after these failures, the written files should contain the correct sequence of numbers.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-stream-sql-test.src.main.java.org.apache.flink.sql.tests.StreamSQLTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-parent-child-classloading-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-dataset-allround-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8981" opendate="2018-3-16 00:00:00" fixdate="2018-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add end-to-end test for running on YARN with Kerberos</summary>
      <description>We should add an end-to-end test which verifies Flink's integration with Kerberos security. In order to do this, we should start a Kerberos secured Hadoop, ZooKeeper and Kafka cluster. Then we should start a Flink cluster with HA enabled and run a job which reads from and writes to Kafka. We could use a simple pipe job for that purpose which has some state for checkpointing to HDFS.See security docs for how more information about Flink's Kerberos integration.</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="8982" opendate="2018-3-16 00:00:00" fixdate="2018-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Queryable state</summary>
      <description>We should add an end-to-end test which verifies that Queryable State is working.florianschmidt and kkl0u could you please provide more details for the description.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8983" opendate="2018-3-16 00:00:00" fixdate="2018-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Confluent schema registry</summary>
      <description>It would be good to add an end-to-end test which verifies that Flink is able to work together with the Confluent schema registry. In order to do that we have to setup a Kafka cluster and write a Flink job which reads from the Confluent schema registry producing an Avro type.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-confluent-schema-registry.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.flink-confluent-schema-registry.src.main.java.org.apache.flink.schema.registry.test.TestAvroConsumerConfluent.java</file>
      <file type="M">flink-end-to-end-tests.flink-confluent-schema-registry.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8984" opendate="2018-3-16 00:00:00" fixdate="2018-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disabling credit based flow control deadlocks Flink on checkpoint</summary>
      <description>This is configuration issue. There are two options: taskmanager.network.credit-based-flow-control.enabledandtaskmanager.exactly-once.blocking.data.enabledIf we disable first one, but remain default value for the second one deadlocks will occur. I think we can safely drop the second config value altogether and always use blocking BarrierBuffer for credit based flow control and spilling BarrierBuffer for non credit based flow control.cc zjwang</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.InputProcessorUtil.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="8985" opendate="2018-3-16 00:00:00" fixdate="2018-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: CLI</summary>
      <description>We should add end-to-end test which verifies that all client commands are working correctly.</description>
      <version>1.5.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.cli.api.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.flink-api-test.src.main.java.org.apache.flink.runtime.tests.PeriodicStreamingJob.java</file>
      <file type="M">flink-end-to-end-tests.flink-api-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8987" opendate="2018-3-16 00:00:00" fixdate="2018-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Avro state evolution</summary>
      <description>We should add an end-to-end test which verifies that we can upgrade Avro types by adding and removing fields. We can use the general purpose job (FLINK-8971) after it added Avro types for that.What should happen is Start general purpose job Take savpoint Change Avro type to have different fields Resume from savepointCheck for different state backends: RocksDB, FsStateBackend</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-pre-commit-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8989" opendate="2018-3-16 00:00:00" fixdate="2018-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: ElasticSearch connector</summary>
      <description>Similar to FLINK-8988 we should add a end-to-end test which tests the ElasticSearch connector. We should run against all three supported ElasticSearch versions: 1.x, 2.x and 5.x.</description>
      <version>None</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.examples.ElasticsearchSinkExample.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.test.java.org.apache.flink.streaming.connectors.elasticsearch5.examples.ElasticsearchSinkExample.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.test.java.org.apache.flink.streaming.connectors.elasticsearch2.examples.ElasticsearchSinkExample.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.elasticsearch125.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.elasticsearch-common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch5-test.src.main.java.org.apache.flink.streaming.tests.Elasticsearch5SinkExample.java</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch5-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch2-test.src.main.java.org.apache.flink.streaming.tests.Elasticsearch2SinkExample.java</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch2-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch1-test.src.main.java.org.apache.flink.streaming.tests.Elasticsearch1SinkExample.java</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch1-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8990" opendate="2018-3-16 00:00:00" fixdate="2018-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Dynamic Kafka partition discovery</summary>
      <description>We should add an end-to-end test which verifies the dynamic partition discovery of Flink's Kafka connector. We can simulate it by reading from a Kafka topic to which we add partitions after the job started. By writing to these new partitions it should be verifiable whether Flink noticed them by checking the output for completeness.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.kafka010.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="8992" opendate="2018-3-16 00:00:00" fixdate="2018-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement source and operator that validate exactly-once</summary>
      <description>We can build this with sources that emit sequences per key and a stateful (keyed) operator that validate for the update of each key that the new value is the old value + 1. This can help to easily detect if events/state were lost or duplicates.</description>
      <version>1.5.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.savepoint.sh</file>
      <file type="M">flink-end-to-end-tests.src.main.java.org.apache.flink.streaming.tests.general.SequenceGeneratorSource.java</file>
      <file type="M">flink-end-to-end-tests.src.main.java.org.apache.flink.streaming.tests.general.SemanticsCheckMapper.java</file>
      <file type="M">flink-end-to-end-tests.src.main.java.org.apache.flink.streaming.tests.general.GeneralPurposeJobTest.java</file>
      <file type="M">flink-end-to-end-tests.src.main.java.org.apache.flink.streaming.tests.general.Event.java</file>
      <file type="M">flink-end-to-end-tests.src.main.java.org.apache.flink.streaming.tests.general.artificialstate.eventpayload.ComplexPayload.java</file>
      <file type="M">flink-end-to-end-tests.src.main.java.org.apache.flink.streaming.tests.general.artificialstate.eventpayload.ArtificialValueStateBuilder.java</file>
      <file type="M">flink-end-to-end-tests.src.main.java.org.apache.flink.streaming.tests.general.artificialstate.eventpayload.ArtificialMapStateBuilder.java</file>
      <file type="M">flink-end-to-end-tests.src.main.java.org.apache.flink.streaming.tests.general.artificialstate.ArtificialKeyedStateMapper.java</file>
      <file type="M">flink-end-to-end-tests.src.main.java.org.apache.flink.streaming.tests.general.artificialstate.ArtificialKeyedStateBuilder.java</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8993" opendate="2018-3-16 00:00:00" fixdate="2018-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a test operator with keyed state that uses Kryo serializer (registered/unregistered/custom)</summary>
      <description>Add an operator with keyed state that uses Kryo serializer (registered/unregistered/custom).</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-stream-stateful-job-upgrade-test.src.main.java.org.apache.flink.streaming.tests.StatefulStreamJobUpgradeTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestJobFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.artificialstate.builder.ArtificialValueStateBuilder.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.artificialstate.builder.ArtificialListStateBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="8995" opendate="2018-3-16 00:00:00" fixdate="2018-10-16 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add a test operator with keyed state that uses custom, stateful serializer</summary>
      <description>This test should figure out problems in places where multiple threads would share the same serializer instead of properly duplicating it.</description>
      <version>1.5.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.java</file>
    </fixedFiles>
  </bug>
  <bug id="8997" opendate="2018-3-16 00:00:00" fixdate="2018-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add sliding window aggregation to the job</summary>
      <description>The test job should also test windowing. Sliding windows are probably the most demanding form, so this would be a good pick for the test.</description>
      <version>1.5.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.SemanticsCheckMapper.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.Event.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestJobFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="9003" opendate="2018-3-16 00:00:00" fixdate="2018-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add operators with input type that goes through custom, stateful serialization</summary>
      <description></description>
      <version>1.5.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateSnapshotTransformerTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.TypeSerializer.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.SequenceGeneratorSource.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.SemanticsCheckMapper.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestJobFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.artificialstate.StatefulComplexPayloadSerializer.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.artificialstate.builder.ArtificialStateBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="9007" opendate="2018-3-16 00:00:00" fixdate="2018-3-16 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>End-to-end test: Kinesis connector</summary>
      <description>Add an end-to-end test which uses Flink's Kinesis connector to read and write to Kinesis. We could use a simple pipe job with simple state for checkpointing purposes. The checkpoints should then be written to S3 using flink-s3-fs-hadoop and flink-s3-fs-presto.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.src.main.java.org.apache.flink.streaming.kinesis.test.KinesisExample.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-pre-commit-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9016" opendate="2018-3-17 00:00:00" fixdate="2018-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unregister jobs from JobMetricGroup after termination</summary>
      <description>In order to free resources and unregister metrics, jobs should be properly unregistered from the JobMetricGroup once they have reached terminal state.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobManagerRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.MiniDispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.SessionClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.JobClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.StandaloneDispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.MiniDispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="9027" opendate="2018-3-19 00:00:00" fixdate="2018-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web UI does not cleanup temporary files</summary>
      <description>The web UI creates two directories in java.io.tmp, namely flink-web-&lt;uuid&gt; and flink-web-upload-&lt;uuid&gt; and both are not cleaned up (running start-cluster.sh and stop-cluster.sh.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpointConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.RestHandlerConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="9028" opendate="2018-3-20 00:00:00" fixdate="2018-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flip6 should check config before starting cluster</summary>
      <description>In flip6, we should perform parameters checking before starting cluster.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnClusterDescriptorTest.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.FlinkYarnSessionCliTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.ContaineredTaskManagerParametersTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.ContaineredTaskManagerParameters.java</file>
    </fixedFiles>
  </bug>
  <bug id="9033" opendate="2018-3-20 00:00:00" fixdate="2018-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace usages of deprecated TASK_MANAGER_NUM_TASK_SLOTS</summary>
      <description>The deprecated ConfigConstants#TASK_MANAGER_NUM_TASK_SLOTS is still used a lot.We should replace these usages with TaskManagerOptions#NUM_TASK_SLOTS.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.taskmanager.TaskManagerFailsITCase.scala</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.jobmanager.JobManagerFailsITCase.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.minicluster.LocalFlinkMiniClusterITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.TaskManagerFailureRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAProcessFailureBatchRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHACheckpointRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.operators.RemoteEnvironmentITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.operators.ExecutionEnvironmentITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.SavepointMigrationTestBase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.MiniClusterResource.java</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingUtils.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.RecoveryITCase.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.akka.AkkaSslITCase.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.backpressure.StackTraceSampleCoordinatorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.backpressure.BackPressureStatsTrackerImplITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderChangeStateCleanupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderChangeJobRecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerHARecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerCleanupITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartialConsumePipelinedResultTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CoordinatorShutdownTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServicesConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkLocalCluster.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualExactlyOnceWithStreamReshardingTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualExactlyOnceTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.LocalExecutor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.ClusterSpecification.java</file>
    </fixedFiles>
  </bug>
  <bug id="9057" opendate="2018-3-22 00:00:00" fixdate="2018-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE in CreditBasedSequenceNumberingViewReader when cancelling before initilization was complete</summary>
      <description>RescalingITCase unveiled an exception which may occur when shutting down before completely initializing the network stack:https://travis-ci.org/apache/flink/jobs/35661210001:08:13,458 WARN org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline - An exception was thrown by a user handler's exceptionCaught() method while handling the following exception:java.lang.NullPointerException at org.apache.flink.runtime.io.network.netty.CreditBasedSequenceNumberingViewReader.releaseAllResources(CreditBasedSequenceNumberingViewReader.java:192) at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.releaseAllResources(PartitionRequestQueue.java:322) at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.channelInactive(PartitionRequestQueue.java:298) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:237) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:223) at org.apache.flink.shaded.netty4.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:237) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:223) at org.apache.flink.shaded.netty4.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:237) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:223) at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:294) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:237) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:223) at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:829) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:610) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) at java.lang.Thread.run(Thread.java:748)</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestServerHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="9059" opendate="2018-3-22 00:00:00" fixdate="2018-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for unified table source and sink declaration in environment file</summary>
      <description>1) Add a common property called "type" with single value 'source'.2) in yaml file, replace "sources" with "tables".</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sources.TestTableSourceFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sources.TableSourceFactoryServiceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.TableSourceDescriptor.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-factory.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Environment.java</file>
      <file type="M">flink-libraries.flink-sql-client.conf.sql-client-defaults.yaml</file>
    </fixedFiles>
  </bug>
  <bug id="9061" opendate="2018-3-22 00:00:00" fixdate="2018-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add entropy to s3 path for better scalability</summary>
      <description>I think we need to modify the way we write checkpoints to S3 for high-scale jobs (those with many total tasks).  The issue is that we are writing all the checkpoint data under a common key prefix.  This is the worst case scenario for S3 performance since the key is used as a partition key. In the worst case checkpoints fail with a 500 status code coming back from S3 and an internal error type of TooBusyException. One possible solution would be to add a hook in the Flink filesystem code that allows me to "rewrite" paths.  For example say I have the checkpoint directory set to: s3://bucket/flink/checkpoints I would hook that and rewrite that path to: s3://bucket/&amp;#91;HASH&amp;#93;/flink/checkpoints, where HASH is the hash of the original path This would distribute the checkpoint write load around the S3 cluster evenly. For reference: https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-performance-improve/ Any other people hit this issue?  Any other ideas for solutions?  This is a pretty serious problem for people trying to checkpoint to S3. -Jamie </description>
      <version>1.4.2,1.5.0</version>
      <fixedVersion>1.6.2,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStorageLocation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStorage.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsCheckpointMetadataOutputStream.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.StringUtilsTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.StringUtils.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.main.java.org.apache.flink.fs.s3.common.HadoopConfigLoader.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.main.java.org.apache.flink.fs.s3.common.FlinkS3FileSystem.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.main.java.org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.java</file>
      <file type="M">docs.ops.filesystems.md</file>
    </fixedFiles>
  </bug>
  <bug id="9064" opendate="2018-3-23 00:00:00" fixdate="2018-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Scaladocs link to documentation</summary>
      <description>Browse to the Apache Flink Documentation page.On the sidebar, under the Javadocs link, I recommend that you add a Scaladocs link.Thanks! </description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.update.branch.version.sh</file>
      <file type="M">tools.releasing.create.release.branch.sh</file>
      <file type="M">docs..includes.sidenav.html</file>
      <file type="M">docs..config.yml</file>
    </fixedFiles>
  </bug>
  <bug id="9067" opendate="2018-3-23 00:00:00" fixdate="2018-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Stream SQL query with failure and retry</summary>
      <description>Implement a test job that runs a streaming SQL query with a temporary failure and recovery.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.sql.sh</file>
      <file type="M">flink-end-to-end-tests.flink-stream-sql-test.src.main.java.org.apache.flink.sql.tests.StreamSQLTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-sql-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9068" opendate="2018-3-24 00:00:00" fixdate="2018-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Website documentation issue - html tag visible on screen</summary>
      <description>In the documentation at the following urlhttps://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/operators/#physical-partitioningIn the section which explains the 'Reduce' operator (ReduceKeyedStream → DataStream), an html tag (&lt;/p&gt;) is visible.</description>
      <version>None</version>
      <fixedVersion>1.4.3,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.operators.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="9076" opendate="2018-3-26 00:00:00" fixdate="2018-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make credit-based floating buffers optional</summary>
      <description>Currently, floating buffers (per gate) are always required in case credit-based flow control is enabled. This, however, increases our minimum number of required network buffers.Instead, without changing too much, we could already work with a minimum of one or zero floating buffers and set the max to the configured value. This way, if there are not enough buffers, all {{LocalBufferPool}}s will at least share the available ones.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.misc.SuccessAfterNetworkBuffersFailureITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkThroughputBenchmarkTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkThroughputBenchmark.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkPointToPointBenchmark.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkBenchmarkEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.InputChannelTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NetworkEnvironmentTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NetworkEnvironment.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="9088" opendate="2018-3-26 00:00:00" fixdate="2018-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Nifi connector dependency to 1.6.0</summary>
      <description>Currently dependency of Nifi is 0.6.1We should upgrade to 1.6.0</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-nifi.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9089" opendate="2018-3-26 00:00:00" fixdate="2018-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Orc dependency to 1.4.3</summary>
      <description>Currently flink-orc uses Orc 1.4.1 release.This issue upgrades to Orc 1.4.3</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-orc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="909" opendate="2014-6-9 00:00:00" fixdate="2014-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pitfall due to additional superstep after the iteration has stopped</summary>
      <description>Currently, after an iteration has exceeded the maximum number of iterations, all tasks are started again for an additional superstep during which they are stopped. This works if a tasks only waits for dynamic input. However, in the case where one has a task, e.g. a coGroup operation, which gets dynamic and static input the execution is not blocked. This can then lead to erroneous behaviour which the user is not aware of.I had this problem implementing ALS. Here one has a loop which gets as dynamic input matrix columns and as static input matrix entries. The columns and the entries are used to construct a matrix which represents a system of linear equations. If the set of columns are empty, then the matrix is singular and thus not solvable. During the additional superstep the task won't receive any columns but would still try to solve the now singular matrix.It would be good to finish the iteration without initiating this additional superstep.---------------- Imported from GitHub ----------------Url: https://github.com/stratosphere/stratosphere/issues/909Created by: tillrohrmannLabels: Created at: Thu Jun 05 17:50:17 CEST 2014State: open</description>
      <version>None</version>
      <fixedVersion>pre-apache</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.aggregators.ConnectedComponentsWithParametrizableAggregatorITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.concurrent.SuperstepKickoffLatchBroker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.concurrent.SuperstepKickoffLatch.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.task.IterationTailPactTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.task.IterationIntermediatePactTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.task.IterationHeadPactTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.task.AbstractIterativePactTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="9091" opendate="2018-3-26 00:00:00" fixdate="2018-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failure while enforcing releasability in building flink-json module</summary>
      <description>Got the following when building flink-json module:[WARNING] Rule 0: org.apache.maven.plugins.enforcer.DependencyConvergence failed with message:Failed while enforcing releasability. See above detailed error message....[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.0.0-M1:enforce (dependency-convergence) on project flink-json: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -&gt; [Help 1]</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9093" opendate="2018-3-27 00:00:00" fixdate="2018-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>If Google can&amp;#39;t be accessed,the document can&amp;#39;t be use</summary>
      <description>these links can't be visited.</description>
      <version>None</version>
      <fixedVersion>1.3.4,1.4.3,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.savepoint.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">docs..layouts.base.html</file>
    </fixedFiles>
  </bug>
  <bug id="9104" opendate="2018-3-29 00:00:00" fixdate="2018-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-generate REST API documentation for FLIP-6</summary>
      <description>The API documentation is missing for several handlers, e.g., SavepointHandlers.</description>
      <version>1.5.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">docs..includes.generated.rest.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="9107" opendate="2018-3-29 00:00:00" fixdate="2018-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document timer coalescing for ProcessFunctions</summary>
      <description>In a ProcessFunction, registering timers for each event via ctx.timerService().registerEventTimeTimer() using times like ctx.timestamp() + timeout will get a millisecond accuracy and may thus create one timer per millisecond which may lead to some overhead in the TimerService.This problem can be mitigated by using timer coalescing if the desired accuracy of the timer can be larger than 1ms. A timer firing at full seconds only, for example, can be realised like this:coalescedTime = ((ctx.timestamp() + timeout) / 1000) * 1000;ctx.timerService().registerEventTimeTimer(coalescedTime);As a result, only a single timer may exist for every second since we do not add timers for timestamps that are already there.This should be documented in the ProcessFunction docs.</description>
      <version>1.3.0,1.4.0,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.operators.process.function.md</file>
    </fixedFiles>
  </bug>
  <bug id="9108" opendate="2018-3-29 00:00:00" fixdate="2018-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>invalid ProcessWindowFunction link in Document</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.side.output.md</file>
    </fixedFiles>
  </bug>
  <bug id="9109" opendate="2018-3-29 00:00:00" fixdate="2018-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add flink modify command to documentation</summary>
      <description>We should add documentation for the flink modify command.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="9121" opendate="2018-4-2 00:00:00" fixdate="2018-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Flip-6 prefixes from code base</summary>
      <description>We should remove all Flip-6 prefixes and other references from the code base since it is not a special case but the new default architecture. Instead we should prefix old code with legacy.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManagerTest.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnClusterDescriptorTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Flip6YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNHighAvailabilityITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnConfigurationITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.TestingYarnClusterDescriptor.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.AbstractOperatorRestoreTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.TaskManagerProcessFailureStreamingRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.TaskManagerProcessFailureBatchRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ProcessFailureCancelingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAProcessFailureBatchRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.operators.RemoteEnvironmentITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.misc.AutoParallelismITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.accumulators.AccumulatorLiveITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTriggerSavepointIT.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.MiniClusterResource.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.AbstractTestBase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.testutils.category.OldAndFlip6.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.testutils.category.Flip6.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.environment.LocalStreamEnvironmentITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.RemoteStreamEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.LocalStreamEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.Flip6LocalStreamEnvironment.java</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellLocalStartupITCase.scala</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskCancelAsyncProducerConsumerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskManagerServicesConfigurationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.slot.TimerServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.NetworkBufferCalculationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.RpcEndpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.RpcConnectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.FencedRpcEndpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.AsyncCallsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.MessageSerializationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.MainThreadValidationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerIdPathParameterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.RestResponseMarshallingTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.RestRequestMarshallingTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.MessageParametersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.TaskManagerMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.SubtaskMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.MetricsFilterParameterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.JobVertexMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.JobMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.JobManagerMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.job.metrics.AbstractMetricsHeadersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.MetricsHandlerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.metrics.AbstractMetricsHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobSubmitHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.BlobServerPortHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotProtocolTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerJobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerHATest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.JobLeaderIdServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.query.KvStateRegistryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.minicluster.MiniClusterITCase.java</file>
      <file type="M">.travis.yml</file>
      <file type="M">docs.monitoring.rest.api.md</file>
      <file type="M">docs.ops.config.md</file>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.DefaultCLI.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.Flip6DefaultCLI.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.Flip6StandaloneClusterDescriptor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.StandaloneClusterDescriptor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.LocalExecutor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.RemoteExecutor.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendPackageProgramTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendTestBase.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.DefaultCLITest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.jobmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-cluster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.stop-cluster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.taskmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-appmaster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-taskmanager.sh</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
      <file type="M">flink-libraries.flink-streaming-python.src.main.java.org.apache.flink.streaming.python.api.environment.PythonEnvironmentFactory.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.main.java.org.apache.flink.queryablestate.client.proxy.KvStateClientProxyImpl.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.client.proxy.KvStateClientProxyImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.SharedSlot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.SimpleSlot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.Slot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.scheduler.CoLocationConstraint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.query.KvStateRegistry.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.FileArchivedExecutionGraphStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.MiniDispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.heartbeat.HeartbeatManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.nonha.standalone.StandaloneHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.SchedulerSlotSharingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobResultTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.AllocatedSlotsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.AvailableSlotsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.DualKeyMapTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolRpcTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolSchedulingTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="9128" opendate="2018-4-4 00:00:00" fixdate="2018-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for scheduleRunAsync for FencedRpcEndpoints</summary>
      <description>Currently, the FencedRpcEndpoint cannot send a scheduleRunAsync message because it is not properly wrapped in a LocalFencedMessage. Due to this, the message will be dropped and the code will not be executed.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.AsyncCallsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActor.java</file>
    </fixedFiles>
  </bug>
  <bug id="9131" opendate="2018-4-4 00:00:00" fixdate="2018-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable spotbugs on travis</summary>
      <description>The misc profile that also runs spotbugs is consistently timing out on travis at the moment.The spotbugs plugin is a major contributor to the compilation time, for example it doubles the compile time for flink-runtime.I suggest to temporarily disable spotbugs, and re-enable it at a lter point when we figure out the daily cron jobs.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
    </fixedFiles>
  </bug>
  <bug id="9140" opendate="2018-4-5 00:00:00" fixdate="2018-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>simplify scalastyle configurations</summary>
      <description>Simplifying &lt;check .....&gt;&lt;/check&gt; to &lt;check ... /&gt;</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.scalastyle-config.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9144" opendate="2018-4-6 00:00:00" fixdate="2018-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Spilling batch job hangs</summary>
      <description>A user on the mailing list reported that his batch job stops to run with Flink 1.5 RC1: https://lists.apache.org/thread.html/43721934405019e7255fda627afb7c9c4ed0d04fb47f1c8f346d4194@%3Cdev.flink.apache.org%3E</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.BufferBuilderTestUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartition.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.batch.allround.sh</file>
    </fixedFiles>
  </bug>
  <bug id="9145" opendate="2018-4-6 00:00:00" fixdate="2018-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Website build is broken</summary>
      <description>The javadoc generation fails with a dependency-convergence error in flink-json:[WARNING] Dependency convergence error for commons-beanutils:commons-beanutils:1.8.0 paths to dependency are:+-org.apache.flink:flink-json:1.6-SNAPSHOT +-org.apache.flink:flink-table_2.11:1.6-SNAPSHOT +-commons-configuration:commons-configuration:1.7 +-commons-digester:commons-digester:1.8.1 +-commons-beanutils:commons-beanutils:1.8.0and+-org.apache.flink:flink-json:1.6-SNAPSHOT +-org.apache.flink:flink-table_2.11:1.6-SNAPSHOT +-commons-configuration:commons-configuration:1.7 +-commons-beanutils:commons-beanutils:1.8.3[WARNING] Dependency convergence error for org.codehaus.janino:commons-compiler:3.0.7 paths to dependency are:+-org.apache.flink:flink-json:1.6-SNAPSHOT +-org.apache.flink:flink-table_2.11:1.6-SNAPSHOT +-org.codehaus.janino:janino:3.0.7 +-org.codehaus.janino:commons-compiler:3.0.7and+-org.apache.flink:flink-json:1.6-SNAPSHOT +-org.apache.flink:flink-table_2.11:1.6-SNAPSHOT +-org.apache.calcite:calcite-core:1.16.0 +-org.codehaus.janino:commons-compiler:2.7.6[WARNING] Dependency convergence error for commons-lang:commons-lang:2.6 paths to dependency are:+-org.apache.flink:flink-json:1.6-SNAPSHOT +-org.apache.flink:flink-table_2.11:1.6-SNAPSHOT +-commons-configuration:commons-configuration:1.7 +-commons-lang:commons-lang:2.6and+-org.apache.flink:flink-json:1.6-SNAPSHOT +-org.apache.flink:flink-table_2.11:1.6-SNAPSHOT +-org.apache.calcite:calcite-core:1.16.0 +-net.hydromatic:aggdesigner-algorithm:6.0 +-commons-lang:commons-lang:2.4[WARNING] Dependency convergence error for org.codehaus.janino:janino:3.0.7 paths to dependency are:+-org.apache.flink:flink-json:1.6-SNAPSHOT +-org.apache.flink:flink-table_2.11:1.6-SNAPSHOT +-org.codehaus.janino:janino:3.0.7and+-org.apache.flink:flink-json:1.6-SNAPSHOT +-org.apache.flink:flink-table_2.11:1.6-SNAPSHOT +-org.apache.calcite:calcite-core:1.16.0 +-org.codehaus.janino:janino:2.7.6and+-org.apache.flink:flink-json:1.6-SNAPSHOT +-org.apache.flink:flink-table_2.11:1.6-SNAPSHOT +-org.codehaus.janino:janino:3.0.7[WARNING] Rule 0: org.apache.maven.plugins.enforcer.DependencyConvergence failed with message:Failed while enforcing releasability. See above detailed error message.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
      <file type="M">flink-libraries.flink-sql-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9147" opendate="2018-4-8 00:00:00" fixdate="2018-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PrometheusReporter jar does not include Prometheus dependencies</summary>
      <description>The PrometheusReporter seems to lack the shaded Prometheus dependencies.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-prometheus.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9154" opendate="2018-4-10 00:00:00" fixdate="2018-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include WebSubmissionExtension in REST API docs</summary>
      <description>The handlers contained in the WebSubmissionExtension are currently not documented in the REST API docs.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">docs..includes.generated.rest.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="9158" opendate="2018-4-12 00:00:00" fixdate="2018-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set default FixedRestartDelayStrategy delay to 0s</summary>
      <description>Set default FixedRestartDelayStrategy delay to 0s.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="9159" opendate="2018-4-12 00:00:00" fixdate="2018-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sanity check default timeout values</summary>
      <description>Check that the default timeout values for resource release are sanely chosen.</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.configuration.MesosOptions.java</file>
      <file type="M">docs..includes.generated.mesos.configuration.html</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerConfiguration.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ResourceManagerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs..includes.generated.slot.manager.configuration.html</file>
      <file type="M">docs..includes.generated.resource.manager.configuration.html</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="9160" opendate="2018-4-12 00:00:00" fixdate="2018-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make subclasses of RuntimeContext internal that should be internal</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContext.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.util.AbstractRuntimeUDFContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="9163" opendate="2018-4-12 00:00:00" fixdate="2018-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden e2e tests&amp;#39; signal traps and config restoration during abort</summary>
      <description>Signal traps on certain systems, e.g. Linux, may be called concurrently when the trap is caught during its own execution. In that case, our cleanup may just be wrong and may also overly eagerly delete flink-conf.yaml.</description>
      <version>1.5.0,1.5.1,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.sql.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.kafka010.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.savepoint.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="917" opendate="2014-6-9 00:00:00" fixdate="2014-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename netty IO Thread count parameters</summary>
      <description>How about we rename the config parameters for `taskmanager.netty.numOutThreads` and `taskmanager.netty.numInThreads`? That way we make it "independent" of the underlying implementation. The same parameter should also configure the number of I/O threads if we should choose to go with zeroMQ for streaming, or so...---------------- Imported from GitHub ----------------Url: https://github.com/stratosphere/stratosphere/issues/917Created by: StephanEwenLabels: Created at: Sun Jun 08 16:12:44 CEST 2014State: open</description>
      <version>None</version>
      <fixedVersion>0.6-incubating</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">stratosphere-runtime.src.main.java.eu.stratosphere.nephele.taskmanager.TaskManager.java</file>
      <file type="M">stratosphere-core.src.main.java.eu.stratosphere.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="9174" opendate="2018-4-14 00:00:00" fixdate="2018-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The type of state created in ProccessWindowFunction.proccess() is inconsistency</summary>
      <description>The type of state created from windowState and globalState in ProcessWindowFunction.process() is inconsistency. For detail,context.windowState().getListState(); // return type is HeapListState or RocksDBListStatecontext.globalState().getListState(); // return type is UserFacingListStateThis cause the problem in the following code,Iterable&lt;T&gt; iterableState = listState.get(); if (terableState.iterator().hasNext()) { for (T value : iterableState) { value.setRetracting(true); collector.collect(value); } state.clear();}If the listState is created from context.globalState() is fine, but when it created from context.windowState() this will cause NPE. I met this in 1.3.2 but I found it also affect 1.5.0.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorContractTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.DefaultKeyedStateStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="9181" opendate="2018-4-16 00:00:00" fixdate="2018-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add SQL Client documentation page</summary>
      <description>The current implementation of the SQL Client implementation needs documentation for the upcoming 1.5 release. </description>
      <version>None</version>
      <fixedVersion>1.5.0,1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.Rowtime.scala</file>
    </fixedFiles>
  </bug>
  <bug id="9186" opendate="2018-4-16 00:00:00" fixdate="2018-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable dependency convergence for flink-libraries</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.pom.xml</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
      <file type="M">flink-libraries.flink-sql-client.pom.xml</file>
      <file type="M">flink-libraries.flink-ml.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9188" opendate="2018-4-17 00:00:00" fixdate="2018-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide a mechanism to configure AmazonKinesisClient in FlinkKinesisConsumer</summary>
      <description>It should be possible to control the ClientConfiguration to set socket timeout and other properties.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.java</file>
    </fixedFiles>
  </bug>
  <bug id="919" opendate="2014-6-9 00:00:00" fixdate="2014-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Added configuration subsection in webinterface</summary>
      <description>I had some troubles with git so I decided to reopen this request in a new branch (#914 | FLINK-914)@rmetzger---------------- Imported from GitHub ----------------Url: https://github.com/stratosphere/stratosphere/pull/919Created by: JonathanH5Labels: Created at: Mon Jun 09 13:49:29 CEST 2014State: open</description>
      <version>None</version>
      <fixedVersion>pre-apache,0.6-incubating</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">stratosphere-runtime.src.main.java.eu.stratosphere.nephele.jobmanager.web.WebInfoServer.java</file>
      <file type="M">stratosphere-runtime.resources.web-docs-infoserver.index.html</file>
      <file type="M">stratosphere-runtime.resources.web-docs-infoserver.history.html</file>
      <file type="M">stratosphere-runtime.resources.web-docs-infoserver.blank-page.html</file>
      <file type="M">stratosphere-runtime.resources.web-docs-infoserver.analyze.html</file>
    </fixedFiles>
  </bug>
  <bug id="9194" opendate="2018-4-17 00:00:00" fixdate="2018-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Finished jobs are not archived to HistoryServer</summary>
      <description>In flip6 mode, jobs are not archived to the HistoryServer.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.history.FsJobArchivist.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.MiniDispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.SessionClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.JobClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.StandaloneDispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.MiniDispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobPlanHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.history.HistoryServerTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServerArchiveFetcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.history.ArchivedJson.java</file>
    </fixedFiles>
  </bug>
  <bug id="9196" opendate="2018-4-17 00:00:00" fixdate="2018-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>YARN: Flink binaries are not deleted from HDFS after cluster shutdown</summary>
      <description>When deploying on YARN in flip6 mode, the Flink binaries are not deleted from HDFS after the cluster shuts down.Steps to reproduce Submit job in YARN job mode, non-detached: HADOOP_CLASSPATH=`hadoop classpath` bin/flink run -m yarn-cluster -yjm 2048 -ytm 2048 ./examples/streaming/WordCount.jar Check contents of /user/hadoop/.flink/&lt;application_id&gt; on HDFS after job is finished:[hadoop@ip-172-31-43-78 flink-1.5.0]$ hdfs dfs -ls /user/hadoop/.flink/application_1523966184826_0016Found 6 items-rw-r--r-- 1 hadoop hadoop 583 2018-04-17 14:54 /user/hadoop/.flink/application_1523966184826_0016/90cf5b3a-039e-4d52-8266-4e9563d74827-taskmanager-conf.yaml-rw-r--r-- 1 hadoop hadoop 332 2018-04-17 14:54 /user/hadoop/.flink/application_1523966184826_0016/application_1523966184826_0016-flink-conf.yaml3818971235442577934.tmp-rw-r--r-- 1 hadoop hadoop 89779342 2018-04-02 17:08 /user/hadoop/.flink/application_1523966184826_0016/flink-dist_2.11-1.5.0.jardrwxrwxrwx - hadoop hadoop 0 2018-04-17 14:54 /user/hadoop/.flink/application_1523966184826_0016/lib-rw-r--r-- 1 hadoop hadoop 1939 2018-04-02 15:37 /user/hadoop/.flink/application_1523966184826_0016/log4j.properties-rw-r--r-- 1 hadoop hadoop 2331 2018-04-02 15:37 /user/hadoop/.flink/application_1523966184826_0016/logback.xml</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerTest.java</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.YarnJobManager.scala</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterClient.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnConfigurationITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.util.NonDeployingYarnClusterDescriptor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.RestfulGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.util.DummyClusterDescriptor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.StandaloneClusterDescriptor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.LegacyStandaloneClusterDescriptor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.ClusterDescriptor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug id="9212" opendate="2018-4-18 00:00:00" fixdate="2018-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port SubtasksAllAccumulatorsHandler to new REST endpoint</summary>
      <description></description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="9213" opendate="2018-4-18 00:00:00" fixdate="2018-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert breaking change in checkpoint detail URL</summary>
      <description>In 1.4, the URL for retrieving detailed checkpoint information is /jobs/:jobid/checkpoints/details/:checkpointid, whereas in 1.5 it is /jobs/:jobid/checkpoints/:checkpointid.This is a breaking change that also affects the WebUI and should thus be reverted.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointStatisticDetailsHeaders.java</file>
      <file type="M">docs..includes.generated.rest.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug id="9216" opendate="2018-4-19 00:00:00" fixdate="2018-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix comparator violation</summary>
      <description></description>
      <version>1.3.3,1.4.2,1.5.0</version>
      <fixedVersion>1.4.3,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.JSONGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="9236" opendate="2018-4-23 00:00:00" fixdate="2018-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Apache Parent POM 20</summary>
      <description>Flink is still using Apache Parent POM 18. Apache Parent POM 20 is out.This will also fix Javadoc generation with JDK 10+</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-libraries.flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9246" opendate="2018-4-24 00:00:00" fixdate="2018-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HistoryServer job overview broken</summary>
      <description>The jobs overview URL was changed from jobsoverview to jobs/overview. The handlers and UI was properly adjusted, but the HistoryServer was not.As a result the job overview isn't merged properly, causing only a single job to be displayed.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServerArchiveFetcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="9249" opendate="2018-4-24 00:00:00" fixdate="2018-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add convenience profile for skipping non-essential plugins</summary>
      <description>When compiling Flink devs can already set a variety of command line options to speed up the process, for example skipping checkstyle. We also do the same thing on travis.However, not only is it difficult to keep track of all possible options, it is also tedious to write and obfuscates the actual command.I propose adding a fast profile that skips non-essential plugins, including: rat checkstyle scalastyle enforcer japicmp javadoc</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9265" opendate="2018-4-27 00:00:00" fixdate="2018-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Prometheus version</summary>
      <description>We're using 0.0.26Latest release is 2.2.1This issue is for upgrading the Prometheus version</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9274" opendate="2018-4-30 00:00:00" fixdate="2018-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add thread name to Kafka Partition Discovery</summary>
      <description>For debugging, threads should have names to filter on and get a quick overview. The Kafka partition discovery thread(s) currently don't have any name assigned.</description>
      <version>1.4.0,1.4.1,1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="9275" opendate="2018-4-30 00:00:00" fixdate="2018-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set more distinctive output flusher thread names</summary>
      <description>All output flusher threads are named "OutputFlusher" while at the only place the StreamWriter is initialized, we already have the task name at hand.</description>
      <version>1.4.0,1.4.1,1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.StreamRecordWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="9284" opendate="2018-5-2 00:00:00" fixdate="2018-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update CLI page</summary>
      <description>The CLI page must be updated for 1.5.The examples using the -m option must be updated to use 8081.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.cli.md</file>
    </fixedFiles>
  </bug>
  <bug id="9285" opendate="2018-5-2 00:00:00" fixdate="2018-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update REST API page</summary>
      <description>The REST API must be updated for 1.5.The Available requests section still predominantly lists legacy calls. These should be either removed or moved to the bottom, and explicitly marked as legacy.The developing section must be updated.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.rest.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="9306" opendate="2018-5-7 00:00:00" fixdate="2018-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Execute YARN IT tests for legacy and new mode</summary>
      <description>Currently, we are not executing the YARN IT cases for legacy mode.I opened a PR that changes that but it's currently failing on one of the tests in legacy mode: https://github.com/apache/flink/pull/5953</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOSecuredITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="932" opendate="2014-6-12 00:00:00" fixdate="2014-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create Semantic Properties from Project Joins</summary>
      <description>The Project Joins do not create semantic properties right now, which looses optimization potential.</description>
      <version>None</version>
      <fixedVersion>0.6-incubating</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">stratosphere-java.src.main.java.eu.stratosphere.api.java.operators.ProjectOperator.java</file>
      <file type="M">stratosphere-java.src.main.java.eu.stratosphere.api.java.operators.JoinOperator.java</file>
      <file type="M">stratosphere-java.src.main.java.eu.stratosphere.api.java.operators.CrossOperator.java</file>
      <file type="M">stratosphere-java.src.main.java.eu.stratosphere.api.java.functions.SemanticPropUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="9322" opendate="2018-5-9 00:00:00" fixdate="2018-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add exception throwing map function that simulates failures to the general purpose DataStream job</summary>
      <description>The general purpose DataStream job currently does not have any functionality to simulate user job failures.We can achieve this by: Adding a simple-pass map function, that throws exceptions after a certain criteria is met To allow for the end-to-end tests that we have in mind, criteria could be to fail after 1) processing X records, and 2) Y completed checkpoints (see FLINK-8977) We should also allow specifying how many times to fail. Some chaos monkey tests (see FLINK-8973) would need to continuously fail several times, while FLINK-8977, for example, only needs to fail once.</description>
      <version>None</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestJobFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="9323" opendate="2018-5-9 00:00:00" fixdate="2018-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move checkstyle configuration to plugin management</summary>
      <description>The checkstyle plugin configuration is currently duplicated in 4 modules. We should instead move the configuration into the plugin-management section.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-optimizer.pom.xml</file>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9326" opendate="2018-5-9 00:00:00" fixdate="2018-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TaskManagerOptions.NUM_TASK_SLOTS does not work for local/embedded mode</summary>
      <description>When attempting to set the number of task slots via the api such ashconfiguration = new Configuration();configuration.setInteger(TaskManagerOptions.NUM_TASK_SLOTS, 16);configuration.setInteger(CoreOptions.DEFAULT_PARALLELISM, 1);I will always end up with the default slot setting based on the number of cores I have where my standalone instance is running, it doesn't matter what I set the the NUM_TASK_SLOTS to, it has no effect</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.LocalStreamEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="9337" opendate="2018-5-11 00:00:00" fixdate="2018-5-11 01:00:00" resolution="Done">
    <buginformation>
      <summary>Implement AvroDeserializationSchema</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.src.test.resources.flink-1.3-avro-type-serializer-snapshot</file>
      <file type="M">flink-formats.flink-avro.src.test.resources.flink-1.3-avro-type-serialized-data</file>
      <file type="M">flink-formats.flink-avro.src.test.resources.avro.user.avsc</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.utils.TestDataGenerator.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.utils.AvroTestUtils.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.typeutils.BackwardsCompatibleAvroSerializerTest.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.typeutils.AvroSerializer.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroRowDeserializationSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="9338" opendate="2018-5-11 00:00:00" fixdate="2018-5-11 01:00:00" resolution="Done">
    <buginformation>
      <summary>Implement RegistryAvroDeserializationSchema</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9339" opendate="2018-5-11 00:00:00" fixdate="2018-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Accumulators are not UI accessible running in FLIP-6 mode</summary>
      <description>Using 1.5-rc2, when I run a job in flip-6 mode and try to access Accumulators in the UI nothing shows. Looking at the Job manager log there is this error:  2018-05-11 17:09:04,707 ERROR org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler - Could not create the handler request.org.apache.flink.runtime.rest.handler.HandlerRequestException: Cannot resolve path parameter (subtaskindex) from value "accumulators". at org.apache.flink.runtime.rest.handler.HandlerRequest.&lt;init&gt;(HandlerRequest.java:61) at org.apache.flink.runtime.rest.AbstractHandler.respondAsLeader(AbstractHandler.java:155) at org.apache.flink.runtime.rest.handler.RedirectHandler.lambda$null$0(RedirectHandler.java:139) at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137) at java.lang.Thread.run(Thread.java:748) This error does not occur when running the same job in legacy mode.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="9353" opendate="2018-5-14 00:00:00" fixdate="2018-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Kubernetes integration</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-pre-commit-tests.sh</file>
      <file type="M">flink-end-to-end-tests.README.md</file>
    </fixedFiles>
  </bug>
  <bug id="9354" opendate="2018-5-14 00:00:00" fixdate="2018-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>print execution times for end-to-end tests</summary>
      <description>We need to modify the end-to-end scripts to include the time it takes for a test to run.We currently don't have any clue how long a test actually runs for.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.savepoint.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.externalized.checkpoints.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-pre-commit-tests.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="9357" opendate="2018-5-14 00:00:00" fixdate="2018-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add margins to yarn exception excerpts</summary>
      <description>The yarn tests check the log files for exceptions to detect test failures. If detected a test will fail and an excerpt from the logs will be printed.The excerpt content is currently the stack of the detected exception. This only works correctly if the stacktrace follows a specific formatting style; for example if an exception message contains an empty line the output will be cut off.I propose including the 10 before/after the found exception to make this a bit more reliable. As a side-effect we also get a little contextual information.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="9358" opendate="2018-5-14 00:00:00" fixdate="2018-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Closing of unestablished RM connections can cause NPE</summary>
      <description>When closing an unestablished RM connection, a NPE is thrown. The reason is that we try to unmonitor a non-existing heartbeat target.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="9365" opendate="2018-5-15 00:00:00" fixdate="2018-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add version information to remote rpc messages</summary>
      <description>In order to quickly detect components which use incompatible RPC versions, we should add a version information to all remote RPC messages. That way we could easily detect if an older incompatible component tries to communicate with a newer version.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActor.java</file>
    </fixedFiles>
  </bug>
  <bug id="9368" opendate="2018-5-15 00:00:00" fixdate="2018-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Python API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-pre-commit-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="9372" opendate="2018-5-15 00:00:00" fixdate="2018-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo on Elasticsearch website link (elastic.io --&gt; elastic.co)</summary>
      <description>Typo on website link in Elasticsearch Java Docs (elastic.io --&gt; elastic.co)</description>
      <version>1.4.1,1.4.2,1.5.0,1.5.1</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.ElasticsearchSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="9373" opendate="2018-5-16 00:00:00" fixdate="2018-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix potential data losing for RocksDBBackend</summary>
      <description>Currently, when using RocksIterator we only use the iterator.isValid() to check whether we have reached the end of the iterator. But that is not enough, if we refer to RocksDB's wiki https://github.com/facebook/rocksdb/wiki/Iterator#error-handling we should find that even if iterator.isValid()=true, there may also exist some internal error. A safer way to use the RocksIterator is to always call the iterator.status() to check the internal error of RocksDB. There is a case from user email seems to lost data because of this http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Missing-MapState-when-Timer-fires-after-restored-state-td20134.html</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBRocksIteratorWrapperTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBMergeIteratorTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.benchmark.RocksDBPerformanceTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMapState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="9380" opendate="2018-5-16 00:00:00" fixdate="2018-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failing end-to-end tests should not clean up logs</summary>
      <description>Some of the end-to-end tests clean up their logs also in the failure case. This makes debugging and understanding the problem extremely difficult. Ideally, the scripts says where it stored the respective logs.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="9387" opendate="2018-5-17 00:00:00" fixdate="2018-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Several log message errors in queryable-state module</summary>
      <description></description>
      <version>1.5.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.main.java.org.apache.flink.queryablestate.client.proxy.KvStateClientProxyHandler.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.AbstractServerHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="9392" opendate="2018-5-17 00:00:00" fixdate="2018-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add @FunctionalInterface annotations to all core functional interfaces</summary>
      <description>The @FunctionalInterface annotation should be added to all SAM interfaces in order to prevent accidentally breaking them (as non SAMs).We had a case of that before for the SinkFunction which was compatible through default methods, but incompatible for users that previously instantiated that interface through a lambda.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.ReduceFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.Partitioner.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.MapPartitionFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.MapFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.JoinFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.GroupReduceFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.GroupCombineFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.FoldFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.FlatMapFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.FlatJoinFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.FilterFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.CrossFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.CombineFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.CoGroupFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.BroadcastVariableInitializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="9402" opendate="2018-5-19 00:00:00" fixdate="2018-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kinesis consumer validation incorrectly requires aws.region property</summary>
      <description>AwsClientBuilder says: Only one of Region or EndpointConfiguration may be set.But KinesisUtil still thinks: The AWS region ('aws.region') must be set in the config.This doesn't affect configuration based on region, but makes testing with Kinesalite impossible. The Flink code needs to match the new opinion in AWS SDK (probably changed with . recent update). </description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="9408" opendate="2018-5-22 00:00:00" fixdate="2018-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retry JM-RM connection in case of explicit disconnect</summary>
      <description>The JM should try to reconnect to the RM not only in the case of a heartbeat timeout but also in case of an explicit disconnect.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.EstablishedResourceManagerConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="9415" opendate="2018-5-23 00:00:00" fixdate="2018-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove reference to StreamingMultipleProgramsTestBase in docs</summary>
      <description></description>
      <version>1.5.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.testing.md</file>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.StreamingOperatorsITCase.scala</file>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.ScalaStreamingMultipleProgramsTestBase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="9416" opendate="2018-5-23 00:00:00" fixdate="2018-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make job submission retriable operation in case of a ongoing leader election</summary>
      <description>When starting a session cluster, it can happen that the job submission fails if the REST server endpoint has already gained leadership but if the leadership election for the Dispatcher is still ongoing. In such a case, we receive a error response saying that the leader election is still ongoing and fail the job submission. I think it would be nicer to also make the submission step a retriable operation in order to avoid this race condition.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="942" opendate="2014-6-16 00:00:00" fixdate="2014-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Config key "env.java.opts" does not effect JVM args</summary>
      <description>Setting custom args for the JVM as in env.java.opts: -Dio.netty.leakDetectionLevel=paranoidhas no effect for the started JVMs.A fix is coming up.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">stratosphere-dist.src.main.stratosphere-bin.bin.taskmanager.sh</file>
      <file type="M">stratosphere-dist.src.main.stratosphere-bin.bin.jobmanager.sh</file>
      <file type="M">stratosphere-dist.src.main.stratosphere-bin.bin.config.sh</file>
    </fixedFiles>
  </bug>
  <bug id="9420" opendate="2018-5-23 00:00:00" fixdate="2018-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests for SQL IN sub-query operator in streaming</summary>
      <description>In FLINK-6094 we implemented non-windowed inner joins. The Table API &amp; SQL should now support the IN operator for sub-queries in streaming. Batch support has been added in FLINK-4565. We need to add unit tests, an IT case, and update the docs about that.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.validation.UnsupportedOpsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.subquery.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug id="9424" opendate="2018-5-23 00:00:00" fixdate="2018-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BlobClientSslTest does not work in all environments</summary>
      <description>It seems that the BlobClientSslTest assumes SSL algorithms that are not present in every environment. Thus, they cause the Flink build to fail. It also affects NettyClientServerSslTest.Environment:Apache Maven 3.5.3 (3383c37e1f9e9b3bc3df5050c29c8aff9f295297; 2018-02-24T20:49:05+01:00)Maven home: /usr/local/Cellar/maven/3.5.3/libexecJava version: 1.8.0_102, vendor: Oracle CorporationJava home: /Library/Java/JavaVirtualMachines/jdk1.8.0_102.jdk/Contents/Home/jreDefault locale: en_US, platform encoding: UTF-8OS name: "mac os x", version: "10.13.3", arch: "x86_64", family: "mac"Exception:java.lang.IllegalArgumentException: Cannot support TLS_DHE_RSA_WITH_AES_256_GCM_SHA384 with currently installed providers at sun.security.ssl.CipherSuiteList.&lt;init&gt;(CipherSuiteList.java:92) at sun.security.ssl.SSLServerSocketImpl.setEnabledCipherSuites(SSLServerSocketImpl.java:200) at org.apache.flink.runtime.net.SSLUtils.setSSLVerAndCipherSuites(SSLUtils.java:84) at org.apache.flink.runtime.blob.BlobServer.&lt;init&gt;(BlobServer.java:207) at org.apache.flink.runtime.blob.BlobClientSslTest.startSSLServer(BlobClientSslTest.java:65)</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
      <file type="M">docs..includes.generated.security.configuration.html</file>
      <file type="M">docs.ops.security-ssl.md</file>
    </fixedFiles>
  </bug>
  <bug id="9428" opendate="2018-5-23 00:00:00" fixdate="2018-5-23 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Allow operators to flush data on checkpoint pre-barrier</summary>
      <description>Some operators maintain some small transient state that may be inefficient to checkpoint, especially when it would need to be checkpointed also in a re-scalable way.An example are opportunistic pre-aggregation operators, which have small the pre-aggregation state that is frequently flushed downstream.Rather that persisting that state in a checkpoint, it can make sense to flush the data downstream upon a checkpoint, to let it be part of the downstream operator's state.This feature is sensitive, because flushing state has a clean implication on the downstream operator's checkpoint alignment. However, used with care, and with the new back-pressure-based checkpoint alignment, this feature can be very useful.Because it is sensitive, I suggest to make this only an internal feature (accessible to operators) and NOT expose it in the public API at this point.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperatorLifecycleTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="9429" opendate="2018-5-23 00:00:00" fixdate="2018-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Quickstart E2E not working locally</summary>
      <description>The quickstart e2e test is not working locally. It seems as if the job does not produce anything into Elasticsearch. Furthermore, the test does not terminate with control-C.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-single-test.sh</file>
    </fixedFiles>
  </bug>
  <bug id="9463" opendate="2018-5-29 00:00:00" fixdate="2018-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setting taskmanager.network.netty.transport to epoll</summary>
      <description>https://github.com/apache/flink-shaded/issues/30 </description>
      <version>1.4.0,1.4.1,1.4.2,1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.ExceptionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="9467" opendate="2018-5-29 00:00:00" fixdate="2018-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>No Watermark display on Web UI</summary>
      <description>Watermark is currently not shown on the web interface,  because it still queries for watermark using the old metric name `currentLowWatermark` instead of the new ones `currentInputWatermark` and `currentOutputWatermark` </description>
      <version>1.5.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.hs.index.js</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobs.jobs.ctrl.coffee</file>
      <file type="M">docs.monitoring.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug id="947" opendate="2014-6-18 00:00:00" fixdate="2014-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for "Named Datasets"</summary>
      <description>This would create an API that is a mix between SQL like declarativity and the power of user defined functions. Example user code could look like this:NamedDataSet one = ...NamedDataSet two = ...NamedDataSet result = one.join(two).where("key").equalTo("otherKey") .project("a", "b", "c") .map( (UserTypeIn in) -&gt; return new UserTypeOut(...) ) .print();</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.pom.xml</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.TryTypeInfo.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.TraversableTypeInfo.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.OptionTypeInfo.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.EitherTypeInfo.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.CaseClassTypeInfo.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.codegen.TreeGen.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.PojoField.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.Keys.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeinfo.TypeInformation.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeinfo.BasicTypeInfo.java</file>
      <file type="M">docs..includes.sidenav.html</file>
      <file type="M">flink-staging.flink-expressions.src.test.scala.org.apache.flink.api.scala.expressions.StringExpressionsITCase.scala</file>
      <file type="M">flink-staging.flink-expressions.src.test.scala.org.apache.flink.api.scala.expressions.SelectITCase.scala</file>
      <file type="M">flink-staging.flink-expressions.src.test.scala.org.apache.flink.api.scala.expressions.PageRankExpressionITCase.java</file>
      <file type="M">flink-staging.flink-expressions.src.test.scala.org.apache.flink.api.scala.expressions.JoinITCase.scala</file>
      <file type="M">flink-staging.flink-expressions.src.test.scala.org.apache.flink.api.scala.expressions.GroupedAggreagationsITCase.scala</file>
      <file type="M">flink-staging.flink-expressions.src.test.scala.org.apache.flink.api.scala.expressions.FilterITCase.scala</file>
      <file type="M">flink-staging.flink-expressions.src.test.scala.org.apache.flink.api.scala.expressions.ExpressionsITCase.scala</file>
      <file type="M">flink-staging.flink-expressions.src.test.scala.org.apache.flink.api.scala.expressions.CastingITCase.scala</file>
      <file type="M">flink-staging.flink-expressions.src.test.scala.org.apache.flink.api.scala.expressions.AsITCase.scala</file>
      <file type="M">flink-staging.flink-expressions.src.test.scala.org.apache.flink.api.scala.expressions.AggregationsITCase.scala</file>
      <file type="M">flink-staging.flink-expressions.src.main.scala.org.apache.flink.api.scala.expressions.JavaStreamingTranslator.scala</file>
      <file type="M">flink-staging.flink-expressions.src.main.scala.org.apache.flink.api.scala.expressions.JavaBatchTranslator.scala</file>
      <file type="M">flink-staging.flink-expressions.src.main.scala.org.apache.flink.api.scala.expressions.expressionDsl.scala</file>
      <file type="M">flink-staging.flink-expressions.src.main.scala.org.apache.flink.api.expressions.ExpressionOperation.scala</file>
      <file type="M">flink-staging.flink-expressions.src.main.scala.org.apache.flink.api.expressions.analysis.PredicateAnalyzer.scala</file>
      <file type="M">flink-staging.flink-expressions.src.main.java.org.apache.flink.examples.java.JavaExpressionExample.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TupleTypeInfoBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.CompositeType.java</file>
    </fixedFiles>
  </bug>
  <bug id="9476" opendate="2018-5-30 00:00:00" fixdate="2018-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lost sideOutPut Late Elements in CEP Operator</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CepOperatorTestUtilities.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.PatternStream.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.SelectTimeoutCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.SelectCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.FlatSelectTimeoutCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.FlatSelectCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.CEPOperatorUtils.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep-scala.src.main.scala.org.apache.flink.cep.scala.PatternStream.scala</file>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug id="9488" opendate="2018-6-1 00:00:00" fixdate="2018-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create common entry point for master and workers</summary>
      <description>To make the container setup easier, we should provide a single cluster entry point which uses leader election to become either the master or a worker which runs the TaskManager.</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.entrypoint.ClusterConfigurationParserFactoryTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="9489" opendate="2018-6-1 00:00:00" fixdate="2018-7-1 01:00:00" resolution="Done">
    <buginformation>
      <summary>Checkpoint timers as part of managed keyed state instead of raw keyed state</summary>
      <description>Timer state should now become part of the keyed state backend snapshot, i.e., stored inside the managed keyed state. This means that we have to connect our preparation for asynchronous checkpoints with the backend, so that the timers are written as part of the state for each key-group. This means that we will also free up the raw keyed state an might expose it to user functions in the future.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.TimerSerializer.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.HeapInternalTimerServiceTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.BackendWritableBroadcastState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.DefaultOperatorStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.HeapBroadcastState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.CachingInternalPriorityQueueSet.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.CopyOnWriteStateTableSnapshot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapPriorityQueue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapPriorityQueueSetFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.KeyGroupPartitionedPriorityQueue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.NestedMapsStateTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.StateTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.StateTableByKeyGroupReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.StateTableByKeyGroupReaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyExtractorFunction.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyGroupPartitioner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.metainfo.StateMetaInfoSnapshot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.metainfo.StateMetaInfoSnapshotReadersWriters.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.PriorityComparator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.PriorityQueueSetFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.RegisteredBroadcastBackendStateMetaInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.RegisteredKeyedBackendStateMetaInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.RegisteredOperatorBackendStateMetaInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.RegisteredStateMetaInfoBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateSnapshot.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TieBreakingPriorityComparator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.CopyOnWriteStateTableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.StateTableSnapshotCompatibilityTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.KeyGroupPartitionerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.metainfo.StateMetaInfoSnapshotEnumConstantsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.SerializationProxiesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateSnapshotCompressionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBAggregatingState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBFoldingState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBListState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBMapState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOrderedSetStore.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBReducingState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBValueState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBWriteBatchWrapper.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.HeapPriorityQueueStateSnapshot.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimersSnapshotReaderWriters.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimeServiceManager.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.TimerHeapInternalTimer.java</file>
    </fixedFiles>
  </bug>
  <bug id="9503" opendate="2018-6-1 00:00:00" fixdate="2018-7-1 01:00:00" resolution="Done">
    <buginformation>
      <summary>Migrate integration tests for iterative aggregators</summary>
      <description>Migrate integration tests in org.apache.flink.test.iterative.aggregators to use collect() instead of temp files. Related to parent jira.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.aggregators.AggregatorsITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="9508" opendate="2018-6-3 00:00:00" fixdate="2018-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>General Spell Check on Flink Docs</summary>
      <description>Fixing Flink docs misspelling </description>
      <version>1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.upgrading.md</file>
      <file type="M">docs.ops.security-ssl.md</file>
      <file type="M">docs.ops.filesystems.md</file>
      <file type="M">docs.ops.cli.md</file>
      <file type="M">docs.dev.execution.configuration.md</file>
    </fixedFiles>
  </bug>
  <bug id="9518" opendate="2018-6-4 00:00:00" fixdate="2018-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SSL setup Docs config example has wrong keys password</summary>
      <description>In creating keystores and turststore section password is set to password but in setup config section it is abc123</description>
      <version>1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.security-ssl.md</file>
    </fixedFiles>
  </bug>
  <bug id="9530" opendate="2018-6-5 00:00:00" fixdate="2018-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Task numRecords metrics broken for chains</summary>
      <description>The numRecordsIn/Out metrics for tasks is currently broken. We are wrongly adding up the numRecordsIn/Out metrics for all operators in the chain, instead of just the head/tail operators.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
    </fixedFiles>
  </bug>
  <bug id="9532" opendate="2018-6-6 00:00:00" fixdate="2018-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink Overview of Jobs Documentation Incorrect</summary>
      <description>Link"Jobs, grouped by status, each with a small summary of its status."This statement is incorrect as per the new response format.</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.rest.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="9539" opendate="2018-6-6 00:00:00" fixdate="2018-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate flink-shaded 4.0</summary>
      <description>With the recent release of flink-shaded 4.0 we should bump the versions for all dependencies (except netty which is handled in FLINK-3952).We can now remove the exclusions from the jackson dependencies as they are now properly hidden.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9549" opendate="2018-6-7 00:00:00" fixdate="2018-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix FlickCEP Docs broken link and minor style changes</summary>
      <description></description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug id="9555" opendate="2018-6-8 00:00:00" fixdate="2018-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support table api in scala shell</summary>
      <description>It would be nice to have table api available in scala shell so that user can experience table api in interactive way. </description>
      <version>1.5.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.start-script.start-scala-shell.sh</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkILoop.scala</file>
      <file type="M">flink-scala-shell.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9576" opendate="2018-6-13 00:00:00" fixdate="2018-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong contiguity documentation</summary>
      <description>Example for the contiguity is first of all wrong, and second of all misleading: To illustrate the above with an example, a pattern sequence "a+ b" (one or more "a"’s followed by a "b") with input "a1", "c", "a2", "b" will have the following results:Strict Contiguity: {a2 b} – the "c" after "a1" causes "a1" to be discarded.Relaxed Contiguity: {a1 b} and {a1 a2 b} – "c" is ignored.Non-Deterministic Relaxed Contiguity: {a1 b}, {a2 b}, and {a1 a2 b}.For looping patterns (e.g. oneOrMore() and times()) the default is relaxed contiguity. If you want strict contiguity, you have to explicitly specify it by using the consecutive() call, and if you want non-deterministic relaxed contiguity you can use the allowCombinations() call. Results for the relaxed contiguity are wrong plus they do not clearly explains the internal contiguity of kleene closure.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug id="9578" opendate="2018-6-13 00:00:00" fixdate="2018-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow to define an auto watermark interval in SQL Client</summary>
      <description>Currently it is not possible to define an auto watermark interval in a non-programmatic way for the SQL Client.</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.EnvironmentTest.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.PropertyStrings.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Execution.java</file>
      <file type="M">flink-libraries.flink-sql-client.conf.sql-client-defaults.yaml</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="9579" opendate="2018-6-13 00:00:00" fixdate="2018-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary clear with cep elementQueue</summary>
      <description>when deal with eventtime, the elementQueueState is cleared when sortedTimestamps isEmpty, but I think this operation is not needed because the elements in elementQueueState are all removed if the sortedTimestamps isEmpty, and do not need to clear again to spend time on rocksdb operation? what's your idea dawidwys?</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="9588" opendate="2018-6-14 00:00:00" fixdate="2018-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reuse the same conditionContext with in a same computationState</summary>
      <description>Now cep checkFilterCondition with a newly created Conditioncontext for each edge, which will result in the repeatable getEventsForPattern because of the different Conditioncontext Object.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.NFA.java</file>
    </fixedFiles>
  </bug>
  <bug id="9590" opendate="2018-6-14 00:00:00" fixdate="2018-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HistogramDump should be immutable</summary>
      <description>The HistogramDump represents the contents of a histogram at one point in time, and should thus not be mutable.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.dump.MetricDump.java</file>
    </fixedFiles>
  </bug>
  <bug id="9595" opendate="2018-6-15 00:00:00" fixdate="2018-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add instructions to docs about ceased support of KPL version used in Kinesis connector</summary>
      <description>The KPL version used in the Kinesis connector for FlinkKinesisProducer is not longer supported by AWS Kinesis Streams: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Flink-1-4-and-below-STOPS-writing-to-Kinesis-after-June-12th-td22687.html#a22701We should add a notice about this to the Kinesis connectors, and how to bypass it by specifying the KPL version when building the Kinesis connector.</description>
      <version>1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug id="9627" opendate="2018-6-20 00:00:00" fixdate="2018-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extending &amp;#39;KafkaJsonTableSource&amp;#39; according to comments will result in NPE</summary>
      <description>According to the comments what is needed to extend the 'KafkaJsonTableSource' looks as follows: A version-agnostic Kafka JSON {@link StreamTableSource}.** &lt;p&gt;The version-specific Kafka consumers need to extend this class and* override {@link #createKafkaConsumer(String, Properties, DeserializationSchema)}}.** &lt;p&gt;The field names are used to parse the JSON file and so are the types.This will cause an NPE, since there is no default value for startupMode in the abstract class itself only in the builder of this class. For the 'getKafkaConsumer' method the switch statement will be executed on non-initialized 'startupMode' field:switch (startupMode) {case EARLIEST:kafkaConsumer.setStartFromEarliest();break;case LATEST:kafkaConsumer.setStartFromLatest();break;case GROUP_OFFSETS:kafkaConsumer.setStartFromGroupOffsets();break;case SPECIFIC_OFFSETS:kafkaConsumer.setStartFromSpecificOffsets(specificStartupOffsets);break;}  </description>
      <version>1.5.0</version>
      <fixedVersion>1.4.3,1.5.1,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="9629" opendate="2018-6-21 00:00:00" fixdate="2018-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Datadog metrics reporter does not have shaded dependencies</summary>
      <description>flink-metrics-datadog-1.5.0.jar does not contain shaded dependencies for okhttp3 and okio</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-datadog.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9633" opendate="2018-6-21 00:00:00" fixdate="2018-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink doesn&amp;#39;t use the Savepoint path&amp;#39;s filesystem to create the OuptutStream on Task.</summary>
      <description>Currently, flink use the Savepoint's filesystem to create the meta output stream in CheckpointCoordinator(JM side), but in StreamTask(TM side) it uses the Checkpoint's filesystem to create the checkpoint data output stream. When the Savepoint &amp; Checkpoint in different filesystem this will lead to problematic.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStorageTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStorageLocation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStorage.java</file>
    </fixedFiles>
  </bug>
  <bug id="9634" opendate="2018-6-21 00:00:00" fixdate="2018-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deactivate previous location based scheduling if local recovery is disabled</summary>
      <description>With Flink 1.5.0 we introduced local recovery. In order to make local recovery work we had to change the scheduling to be aware of the previous location of the Execution. This scheduling strategy is also active if local recovery is deactivated. I suggest to also disable the scheduling strategy if local recovery is not enabled.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolSchedulingTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPoolRpcTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.AvailableSlotsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.SchedulerTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.SlotProfileTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.SlotPool.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.DefaultSlotPoolFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.SlotProfile.java</file>
    </fixedFiles>
  </bug>
  <bug id="9655" opendate="2018-6-25 00:00:00" fixdate="2018-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Externalized checkpoint E2E test fails on travis</summary>
      <description>https://travis-ci.org/zentol/flink-ci/builds/396395491 ==============================================================================Running 'Resuming Externalized Checkpoint after terminal failure (file, sync) end-to-end test'==============================================================================Flink dist directory: /home/travis/build/zentol/flink-ci/flink/build-targetTEST_DATA_DIR: /home/travis/build/zentol/flink-ci/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-47420177246Starting cluster.Starting standalonesession daemon on host travis-job-b06f9adc-9d05-4569-b4bc-af11e6a0bc6e.Starting taskexecutor daemon on host travis-job-b06f9adc-9d05-4569-b4bc-af11e6a0bc6e.Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Dispatcher REST endpoint is up.Running externalized checkpoints test, with ORIGINAL_DOP=file NEW_DOP=false and STATE_BACKEND_TYPE=false STATE_BACKEND_FILE_ASYNC=true STATE_BACKEND_ROCKSDB_INCREMENTAL=false SIMULATE_FAILURE=false ...Job () is running.Waiting for job (1) to have at least completed checkpoints ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="9666" opendate="2018-6-27 00:00:00" fixdate="2018-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>short-circuit logic should be used in boolean contexts</summary>
      <description>short-circuit logic should be used in boolean contexts</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.UnilateralSortMerger.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.NormalizedKeySorter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.ReOpenableMutableHashTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.SlotSharingGroupAssignment.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.operators.CartesianProductDescriptor.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dag.TwoInputNode.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.StringValue.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.runtime.PojoComparator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="9672" opendate="2018-6-27 00:00:00" fixdate="2018-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fail fatally if we cannot submit job on added JobGraph signal</summary>
      <description>The SubmittedJobGraphStore signals when new JobGraphs are added. If this happens, then the leader should recover this job and submit it. If the recovery/submission should fail for some reason, then we should fail fatally to restart the process which will then try to recover the jobs again.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="9681" opendate="2018-6-27 00:00:00" fixdate="2018-7-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Make sure minRetentionTime not equal to maxRetentionTime</summary>
      <description>Currently, for a group by(or other operators), if minRetentionTime equals to maxRetentionTime, the group by operator will register a timer for each record coming at different time which cause performance problem. The reasoning for having two parameters is that we can avoid to register many timers if we have more freedom when to discard state. As min equals to max cause performance problem it is better to make sure these two parameters are not same.Any suggestions are welcome.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.operators.ProcessFunctionWithCleanupStateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.operators.KeyedProcessFunctionWithCleanupStateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.StateCleaningCountTriggerHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.OverWindowHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.NonWindowHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.JoinHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.queryConfig.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Execution.java</file>
      <file type="M">docs.dev.table.streaming.md</file>
    </fixedFiles>
  </bug>
  <bug id="9697" opendate="2018-6-30 00:00:00" fixdate="2018-11-30 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Provide connector for modern Kafka</summary>
      <description>Kafka 2.0.0 would be released soon.Here is vote thread:http://search-hadoop.com/m/Kafka/uyzND1vxnEd23QLxb?subj=+VOTE+2+0+0+RC1We should provide connector for Kafka 2.0.0 once it is released.Upgrade to 2.0 documentation : http://kafka.apache.org/20/documentation.html#upgrade_2_0_0</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSinkTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.table.descriptors.KafkaValidator.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.metrics.KafkaMetricMuttableWrapper.java</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010TableSourceSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010AvroTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010JsonTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010TableSourceSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka011TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka011TableSourceSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka011AvroTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka011JsonTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka011TableSourceSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08TableSourceSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08AvroTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08TableSourceSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09TableSourceSinkFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09AvroTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09TableSourceSinkFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaAvroTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaJsonTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaAvroTableSourceTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaJsonTableSourceFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceBuilderTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerCallBridge010.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThreadTest.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010JsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010TableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010JsonTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka011TableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08TableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09TableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaJsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="970" opendate="2014-6-24 00:00:00" fixdate="2014-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement a first(n) operator</summary>
      <description>It is only syntactic sugar, but I had many cases where I just needed the first element or the first 2 elements in a GroupReduce.E.g. Instead of.reduceGroup(new GroupReduceFunction&lt;String, String&gt;() { @Override public void reduce(Iterator&lt;String&gt; values, Collector&lt;String&gt; out) throws Exception { out.collect(values.next()); } }).first()</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.UnsortedGrouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SortedGrouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.DataSet.java</file>
    </fixedFiles>
  </bug>
  <bug id="9701" opendate="2018-7-2 00:00:00" fixdate="2018-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate TTL in state descriptors</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.StateBackendITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TestSpyWrapperStateBackend.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTerminationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImplTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlValueStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlStateTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlReducingStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlMergingStateBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlMapStateTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlMapStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlMapStatePerElementTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlListStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlFoldingStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.TtlAggregatingStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockKeyedStateFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockInternalMapState.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockInternalKvState.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.MockTimeProvider.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateSnapshotCompressionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.HeapStateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackendSnapshotMigrationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.HeapKeyedStateBackendAsyncByDefaultTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointSettingsSerializableTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.TtlTimeProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.TtlReducingState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.TtlMapState.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.AbstractTtlDecorator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.MemoryStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyedStateFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractKeyedStateBackend.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KvStateServerHandlerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KvStateRequestSerializerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KVStateRequestSerializerRocksDBTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.CompositeSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.ttl.TtlStateFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.StateTtlConfiguration.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.StateDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="9712" opendate="2018-7-3 00:00:00" fixdate="2018-4-3 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support enrichment joins in Flink SQL/Table API</summary>
      <description>As described here:https://docs.google.com/document/d/1KaAkPZjWFeu-ffrC9FhYuxE6CIKsatHTTxyrxSBR8Sk/edit?usp=sharing</description>
      <version>1.5.0</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.streaming.index.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug id="9713" opendate="2018-7-3 00:00:00" fixdate="2018-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support versioned joins in planning phase</summary>
      <description>Queries like:SELECT o.amount * r.rate FROM Orders AS o, LATERAL TABLE (Rates(o.rowtime)) AS r WHERE o.currency = r.currencyshould evaluate to valid plan with versioned joins plan node.</description>
      <version>1.5.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.validation.TemporalTableJoinValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.TemporalTableJoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug id="9714" opendate="2018-7-3 00:00:00" fixdate="2018-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support versioned joins with processing time</summary>
      <description>Queries like:SELECT o.amount * r.rate FROM Orders AS o, LATERAL TABLE (Rates(o.proctime)) AS r WHERE o.currency = r.currencyshould work for processing time</description>
      <version>1.5.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.utils.StreamingWithStateTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.validation.TemporalTableJoinValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamTemporalTableJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.CommonJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalTemporalTableJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug id="9715" opendate="2018-7-3 00:00:00" fixdate="2018-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support versioned joins with event time</summary>
      <description>Queries like:SELECT o.amount * r.rate FROM Orders AS o, LATERAL TABLE (Rates(o.rowtime)) AS r WHERE o.currency = r.currencyshould work with event time</description>
      <version>1.5.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.TemporalJoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.TemporalJoinHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.validation.TemporalTableJoinValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamTemporalJoinToCoProcessTranslator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="9730" opendate="2018-7-3 00:00:00" fixdate="2018-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>avoid access static via class reference</summary>
      <description>&amp;#91;code refactor&amp;#93; access static via class reference</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.AbstractServerHandler.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.async.AsyncIOExample.java</file>
    </fixedFiles>
  </bug>
  <bug id="9735" opendate="2018-7-4 00:00:00" fixdate="2018-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Potential resource leak in RocksDBStateBackend#getDbOptions</summary>
      <description>Here is related code: if (optionsFactory != null) { opt = optionsFactory.createDBOptions(opt); }opt, an DBOptions instance, should be closed before being rewritten.getColumnOptions has similar issue.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBResource.java</file>
    </fixedFiles>
  </bug>
  <bug id="9737" opendate="2018-7-4 00:00:00" fixdate="2018-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow to define Table Version Functions from SQL environment file</summary>
      <description>It should be possible to define TVF from SQL environment file.</description>
      <version>1.5.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.FunctionService.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.DescriptorProperties.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-factory.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-factory-services-file</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.TestTableSourceFactory.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.TestTableSinkFactory.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.EnvironmentTest.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.SessionContext.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ResultStore.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.UserDefinedFunction.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.SourceSink.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Source.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Sink.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.PropertyStrings.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Execution.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Environment.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Deployment.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.ConfigUtil.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
      <file type="M">flink-libraries.flink-sql-client.conf.sql-client-defaults.yaml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="9738" opendate="2018-7-4 00:00:00" fixdate="2018-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide a way to define Table Version Functions in Table API</summary>
      <description></description>
      <version>1.5.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalTableFunctionScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.fieldExpression.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.table.scala</file>
    </fixedFiles>
  </bug>
  <bug id="9742" opendate="2018-7-4 00:00:00" fixdate="2018-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose Expression.resultType to public</summary>
      <description>I have use case of TableSource which requires custom implementation of TimestampExtractor. To ensure new TimestampExtractor to cover more general use cases, accessing Expression.resultType is necessary, but its scope is now defined as package private for "org.apache.flink".Below is the implementation of custom TimestampExtractor which leverages Expression.resultType, hence had to place it to org.apache.flink package (looks like a hack).class IsoDateStringAwareExistingField(val field: String) extends TimestampExtractor { override def getArgumentFields: Array[String] = Array(field) override def validateArgumentFields(argumentFieldTypes: Array[TypeInformation[_]]): Unit = { val fieldType = argumentFieldTypes(0) fieldType match { case Types.LONG =&gt; // OK case Types.SQL_TIMESTAMP =&gt; // OK case Types.STRING =&gt; // OK case _: TypeInformation[_] =&gt; throw ValidationException( s"Field '$field' must be of type Long or Timestamp or String but is of type $fieldType.") } } override def getExpression(fieldAccesses: Array[ResolvedFieldReference]): Expression = { val fieldAccess: Expression = fieldAccesses(0) fieldAccess.resultType match { case Types.LONG =&gt; // access LONG field fieldAccess case Types.SQL_TIMESTAMP =&gt; // cast timestamp to long Cast(fieldAccess, Types.LONG) case Types.STRING =&gt; Cast(Cast(fieldAccess, SqlTimeTypeInfo.TIMESTAMP), Types.LONG) } }}It would be better to just make Expression.resultType public to cover other cases as well. (I'm not sure other methods would be also better to be public as well.)</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.ExpressionUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug id="9765" opendate="2018-7-5 00:00:00" fixdate="2018-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve CLI responsiveness when cluster is not reachable</summary>
      <description>If the cluster was not started or is not reachable it takes a long time to cancel a result. This should not affect the main thread. The CLI should be responsive at all times.</description>
      <version>1.5.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliResultView.java</file>
    </fixedFiles>
  </bug>
  <bug id="9771" opendate="2018-7-6 00:00:00" fixdate="2018-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Show Plan" option under Submit New Job in WebUI not working</summary>
      <description>Show Plan button under Submit new job in WebUI not working.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobPlanInfoTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobPlanInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="9772" opendate="2018-7-6 00:00:00" fixdate="2018-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation of Hadoop API outdated</summary>
      <description>It looks like the documentation of the Hadoop Compatibility is somewhat outdated? At least the text and examples in section Using Hadoop InputFormats mention methodsenv.readHadoopFile and env.createHadoopInputwhich do not exist anymore since 1.4.0.   </description>
      <version>1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.batch.index.md</file>
      <file type="M">docs.dev.batch.hadoop.compatibility.md</file>
    </fixedFiles>
  </bug>
  <bug id="9781" opendate="2018-7-9 00:00:00" fixdate="2018-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>scala-maven-plugin fails on java 9</summary>
      <description>https://travis-ci.org/zentol/flink/jobs/40171125811:10:02.157 [INFO] --- scala-maven-plugin:3.2.2:compile (scala-compile-first) @ flink-runtime_2.11 ---11:10:04.861 [INFO] /home/travis/build/zentol/flink/flink-runtime/src/main/java:-1: info: compiling11:10:04.862 [INFO] /home/travis/build/zentol/flink/flink-runtime/src/main/scala:-1: info: compiling11:10:04.862 [INFO] Compiling 1486 source files to /home/travis/build/zentol/flink/flink-runtime/target/classes at 153113460486211:10:06.135 [ERROR] error: java.lang.NoClassDefFoundError: javax/tools/ToolProvider11:10:06.135 [INFO] at scala.reflect.io.JavaToolsPlatformArchive.iterator(ZipArchive.scala:301)11:10:06.135 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)11:10:06.135 [INFO] at scala.reflect.io.AbstractFile.foreach(AbstractFile.scala:92)11:10:06.135 [INFO] at scala.tools.nsc.util.DirectoryClassPath.traverse(ClassPath.scala:277)11:10:06.135 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15$lzycompute(ClassPath.scala:299)11:10:06.136 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15(ClassPath.scala:299)11:10:06.136 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages$lzycompute(ClassPath.scala:299)11:10:06.136 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:299)11:10:06.136 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:264)11:10:06.136 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)11:10:06.136 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)11:10:06.136 [INFO] at scala.collection.Iterator$class.foreach(Iterator.scala:891)11:10:06.136 [INFO] at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)11:10:06.136 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)11:10:06.136 [INFO] at scala.collection.AbstractIterable.foreach(Iterable.scala:54)11:10:06.136 [INFO] at scala.tools.nsc.util.MergedClassPath.packages$lzycompute(ClassPath.scala:358)11:10:06.136 [INFO] at scala.tools.nsc.util.MergedClassPath.packages(ClassPath.scala:353)11:10:06.136 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply$mcV$sp(SymbolLoaders.scala:269)11:10:06.136 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)11:10:06.136 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)11:10:06.136 [INFO] at scala.reflect.internal.SymbolTable.enteringPhase(SymbolTable.scala:235)11:10:06.136 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader.doComplete(SymbolLoaders.scala:260)11:10:06.136 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:211)11:10:06.136 [INFO] at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1535)11:10:06.136 [INFO] at scala.reflect.internal.Mirrors$RootsBase.init(Mirrors.scala:256)11:10:06.136 [INFO] at scala.tools.nsc.Global.rootMirror$lzycompute(Global.scala:73)11:10:06.136 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:71)11:10:06.136 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:39)11:10:06.136 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute(Definitions.scala:257)11:10:06.136 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass(Definitions.scala:257)11:10:06.136 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.init(Definitions.scala:1390)11:10:06.136 [INFO] at scala.tools.nsc.Global$Run.&lt;init&gt;(Global.scala:1242)11:10:06.136 [INFO] at scala.tools.nsc.Driver.doCompile(Driver.scala:31)11:10:06.136 [INFO] at scala.tools.nsc.MainClass.doCompile(Main.scala:23)11:10:06.136 [INFO] at scala.tools.nsc.Driver.process(Driver.scala:51)11:10:06.136 [INFO] at scala.tools.nsc.Driver.main(Driver.scala:64)11:10:06.136 [INFO] at scala.tools.nsc.Main.main(Main.scala)11:10:06.136 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)11:10:06.136 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)11:10:06.136 [INFO] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)11:10:06.137 [INFO] at java.base/java.lang.reflect.Method.invoke(Method.java:564)11:10:06.137 [INFO] at scala_maven_executions.MainHelper.runMain(MainHelper.java:164)11:10:06.137 [INFO] at scala_maven_executions.MainWithArgsInFile.main(MainWithArgsInFile.java:26)11:10:06.137 [INFO] java.lang.reflect.InvocationTargetException11:10:06.137 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)11:10:06.137 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)11:10:06.137 [INFO] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)11:10:06.137 [INFO] at java.base/java.lang.reflect.Method.invoke(Method.java:564)11:10:06.137 [INFO] at scala_maven_executions.MainHelper.runMain(MainHelper.java:164)11:10:06.137 [INFO] at scala_maven_executions.MainWithArgsInFile.main(MainWithArgsInFile.java:26)11:10:06.137 [ERROR] Caused by: java.lang.NoClassDefFoundError: javax/tools/ToolProvider11:10:06.137 [INFO] at scala.reflect.io.JavaToolsPlatformArchive.iterator(ZipArchive.scala:301)11:10:06.137 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)11:10:06.137 [INFO] at scala.reflect.io.AbstractFile.foreach(AbstractFile.scala:92)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.traverse(ClassPath.scala:277)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15$lzycompute(ClassPath.scala:299)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15(ClassPath.scala:299)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages$lzycompute(ClassPath.scala:299)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:299)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:264)11:10:06.138 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)11:10:06.138 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)11:10:06.138 [INFO] at scala.collection.Iterator$class.foreach(Iterator.scala:891)11:10:06.138 [INFO] at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)11:10:06.138 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)11:10:06.138 [INFO] at scala.collection.AbstractIterable.foreach(Iterable.scala:54)11:10:06.148 [INFO] at scala.tools.nsc.util.MergedClassPath.packages$lzycompute(ClassPath.scala:358)11:10:06.148 [INFO] at scala.tools.nsc.util.MergedClassPath.packages(ClassPath.scala:353)11:10:06.149 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply$mcV$sp(SymbolLoaders.scala:269)11:10:06.149 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)11:10:06.149 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)11:10:06.149 [INFO] at scala.reflect.internal.SymbolTable.enteringPhase(SymbolTable.scala:235)11:10:06.149 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader.doComplete(SymbolLoaders.scala:260)11:10:06.149 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:211)11:10:06.149 [INFO] at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1535)11:10:06.149 [INFO] at scala.reflect.internal.Mirrors$RootsBase.init(Mirrors.scala:256)11:10:06.149 [INFO] at scala.tools.nsc.Global.rootMirror$lzycompute(Global.scala:73)11:10:06.149 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:71)11:10:06.149 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:39)11:10:06.151 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute(Definitions.scala:257)11:10:06.153 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass(Definitions.scala:257)11:10:06.153 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.init(Definitions.scala:1390)11:10:06.153 [INFO] at scala.tools.nsc.Global$Run.&lt;init&gt;(Global.scala:1242)11:10:06.153 [INFO] at scala.tools.nsc.Driver.doCompile(Driver.scala:31)11:10:06.153 [INFO] at scala.tools.nsc.MainClass.doCompile(Main.scala:23)11:10:06.172 [INFO] at scala.tools.nsc.Driver.process(Driver.scala:51)11:10:06.172 [INFO] at scala.tools.nsc.Driver.main(Driver.scala:64)11:10:06.172 [INFO] at scala.tools.nsc.Main.main(Main.scala)11:10:06.172 [INFO] ... 6 more11:10:06.196 [INFO] ------------------</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9785" opendate="2018-7-9 00:00:00" fixdate="2018-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add remote addresses to LocalTransportException instances</summary>
      <description>LocalTransportException instantiations do not always add the remote address into the exception message. Having this will ease debugging and make correlating different log files much easier.There always is an address associated with these exceptions but it does not seem like this is used anywhere for now and in that case it is also the local address (looks like its intended to be the error source's address).</description>
      <version>1.4.2,1.5.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="9789" opendate="2018-7-10 00:00:00" fixdate="2018-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Watermark metrics for an operator&amp;task shadow each other</summary>
      <description>In FLINK-4812 we reworked the watermark metrics to be exposed for each operator.In FLINK-9467 we made further modifications to also expose these metrics again for tasks.In both JIRAs we register a single metric multiple times, for example the input watermark metric is registered for both the first operator in the task, and the task itself.Unfortunately, the metric system assumes metric objects to be unique, as can be seen in virtually all reporter implementations as well as the MetricQueryService.As a result the watermark metrics override each other in the reporter, causing only one to be reported, whichever was registered last.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="9792" opendate="2018-7-10 00:00:00" fixdate="2018-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot add html tags in options description</summary>
      <description>Right now it is impossible to add any html tags in options description, because all "&lt;" and "&gt;" are escaped. Therefore some links there do not work.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.configuration.ConfigOptionsDocsCompletenessITCase.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.configuration.ConfigOptionsDocGeneratorTest.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.util.Utils.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigOption.java</file>
      <file type="M">docs..includes.generated.security.configuration.html</file>
      <file type="M">docs..includes.generated.metric.configuration.html</file>
      <file type="M">docs..includes.generated.akka.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="9793" opendate="2018-7-10 00:00:00" fixdate="2018-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When submitting a flink job with yarn-cluster, flink-dist*.jar is repeatedly uploaded</summary>
      <description>We are using flink1.4.2 in our company. When the flink job is submitted to run on yarn with yarn-cluster mode，we actually find that the flink-dist_2.11-1.4.2.jar is uploaded to HDFS. The jars lies in different directories e.g.,1..flink/application_1525941455002_539197/flink-dist_2.11-1.4.2.jar2..flink/application_1525941455002_539197/lib/flink-dist_2.11-1.4.2.jarThrough reviewing source code of flink and having some tests, we suppose that the code below may have a bug in the Linux environment.java.nio.file.Path file!(file.getFileName().startsWith("flink-dist") &amp;&amp;file.getFileName().endsWith("jar")) is Always be true</description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>1.4.3,1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="9795" opendate="2018-7-10 00:00:00" fixdate="2018-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Mesos documentation for flip6</summary>
      <description>Mesos documentation would benefit from an overhaul after flip6 became the default cluster management model.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.configuration.MesosOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HighAvailabilityOptions.java</file>
      <file type="M">docs..includes.generated.mesos.task.manager.configuration.html</file>
      <file type="M">docs..includes.generated.mesos.configuration.html</file>
      <file type="M">docs..includes.generated.high.availability.zookeeper.configuration.html</file>
      <file type="M">docs.ops.deployment.mesos.md</file>
    </fixedFiles>
  </bug>
  <bug id="9799" opendate="2018-7-11 00:00:00" fixdate="2018-7-11 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Generalize/unify state meta info</summary>
      <description>Flink currently has a couple of classes that describe the meta data of state (e.g. for keyed state, operator state, broadcast state, ...) and they typically come with their own serialization proxy and backwards compatibility story. However, the differences between those meta data classes are very small, like different option flags and a different set of serializers. Before introducing yet another meta data for timers, we should unify them in a general state meta data class.</description>
      <version>1.5.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBFoldingState.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.SerializationProxiesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.StateTableSnapshotCompatibilityTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.CopyOnWriteStateTableTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.RegisteredOperatorBackendStateMetaInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.RegisteredKeyedBackendStateMetaInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.RegisteredBroadcastBackendStateMetaInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.OperatorBackendStateMetaInfoSnapshotReaderWriters.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.OperatorBackendSerializationProxy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyedBackendStateMetaInfoSnapshotReaderWriters.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.KeyedBackendSerializationProxy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.StateTableByKeyGroupReaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.DefaultOperatorStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="9805" opendate="2018-7-11 00:00:00" fixdate="2018-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HTTP Redirect to Active JM in Flink CLI</summary>
      <description>Flink CLI allows specifying job manager address via --jobmanager flag. However, in HA mode the JM can change and then standby JM does HTTP redirect to the active one. However, during deployment via flink CLI with --jobmanager flag option the CLI does not redirect to the active one. Thus fails to submit job with "Could not complete the operation. Number of retries has been exhausted"  Proposal:Honor JM HTTP redirect in case leadership changes in flink CLI with --jobmanager flag active. </description>
      <version>1.5.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="9806" opendate="2018-7-11 00:00:00" fixdate="2018-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a canonical link element to documentation HTML</summary>
      <description>Flink has suffered for a while with non-optimal SEO for its documentation, meaning a web search for a topic covered in the documentation often produces results for many versions of Flink, even preferring older versions since those pages have been around for longer.Using a canonical link element (see references) may alleviate this by informing search engines about where to find the latest documentation (i.e. pages hosted under https://ci.apache.org/projects/flink/flink-docs-master/).I think this is at least worth experimenting with, and if it doesn't cause problems, even backporting it to the older release branches to eventually clean up the Flink docs' SEO and converge on advertising only the latest docs (unless a specific version is specified).References: https://moz.com/learn/seo/canonicalization https://yoast.com/rel-canonical/ https://support.google.com/webmasters/answer/139066?hl=en https://en.wikipedia.org/wiki/Canonical_link_element</description>
      <version>1.5.0</version>
      <fixedVersion>1.3.4,1.4.3,1.5.3,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..layouts.base.html</file>
      <file type="M">docs..config.yml</file>
    </fixedFiles>
  </bug>
  <bug id="9808" opendate="2018-7-11 00:00:00" fixdate="2018-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement state conversion procedure in state backends</summary>
      <description>With FLINK-9377 in place and that config snapshots serve as the single source of truth for recreating restore serializers, the next step would be to utilize this when performing a full-pass state conversion (i.e., read with old / restore serializer, write with new serializer).For Flink's heap-based backends, it can be seen that state conversion inherently happens, since all state is always deserialized after restore with the restore serializer, and written with the new serializer on snapshots.For the RocksDB state backend, since state is lazily deserialized, state conversion needs to happen for per-registered state on their first access if the registered new serializer has a different serialization schema than the previous serializer.This task should consist of three parts:1. Allow CompatibilityResult to correctly distinguish between whether the new serializer's schema is a) compatible with the serializer as it is, b) compatible after the serializer has been reconfigured, or c) incompatible.2. Introduce state conversion procedures in the RocksDB state backend. This should occur on the first state access.3. Make sure that all other backends no longer do redundant serializer compatibility checks. That is not required because those backends always perform full-pass state conversions.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBListState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.AbstractRocksDBState.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.RegisteredKeyValueStateBackendMetaInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug id="983" opendate="2014-6-25 00:00:00" fixdate="2014-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebClient makes names of functions unreadable</summary>
      <description>The named of Functions are truncated and abbreviated like "Properties of ...reeDistribution, delimiter: ) - ID = 7".There should be a place where the names are printed fully, not abbreviated/truncated.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.resources.web-docs.js.graphCreator.js</file>
    </fixedFiles>
  </bug>
  <bug id="9841" opendate="2018-7-13 00:00:00" fixdate="2018-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web UI only show partial taskmanager log</summary>
      <description> In the web UI, we select a task manager and click the "log" tab, but the UI only show the partial log (first part), can never update even if we click the "refresh" button.However, the job manager is always OK.The reason is the resource be closed twice.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="9865" opendate="2018-7-16 00:00:00" fixdate="2018-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-hadoop-compatibility should assume Hadoop as provided</summary>
      <description>The flink-hadoop-compatibility project as a compile scope dependency on Hadoop (flink-hadoop-shaded). Because of that, the hadoop dependencies are pulled into the user application.Like in other Hadoop-dependent modules, we should assume that Hadoop is provided in the framework classpath already.</description>
      <version>1.5.0,1.5.1</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-hcatalog.pom.xml</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9873" opendate="2018-7-17 00:00:00" fixdate="2018-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log actual state when aborting checkpoint due to task not running</summary>
      <description>Currently, if a checkpoint is triggered while a task s not in a RUNNING state the following message is logged:Checkpoint triggering task {} of job {} is not being executed at the moment.We can improve this message to include the actual task state to help diagnose problems.This message is also a bit ambiguous, as "being executed" could mean many things, from not "RUNNING", to not being "DEPLOYED", or to not existing at all.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug id="9878" opendate="2018-7-17 00:00:00" fixdate="2018-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>IO worker threads BLOCKED on SSL Session Cache while CMS full gc</summary>
      <description>According to https://github.com/netty/netty/issues/832, there is a JDK issue during garbage collection when the SSL session cache is not limited. We should allow the user to configure this and further (advanced) SSL parameters for fine-tuning to fix this and similar issues. In particular, the following parameters should be configurable: SSL session cache size SSL session timeout SSL handshake timeout SSL close notify flush timeout</description>
      <version>1.5.0,1.5.1,1.6.0</version>
      <fixedVersion>1.5.4,1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.net.SSLUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpointConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClientConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.SSLUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.SSLEngineFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.RedirectingSslHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyServer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyConfig.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyClient.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.utils.WebFrontendBootstrap.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServer.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosArtifactServer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
      <file type="M">docs..includes.generated.security.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="9923" opendate="2018-7-23 00:00:00" fixdate="2018-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>OneInputStreamTaskTest.testWatermarkMetrics fails on Travis</summary>
      <description>OneInputStreamTaskTest.testWatermarkMetrics fails on Travis withjava.lang.AssertionError: expected:&lt;1&gt; but was:&lt;-9223372036854775808&gt; at org.junit.Assert.fail(Assert.java:88) at org.junit.Assert.failNotEquals(Assert.java:834) at org.junit.Assert.assertEquals(Assert.java:645) at org.junit.Assert.assertEquals(Assert.java:631) at org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.testWatermarkMetrics(OneInputStreamTaskTest.java:731)https://api.travis-ci.org/v3/job/407196285/log.txt</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.metrics.WatermarkGauge.java</file>
    </fixedFiles>
  </bug>
  <bug id="9936" opendate="2018-7-24 00:00:00" fixdate="2018-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mesos resource manager unable to connect to master after failover</summary>
      <description>When deployed in mesos session cluster mode, the connector monitor keeps reporting unable to connect to mesos after restart. In fact, scheduler driver already connected to mesos master, but when the connected message is lost. This is because leadership is not granted yet and fence id is not set, the rpc service ignores the connected message. So we should connect to mesos master after leadership is granted.</description>
      <version>1.5.0,1.5.1,1.6.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.java</file>
      <file type="M">flink-jepsen.scripts.run-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="9942" opendate="2018-7-25 00:00:00" fixdate="2018-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Guard handlers against null fields in requests</summary>
      <description>In FLINK-8233 the ObjectMapper used for the REST API was modified to not fail on missing creator properties. This means that any field for any request may be null.Since fields not being null was an assumption that handlers were previously built on, we now have to scan every implementation to ensure they can't fail with an NPE.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.savepoints.SavepointDisposalRequest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.JobSubmitRequestBody.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobSubmitHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="9969" opendate="2018-7-26 00:00:00" fixdate="2018-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unreasonable memory requirements to complete examples/batch/WordCount</summary>
      <description>setup on AWS EMR: 5 worker nodes (m4.4xlarge nodes)  1 master node (m4.large)following command fails with out of memory errors:export HADOOP_CLASSPATH=`hadoop classpath`./bin/flink run -m yarn-cluster -p 20 -yn 5 -ys 4 -ytm 16000 examples/batch/WordCount.jarOnly increasing memory over 17.2GB example completes. At the same time after disabling flip6 following command succeeds:export HADOOP_CLASSPATH=`hadoop classpath`./bin/flink run -m yarn-cluster -p 20 -yn 5 -ys 4 -ytm 1000 examples/batch/WordCount.jar</description>
      <version>1.5.0,1.5.1,1.5.2,1.6.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.UnilateralSortMerger.java</file>
    </fixedFiles>
  </bug>
  <bug id="9972" opendate="2018-7-26 00:00:00" fixdate="2018-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Debug memory logging not working</summary>
      <description>It seems like with introduction of Flip6, debug memory logging is not being initialised anymore and following config properties are ignored:  `taskmanager.debug.memory.log` `taskmanager.debug.memory.log-interval`after disabling flip6 it works just fine.</description>
      <version>1.5.0,1.5.1,1.5.2,1.6.0</version>
      <fixedVersion>1.5.3,1.6.1,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.MemoryLogger.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="9975" opendate="2018-7-26 00:00:00" fixdate="2018-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Netty dependency of Hadoop &gt;= 2.7 is not relocated</summary>
      <description>Previously, in flink-shaded-hadoop, we also relocate Netty (org.jboss.netty) to not conflict with user code. Since Hadoop 2.7 the Netty version they depend on has been upgraded and we missed relocating io.netty accordingly.</description>
      <version>1.4.2,1.5.0,1.5.1,1.6.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9988" opendate="2018-7-27 00:00:00" fixdate="2018-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>job manager does not respect property jobmanager.web.address</summary>
      <description>As flink does not have any built in authentication mechanism, we used to setup nginx in front of it and start jobmanager on 127.0.0.1.but starting from version 1.5.0 - it does not work anymore.distespecting on jobmanager.web.address it always start on 0.0.0.0</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="9990" opendate="2018-7-29 00:00:00" fixdate="2018-10-29 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add regexp_extract supported in TableAPI and SQL</summary>
      <description>regex_extract is a very useful function, it returns a string based on a regex pattern and a index.For example : regexp_extract('foothebar', 'foo(.*?)(bar)', 2) // returns 'bar.'It is provided as a UDF in Hive, more details please see&amp;#91;1&amp;#93;.&amp;#91;1&amp;#93;: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.functions.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.sql.ScalarSqlFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.functions.md</file>
    </fixedFiles>
  </bug>
  <bug id="9991" opendate="2018-7-29 00:00:00" fixdate="2018-9-29 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add regexp_replace supported in TableAPI and SQL</summary>
      <description>regexp_replace is a very userful function to process String. For example :regexp_replace("foobar", "oo|ar", "") //returns 'fb.'It is supported as a UDF in Hive, more details please see&amp;#91;1&amp;#93;.&amp;#91;1&amp;#93;: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF </description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.functions.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.sql.ScalarSqlFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.functions.md</file>
    </fixedFiles>
  </bug>
</bugrepository>
