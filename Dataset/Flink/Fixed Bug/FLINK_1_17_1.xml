<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="31182" opendate="2023-2-22 00:00:00" fixdate="2023-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CompiledPlan cannot deserialize BridgingSqlFunction with MissingTypeStrategy</summary>
      <description>This issue is reported from the user mail list.The stacktrace is Unable to find source-code formatter for language: text. Available languages are: actionscript, ada, applescript, bash, c, c#, c++, cpp, css, erlang, go, groovy, haskell, html, java, javascript, js, json, lua, none, nyan, objc, perl, php, python, r, rainbow, ruby, scala, sh, sql, swift, visualbasic, xml, yamlCaused by: org.apache.flink.table.api.TableException: Could not resolve internal system function '$UNNEST_ROWS$1'. This is a bug, please file an issue.    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeInternalFunction(RexNodeJsonDeserializer.java:392)    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeSqlOperator(RexNodeJsonDeserializer.java:337)    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeCall(RexNodeJsonDeserializer.java:307)    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:146)    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:128)    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:115) The root cause is that although ModuleManager can resolve '$UNNEST_ROWS$1', the output type strategy is "Missing"; as a result, FunctionCatalogOperatorTable#convertToBridgingSqlFunction returns empty.</description>
      <version>1.17.0,1.18.0,1.17.1</version>
      <fixedVersion>1.17.0,1.16.2,1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.CorrelateJsonPlanITCase.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.inference.strategies.SpecificTypeStrategies.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
    </fixedFiles>
  </bug>
  <bug id="31222" opendate="2023-2-25 00:00:00" fixdate="2023-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove usage of deprecated ConverterUtils.toApplicationId</summary>
      <description>When reading the code, I found that we use ConverterUtils.toApplicationId to convert applicationId, this method is deprecated, we should use ApplicationId.fromString</description>
      <version>1.17.1</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterClientFactory.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
    </fixedFiles>
  </bug>
  <bug id="31575" opendate="2023-3-22 00:00:00" fixdate="2023-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t swap table-planner-loader and table-planner to use hive dialect</summary>
      <description>From Flink 1.15,  to use Hive dialect, user have to swap the flink-table-planner-loader jar with flink-table-planner.jar.It really bothers some users who want to use Hive dialect like FLINK-27020, FLINK-28618Althogh we has paid much effort like FLINK-29350, FLINK-29045 to tell users to do the swap, but it'll still not convenient.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-loader.src.main.java.org.apache.flink.table.planner.loader.PlannerModule.java</file>
      <file type="M">flink-table.flink-table-planner-loader.src.main.java.org.apache.flink.table.planner.loader.DelegatePlannerFactory.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-hive.src.test.java.org.apache.flink.tests.hive.HiveITCase.java</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.3.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.3.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveReflectionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="31962" opendate="2023-4-27 00:00:00" fixdate="2023-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>libssl not found when running CI</summary>
      <description>Installed Maven 3.2.5 to /home/vsts/maven_cache/apache-maven-3.2.5Installing required softwareReading package lists...Building dependency tree...Reading state information...bc is already the newest version (1.07.1-2build1).bc set to manually installed.libapr1 is already the newest version (1.6.5-1ubuntu1).libapr1 set to manually installed.0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.--2023-04-27 11:42:53-- http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.11_amd64.debResolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 185.125.190.36, 185.125.190.39, ...Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.HTTP request sent, awaiting response... 404 Not Found2023-04-27 11:42:53 ERROR 404: Not Found.</description>
      <version>1.16.2,1.18.0,1.17.1</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
    </fixedFiles>
  </bug>
  <bug id="3224" opendate="2016-1-12 00:00:00" fixdate="2016-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The Streaming API does not call setInputType if a format implements InputTypeConfigurable</summary>
      <description>Per parent JIRA.</description>
      <version>None</version>
      <fixedVersion>0.10.2,1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.FileSinkFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="3225" opendate="2016-1-12 00:00:00" fixdate="2016-3-12 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Optimize logical Table API plans in Calcite</summary>
      <description>This task implements the optimization of logical Table API plans with Apache Calcite. The input of the optimization process is a logical query plan consisting of Calcite RelNodes. FLINK-3223 translates Table API queries into this representation.The result of this issue is an optimized logical plan.Calcite's rule-based optimizer applies query rewriting and optimization rules. For Batch SQL, we can use (a subset of) Calcite’s default optimization rules. For this issue we have to add the Calcite optimizer to the translation process select an appropriate set of batch optimization rules from Calcite’s default rules. We can reuse the rules selected by Timo’s first SQL implementation.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.UnionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.StringExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.SelectITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.GroupedAggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.ExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.CastingITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.AsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.UnionITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.StringExpressionsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.SelectITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.PojoGroupingITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.JoinITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.GroupedAggregationsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.FilterITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.ExpressionsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.CastingITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.AsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.AggregationsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.operators.DataSetTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.api.table.sql.calcite.node.DataSetUnion.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.api.table.sql.calcite.node.DataSetSource.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.api.table.sql.calcite.node.DataSetSort.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.api.table.sql.calcite.node.DataSetReduceGroup.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.api.table.sql.calcite.node.DataSetReduce.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.api.table.sql.calcite.node.DataSetMap.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.api.table.sql.calcite.node.DataSetJoin.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.api.table.sql.calcite.node.DataSetFlatMap.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.api.table.sql.calcite.node.DataSetExchange.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.api.table.sql.calcite.DataSetRelNode.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.api.table.package-info.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.JavaBatchTranslator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="32277" opendate="2023-6-7 00:00:00" fixdate="2023-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce operator fusion codegen basic framework</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.multipleinput.input.InputSelectionHandlerTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.input.InputSpec.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.input.InputSelectionHandler.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.BatchMultipleInputStreamOperator.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.TestingBatchExecNode.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNode.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.java</file>
    </fixedFiles>
  </bug>
  <bug id="32278" opendate="2023-6-7 00:00:00" fixdate="2023-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HashJoin support operator fusion codegen</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.fusion.FusionStreamOperatorBase.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.hashtable.LongHybridHashTable.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.hashtable.BinaryHashTable.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.hashtable.BaseHybridHashTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.MultipleInputITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalHashJoin.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.fusion.spec.HashJoinFusionCodegenSpec.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.MultipleInputNodeCreationProcessor.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.ForwardHashExchangeProcessor.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.FusionCodegenExecNode.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecMultipleInput.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecHashJoin.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecCalc.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.fusion.OpFusionCodegenSpec.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.ExecutionConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.config.configuration.html</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.join.HashJoinType.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.fusion.OpFusionCodegenSpecGeneratorBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.fusion.impl.TwoInputOpFusionCodegenSpecGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.fusion.impl.OneInputOpFusionCodegenSpecGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.LongHashJoinGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.fusion.OpFusionCodegenSpecGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="32279" opendate="2023-6-7 00:00:00" fixdate="2023-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shuffle HashJoin support spill to disk when enable operator fusion codegen</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.hashtable.LongHybridHashTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.OperatorFusionCodegenITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.fusion.spec.HashJoinFusionCodegenSpec.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.fusion.spec.CalcFusionCodegenSpec.scala</file>
    </fixedFiles>
  </bug>
  <bug id="32304" opendate="2023-6-9 00:00:00" fixdate="2023-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce rpc-akka jar size</summary>
      <description>We bundle unnecessary dependencies in the rpc-akka jar; we can easily shave of 15mb of dependencies.</description>
      <version>None</version>
      <fixedVersion>1.18.0,1.17.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-rpc.flink-rpc-akka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32349" opendate="2023-6-15 00:00:00" fixdate="2023-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support atomic for CREATE TABLE AS SELECT(CTAS) statement</summary>
      <description>For detailed information, see FLIP-305https://cwiki.apache.org/confluence/display/FLINK/FLIP-305%3A+Support+atomic+for+CREATE+TABLE+AS+SELECT%28CTAS%29+statement</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.delegation.DefaultExecutor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.ExecutorMock.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.CreateTableASOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.delegation.Executor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.TableConfigOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.executor.python.ChainingOptimizingExecutor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.execution.JobStatusHook.java</file>
      <file type="M">docs.layouts.shortcodes.generated.table.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="32351" opendate="2023-6-15 00:00:00" fixdate="2023-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce base interfaces for call procedure</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.Catalog.java</file>
      <file type="M">flink-python.pyflink.table.catalog.py</file>
    </fixedFiles>
  </bug>
  <bug id="32354" opendate="2023-6-15 00:00:00" fixdate="2023-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to execute the call procedure operation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.ProcedureITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.PlannerCallProcedureOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.CallProcedureOperation.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlNodeConvertContext.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConverters.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.bridging.BridgingUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.calcite.SqlToRexConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug id="32358" opendate="2023-6-15 00:00:00" fixdate="2023-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CI may unintentionally use fallback akka loader</summary>
      <description>We have a fallback akka loader for developer convenience in the IDE, that is on the classpath of most modules. Depending on the order of jars on the classpath it can happen that the fallback loader appears first, which we dont want because it slows down the build and creates noisy logs.We can add a simple prioritization scheme to the rpc system loading to remedy that.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.RpcSystemLoader.java</file>
      <file type="M">flink-rpc.flink-rpc-core.src.main.java.org.apache.flink.runtime.rpc.RpcSystem.java</file>
      <file type="M">flink-rpc.flink-rpc-akka-loader.src.test.java.org.apache.flink.runtime.rpc.akka.FallbackAkkaRpcSystemLoader.java</file>
      <file type="M">flink-rpc.flink-rpc-akka-loader.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcSystemLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="32365" opendate="2023-6-16 00:00:00" fixdate="2023-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>orc format get table statistics slow</summary>
      <description>Orc format get table statistics slow when task have many  files。Previously, it was single parallel reading, but it needs to be improved to multi-parallel reading files row count. </description>
      <version>1.17.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.utils.ParquetFormatStatisticsReportUtil.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.util.OrcFormatStatisticsReportUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveOptions.java</file>
      <file type="M">docs.content.docs.connectors.table.hive.hive.read.write.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.read.write.md</file>
    </fixedFiles>
  </bug>
  <bug id="32369" opendate="2023-6-16 00:00:00" fixdate="2023-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setup cron build</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.compile.sh</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
    </fixedFiles>
  </bug>
  <bug id="32426" opendate="2023-6-25 00:00:00" fixdate="2023-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix adaptive local hash agg can&amp;#39;t work when auxGrouping exist</summary>
      <description>For the following case, the field `a` is primary key,  we select from `AuxGroupingTable` and group by a, b. Since a is primary key, it also guarantee the unique, so planner will extract b as auxGrouping field.registerCollection( "AuxGroupingTable", data2, type2, "a, b, c, d, e", nullablesOfData2, FlinkStatistic.builder().uniqueKeys(Set(Set("a").asJava).asJava).build())checkResult( "SELECT a, b, COUNT(c) FROM AuxGroupingTable GROUP BY a, b", Seq( row(1, 1, 1), row(2, 3, 2), row(3, 4, 3), row(4, 10, 4), row(5, 11, 5) ))  Due to the generated code doesn't get auxGrouping fields from input RowData and then setting it to aggBuffer, the aggBuffer RowData loses some fields, and it will throw an index Exception when get the field from it. As following:Caused by: java.lang.AssertionError: index (1) should &lt; 1    at org.apache.flink.table.data.binary.BinaryRowData.assertIndexIsValid(BinaryRowData.java:127)    at org.apache.flink.table.data.binary.BinaryRowData.isNullAt(BinaryRowData.java:156)    at org.apache.flink.table.data.utils.JoinedRowData.isNullAt(JoinedRowData.java:113)    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.toBinaryRow(RowDataSerializer.java:201)    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.serialize(RowDataSerializer.java:103)    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.serialize(RowDataSerializer.java:48)    at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.serialize(StreamElementSerializer.java:165)    at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.serialize(StreamElementSerializer.java:43)    at org.apache.flink.runtime.plugable.SerializationDelegate.write(SerializationDelegate.java:54)    at org.apache.flink.runtime.io.network.api.writer.RecordWriter.serializeRecord(RecordWriter.java:141)    at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:107)    at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:55)    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:134)    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collectAndCheckIfChained(RecordWriterOutput.java:114)    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:95)    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:48)    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:59)    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:31)    at LocalHashAggregateWithKeys$39.processElement_split2(Unknown Source)    at LocalHashAggregateWithKeys$39.processElement(Unknown Source)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:108)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:77)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)    at BatchExecCalc$10.processElement(Unknown Source)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:108)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:77)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)    at SourceConversion$6.processElement(Unknown Source)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:108)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:77)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)    at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)    at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)    at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)    at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)</description>
      <version>1.18.0,1.17.1</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.HashAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ProjectionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="32428" opendate="2023-6-25 00:00:00" fixdate="2023-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce base interfaces for CatalogStore</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.FactoryUtilTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FactoryUtil.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
    </fixedFiles>
  </bug>
  <bug id="32457" opendate="2023-6-28 00:00:00" fixdate="2023-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update current documentation of JSON_OBJECTAGG/JSON_ARRAYAGG to clarify the limitation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0,1.17.2</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.WrapJsonAggFunctionArgumentsRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.logical.WrapJsonAggFunctionArgumentsRuleTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="32469" opendate="2023-6-28 00:00:00" fixdate="2023-7-28 01:00:00" resolution="Done">
    <buginformation>
      <summary>Improve checkpoint REST APIs for programmatic access</summary>
      <description>WhyWe want to enable programmatic use of the checkpoints REST API, independent of the Flink dashboard.Currently, REST APIs that retrieve information relating to a given Flink job passes through the ExecutionGraphCache. This means that all these APIs will retrieve stale data depending on the web.refresh-interval, which defaults to 3s. For programmatic use of the REST API, we should be able to retrieve the latest / cached version depending on the client (Flink dashboard gets the cached version, other clients get the updated version).For example, a user might want to use the REST API to retrieve the latest completed checkpoint for a given Flink job. This might be useful when trying to use existing checkpoints as state store when migrating a Flink job from one cluster to another. See Appendix for example.WhatThis change is about separating out the cache used for the checkpoints REST APIs to a separate cache. This way, a user can set the timeout for the checkpoints cache to 0s (disable cache), without causing much effect on the user experience on the Flink dashboard.In addition, the checkpoint handlers first retrieve the ExecutionGraph, then retrieve the CheckpointStatsSnapshot from the graph. This is not needed, since the checkpoint handlers only need the CheckpointStatsSnapshot. This change will mean these handlers retrieve the minimal required information (CheckpointStatsSnapshot) to construct a reply. Example use caseWhen performing security patching / maintenance of the infrastructure supporting the Flink cluster, we might want to transfer a given Flink job to another cluster, whilst maintaining state. We can do this via the below steps: Old cluster - Select completed checkpoint on existing Flink job Old cluster - Stop the existing Flink job New cluster - Start a new Flink job with selected checkpointStep 1 requires us to query the checkpoints REST API for the latest completed checkpoint. With the status quo, we need to wait 3s (or whatever the ExecutionGraphCache expiry may be). This is undesirable because this means the Flink job will have to reprocess data equivalent to 3s / whatever the execution graph cache timeout is.</description>
      <version>1.16.2,1.17.1</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingRestfulGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingDispatcherGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.TestingSchedulerNG.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.RestHandlerConfigurationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.TestingJobMasterGatewayBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.TestingJobMasterGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.TestingJobManagerRunner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.RestfulGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.NonLeaderRetrievalRestfulGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerNG.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.RestHandlerConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.checkpoints.AbstractCheckpointHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.expert.rest.section.html</file>
    </fixedFiles>
  </bug>
  <bug id="32501" opendate="2023-6-30 00:00:00" fixdate="2023-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong execution plan of a proctime window aggregation generated due to incorrect cost evaluation</summary>
      <description>Currently when uses window aggregation referring a windowing tvf with a filter condition, may encounter wrong plan which may hang forever in runtime(the window aggregate operator never output)for such a case:insert into sink select window_start, window_end, b, COALESCE(sum(case when a = 11 then 1 end), 0) c from TABLE( TUMBLE(TABLE source, DESCRIPTOR(proctime), INTERVAL '10' SECONDS) ) where a in (1, 5, 7, 9, 11) GROUP BY window_start, window_end, bgenerate wrong plan which didn't combine the proctime WindowTableFunction into WindowAggregate (so when translate to execution plan the WindowAggregate will wrongly recognize the window as an event-time window, then the WindowAggregateOperator will not receive watermark nor setup timers to fire any windows in runtime)Sink(table=[default_catalog.default_database.sink], fields=[ws, we, b, c])+- Calc(select=[CAST(window_start AS TIMESTAMP(6)) AS ws, CAST(window_end AS TIMESTAMP(6)) AS we, b, CAST(COALESCE($f1, 0) AS BIGINT) AS c]) +- WindowAggregate(groupBy=[b], window=[TUMBLE(win_start=[window_start], win_end=[window_end], size=[10 s])], select=[b, SUM($f3) AS $f1, start('w$) AS window_start, end('w$) AS window_end]) +- Exchange(distribution=[hash[b]]) +- Calc(select=[window_start, window_end, b, CASE((a = 11), 1, null:INTEGER) AS $f3], where=[SEARCH(a, Sarg[1, 5, 7, 9, 11])]) +- WindowTableFunction(window=[TUMBLE(time_col=[proctime], size=[10 s])]) +- Calc(select=[a, b, PROCTIME() AS proctime]) +- TableSourceScan(table=[[default_catalog, default_database, source, project=[a, b], metadata=[]]], fields=[a, b])expected plan:Sink(table=[default_catalog.default_database.sink], fields=[ws, we, b, c])+- Calc(select=[CAST(window_start AS TIMESTAMP(6)) AS ws, CAST(window_end AS TIMESTAMP(6)) AS we, b, CAST(COALESCE($f1, 0) AS BIGINT) AS c]) +- WindowAggregate(groupBy=[b], window=[TUMBLE(time_col=[proctime], size=[10 s])], select=[b, SUM($f3) AS $f1, start('w$) AS window_start, end('w$) AS window_end]) +- Exchange(distribution=[hash[b]]) +- Calc(select=[b, CASE((a = 11), 1, null:INTEGER) AS $f3, PROCTIME() AS proctime], where=[SEARCH(a, Sarg[1, 5, 7, 9, 11])]) +- TableSourceScan(table=[[default_catalog, default_database, source, project=[a, b], metadata=[]]], fields=[a, b])</description>
      <version>1.16.2,1.17.1</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdSelectivityTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdSelectivity.scala</file>
    </fixedFiles>
  </bug>
  <bug id="32514" opendate="2023-7-3 00:00:00" fixdate="2023-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FLIP-309: Support using larger checkpointing interval when source is processing backlog</summary>
      <description>Umbrella issue for https://cwiki.apache.org/confluence/display/FLINK/FLIP-309%3A+Support+using+larger+checkpointing+interval+when+source+is+processing+backlog</description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorEventSendingCheckpointITCase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.ManuallyTriggeredScheduledExecutorService.java</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testutils.source.reader.TestingSplitEnumeratorContext.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.CheckpointConfig.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.MockOperatorCoordinatorContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStoreITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.FailoverStrategyCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDeciderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTriggeringTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.VertexEndOfDataListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.CheckpointCoordinatorConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDecider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.concurrent.ManuallyTriggeredScheduledExecutor.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.connector.source.mocks.MockSplitEnumeratorContext.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.connector.source.SplitEnumeratorContext.java</file>
      <file type="M">flink-connectors.flink-connector-base.src.main.java.org.apache.flink.connector.base.source.hybrid.HybridSourceSplitEnumerator.java</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.checkpointing.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="32516" opendate="2023-7-3 00:00:00" fixdate="2023-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to parse [CREATE OR ] REPLACE TABLE AS statement</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug id="32517" opendate="2023-7-3 00:00:00" fixdate="2023-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to execute [CREATE OR] REPLACE TABLE AS statement</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.utils.OperationConverterUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlCreateTableConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConverters.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ModifyOperationVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlReplaceTableAs.java</file>
    </fixedFiles>
  </bug>
  <bug id="32518" opendate="2023-7-3 00:00:00" fixdate="2023-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable atomicity for [CREATE OR] REPLACE table as statement</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.utils.AtomicCtasITCaseBase.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.sink.abilities.SupportsStaging.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.StagedTable.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.StagedSinkModifyOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ReplaceTableAsOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.execution.CtasJobStatusHook.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.TableConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.table.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="32519" opendate="2023-7-3 00:00:00" fixdate="2023-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add doc for [CREATE OR] REPLACE TABLE AS statement</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.create.md</file>
    </fixedFiles>
  </bug>
  <bug id="32547" opendate="2023-7-6 00:00:00" fixdate="2023-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add missing doc for Timestamp support in ProtoBuf format</summary>
      <description>In FLINK-30093, we have support Timestamp type, and added the doc for it, but missed to updating the English version.</description>
      <version>1.17.1</version>
      <fixedVersion>1.18.0,1.17.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.formats.protobuf.md</file>
    </fixedFiles>
  </bug>
  <bug id="32578" opendate="2023-7-11 00:00:00" fixdate="2023-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cascaded group by window time columns on a proctime window aggregate may result hang for ever</summary>
      <description>Currently when group by window time columns on a proctime window aggregate result will get a wrong plan which may result hang for ever in runtime.For such a query:insert into s1SELECT window_start, window_end, sum(cnt), count(*)FROM ( SELECT a, b, window_start, window_end, count(*) as cnt, sum(d) as sum_d, max(d) as max_d FROM TABLE(TUMBLE(TABLE src1, DESCRIPTOR(proctime), INTERVAL '5' MINUTE)) GROUP BY a, window_start, window_end, b)GROUP BY a, window_start, window_endthe inner proctime window works fine, but the outer one doesn't work due to a wrong plan which will translate to a unexpected event mode window operator:Sink(table=[default_catalog.default_database.s1], fields=[ws, we, b, c])+- Calc(select=[CAST(window_start AS TIMESTAMP(6)) AS ws, CAST(window_end AS TIMESTAMP(6)) AS we, CAST(EXPR$2 AS BIGINT) AS b, CAST(EXPR$3 AS BIGINT) AS c]) +- WindowAggregate(groupBy=[a], window=[TUMBLE(win_start=[window_start], win_end=[window_end], size=[5 min])], select=[a, SUM(cnt) AS EXPR$2, COUNT(*) AS EXPR$3, start('w$) AS window_start, end('w$) AS window_end]) +- Exchange(distribution=[hash[a]]) +- Calc(select=[a, window_start, window_end, cnt]) +- WindowAggregate(groupBy=[a, b], window=[TUMBLE(time_col=[proctime], size=[5 min])], select=[a, b, COUNT(*) AS cnt, start('w$) AS window_start, end('w$) AS window_end]) +- Exchange(distribution=[hash[a, b]]) +- Calc(select=[a, b, d, PROCTIME() AS proctime]) +- TableSourceScan(table=[[default_catalog, default_database, src1, project=[a, b, d], metadata=[]]], fields=[a, b, d])</description>
      <version>1.17.1</version>
      <fixedVersion>1.18.0,1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.WindowAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalGroupAggregateRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="32581" opendate="2023-7-12 00:00:00" fixdate="2023-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for atomic CTAS</summary>
      <description>add docs for atomic CTAS</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.docs.dev.table.sourcesSinks.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sourcesSinks.md</file>
    </fixedFiles>
  </bug>
  <bug id="32592" opendate="2023-7-14 00:00:00" fixdate="2023-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>(Stream)ExEnv#initializeContextEnvironment isn&amp;#39;t thread-safe</summary>
      <description>ContextWe are using the flink-k8s-operator to deploy multiple jobs (up to 32) to a single session cluster. The job submissions done by the operator happen concurrently, basically at the same time.Operator version: 1.5.0Flink version:  1.15.4, 1.7.1, 1.18 (master@f37d41cf)ProblemRarely (~once every 50 deployments) one of the jobs will not be executed. In the following incident 4 jobs are deployed at the same time: gorner-task-staging-e5730831 gorner-facility-staging-e5730831 gorner-aepp-staging-e5730831 gorner-session-staging-e5730831 The operator submits the job, they all get a reasonable jobID:2023-07-14 10:25:35,295 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/gorner-task-staging-e5730831] Submitting job: 4968b186061e44390000000000000002 to session cluster.2023-07-14 10:25:35,297 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/gorner-facility-staging-e5730831] Submitting job: 91a5260d916c4dff0000000000000002 to session cluster.2023-07-14 10:25:35,301 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/gorner-aepp-staging-e5730831] Submitting job: 103c0446e14749a10000000000000002 to session cluster.2023-07-14 10:25:35,302 o.a.f.k.o.s.AbstractFlinkService [INFO ][aelps-staging/gorner-session-staging-e5730831] Submitting job: de59304d370b4b8e0000000000000002 to session cluster.In the cluster the JarRunHandler's handleRequest() method will get the request, all 4 jobIDs are present (also all args, etc are correct):2023-07-14 10:25:35,320 WARN org.apache.flink.runtime.webmonitor.handlers.JarRunHandler [] - handleRequest - requestBody.jobId: 4968b186061e443900000000000000022023-07-14 10:25:35,321 WARN org.apache.flink.runtime.webmonitor.handlers.JarRunHandler [] - handleRequest - requestBody.jobId: de59304d370b4b8e00000000000000022023-07-14 10:25:35,321 WARN org.apache.flink.runtime.webmonitor.handlers.JarRunHandler [] - handleRequest - requestBody.jobId: 91a5260d916c4dff00000000000000022023-07-14 10:25:35,321 WARN org.apache.flink.runtime.webmonitor.handlers.JarRunHandler [] - handleRequest - requestBody.jobId: 103c0446e14749a10000000000000002But once the EmbeddedExecutor's submitAndGetJobClientFuture() method is called instead of getting 1 call per jobID we have 4 calls but one of the jobIDs twice:2023-07-14 10:25:35,616 WARN org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - optJobId: Optional[4968b186061e44390000000000000002]2023-07-14 10:25:35,616 WARN org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - optJobId: Optional[103c0446e14749a10000000000000002]2023-07-14 10:25:35,616 WARN org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - optJobId: Optional[de59304d370b4b8e0000000000000002]2023-07-14 10:25:35,721 WARN org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - optJobId: Optional[de59304d370b4b8e0000000000000002]If this is important: the jobGraph obtained does not match the jobID. We get 2 times de59304d370b4b8e0000000000000002 but the jobgraph for this jobID is never returned by getJobGraph() in EmbeddedExecutor.submitAndGetJobClientFuture().This will then lead to the job already existing:2023-07-14 10:25:35,616 WARN org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - submittedJobIds: []2023-07-14 10:25:35,616 WARN org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - submittedJobIds: []2023-07-14 10:25:35,616 WARN org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - submittedJobIds: []2023-07-14 10:25:35,721 WARN org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - execute - submittedJobIds: [de59304d370b4b8e0000000000000002]But since the jobs are completely different the execution will fail. Depending on the timing with one of the following exceptions: RestHandlerException: No jobs included in application ClassNotFoundException: io.dectris.aelps.pipelines.gorner.facility.FacilityEventProcessor </description>
      <version>1.15.4,1.18.0,1.17.1</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.StreamExecutionEnvironmentTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="32680" opendate="2023-7-26 00:00:00" fixdate="2023-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Job vertex names get messed up once there is a source vertex chained with a MultipleInput vertex in job graph</summary>
      <description>Take the following test(put it to MultipleInputITCase) as example: @Test public void testMultipleInputDoesNotChainedWithSource() throws Exception { testJobVertexName(false); } @Test public void testMultipleInputChainedWithSource() throws Exception { testJobVertexName(true); } public void testJobVertexName(boolean chain) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); TestListResultSink&lt;Long&gt; resultSink = new TestListResultSink&lt;&gt;(); DataStream&lt;Long&gt; source1 = env.fromSequence(0L, 3L).name("source1"); DataStream&lt;Long&gt; source2 = env.fromElements(4L, 6L).name("source2"); DataStream&lt;Long&gt; source3 = env.fromElements(7L, 9L).name("source3"); KeyedMultipleInputTransformation&lt;Long&gt; transform = new KeyedMultipleInputTransformation&lt;&gt;( "MultipleInput", new KeyedSumMultipleInputOperatorFactory(), BasicTypeInfo.LONG_TYPE_INFO, 1, BasicTypeInfo.LONG_TYPE_INFO); if (chain) { transform.setChainingStrategy(ChainingStrategy.HEAD_WITH_SOURCES); } KeySelector&lt;Long, Long&gt; keySelector = (KeySelector&lt;Long, Long&gt;) value -&gt; value % 3; env.addOperator( transform .addInput(source1.getTransformation(), keySelector) .addInput(source2.getTransformation(), keySelector) .addInput(source3.getTransformation(), keySelector)); new MultipleConnectedStreams(env).transform(transform).rebalance().addSink(resultSink).name("sink"); env.execute(); } When we run testMultipleInputDoesNotChainedWithSource , all job vertex names are normal:When we run testMultipleInputChainedWithSource (the MultipleInput chained with source1), job vertex names get messed up (all job vertex names contain Source: source1): I think it's a bug.</description>
      <version>1.16.2,1.18.0,1.17.1</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="32703" opendate="2023-7-27 00:00:00" fixdate="2023-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hotfix] flink-python POM has a typo for protobuf-java in shading config</summary>
      <description>Fix typo. `inculde` -&gt; `include`                                  &lt;includes combine.children="append"&gt;                                    &lt;include&gt;net.razorvine:*&lt;/include&gt;                                    &lt;include&gt;net.sf.py4j:*&lt;/include&gt;                                    &lt;include&gt;org.apache.beam:*&lt;/include&gt;                                    &lt;include&gt;com.fasterxml.jackson.core:*&lt;/include&gt;                                    &lt;include&gt;joda-time:*&lt;/include&gt;                                    &lt;inculde&gt;com.google.protobuf:*&lt;/inculde&gt;                                    &lt;include&gt;org.apache.arrow:*&lt;/include&gt;                                    &lt;include&gt;io.netty:*&lt;/include&gt;                                    &lt;include&gt;com.google.flatbuffers:*&lt;/include&gt;                                    &lt;include&gt;com.alibaba:pemja&lt;/include&gt;                                &lt;/includes&gt;</description>
      <version>1.16.2,1.18.0,1.17.1</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32755" opendate="2023-8-4 00:00:00" fixdate="2023-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add quick start guide for Flink OLAP</summary>
      <description>I propose to add a new QUICKSTART.md guide that provides instructions for beginner to build a production ready Flink OLAP Service by using flink-jdbc-driver, flink-sql-gateway and flink session cluster.</description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.overview.md</file>
    </fixedFiles>
  </bug>
  <bug id="32759" opendate="2023-8-5 00:00:00" fixdate="2023-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the removed config in the doc</summary>
      <description>The cluster.declarative-resource-management.enabled was removed at FLINK-21095(https://github.com/apache/flink/pull/15838/files), so it doesn't work now.However, the flink doc still have it.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.elastic.scaling.md</file>
      <file type="M">docs.content.zh.docs.deployment.elastic.scaling.md</file>
    </fixedFiles>
  </bug>
  <bug id="32760" opendate="2023-8-5 00:00:00" fixdate="2023-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Version Conflict in flink-sql-connector-hive for shaded.parquet prefix packages</summary>
      <description>SummaryIn https://issues.apache.org/jira/browse/FLINK-23074 it seems like shading parquet dependency from hive-exec is done. But I think this is not enough and causing errors like below when I try to read parquet file using sql-gateway which requires both flink-parquet and flink-sql-connector-hive dependencies. CauseParquet dependency not only includes org.apache.parquet but also shaded.parquet prefix dependencies. (ref)So we need to shade both.- flink-parquet depends on Parquet 1.12.3 with shaded Thrift 0.16.0 (prefix: shaded.parquet)- flink-sql-connector-hive depends on hive-exec 3.1.3 with Parquet 1.10.0 and shaded Thrift 0.9.3 (prefix: shaded.parquet)- Code compiled against Thrift 0.16.0 attempts to run against 0.9.3, causing the error.Proposed solutionAdding new shading rule to flink-sql-connector-hive project.I have confirmed that adding this rule could resolve the above error.&lt;relocation&gt; &lt;pattern&gt;shaded.parquet&lt;/pattern&gt; &lt;shadedPattern&gt;shaded.parquet.flink.hive.shaded&lt;/shadedPattern&gt;&lt;/relocation&gt; I would be happy to implement it if the proposal is accepted. Thanks </description>
      <version>1.17.1</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.3.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.9.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="32824" opendate="2023-8-10 00:00:00" fixdate="2023-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port Calcite&amp;#39;s fix for the sql like operator</summary>
      <description>we should port the bugfix of sql like operator https://issues.apache.org/jira/browse/CALCITE-1898The LIKE operator must match '.' (period) literally, not treat it as a wild-card. Currently it treats it the same as '_'.</description>
      <version>1.18.0,1.17.1</version>
      <fixedVersion>1.18.0,1.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.functions.SqlLikeUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="32835" opendate="2023-8-11 00:00:00" fixdate="2023-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] The accumulators, blob and blocklist packages of flink-runtime module</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.TransientBlobCacheTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.TestingBlobUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.PermanentBlobCacheTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.PermanentBlobCacheSizeLimitTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.FileSystemBlobStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobUtilsNonWritableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobServerSSLTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobServerRecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobServerRangeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobServerPutTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobServerGetTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobServerDeleteTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobServerCorruptionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobServerCleanupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobKeyTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobClientTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobClientSslTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobCacheSuccessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobCacheSizeTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobCacheRetriesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobCacheRecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobCachePutTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobCacheGetTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobCacheDeleteTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobCacheCorruptionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobCacheCleanupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.accumulators.StringifiedAccumulatorResultTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.HDFSTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="32836" opendate="2023-8-11 00:00:00" fixdate="2023-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] The checkpoint package of flink-runtime module</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.VertexFinishedStateCheckerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.TaskStateStatsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.TaskStateSnapshotTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.SubtaskStateStatsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StatsSummaryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StateObjectCollectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.RestoredCheckpointStatsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.RescaleMappingsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PrioritizedOperatorSubtaskStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PerJobCheckpointRecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.PendingCheckpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.OperatorSubtaskStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.metadata.MetadataV4SerializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.metadata.MetadataV3SerializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.metadata.CheckpointTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.metadata.CheckpointMetadataTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.MappingBasedRepartitionerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.hooks.MasterHooksTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.FullyFinishedOperatorStateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.DefaultSchedulerCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStatsTrackerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStatsStatusTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStatsSnapshotTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStatsHistoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStatsCountsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointSettingsSerializableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointsCleanerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointRequestDeciderTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointPropertiesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointOptionsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointMetricsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointMetadataLoadingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointIDCounterTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointFailureManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTriggeringTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorRestoringTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="32837" opendate="2023-8-11 00:00:00" fixdate="2023-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] The client, clusterframework and concurrent packages of flink-runtime module</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerProcessUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.ResourceProfileTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.ResourceBudgetManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.TaskExecutorProcessUtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.BootstrapToolsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.ApplicationStatusTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.client.SerializedJobExecutionResultTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.client.ClientUtilsTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="32863" opendate="2023-8-14 00:00:00" fixdate="2023-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Flink UI&amp;#39;s time precision from second level to millisecond level</summary>
      <description>This an UI improvement for OLAP jobs.OLAP queries are generally small queries which will finish at the seconds or milliseconds, but currently the time precision displayed is second level and not enough for OLAP queries. Millisecond part of time is very important for users and developers, to see accurate time, for performance measurement and optimization. The displayed time includes job duration, task duration, task start time, end time and so on.It would be nice to improve this for better OLAP user experience.</description>
      <version>1.17.1</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.log-list.task-manager-log-list.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.timeline.job-timeline.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.taskmanagers.job-overview-drawer-taskmanagers.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.list.job-overview-list.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.detail.job-overview-drawer-detail.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.job-detail.status.job-status.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.subtask.job-checkpoints-subtask.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.detail.job-checkpoints-detail.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.log-list.job-manager-log-list.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.components.job-list.job-list.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.components.humanize-duration.pipe.ts</file>
    </fixedFiles>
  </bug>
  <bug id="32865" opendate="2023-8-14 00:00:00" fixdate="2023-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DynamicFilteringDataCollectorOperator can&amp;#39;t chain with the upstream operator when the parallelism is inconsistent</summary>
      <description> If the DynamicFilteringDataCollectorOperator parallelism is not consistent with the upstream operator, they can't chain together, this will the DynamicFilteringDataCollectorOperator to execute after the fact source, so the dpp won't work. Due to the operator parallelism being decided during runtime, so we should add scheduler dependency forcibly in compile phase.</description>
      <version>1.16.2,1.18.0,1.17.1</version>
      <fixedVersion>1.18.0,1.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.dynamicfiltering.ExecutionOrderEnforcerOperatorFactory.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.dynamicfiltering.ExecutionOrderEnforcerOperator.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.optimize.program.FlinkRuntimeFilterProgramTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.optimize.program.DynamicPartitionPruningProgramTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.operator.BatchOperatorNameTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DynamicFilteringTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.processor.ResetTransformationProcessorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.ResetTransformationProcessor.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.DynamicFilteringDependencyProcessor.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecMultipleInput.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.DynamicFilteringITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecExecutionOrderEnforcer.java</file>
    </fixedFiles>
  </bug>
  <bug id="32963" opendate="2023-8-26 00:00:00" fixdate="2023-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make the test "testKeyedMapStateStateMigration" stable</summary>
      <description>We are proposing to make the following test stable:org.apache.flink.runtime.state.FileStateBackendMigrationTest.testKeyedMapStateStateMigrationThe test is currently flaky because the order of elements returned by the iterator is non-deterministic.The following PR fixes the flaky test by making it independent of the order of elements returned by the iterator:https://github.com/apache/flink/pull/23298We detected this using the NonDex tool using the following command:mvn edu.illinois:nondex-maven-plugin:2.1.1:nondex -pl flink-runtime -DnondexRuns=10 -Dtest=org.apache.flink.runtime.state.FileStateBackendMigrationTest#testKeyedMapStateStateMigrationPlease see the following Continuous Integration log that shows the flakiness:https://github.com/asha-boyapati/flink/actions/runs/5909136145/job/16029377793Please see the following Continuous Integration log that shows that the flakiness is fixed by this change:https://github.com/asha-boyapati/flink/actions/runs/5909183468/job/16029467973</description>
      <version>1.17.1</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendMigrationTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="33053" opendate="2023-9-7 00:00:00" fixdate="2023-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Watcher leak in Zookeeper HA mode</summary>
      <description>We observe a watcher leak in our OLAP stress test when enabling Zookeeper HA mode. TM's watches on the leader of JobMaster has not been stopped after job finished.Here is how we re-produce this issue: Start a session cluster and enable Zookeeper HA mode. Continuously and concurrently submit short queries, e.g. WordCount to the cluster. echo -n wchp | nc {zk host} {zk port} to get current watches.We can see a lot of watches on /flink/{cluster_name}/leader/{job_id}/connection_info.</description>
      <version>1.17.0,1.18.0,1.17.1</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="33083" opendate="2023-9-13 00:00:00" fixdate="2023-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SupportsReadingMetadata is not applied when loading a CompiledPlan</summary>
      <description>If a few conditions are met, we can not apply ReadingMetadata interface: source overwrites: @Override public boolean supportsMetadataProjection() { return false; } source does not implement SupportsProjectionPushDown table has metadata columns e.g.CREATE TABLE src ( physical_name STRING, physical_sum INT, timestamp TIMESTAMP_LTZ(3) NOT NULL METADATA VIRTUAL) we query the table SELECT * FROM srcIt fails with:Caused by: java.lang.IllegalArgumentException: Row arity: 1, but serializer arity: 2 at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:124)The reason is SupportsReadingMetadataSpec is created only in the PushProjectIntoTableSourceScanRule, but the rule is not applied when 1 &amp; 2</description>
      <version>1.16.2,1.17.1</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.TableSourceJsonPlanITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSourceUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSinkUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.NonDeterministicDagTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.WatermarkAssignerChangelogNormalizeTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.PushLocalAggIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.optimize.ScanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testReuseSourceWithoutProjectionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.connector.file.table.FileSystemTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.spec.DynamicTableSourceSpec.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
