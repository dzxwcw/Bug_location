<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="11136" opendate="2018-12-12 00:00:00" fixdate="2018-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the logical of merge for DISTINCT aggregates</summary>
      <description>The logic of merge for DISTINCT aggregates has bug. For the following query:SELECT c, COUNT(DISTINCT b), SUM(DISTINCT b), SESSION_END(rowtime, INTERVAL '0.005' SECOND)FROM MyTableGROUP BY SESSION(rowtime, INTERVAL '0.005' SECOND), cthe following exception will be thrown:Caused by: java.lang.ClassCastException: org.apache.flink.types.Row cannot be cast to java.lang.Integerat scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:101)at scala.math.Numeric$IntIsIntegral$.plus(Numeric.scala:58)at org.apache.flink.table.functions.aggfunctions.SumAggFunction.accumulate(SumAggFunction.scala:50)at GroupingWindowAggregateHelper$18.mergeAccumulatorsPair(Unknown Source)at org.apache.flink.table.runtime.aggregate.AggregateAggFunction.merge(AggregateAggFunction.scala:66)at org.apache.flink.table.runtime.aggregate.AggregateAggFunction.merge(AggregateAggFunction.scala:33)at org.apache.flink.runtime.state.heap.HeapAggregatingState.mergeState(HeapAggregatingState.java:117)at org.apache.flink.runtime.state.heap.AbstractHeapMergingState$MergeTransformation.apply(AbstractHeapMergingState.java:102)at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.transform(CopyOnWriteStateTable.java:463)at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.transform(CopyOnWriteStateTable.java:341)at org.apache.flink.runtime.state.heap.AbstractHeapMergingState.mergeNamespaces(AbstractHeapMergingState.java:91)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator$2.merge(WindowOperator.java:341)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator$2.merge(WindowOperator.java:311)at org.apache.flink.streaming.runtime.operators.windowing.MergingWindowSet.addWindow(MergingWindowSet.java:212)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.processElement(WindowOperator.java:311)at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:202)at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:105)at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)at org.apache.flink.runtime.taskmanager.Task.run(Task.java:704)at java.lang.Thread.run(Thread.java:745)</description>
      <version>None</version>
      <fixedVersion>1.6.3,1.7.1,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug id="13999" opendate="2019-9-7 00:00:00" fixdate="2019-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct the documentation of MATCH_RECOGNIZE</summary>
      <description>Regarding to the following example in the doc:SELECT *FROM Ticker MATCH_RECOGNIZE ( PARTITION BY symbol ORDER BY rowtime MEASURES FIRST(A.rowtime) AS start_tstamp, LAST(A.rowtime) AS end_tstamp, AVG(A.price) AS avgPrice ONE ROW PER MATCH AFTER MATCH SKIP TO FIRST B PATTERN (A+ B) DEFINE A AS AVG(A.price) &lt; 15 ) MR;Given the inputs shown in the doc, it should be: symbol start_tstamp end_tstamp avgPrice========= ================== ================== ============ACME 01-APR-11 10:00:00 01-APR-11 10:00:03 14.5instead of: symbol start_tstamp end_tstamp avgPrice========= ================== ================== ============ACME 01-APR-11 10:00:00 01-APR-11 10:00:03 14.5ACME 01-APR-11 10:00:04 01-APR-11 10:00:09 13.5</description>
      <version>1.8.2,1.9.1</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.streaming.match.recognize.md</file>
    </fixedFiles>
  </bug>
  <bug id="14126" opendate="2019-9-19 00:00:00" fixdate="2019-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elasticsearch Xpack Machine Learning doesn&amp;#39;t support ARM</summary>
      <description>Elasticsearch Xpack Machine Learning function is enabled by default if the version is &gt;=6.0. But This feature doesn't support ARM arch. So that in some e2e tests, Elasticsearch is failed to start.We should disable ML feature in this case on ARM.</description>
      <version>1.9.1</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.quickstarts.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.elasticsearch.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.elasticsearch-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="14459" opendate="2019-10-19 00:00:00" fixdate="2019-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python module build hangs</summary>
      <description>The build of python module hangs when installing conda. See travis log: https://api.travis-ci.org/v3/job/599704570/log.txtCan't reproduce it neither on my local mac nor on my repo with travis.</description>
      <version>1.9.0,1.9.1</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug id="14476" opendate="2019-10-21 00:00:00" fixdate="2019-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend PartitionTracker to support partition promotions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.TestingJobMasterPartitionTracker.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.NoOpJobMasterPartitionTracker.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.JobMasterPartitionTrackerImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.JobMasterPartitionTrackerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.JobMasterPartitionTracker.java</file>
    </fixedFiles>
  </bug>
  <bug id="14481" opendate="2019-10-22 00:00:00" fixdate="2019-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify the Flink valid socket port check to 0 to 65535.</summary>
      <description>In Flink, I found that Flink's socket port check is 'port &gt;= 0 &amp;&amp; port &lt;= 65536.checkArgument(serverPort &gt;= 0 &amp;&amp; serverPort &lt;= 65536, "Invalid port number.");But in the process of binding the port, the valid port is 0 to 65535(A port number of zero will let the System pick up anephemeral port in a bin operation). Although the 65536 port will fail due to the port out of range when actually binding, Flink has already done a valid range check on the port, which seems to be very confusing. Should we modify Flink's port check to 0 to 65535?</description>
      <version>1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClient.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.client.QueryableStateClient.java</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.main.java.org.apache.flink.metrics.influxdb.InfluxdbReporter.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.NetUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyConfig.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.SocketTextStreamFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.SocketClientSink.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="14482" opendate="2019-10-22 00:00:00" fixdate="2019-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump up rocksdb version</summary>
      <description>This JIRA aims at rebasing frocksdb to newer version of official RocksDB.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainerTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBInitTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksIteratorWrapper.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBConfigurableOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.configurable.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="14484" opendate="2019-10-22 00:00:00" fixdate="2019-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify RocksDB backend to bound total memory via Cache with WriteBufferManager</summary>
      <description>Support to allow RocksDB statebackend to enable write buffer manager feature via options. And we would use the same LRUCache to limit the memory usage for write buffer and block cache.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendBoundedMemoryTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBSharedObjects.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointEnvironment.java</file>
      <file type="M">docs..includes.generated.rocks.db.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="14488" opendate="2019-10-22 00:00:00" fixdate="2019-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update python table API with temporary tables &amp; views methods</summary>
      <description>Update python table API with new methods introduced in Java/Scala APIThis should cover the scope of FLINK-14490</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.table.descriptors.py</file>
    </fixedFiles>
  </bug>
  <bug id="14522" opendate="2019-10-24 00:00:00" fixdate="2019-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adjust GC Cleaner for unsafe memory and Java 11</summary>
      <description>sun.misc.Cleaner is not available since Java 9.It was moved to jdk.internal.ref.Cleaner of java.base module (Open JDK-8148117), but another new public API  was introduced to achieve the same behaviour (java.lang.ref.Cleaner);A popular solution is use reflection to look up for the location of the Cleaner class depending on running JVM version.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.MemoryUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.MemorySegmentFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="14524" opendate="2019-10-25 00:00:00" fixdate="2019-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PostgreSQL JDBC sink generates invalid SQL in upsert mode</summary>
      <description>The "upsert" query generated for the PostgreSQL dialect is missing a closing parenthesis in the ON CONFLICT clause, causing the INSERT statement to error out with the error ERROR o.a.f.s.runtime.tasks.StreamTask - Error during disposal of stream operator.java.lang.RuntimeException: Writing records to JDBC failed.{{ at org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.checkFlushException(JDBCUpsertOutputFormat.java:135)}}{{ at org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.close(JDBCUpsertOutputFormat.java:184)}}{{ at org.apache.flink.api.java.io.jdbc.JDBCUpsertSinkFunction.close(JDBCUpsertSinkFunction.java:61)}}{{ at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:43)}}{{ at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.dispose(AbstractUdfStreamOperator.java:117)}}{{ at org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:585)}}{{ at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:484)}}{{ at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:705)}}{{ at org.apache.flink.runtime.taskmanager.Task.run(Task.java:530)}}{{ at java.lang.Thread.run(Thread.java:748)}}Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO "public.temperature"("id", "timestamp", "temperature") VALUES ('sensor_17', '2019-10-25 00:39:10-05', 20.27573964210997) ON CONFLICT ("id", "timestamp" DO UPDATE SET "id"=EXCLUDED."id", "timestamp"=EXCLUDED."timestamp", "temperature"=EXCLUDED."temperature" was aborted: ERROR: syntax error at or near "DO"{{ Position: 119 Call getNextException to see other errors in the batch.}}{{ at org.postgresql.jdbc.BatchResultHandler.handleCompletion(BatchResultHandler.java:163)}}{{ at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:838)}}{{ at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1546)}}{{ at org.apache.flink.api.java.io.jdbc.writer.UpsertWriter$UpsertWriterUsingUpsertStatement.internalExecuteBatch(UpsertWriter.java:177)}}{{ at org.apache.flink.api.java.io.jdbc.writer.UpsertWriter.executeBatch(UpsertWriter.java:117)}}{{ at org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.flush(JDBCUpsertOutputFormat.java:159)}}{{ at org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.lambda$open$0(JDBCUpsertOutputFormat.java:124)}}{{ at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)}}{{ at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)}}{{ at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)}}{{ at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)}}{{ at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)}}{{ at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)}}{{ ... 1 common frames omitted}}Caused by: org.postgresql.util.PSQLException: ERROR: syntax error at or near "DO"{{ Position: 119}}{{ at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2497)}}{{ at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2233)}}{{ at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:310)}}{{ at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:834)}}{{ ... 12 common frames omitted}}</description>
      <version>1.9.1,1.10.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.dialect.JDBCDialects.java</file>
    </fixedFiles>
  </bug>
  <bug id="14535" opendate="2019-10-26 00:00:00" fixdate="2019-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cast exception is thrown when count distinct on decimal fields</summary>
      <description>DecimalType in DistinctInfo bridged to wrong external BigDecimal type, which causes failures count distinct on decimal type.</description>
      <version>1.9.1</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.DateTimeTestUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SplitAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14546" opendate="2019-10-28 00:00:00" fixdate="2019-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support map type in flink-json</summary>
      <description>Currently in flink-json, we don't treat map type as a special type, which results that the type of map is deduced from fastjson.For example, when we set `map&lt;varchar, int&gt;` in SQL DDL，and get a value large than int_max, which will result in `Long` from fastjson, and will cause a runtime error.</description>
      <version>1.8.2,1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonRowDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDeserializationSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="14561" opendate="2019-10-29 00:00:00" fixdate="2019-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t write FLINK_PLUGINS_DIR ENV variable to Flink configuration</summary>
      <description>With FLINK-12143 we introduced the plugin mechanism. As part of this feature, we now write the FLINK_PLUGINS_DIR environment variable to the Flink Configuration we use for the cluster components. This is problematic, because we also use this Configuration to start new processes (Yarn and Mesos TaskExecutors). If the Configuration contains a configured FLINK_PLUGINS_DIR which differs from the one used by the newly created process, then this leads to problems.In order to solve this problem, I suggest to not write env variables which are intended for local usage within the started process into the Configuration. Instead we should directly read the environment variable at the required site similar to what we do with the env variable FLINK_LIB_DIR.</description>
      <version>1.9.1,1.10.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.overlays.FlinkDistributionOverlayTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.overlays.FlinkDistributionOverlay.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.plugin.PluginConfig.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.GlobalConfiguration.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="14584" opendate="2019-10-31 00:00:00" fixdate="2019-12-31 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support complex data types in Python user-defined functions</summary>
      <description>This jira is a sub-task of FLINK-14388. In this jira, complex data types which include ArrayType, DecimalType, MapType and MultisetType are dedicated to be supported for python UDF.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.functions.python.PythonTypeUtilsTest.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtils.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.coders.test.common.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  <bug id="1460" opendate="2015-1-29 00:00:00" fixdate="2015-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo fixes</summary>
      <description>Fix some typos. Also fix some inconsistent uses of partition operator and partitioning operator in the codebase.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.compiler.PartitionOperatorTranslationTest.scala</file>
      <file type="M">flink-examples.flink-scala-examples.src.main.scala.org.apache.flink.examples.scala.ml.LinearRegression.scala</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.java.PartitioningOperatorTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="14601" opendate="2019-11-4 00:00:00" fixdate="2019-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CLI documentation for list is missing &amp;#39;-a&amp;#39;</summary>
      <description>It is shown in the examples here https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/cli.html#job-management-examples however it is not in the help docs or any other documentation I can find  $ ./bin/flink list --helpAction "list" lists running and scheduled programs. Syntax: list [OPTIONS] "list" action options: -r,--running Show only running programs and their JobIDs -s,--scheduled Show only scheduled programs and their JobIDs Options for default mode: -m,--jobmanager &lt;arg&gt; Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. -z,--zookeeperNamespace &lt;arg&gt; Namespace to create the Zookeeper sub-paths for high availability mode</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="14605" opendate="2019-11-5 00:00:00" fixdate="2019-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Hive-1.1.0 as the profile to test against 1.1.x</summary>
      <description>Hive-1.1.1 has the issue that it can't properly handle stored as file_format syntax. So let's test against 1.1.0.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1461" opendate="2015-1-29 00:00:00" fixdate="2015-2-29 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add sortPartition operator</summary>
      <description>A sortPartition() operator can be used to sort the input of a mapPartition() operator enforce a certain sorting of the input of a given operator of a program.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.util.CollectionDataSets.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.DataSet.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.DataSet.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.PactCompiler.java</file>
      <file type="M">docs.programming.guide.md</file>
      <file type="M">docs.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  <bug id="14610" opendate="2019-11-5 00:00:00" fixdate="2019-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for how to define time attribute in DDL</summary>
      <description>Add documentation for how to use watermark syntax and computed column in DDL to define processing time attribute and event time attribute.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.streaming.time.attributes.zh.md</file>
      <file type="M">docs.dev.table.streaming.time.attributes.md</file>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.sourceSinks.zh.md</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
    </fixedFiles>
  </bug>
  <bug id="14636" opendate="2019-11-6 00:00:00" fixdate="2019-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle schedule mode LAZY_FROM_SOURCES_WITH_BATCH_SLOT_REQUEST correctly in DefaultScheduler</summary>
      <description>It should be possible to schedule a job with ScheduleMode.LAZY_FROM_SOURCES_WITH_BATCH_SLOT_REQUEST.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="14637" opendate="2019-11-6 00:00:00" fixdate="2019-11-6 01:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce framework off heap memory config</summary>
      <description>At the moment after FLINK-13982, when we do not account for adhoc direct memory allocations for Flink framework (except network buffers) or done by some libraries used in Flink. In general, we expect this allocations to stay under a certain reasonably low limit but we have to have some margin for them so that JVM direct memory limit is not exactly equal to network buffers and does not fail. We can address it by introducing framework off heap memory config option.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.TaskExecutorResourceSpec.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs..includes.generated.task.manager.memory.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="14665" opendate="2019-11-8 00:00:00" fixdate="2019-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support computed column in blink-planner</summary>
      <description></description>
      <version>1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TestData.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.InnerJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.GroupingSetsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.DistinctAggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.factories.utils.TestCollectionTableFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.WatermarkGeneratorCodeGenTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.digest.testGetDigestWithDynamicFunctionView.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.sources.TableSourceUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.FlinkRelOptTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.PhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.delegation.PlannerContext.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.QueryOperationCatalogViewTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="14724" opendate="2019-11-12 00:00:00" fixdate="2019-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Join condition could be simplified in logical phase</summary>
      <description>currently the plan of tpcds q38.sql contains NestedLoopJoin, because it's join condition is CAST(AND(IS NOT DISTINCT FROM($2, $3), IS NOT DISTINCT FROM($1, $4), IS NOT DISTINCT FROM($0, $5))):BOOLEAN, and planner can't find equal join keys from the condition by Join#analyzeCondition.SimplifyJoinConditionRule could solve this.</description>
      <version>1.9.0,1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14727" opendate="2019-11-13 00:00:00" fixdate="2019-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update doc of supported Hive versions</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="14728" opendate="2019-11-13 00:00:00" fixdate="2019-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add reminder for users of potential thread safety issues of hive built-in function</summary>
      <description>remind users of https://issues.apache.org/jira/browse/HIVE-16183</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.hive.functions.zh.md</file>
      <file type="M">docs.dev.table.hive.hive.functions.md</file>
    </fixedFiles>
  </bug>
  <bug id="14746" opendate="2019-11-13 00:00:00" fixdate="2019-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>History server does not handle uncaught exceptions in archive fetcher.</summary>
      <description>In case archive fetcher fails with an error - eg. OOM while parsing json archives, the error is swallowed by ScheduledExectutorService and the submitted runnable is never rescheduled.</description>
      <version>1.8.2,1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServerArchiveFetcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="14747" opendate="2019-11-13 00:00:00" fixdate="2019-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression with latest changes to Mailbox</summary>
      <description>The commits edeec8d7420185d1c49b2739827bd921d2c2d485 .. 809533e5b5c686e2d21b64361d22178ccb92ec26 introduced a performance regression in the course of FLINK-14304 .The root cause seems to be the removal of the volatile variable for speed up the hotpath in case of an empty mailbox queue resulting in unnecessary lock acquisitions.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="1475" opendate="2015-2-4 00:00:00" fixdate="2015-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minimize log output of yarn test cases</summary>
      <description>The new yarn test cases are quite verbose. Maybe we could increase the log level for these tests.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.resources.log4j-test.properties</file>
    </fixedFiles>
  </bug>
  <bug id="14800" opendate="2019-11-15 00:00:00" fixdate="2019-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce parallelism inference for HiveSource</summary>
      <description>Because new source api is not ready, we can not finish FLINK-14676Let's introduce parallelism inference for HiveSource first.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveOptions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14807" opendate="2019-11-15 00:00:00" fixdate="2019-5-15 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add TableResult#collect api for fetching data to client</summary>
      <description>Currently, it is very unconvinient for user to fetch data of flink job unless specify sink expclitly and then fetch data from this sink via its api (e.g. write to hdfs sink, then read data from hdfs). However, most of time user just want to get the data and do whatever processing he want. So it is very necessary for flink to provide api Table#collect for this purpose.  Other apis such as Table#head, Table#print is also helpful.   </description>
      <version>1.9.1</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.api.environment.RemoteStreamEnvironmentTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingRestfulGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.TestingDispatcherGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorSchedulerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.TestingJobMasterGatewayBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.utils.TestingJobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.RestfulGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerNG.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.TestingClusterClient.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PerJobMiniClusterFactory.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.MiniClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ClusterClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.ClusterClientJobClientAdapter.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.deployment.application.EmbeddedJobClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="14816" opendate="2019-11-15 00:00:00" fixdate="2019-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add thread dump feature for taskmanager</summary>
      <description>Add thread dump feature for taskmanager, so use can get thread information easily.</description>
      <version>1.9.1</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGatewayBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.utils.TestingResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.JvmUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerThreadDumpFileHeaders.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpFileHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.task-manager.ts</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.FileType.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.task-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.task-manager.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.task-manager-routing.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.status.task-manager-status.component.ts</file>
    </fixedFiles>
  </bug>
  <bug id="14830" opendate="2019-11-16 00:00:00" fixdate="2019-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct the link for chinese version stream_checkpointing page</summary>
      <description>Currently, in Chinese version of stream_checkpointing page, there are some links not correct set to the Chinese version, but set to the English version.Such as {{site.baseurl }}/dev/stream/state/index.html &amp;#91;state backend&amp;#93;({{ site.baseurl }}/ops/state/state_backends.html).&amp;#91;State Backends&amp;#93;({{ site.baseurl }}/ops/state/state_backends.html)&amp;#91;Restart Strategies&amp;#93;({{ site.baseurl }}/dev/restart_strategies.html)  This issue wants to fix the problem.</description>
      <version>1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.internals.stream.checkpointing.zh.md</file>
    </fixedFiles>
  </bug>
  <bug id="14845" opendate="2019-11-19 00:00:00" fixdate="2019-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce data compression to blocking shuffle.</summary>
      <description>Currently, blocking shuffle writer writes raw output data to disk without compression. For IO bounded scenario, this can be optimized by compressing the output data. It is better to introduce a compression mechanism and offer users a config option to let the user decide whether to compress the shuffle data. Actually, we hava implemented compression in our inner Flink version and  here are some key points:1. Where to compress/decompress?Compressing at upstream and decompressing at downstream.2. Which thread do compress/decompress?Task threads do compress/decompress.3. Data compression granularity.Per buffer.4. How to handle that when data size become even bigger after compression?Give up compression in this case and introduce an extra flag to identify if the data was compressed, that is, the output may be a mixture of compressed and uncompressed data. We'd like to introduce blocking shuffle data compression to Flink if there are interests. </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.compression.BlockCompressor.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.io.CompressedHeaderlessChannelTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.compression.BlockCompressionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.FileChannelUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.BufferedKVExternalSorter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.BinaryKVExternalMerger.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.BinaryExternalSorter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.BinaryExternalMerger.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.AbstractBinaryExternalMerger.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.io.CompressedHeaderlessChannelWriterOutputView.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.io.CompressedHeaderlessChannelReaderInputView.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.io.CompressedBlockChannelWriter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.io.CompressedBlockChannelReader.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.LongHybridHashTable.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.BinaryHashPartition.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.BaseHybridHashTable.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.compression.Lz4BlockDecompressor.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.compression.Lz4BlockCompressor.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.compression.Lz4BlockCompressionFactory.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.compression.InsufficientBufferException.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.compression.DataCorruptionException.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.compression.BlockDecompressor.java</file>
      <file type="M">docs..includes.generated.netty.shuffle.environment.configuration.html</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.NettyShuffleEnvironmentOptions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.Buffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.NetworkBuffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.ReadOnlySlicedNetworkBuffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NettyShuffleServiceFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.CreditBasedSequenceNumberingViewReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyMessage.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartitionReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.BufferReaderWriterUtil.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.InputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.NoOpResultSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ReleaseOnConsumptionResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultSubpartitionView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.NettyShuffleEnvironmentConfiguration.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NettyShuffleEnvironmentBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.CancelPartitionRequestTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandlerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyMessageSerializationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.PartitionRequestQueueTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartitionAvailabilityTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartitionWriteReadTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.BoundedDataTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.FileChannelBoundedDataTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.InputGateFairnessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PartitionTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionWithReadViewTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.ResultPartitionFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SubpartitionTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.util.TestSubpartitionConsumer.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-runtime.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-runtime-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.compression.BlockCompressionFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="14847" opendate="2019-11-19 00:00:00" fixdate="2019-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support retrieving Hive PK constraints</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV210.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV100.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="14849" opendate="2019-11-19 00:00:00" fixdate="2019-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix documentation about Hive dependencies</summary>
      <description>With:&lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt;&lt;/dependency&gt;Caused by: java.lang.ClassCastException: org.codehaus.janino.CompilerFactory cannot be cast to org.codehaus.commons.compiler.ICompilerFactory at org.codehaus.commons.compiler.CompilerFactoryFactory.getCompilerFactory(CompilerFactoryFactory.java:129) at org.codehaus.commons.compiler.CompilerFactoryFactory.getDefaultCompilerFactory(CompilerFactoryFactory.java:79) at org.apache.calcite.rel.metadata.JaninoRelMetadataProvider.compile(JaninoRelMetadataProvider.java:432) ... 68 moreAfter https://issues.apache.org/jira/browse/FLINK-13749 , flink-client will use default child-first resolve-order.If user jar has some conflict dependents, there will be some problem.Maybe we should update document to add some exclusions to hive dependents.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="14854" opendate="2019-11-19 00:00:00" fixdate="2019-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add executeAsync() method to execution environments</summary>
      <description>The new executeAsync() method should return a JobClient. This exposes the new executor/job client work on the user API.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.CollectionTestEnvironment.java</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamPlanEnvironment.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="14857" opendate="2019-11-19 00:00:00" fixdate="2019-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecate checkpoint lock from the Operators API</summary>
      <description>Drop `this.checkpointLock = getContainingTask().getCheckpointLock();` access (used for example in `ContinuousFileReaderOperator`).Redirect users to the mailbox (enqueuing/yielding).</description>
      <version>1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="14872" opendate="2019-11-20 00:00:00" fixdate="2019-12-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>(Partial fix) Potential deadlock for task reading from blocking ResultPartition.</summary>
      <description>Currently, the buffer pool size of InputGate reading from blocking ResultPartition is unbounded which have a potential of using too many buffers and may lead to ResultPartition of the same task can not acquire enough core buffers and finally lead to deadlock.Considers the following case:Core buffers are reserved for InputGate and ResultPartition -&gt; InputGate consumes lots of Buffer (not including the buffer reserved for ResultPartition) -&gt; Other tasks acquire exclusive buffer for InputGate and trigger redistribute of Buffers (Buffers taken by previous InputGate can not be released) -&gt; The first task of which InputGate uses lots of buffers begin to emit records but can not acquire enough core Buffers (Some operators may not emit records out immediately or there is just nothing to emit) -&gt; Deadlock. I think we can fix this problem by limit the number of Buffers can be allocated by a InputGate which reads from blocking ResultPartition.</description>
      <version>1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NettyShuffleEnvironmentTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="14874" opendate="2019-11-20 00:00:00" fixdate="2019-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add local aggregate to solve data skew for ROLLUP/CUBE case</summary>
      <description>Many tpc-ds queries have rollup keyword, which will be translated to multiple groups. for example: group by rollup (channel, id) is equivalent group by (channel, id) + group by (channel) + group by (). All data on empty group will be shuffled to a single node, It is a typical data skew case. If there is a local aggregate, the data size shuffled to the single node will be greatly reduced. However, currently the cost mode can't estimate the local aggregate's cost, and the plan with local aggregate may be chose even the query has rollup keyword.we could add a rule based phase (after physical phase) to enforce local aggregate if it's input has empty group.</description>
      <version>1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateReduceGroupingTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecSortAggRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecHashAggRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecAggRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkBatchProgram.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14881" opendate="2019-11-20 00:00:00" fixdate="2019-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade AWS SDK to support "IAM Roles for Service Accounts" in AWS EKS</summary>
      <description>In order to use IAM Roles for Service Accounts in AWS EKS, the minimum required version of the AWS SDK  is 1.11.625.https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts-minimum-sdk.html</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.AWSConfigConstants.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
      <file type="M">docs.dev.connectors.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug id="14899" opendate="2019-11-21 00:00:00" fixdate="2019-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>can not be translated to StreamExecDeduplicate when PROCTIME() is defined in query</summary>
      <description>CREATE TABLE user_log ( user_id VARCHAR, item_id VARCHAR, category_id VARCHAR, behavior VARCHAR, ts TIMESTAMP) WITH ( 'connector.type' = 'kafka', 'connector.version' = 'universal', 'connector.topic' = 'user_behavior', 'connector.startup-mode' = 'earliest-offset', 'connector.properties.0.key' = 'zookeeper.connect', 'connector.properties.0.value' = 'localhost:2181', 'connector.properties.1.key' = 'bootstrap.servers', 'connector.properties.1.value' = 'localhost:9092', 'update-mode' = 'append', 'format.type' = 'json', 'format.derive-schema' = 'true');CREATE TABLE user_dist ( dt VARCHAR, user_id VARCHAR, behavior VARCHAR) WITH ( 'connector.type' = 'jdbc', 'connector.url' = 'jdbc:mysql://localhost:3306/flink-test', 'connector.table' = 'user_behavior_dup', 'connector.username' = 'root', 'connector.password' = ‘******', 'connector.write.flush.max-rows' = '1');INSERT INTO user_distSELECT dt, user_id, behaviorFROM ( SELECT dt, user_id, behavior, ROW_NUMBER() OVER (PARTITION BY dt, user_id, behavior ORDER BY proc asc ) AS rownum FROM (select DATE_FORMAT(ts, 'yyyy-MM-dd HH:00') as dt,user_id,behavior,PROCTIME() as proc from user_log) )WHERE rownum = 1;Exception in thread "main" org.apache.flink.table.api.TableException: UpsertStreamTableSink requires that Table has a full primary keys if it is updated.at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:114)at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:50)at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:54)at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlan(StreamExecSink.scala:50)at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:61)at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:60)at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)at scala.collection.Iterator$class.foreach(Iterator.scala:891)at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)at scala.collection.AbstractIterable.foreach(Iterable.scala:54)at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)at scala.collection.AbstractTraversable.map(Traversable.scala:104)at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:60)at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:149)at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:439)at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:348)</description>
      <version>1.9.0,1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.DeduplicateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.OverAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14935" opendate="2019-11-24 00:00:00" fixdate="2019-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve mailbox exception handling</summary>
      <description>Errors in mail processing are currently wrapped in IllegalStateException, but WrappingRuntimeException would be more canonical Usage of mailbox would be easier if we support CheckedRunnables in additional/instead of Runnables.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxProcessorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImplTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImplTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.MailboxOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.Mail.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.MailboxExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.concurrent.FutureUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.function.FunctionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="14952" opendate="2019-11-26 00:00:00" fixdate="2019-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn containers can exceed physical memory limits when using BoundedBlockingSubpartition.</summary>
      <description>As reported by a user on the user mailing list, combination of using BoundedBlockingSubpartition with yarn containers can cause yarn container to exceed memory limits.2019-11-19 12:49:23,068 INFO org.apache.flink.yarn.YarnResourceManager - Closing TaskExecutor connection container_e42_1574076744505_9444_01_000004 because: Container &amp;#91;pid=42774,containerID=container_e42_1574076744505_9444_01_000004&amp;#93; is running beyond physical memory limits. Current usage: 12.0 GB of 12 GB physical memory used; 13.9 GB of 25.2 GB virtual memory used. Killing container.This is probably happening because memory usage of mmap is not capped and not accounted by configured memory limits, however yarn is tracking this memory usage and once Flink exceeds some threshold, container is being killed.Workaround is to overrule default value and force Flink to not user mmap, by setting a secret (🤫) config option:taskmanager.network.bounded-blocking-subpartition-type: file</description>
      <version>1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.FileBufferReaderITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.NettyShuffleEnvironmentConfiguration.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.NettyShuffleEnvironmentOptions.java</file>
      <file type="M">docs..includes.generated.netty.shuffle.environment.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="14954" opendate="2019-11-26 00:00:00" fixdate="2019-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Publish OpenAPI specification of REST Monitoring API</summary>
      <description>Hello,Flink provides a very helpful REST Monitoring API.OpenAPI is convenient standard to generate clients in a variety of language for REST API documented according to their specification. In this case, clients would be helpful to automate management of Flink clusters.Currently, there is no "official" OpenAPI specification of Flink REST Monitoring API. Some have written by users, but their consistency across Flink releases is uncertain.I think it would be beneficial to have an OpenAPI specification provided and maintained by the Flink project. Kind regards, </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.json.SerializedThrowableSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.async.AsynchronousOperationResult.java</file>
      <file type="M">flink-docs.README.md</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">docs.content.docs.ops.rest.api.md</file>
      <file type="M">docs.content.zh.docs.ops.rest.api.md</file>
    </fixedFiles>
  </bug>
  <bug id="14976" opendate="2019-11-27 00:00:00" fixdate="2019-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cassandra Connector leaks Semaphore on Throwable; hangs on close</summary>
      <description>This issue was mostly fixed in FLINK-13059; unfortunately, the fix only caught Exception so any non-Exception Throwable can still cause the issue of leaking semaphores.</description>
      <version>1.8.2,1.9.1,1.10.0</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraSinkBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.java.org.apache.flink.streaming.connectors.cassandra.CassandraSinkBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="14978" opendate="2019-11-27 00:00:00" fixdate="2019-12-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce constraint class hierarchy required for primary keys</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.api.TableSchemaTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.TableSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="14992" opendate="2019-11-29 00:00:00" fixdate="2019-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add job submission listener to execution environments</summary>
      <description>We should add a way of registering listeners that are notified when a job is submitted for execution using an environment. This is useful for cases where a framework, for example the Zeppelin Notebook, creates an environment for the user, the user can submit jobs, but the framework needs a handle to the job in order to manage it.This can be as simple asinterface JobSubmissionListener { void notify(JobClient jobClient)}with a method registerJobSubmissionListener(JobSubmissionListener) on the environments.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.execution.JobListenerITCase.java</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.dataset.tests.test.execution.environment.completeness.py</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="14997" opendate="2019-12-1 00:00:00" fixdate="2019-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid to call unnecessary delete within RocksDBState&amp;#39;s mergeNamespaces implementation</summary>
      <description>After FLINK-7700, we delete old state when merging namespaces of RocksDB state. However, this change still leaves some performance space to improve:byte[] valueBytes = backend.db.get(columnFamily, sourceKey);backend.db.delete(columnFamily, writeOptions, sourceKey);if (valueBytes != null) { backend.db.merge(columnFamily, writeOptions, targetKey, valueBytes);}As you can see, we would call delete instantly after we get the valueBytes. However, since we did not allow null in list state value and the delete operation could be removed to the statement block which has verified the valueBytes is not null. By doing this, we could avoid unnecessary delete calls if sourceKey not existed actually.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBReducingState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBListState.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBAggregatingState.java</file>
    </fixedFiles>
  </bug>
  <bug id="15001" opendate="2019-12-2 00:00:00" fixdate="2019-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The digest of sub-plan reuse should contain retraction traits for stream physical nodes</summary>
      <description>This bug is found in FLINK-14946:The plan for the given sql in FLINK-14946 is however, the plan after sub-plan reuse is: in the first picture, we could find that the accMode of two joins are different, but the two joins are reused in the second picture. The reason is the digest of sub-plan reuse does not contain retraction traits for stream physical nodes now.</description>
      <version>1.9.0,1.9.1</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.RelDigestUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.digest.testGetDigestWithDynamicFunctionView.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.digest.testGetDigestWithDynamicFunction.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelTreeWriterImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelDigestUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.reuse.SubplanReuser.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15006" opendate="2019-12-2 00:00:00" fixdate="2019-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to close shuffle when dynamic partition inserting</summary>
      <description>When partition values are rare or have skew, if we shuffle by dynamic partitions, will break the performance.We can have an option to close shuffle in such cases:‘connector.sink.shuffle-by-partition.enable’ = ...</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.PartitionableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.PartitionableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.PartitionableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.PartitionableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecSinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecSinkRule.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15010" opendate="2019-12-2 00:00:00" fixdate="2019-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Temp directories flink-netty-shuffle-* are not cleaned up</summary>
      <description>Starting a Flink cluster with 2 TMs and stopping it again will leave 2 temporary directories (and not delete them): flink-netty-shuffle-&lt;uid&gt;</description>
      <version>1.9.0,1.9.1,1.9.2</version>
      <fixedVersion>1.9.3,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.FileChannelManagerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="15026" opendate="2019-12-3 00:00:00" fixdate="2019-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support database DDLs in SQL CLI</summary>
      <description>Support DDL as following:CREATE DATABASE [ IF NOT EXISTS ] [ catalogName.] dataBaseName [ COMMENT database_comment ] [WITH ( name=value [, name=value]*)]DROP DATABASE [ IF EXISTS ] [ catalogName.] dataBaseName [ (RESTRICT|CASCADE)]ALTER DATABASE [ catalogName.] dataBaseName SET ( name=value [, name=value]*)USE [ catalogName.] dataBaseName</description>
      <version>1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="15027" opendate="2019-12-3 00:00:00" fixdate="2019-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support alter table DDLs in SQL CLI</summary>
      <description>Support syntax as following:ALTER TABLE [[catalogName.] dataBasesName].tableName RENAME TO newTableNameALTER TABLE [[catalogName.] dataBasesName].tableName SET ( name=value [, name=value]*)</description>
      <version>1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="15052" opendate="2019-12-4 00:00:00" fixdate="2019-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test transformation.clear() in sqlClient</summary>
      <description>when executing multiple commands from sql client, the later job graph will include all job graphs which already executed. </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="15062" opendate="2019-12-5 00:00:00" fixdate="2019-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Orc reader should use java.sql.Timestamp to read for respecting time zone</summary>
      <description>Hive orc use java.sql.Timestamp to read and write orc files... default, timestamp will consider time zone to adjust seconds.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcColumnarRowSplitReaderTest.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.vector.OrcTimestampColumnVector.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.vector.AbstractOrcColumnVector.java</file>
    </fixedFiles>
  </bug>
  <bug id="15068" opendate="2019-12-5 00:00:00" fixdate="2019-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable RocksDB&amp;#39;s local LOG by default</summary>
      <description>With Flink's default settings for RocksDB, it will write a log file (not the WAL, but pure logging statements) into the data folder. Besides periodic statistics, it will log compaction attempts, new memtable creations, flushes, etc.A few things to note about this practice: this LOG file is growing over time with no limit the default logging level is INFO the statistics in there may help looking into performance and/or disk space problems (but maybe you should be looking and monitoring metrics instead) this file is not useful for debugging errors since it will be deleted along with the local dir when the TM goes downWith a custom OptionsFactory, the user can change the behaviour like the following: @Override public DBOptions createDBOptions(DBOptions currentOptions) { currentOptions = super.createDBOptions(currentOptions); currentOptions.setKeepLogFileNum(10); currentOptions.setInfoLogLevel(InfoLogLevel.WARN_LEVEL); currentOptions.setStatsDumpPeriodSec(0); currentOptions.setMaxLogFileSize(1024 * 1024); // 1 MB each return currentOptions; }However, the rotating logger does currently not work (it will not delete old log files - see https://github.com/dataArtisans/frocksdb/pull/12). Also, the user should not have to write his own OptionsFactory to get a sensible default.To prevent this file from filling up the disk, I propose to change Flink's default RocksDB settings so that the LOG file is effectively disabled (nothing is written to it by default).</description>
      <version>1.7.2,1.8.2,1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.PredefinedOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="15113" opendate="2019-12-6 00:00:00" fixdate="2019-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fs.azure.account.key not hidden from global configuration</summary>
      <description>For access to Azure's Blob Storage, you need to provide the (secret) key withfs.azure.account.key.&lt;storageaccount&gt;.core.windows.netThis value, however, is not hidden from the global configuration which only specifies configurations with keys containing "password" or "secret" as sensitive.We should add fs.azure.account.key to that list as well.</description>
      <version>1.9.1</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.GlobalConfigurationTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.GlobalConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="15114" opendate="2019-12-6 00:00:00" fixdate="2019-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add execute result info for alter/create/drop database in sql client.</summary>
      <description>Add execute result info for alter/create/drop database in sql-client</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="15135" opendate="2019-12-9 00:00:00" fixdate="2019-12-9 01:00:00" resolution="Done">
    <buginformation>
      <summary>Adding e2e tests for Flink&amp;#39;s Mesos integration</summary>
      <description>Currently, there is no end to end test or IT case for Mesos deployment. We want to add Mesos end-to-end tests which will benefit both Mesos users and contributors.More discussion could be found here.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.yarn.docker.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">tools.travis.splits.split.container.sh</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="15152" opendate="2019-12-9 00:00:00" fixdate="2019-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Job running without periodic checkpoint for stop failed at the beginning</summary>
      <description>I have a streaming job configured with periodically checkpoint, but after one week running, I found there isn't any checkpoint file.Reproduce the problem:1. Job was submitted to YARN:bin/flink run -m yarn-cluster -p 1 -yjm 1024m -ytm 4096m flink-example-1.0-SNAPSHOT.jar2. Then immediately, before all the task switch to RUNNING (about seconds), I(actually a job control script) send a "stop with savepoint" command by flink cli:bin/flink stop -yid application_1575872737452_0019 f75ca6f457828427ed3d413031b92722 -p file:///tmp/some_dirlog in jobmanager.log:2019-12-09 17:56:56,512 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Checkpoint triggering task Source: Socket Stream -&gt; Map (1/1) of job f75ca6f457828427ed3d413031b92722 is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.Then the job task(taskmanager) continues to run normally without checkpoint.The cause of the problem:1. "stop with savepoint" command call the code stopCheckpointScheduler(org/apache/flink/runtime/scheduler/LegacyScheduler.java:612) and then triggerSynchronousSavepoint:// we stop the checkpoint coordinator so that we are guaranteed// to have only the data of the synchronous savepoint committed.// in case of failure, and if the job restarts, the coordinator// will be restarted by the CheckpointCoordinatorDeActivator.checkpointCoordinator.stopCheckpointScheduler();2. but "before all the task switch to RUNNING", triggerSynchronousSavepoint failed at org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java:509LOG.info("Checkpoint triggering task {} of job {} is not in state {} but {} instead. Aborting checkpoint.", tasksToTrigger[i].getTaskNameWithSubtaskIndex(), job, ExecutionState.RUNNING, ee.getState());throw new CheckpointException(CheckpointFailureReason.NOT_ALL_REQUIRED_TASKS_RUNNING);3. finally, "stop with savepoint" failed, with "checkpointCoordinator.stopCheckpointScheduler()" but without the termination of the job. The job is still running without periodically checkpoint.  sample code for reproduce:public class StreamingJob { private static StateBackend makeRocksdbBackend() throws IOException { RocksDBStateBackend rocksdbBackend = new RocksDBStateBackend("file:///tmp/aaa"); rocksdbBackend.enableTtlCompactionFilter(); rocksdbBackend.setPredefinedOptions(PredefinedOptions.SPINNING_DISK_OPTIMIZED); return rocksdbBackend; } public static void main(String[] args) throws Exception { // set up the streaming execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 10 sec env.enableCheckpointing(10_000L, CheckpointingMode.AT_LEAST_ONCE); env.setStateBackend(makeRocksdbBackend()); env.setRestartStrategy(RestartStrategies.noRestart()); CheckpointConfig checkpointConfig = env.getCheckpointConfig(); checkpointConfig.enableExternalizedCheckpoints( CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION); checkpointConfig.setFailOnCheckpointingErrors(true); DataStream&lt;String&gt; text = env.socketTextStream("127.0.0.1", 8912, "\n"); text.map(new MapFunction&lt;String, Tuple2&lt;Long, Long&gt;&gt;() { @Override public Tuple2&lt;Long, Long&gt; map(String s) { String[] s1 = s.split(" "); return Tuple2.of(Long.parseLong(s1[0]), Long.parseLong(s1[1])); } }).keyBy(0).flatMap(new CountWindowAverage()).print(); env.execute("Flink Streaming Java API Skeleton"); } public static class CountWindowAverage extends RichFlatMapFunction&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; { private transient ValueState&lt;Tuple2&lt;Long, Long&gt;&gt; sum; @Override public void flatMap(Tuple2&lt;Long, Long&gt; input, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out) throws Exception { Tuple2&lt;Long, Long&gt; currentSum = sum.value(); currentSum.f0 += 1; currentSum.f1 += input.f1; sum.update(currentSum); out.collect(new Tuple2&lt;&gt;(input.f0, currentSum.f1)); } @Override public void open(Configuration config) { ValueStateDescriptor&lt;Tuple2&lt;Long, Long&gt;&gt; descriptor = new ValueStateDescriptor&lt;&gt;( "average", // the state name TypeInformation.of(new TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() { }), // type information Tuple2.of(0L, 0L)); // default value of the state, if nothing was set sum = getRuntimeContext().getState(descriptor); } }}</description>
      <version>1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointIT.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="15153" opendate="2019-12-9 00:00:00" fixdate="2019-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Service selector needs to contain jobmanager component label</summary>
      <description>The jobmanager label needs to be added to service selector. Otherwise, it may select the wrong backend pods(taskmanager).The internal service is used for taskmanager talking to jobmanager. If it does not have correct backend pods, the taskmanager may fail to register.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.Fabric8ClientTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.ServiceDecorator.java</file>
    </fixedFiles>
  </bug>
  <bug id="15154" opendate="2019-12-9 00:00:00" fixdate="2019-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change Flink binding addresses in local mode</summary>
      <description>Flink (or some of its services) listens on three random TCP portsduring the local&amp;#91;1&amp;#93; execution, e.g., 39951, 41009 and 42849.&amp;#91;1&amp;#93;: https://ci.apache.org/projects/flink/flink-docs-stable/dev/local_execution.html#local-environmentThe sockets listens on `0.0.0.0` and since I need to run somelong-running tests on an Internet-facing machine I was wondering howto make them listen on `localhost` instead or if there is anythingelse I can do to improve the security in this scenario.Here's what I tried (with little luck):```Configuration config = new Configuration();config.setString("taskmanager.host", "127.0.0.1");cconfig.setString("rest.bind-address", "127.0.0.1"); // OKconfig.setString("jobmanager.rpc.address", "127.0.0.1");StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(StreamExecutionEnvironment.getDefaultLocalParallelism(), config);```Only the `rest.bind-address` configuration actually changes thebinding address of one of those ports. Are there other parameters thatI'm not aware of or this is not the right approach in local mode?</description>
      <version>1.9.1</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.util.MetricUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.util.MetricUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="15156" opendate="2019-12-9 00:00:00" fixdate="2019-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Warn user if System.exit() is called in user code</summary>
      <description>It would make debugging Flink errors easier if we would intercept and log calls to System.exit() through the SecurityManager.A user recently had an error where the JobManager was shutting down because of a System.exit() in the user code: https://lists.apache.org/thread.html/b28dabcf3068d489f38399c456c80d48569fcdf74b15f8bb95d532d0%40%3Cuser.flink.apache.org%3EIf I remember correctly, we had such issues before.I put this ticket into the "Runtime / Coordination" component, as it is mostly about improving the usability / debuggability in that area.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.ExitTrappingSecurityManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.ExitTrappingSecurityManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgram.java</file>
      <file type="M">docs..includes.generated.expert.cluster.section.html</file>
      <file type="M">docs..includes.generated.cluster.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="15157" opendate="2019-12-9 00:00:00" fixdate="2019-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make ScalaShell ensureYarnConfig() and fetchConnectionInfo() public</summary>
      <description>This allows users of the Scala Shell, such as Zeppelin to work better with the shell.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15159" opendate="2019-12-9 00:00:00" fixdate="2019-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the mapping of JSON schema string type to Flink SQL STRING type</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
    </fixedFiles>
  </bug>
  <bug id="15181" opendate="2019-12-10 00:00:00" fixdate="2019-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix minor typos and mistakes in table documentation</summary>
      <description>Minor documentation corrections.</description>
      <version>1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.streaming.dynamic.tables.zh.md</file>
      <file type="M">docs.dev.table.streaming.dynamic.tables.md</file>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.common.zh.md</file>
      <file type="M">docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  <bug id="15185" opendate="2019-12-11 00:00:00" fixdate="2019-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive sink can not run in standalone mode</summary>
      <description>Now in hive HadoopFileSystemFactory, we use org.apache.flink.runtime.fs.hdfs.HadoopFileSystem to get FileSystem.But it should not work after we setting default child first class loader. Because in standalone mode, the cluster has no hadoop dependency. So the solution is: Add `flink-hadoop-fs` dependency to hive module, not work, because classes with "org.apache.flink" prefix will always be loaded by parent class loader  User add hadoop dependency to standalone cluster, it breaks out-of-the-box. Shade hadoop FileSystem in hive module, not complex, good.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15214" opendate="2019-12-12 00:00:00" fixdate="2019-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding multiple submission e2e test for Flink&amp;#39;s Mesos integration</summary>
      <description>As discussed, we need a e2e test to verify the user's flow of submitting multiple jobs, in which the second job should reuse the slots of the first job.More detail could be found in ML.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.splits.split.container.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-mesos-cluster.docker-compose.yml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.mesos.docker.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="15266" opendate="2019-12-16 00:00:00" fixdate="2019-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE in blink planner code gen</summary>
      <description>`cast` function in blink planner and old planner are different:in legacy planner:cast('' as int) -&gt; throw NumberFormatExceptioncast(null as int) -&gt; throw NullPointerExceptioncast('abc' as int) -&gt; throw NumberFormatExceptionbut in blink planner:cast('' as int) -&gt; return nullcast(null as int) -&gt; return nullcast('abc' as int) -&gt; return nullA step forward:```create table source { age int, id varchar};select case when age &lt; 20 then cast(id as bigint) else 0 end from source;```queries like above will throw NPE because we will try assign a `null` to a `long` field when the input satisfy `age &lt; 20`.</description>
      <version>1.9.1</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15360" opendate="2019-12-22 00:00:00" fixdate="2019-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn e2e test is broken with building docker image</summary>
      <description>Yarn e2e test is broken with building docker image. This is because this change https://github.com/apache/flink/commit/cce1cef50d993aba5060ea5ac597174525ae895e. Shell function retry_times do not support passing a command as multiple parts. For example, retry_times 5 0 docker build image could not work. cc karmagyz</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="15362" opendate="2019-12-23 00:00:00" fixdate="2019-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Kafka client version to 2.4.1 for universal Kafka connector</summary>
      <description>Kafka 2.4 has been released recently. There are many features are involved in this version. More details: https://blogs.apache.org/kafka/entry/what-s-new-in-apache1IMO, it would be better to bump the Kafka client version to 2.4.0.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.StreamingKafkaITCase.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.java.org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.java</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15402" opendate="2019-12-26 00:00:00" fixdate="2019-2-26 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add disable quote character feature for serializing row to csv format</summary>
      <description>We'd like to send row to kafka topic with csv format and that without any quote character. For example, input data is like Row.of("Test", 12, "2019-12-26 12:12:12")and expect serialized result is Test,12,2019-12-26 12:12:12 rather than Test,12,"2019-12-26 12:12:12" by default.</description>
      <version>1.9.1</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvRowFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-csv.src.test.java.org.apache.flink.formats.csv.CsvRowDeSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.table.descriptors.CsvValidator.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.table.descriptors.Csv.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowSerializationSchema.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowFormatFactory.java</file>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
    </fixedFiles>
  </bug>
  <bug id="15420" opendate="2019-12-27 00:00:00" fixdate="2019-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cast string to timestamp will loose precision</summary>
      <description>cast('2010-10-14 12:22:22.123456' as timestamp(9))Will produce "2010-10-14 12:22:22.123" in blink planner, this should not happen.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlDateTimeUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15438" opendate="2019-12-30 00:00:00" fixdate="2019-3-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Counter metrics are incorrectly reported as total counts to DataDog</summary>
      <description>The Flink semantics of a counter are not matching with the counters in DataDog.In Flink a counter counts the total of increment and decrement calls.In DataDog a counter is a rate over the reporting interval. The Flink implementation of the DataDog reporter seems to send the Flink counter value each time the metrics are reported. Correct would be to send the delta of the counter since the last report.There are some features in DataDog which are easier to use if this could be fixed, e.g. alerts based on counters.</description>
      <version>1.9.1</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DCounter.java</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DatadogHttpReporter.java</file>
      <file type="M">flink-metrics.flink-metrics-datadog.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15443" opendate="2019-12-31 00:00:00" fixdate="2019-1-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use JDBC connector write FLOAT value occur ClassCastException</summary>
      <description>I defined a float type field in mysql table, when I use jdbc connector write float value into db, there are ClassCastException occurs.//代码占位符Caused by: java.lang.ClassCastException: java.lang.Float cannot be cast to java.lang.Double, field index: 6, field value: 0.1.Caused by: java.lang.ClassCastException: java.lang.Float cannot be cast to java.lang.Double, field index: 6, field value: 0.1. Caused by: java.lang.ClassCastException: java.lang.Float cannot be cast to java.lang.Double, field index: 6, field value: 0.1.Caused by: java.lang.ClassCastException: java.lang.Float cannot be cast to java.lang.Double, field index: 6, field value: 0.1.  at org.apache.flink.api.java.io.jdbc.JDBCUtils.setField(JDBCUtils.java:106)  at org.apache.flink.api.java.io.jdbc.JDBCUtils.setRecordToStatement(JDBCUtils.java:63) at org.apache.flink.api.java.io.jdbc.writer.AppendOnlyWriter.addRecord(AppendOnlyWriter.java:56) at org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.writeRecord(JDBCUpsertOutputFormat.java:144) </description>
      <version>1.9.1</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCUpsertTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCTypeUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="15485" opendate="2020-1-6 00:00:00" fixdate="2020-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reopen tests when blocking issue has been resolved</summary>
      <description>Sometimes we  close test and left comment like 'TODO/when FLINK-xx is closed/when FLINK-xx is merged' for various reasons and ready to reopen  it after they are really fixed.Unfortunately we missed some of them. This issue aims to reopen tests that close by FLINK-12088   FLINK-13740  CALCITE-1860  </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.ExpressionReductionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.GroupWindowTableAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.MiscITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CorrelateITCase2.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.ModifiedMonotonicityTest.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15489" opendate="2020-1-6 00:00:00" fixdate="2020-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebUI log refresh not working</summary>
      <description>There is no way to query the latest state of logs of jobmanager/taskmanager.The Web UI show only the first version that was ever displayed.How to reproduce: (not sure if necessary) configure logback as described here: https://ci.apache.org/projects/flink/flink-docs-stable/dev/best_practices.html#use-logback-when-running-flink-on-a-cluster start a cluster show jobmanager logs in the Web UI run example job check again the jobmanager logs, there is no trace of the job. Clicking the refresh button does not help</description>
      <version>1.9.1,1.10.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.task-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job-manager.service.ts</file>
    </fixedFiles>
  </bug>
  <bug id="15495" opendate="2020-1-7 00:00:00" fixdate="2020-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set default planner for SQL Client to Blink planner</summary>
      <description>As discussed in the mailing list &amp;#91;1&amp;#93;, we will change the default planner to Blink planner for SQL CLI. &amp;#91;1&amp;#93;: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Set-default-planner-for-SQL-Client-to-Blink-planner-in-1-10-release-td36379.html</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.conf.sql-client-defaults.yaml</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug id="15497" opendate="2020-1-7 00:00:00" fixdate="2020-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming TopN operator doesn&amp;#39;t reduce outputs when rank number is not required</summary>
      <description>As we described in the doc: https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/table/sql.html#top-nwhen rank number is not required, we can reduce some output, like unnecessary retract messages. Here is an example which can re-produce:val data = List( ("aaa", 97.0, 200.0), ("bbb", 67.0, 200.0), ("bbb", 162.0, 200.0))val ds = failingDataSource(data).toTable(tEnv, 'guid, 'a, 'b)tEnv.registerTable("T", ds)val aggreagtedTable = tEnv.sqlQuery( """ |select guid, | sum(a) as reached_score, | sum(b) as max_score, | sum(a) / sum(b) as score |from T group by guid |""".stripMargin)tEnv.registerTable("T2", aggreagtedTable)val sql = """ |SELECT guid, reached_score, max_score, score |FROM ( | SELECT *, | ROW_NUMBER() OVER (ORDER BY score DESC) as rank_num | FROM T2) |WHERE rank_num &lt;= 5 """.stripMarginval sink = new TestingRetractSinktEnv.sqlQuery(sql).toRetractStream[Row].addSink(sink).setParallelism(1)env.execute()In this case, the output is:(true,aaa,97.0,200.0,0.485)(true,bbb,67.0,200.0,0.335) (false,bbb,67.0,200.0,0.335) (true,bbb,229.0,400.0,0.5725) (false,aaa,97.0,200.0,0.485) (true,aaa,97.0,200.0,0.485)But the last 2 messages are unnecessary. </description>
      <version>1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.RetractableTopNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.UpdatableTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.RetractableTopNFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.RankITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15515" opendate="2020-1-8 00:00:00" fixdate="2020-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document that Hive connector should be used with blink planner</summary>
      <description>HiveCatalog works with both old and blink planner. But read/write Hive tables only works with blink planner.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.scala.shell.hive.zh.md</file>
      <file type="M">docs.dev.table.hive.scala.shell.hive.md</file>
      <file type="M">docs.dev.table.hive.read.write.hive.zh.md</file>
      <file type="M">docs.dev.table.hive.read.write.hive.md</file>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug id="15537" opendate="2020-1-9 00:00:00" fixdate="2020-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Type of keys should be `BinaryRow` when manipulating map state with `BaseRow` as key type.</summary>
      <description>`BaseRow` is serialized and deserialized as `BinaryRow` by default, so when the key type of the map state is `BaseRow`, we should construct map keys with `BinaryRow` as type to get value from map state, otherwise, you would  always get Null...Try it with following SQL:// (b: Int, c: String)SELECT b, listagg(DISTINCT c, '#')FROM MyTableGROUP BY b </description>
      <version>1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWsWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.DistinctAggCodeGen.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15575" opendate="2020-1-13 00:00:00" fixdate="2020-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Azure Filesystem Shades Wrong Package "httpcomponents"</summary>
      <description>Instead of shading "org.apache.httpcomponents" (this package does not exist) the azure filesystem should shade "org.apache.http". This e.g. causes problems when the azure filesystem and elasticsearch6 connector are both on the classpath.</description>
      <version>1.9.1,1.10.0</version>
      <fixedVersion>1.9.3,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15577" opendate="2020-1-13 00:00:00" fixdate="2020-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WindowAggregate RelNodes missing Window specs in digest</summary>
      <description>The RelNode's digest (AbstractRelNode.getDigest()), along with its RowType, is used by the Calcite HepPlanner to avoid adding duplicate Vertices to the graph. If an equivalent vertex is already present in the graph, then that vertex is used in place of the newly generated one: https://github.com/apache/calcite/blob/branch-1.21/core/src/main/java/org/apache/calcite/plan/hep/HepPlanner.java#L828This means that the digest needs to contain all the information necessary to identify a vertex and distinguish it from similar - but not equivalent - vertices.In the case of `LogicalWindowAggregation` and `FlinkLogicalWindowAggregation`, the window specs are currently not in the digest, meaning that two aggregations with the same signatures and expressions but different windows are considered equivalent by the planner, which is not correct and will lead to an invalid Physical Plan.For instance, the following query would give an invalid plan:WITH window_1h AS ( SELECT HOP_ROWTIME(`timestamp`, INTERVAL '1' HOUR, INTERVAL '1' HOUR) as `timestamp` FROM my_table GROUP BY HOP(`timestamp`, INTERVAL '1' HOUR, INTERVAL '1' HOUR)),window_2h AS ( SELECT HOP_ROWTIME(`timestamp`, INTERVAL '1' HOUR, INTERVAL '2' HOUR) as `timestamp` FROM my_table GROUP BY HOP(`timestamp`, INTERVAL '1' HOUR, INTERVAL '2' HOUR))(SELECT * FROM window_1h)UNION ALL(SELECT * FROM window_2h)The invalid plan generated by the planner is the following (Please note the windows in the two DataStreamGroupWindowAggregates nodes being the same when they should be different):DataStreamUnion(all=[true], union all=[timestamp]): rowcount = 200.0, cumulative cost = {800.0 rows, 802.0 cpu, 0.0 io}, id = 176 DataStreamCalc(select=[w$rowtime AS timestamp]): rowcount = 100.0, cumulative cost = {300.0 rows, 301.0 cpu, 0.0 io}, id = 173 DataStreamGroupWindowAggregate(window=[SlidingGroupWindow('w$, 'timestamp, 7200000.millis, 3600000.millis)], select=[start('w$) AS w$start, end('w$) AS w$end, rowtime('w$) AS w$rowtime, proctime('w$) AS w$proctime]): rowcount = 100.0, cumulative cost = {200.0 rows, 201.0 cpu, 0.0 io}, id = 172 DataStreamScan(id=[1], fields=[timestamp]): rowcount = 100.0, cumulative cost = {100.0 rows, 101.0 cpu, 0.0 io}, id = 171 DataStreamCalc(select=[w$rowtime AS timestamp]): rowcount = 100.0, cumulative cost = {300.0 rows, 301.0 cpu, 0.0 io}, id = 175 DataStreamGroupWindowAggregate(window=[SlidingGroupWindow('w$, 'timestamp, 7200000.millis, 3600000.millis)], select=[start('w$) AS w$start, end('w$) AS w$end, rowtime('w$) AS w$rowtime, proctime('w$) AS w$proctime]): rowcount = 100.0, cumulative cost = {200.0 rows, 201.0 cpu, 0.0 io}, id = 174 DataStreamScan(id=[1], fields=[timestamp]): rowcount = 100.0, cumulative cost = {100.0 rows, 101.0 cpu, 0.0 io}, id = 171</description>
      <version>1.9.1</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.GroupWindowTableAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalWindowTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalWindowTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalWindowAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15578" opendate="2020-1-13 00:00:00" fixdate="2020-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement exactly-once JDBC sink</summary>
      <description>As per discussion in the dev mailing list, there are two options: Write-ahead log Two-phase commit (XA)the latter being preferable. </description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.testutils.migration.MigrationVersion.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcTestFixture.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.DbMetadata.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.JdbcSink.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
      <file type="M">docs.dev.connectors.jdbc.md</file>
    </fixedFiles>
  </bug>
  <bug id="15584" opendate="2020-1-14 00:00:00" fixdate="2020-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Give nested data type of ROWs in ValidationException</summary>
      <description>In INSERT INTO baz_sinkSELECT  a, ROW(b, c)FROM foo_sourceSchema mismatch mistakes will not get proper detail level, yielding the following:Caused by: org.apache.flink.table.api.ValidationException: Field types of query result and registered TableSink &amp;#91;baz_sink&amp;#93; do not match. Query result schema: &amp;#91;a: Integer, EXPR$2: Row&amp;#93; TableSink schema: &amp;#91;a: Integer, payload: Row&amp;#93;Leaving the user with an opaque 'Row' type to debug. </description>
      <version>1.9.1,1.10.0,1.11.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.validation.InsertIntoValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sinks.TableSinkUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug id="15694" opendate="2020-1-20 00:00:00" fixdate="2020-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove HDFS Section of Configuration Page</summary>
      <description>The section "HDFS" is outdated (and flagged as such).</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.config.zh.md</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  <bug id="15728" opendate="2020-1-21 00:00:00" fixdate="2020-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBCUpsertOutputFormat does not set bind parameter keyFields in updateStatement</summary>
      <description>When using JDBCUpsertOutputFormat custom dialect e.g. H2/Oracle which uses UpsertWriterUsingInsertUpdateStatement, code fails with below error.Caused by: org.h2.jdbc.JdbcSQLDataException: Parameter "#6" is not set [90012-200] at org.h2.message.DbException.getJdbcSQLException(DbException.java:590) at org.h2.message.DbException.getJdbcSQLException(DbException.java:429) at org.h2.message.DbException.get(DbException.java:205) at org.h2.message.DbException.get(DbException.java:181) at org.h2.expression.Parameter.checkSet(Parameter.java:83) at org.h2.jdbc.JdbcPreparedStatement.addBatch(JdbcPreparedStatement.java:1275) at org.apache.flink.api.java.io.jdbc.writer.UpsertWriter$UpsertWriterUsingInsertUpdateStatement.processOneRowInBatch(UpsertWriter.java:233) at org.apache.flink.api.java.io.jdbc.writer.UpsertWriter.executeBatch(UpsertWriter.java:111) This is due to UpsertWriterUsingInsertUpdateStatement#processOneRowInBatch does not set all bind paramters in case of Update.This bug does get surfaced while using Derby DB. In JDBCUpsertOutputFormatTest if we replace Derby with H2 we can reproduce the bug.The fix is trivial. Happy to raise PR.//for update case replace belowsetRecordToStatement(updateStatement, fieldTypes, row); //withsetRecordToStatement(updateStatement, fieldTypes + pkTypes, row + pkRow);//NOTE: as prepared updateStatement contains additional where clause we need pass additional bind values and its sql Types</description>
      <version>1.9.1</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcLookupTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunction.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcLookupFunction.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.table.JdbcDynamicOutputFormatBuilder.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.TableJdbcUpsertOutputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.executor.InsertOrUpdateJdbcExecutor.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.executor.BufferReduceStatementExecutor.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.converter.AbstractJdbcRowConverter.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.dialect.JdbcDialect.java</file>
    </fixedFiles>
  </bug>
  <bug id="15736" opendate="2020-1-23 00:00:00" fixdate="2020-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Java 17 (LTS)</summary>
      <description>Long-term issue for preparing Flink for Java 17.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.memory..index.md</file>
    </fixedFiles>
  </bug>
  <bug id="15833" opendate="2020-1-31 00:00:00" fixdate="2020-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix &amp;#39;Kerberized YARN on Docker&amp;#39; e2e test on Azure Pipelines</summary>
      <description>AZP support has been introduced in FLINK-13978.The YARN on Docker tests are disabled in the PR introducing Azure support.The issue is likely occurring due to different resources available on the Azure machines, compared to Travis.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.yarn.kerberos.docker.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.yarn.docker.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug id="15834" opendate="2020-1-31 00:00:00" fixdate="2020-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set up nightly cron jobs on Azure Pipelines build</summary>
      <description>FLINK-13978 introduced support for Azure Pipelines, however limited to building pull requests and pushes.The scope of this issue is to add the cron jobs available in travis also to the Azure setup.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.prepare.precommit.sh</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.yarn.kerberos.docker.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.config.yarn-site.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.yarn.docker.sh</file>
      <file type="M">tools.azure.controller.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.queryable.state.base.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.kubernetes.sh</file>
    </fixedFiles>
  </bug>
  <bug id="16033" opendate="2020-2-13 00:00:00" fixdate="2020-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce a Java Expression DSL</summary>
      <description>Introduce the basic expressions. The new Java expression DSL should be feature equivalent to string based expression parser. It does not support calls with new inference stack yet.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.converter.CustomizedConvertRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.RexProgramExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.ExpressionBridge.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.ExpressionBridge.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.CallExpressionResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionLookup.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.KeywordParseTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.descriptors.RowtimeTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sources.TableSourceUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.RexProgramExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionParserImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.testTableSources.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.RexNodeExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.MapTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.KeywordParseTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.DecimalTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.sources.TableSourceUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RexNodeExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalWindowAggregateRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.DeclarativeAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.AggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionParserImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.SumWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.SumAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.Sum0AggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.SingleValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.RowNumberAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.RankLikeAggFunctionBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.RankAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.MinAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.MaxAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.LeadLagAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.IncrSumWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.IncrSumAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.DeclarativeAggregateFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.CountAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.Count1AggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.AvgAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.SqlAggFunctionVisitor.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.ExpressionBuilder.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.DeclarativeExpressionResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.GroupWindow.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.SessionWithGap.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.SessionWithGapOnTime.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.SessionWithGapOnTimeWithAlias.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.SlideWithSize.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.SlideWithSizeAndSlide.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.SlideWithSizeAndSlideOnTime.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.SlideWithSizeAndSlideOnTimeWithAlias.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TumbleWithSize.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TumbleWithSizeOnTime.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TumbleWithSizeOnTimeWithAlias.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.ApiExpressionUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.LookupCallExpression.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.ExpressionResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.LookupCallResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.OverWindowResolverRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.ResolverRules.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.UnresolvedCallExpression.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.typeutils.FieldInfoUtils.java</file>
      <file type="M">flink-table.flink-table-api-scala.src.main.scala.org.apache.flink.table.api.expressionDsl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.ExpandColumnFunctionsRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.ReferenceResolverRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.utils.OperationExpressionsUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.batch.table.JavaTableEnvironmentITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.ColumnFunctionsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.ColumnFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.stringexpr.CalcStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.stringexpr.OverWindowStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.stringexpr.TableAggregateStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.OverWindowPartitionedOrdered.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.LocalReferenceExpression.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.QualifyBuiltInFunctionsRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.TableReferenceExpression.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.UnresolvedReferenceExpression.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.utils.ApiExpressionUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.utils.factories.AggregateOperationFactory.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.utils.factories.AliasOperationUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.utils.factories.CalculatedTableFactory.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.utils.factories.ColumnOperationUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.utils.OperationTreeBuilder.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.sources.tsextractors.ExistingField.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.sources.tsextractors.StreamRecordTimestamp.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.operations.QueryOperationTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="16034" opendate="2020-2-13 00:00:00" fixdate="2020-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the string based expressions to the java dsl in documentation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.streaming.time.attributes.zh.md</file>
      <file type="M">docs.dev.table.streaming.time.attributes.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.zh.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.md</file>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
      <file type="M">docs.dev.table.common.zh.md</file>
      <file type="M">docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  <bug id="16035" opendate="2020-2-13 00:00:00" fixdate="2020-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the Java&amp;#39;s TableEnvironment/Table to accept the Java Expression DSL</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.sql.InsertIntoITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.SortITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.match.PatternTranslatorTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.TableEnvironmentValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.TableAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.stringexpr.CorrelateStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.UnionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.stringexpr.CorrelateStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.stream.sql.JavaSqlITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.batch.table.JavaTableEnvironmentITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.batch.sql.JavaSqlITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.batch.sql.GroupingSetsITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.java.internal.BatchTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamTableEnvUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTableEnvUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.PruneAggregateCallITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateRemoveITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.stringexpr.CorrelateStringExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.match.PatternTranslatorTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.api.TableUtilsStreamingITCase.java</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.java.org.apache.flink.table.operations.ScalaDataStreamQueryOperation.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.operations.JavaDataStreamQueryOperation.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.java.StreamTableEnvironment.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.java.BatchTableEnvironment.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperatorTestBase.java</file>
      <file type="M">flink-ml-parent.flink-ml-lib.src.main.java.org.apache.flink.ml.common.utils.DataStreamConversionUtil.java</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.StreamSQLExample.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.StreamSQLExample.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCUpsertTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCLookupFunctionITCase.java</file>
      <file type="M">flink-connectors.flink-hbase.src.test.java.org.apache.flink.addons.hbase.HBaseConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.java</file>
    </fixedFiles>
  </bug>
  <bug id="16036" opendate="2020-2-13 00:00:00" fixdate="2020-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecate String based Expression DSL in TableEnvironments</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.BatchTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.java.StreamTableEnvironment.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.java.BatchTableEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug id="16037" opendate="2020-2-13 00:00:00" fixdate="2020-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>maven-dependency-plugin not fully compatible with Java 11</summary>
      <description>The maven-dependency-plugin 3.1.1 is not fully compatible with Java 11; dependency analysis and listing of dependencies is currently failing.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-tpch-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-tpcds-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1643" opendate="2015-3-4 00:00:00" fixdate="2015-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Detect tumbling policies where trigger and eviction match</summary>
      <description>The windowing api should automatically detect matching trigger and eviction policies so it can apply optimizations for tumbling policies.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.windowing.StreamDiscretizerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.WindowUtils.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingTimePreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingCountPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.StreamWindow.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.TimeTriggerPolicy.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.CountTriggerPolicy.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.CountEvictionPolicy.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.helper.Time.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.operator.windowing.WindowBufferInvokable.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.WindowedDataStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="16524" opendate="2020-3-10 00:00:00" fixdate="2020-3-10 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Optimize the execution of Python UDF to use generator to eliminate unnecessary function calls</summary>
      <description>Optimize the result of FlattenRowCoder and ArrowCoder to generator to eliminate unnecessary function calls.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.tests.coders.test.common.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  <bug id="1682" opendate="2015-3-10 00:00:00" fixdate="2015-5-10 01:00:00" resolution="Done">
    <buginformation>
      <summary>Port Record-API based optimizer tests to new Java API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.WorksetIterationsRecordApiCompilerTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.WorksetIterationCornerCasesTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.util.OperatorResolver.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.util.IdentityReduce.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.util.IdentityMap.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.util.DummyOutputFormat.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.util.DummyNonPreservingMatchStub.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.util.DummyMatchStub.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.util.DummyInputFormat.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.util.DummyCrossStub.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.util.DummyCoGroupStub.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.UnionReplacementTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.UnionPropertyPropagationTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.UnionBetweenDynamicAndStaticPathTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.testfunctions.IdentityGroupReducer.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.SortPartialReuseTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.SemanticPropertiesAPIToPlanTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.ReplicatingDataSourceTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.ReduceAllTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.PropertyDataSourceTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.PipelineBreakerTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.PartitionPushdownTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.PartitioningReusageTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.ParallelismChangeTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.NestedIterationsTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.java.PartitionOperatorTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.IterationsCompilerTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.HardPlansCompilationTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.GroupOrderTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.DistinctCompilationTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.DisjointDataFlowsTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.custompartition.JoinCustomPartitioningTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.custompartition.GroupingTupleTranslationTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.custompartition.GroupingPojoTranslationTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.custompartition.GroupingKeySelectorTranslationTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.custompartition.CustomPartitioningGlobalOptimizationTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.custompartition.CoGroupCustomPartitioningTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.CoGroupSolutionSetFirstTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.CachedMatchStrategyCompilerTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.BroadcastVariablePipelinebreakerTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.BranchingPlansCompilerTest.java</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.AdditionalOperatorsTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="16820" opendate="2020-3-27 00:00:00" fixdate="2020-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support reading timestamp, data, and time in JDBCTableSource</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JdbcTypeUtil.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.dialect.JDBCDialects.java</file>
    </fixedFiles>
  </bug>
  <bug id="16823" opendate="2020-3-27 00:00:00" fixdate="2020-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The functioin TIMESTAMPDIFF doesn&amp;#39;t perform expected result</summary>
      <description>For example,In mysql bellow sql get result 6, but in flink the output is 5SELECT timestampdiff (MONTH, TIMESTAMP '2019-09-01 00:00:00',TIMESTAMP '2020-03-01 00:00:00' )   </description>
      <version>1.9.1,1.10.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlFunctionUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.TimestampDiffCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.BuiltInMethods.scala</file>
    </fixedFiles>
  </bug>
  <bug id="2023" opendate="2015-5-15 00:00:00" fixdate="2015-1-15 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>TypeExtractor does not work for (some) Scala Classes</summary>
      <description>vanaepi discovered some problems while working on the Scala Gelly API where, for example, a Scala MapFunction can not be correctly analyzed by the type extractor. For example, generic types will not be correctly detected.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.types.TypeInformationGenTest.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.codegen.TypeInformationGen.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.codegen.TypeDescriptors.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.codegen.TypeAnalyzer.scala</file>
    </fixedFiles>
  </bug>
  <bug id="2083" opendate="2015-5-22 00:00:00" fixdate="2015-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure high quality docs for FlinkML in 0.9</summary>
      <description>As defined in our vision for FlinkML, providing high-quality documentation is a primary goal for us.This issue concerns the docs that will be included in 0.9, and will track improvements and additions for the release.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..layouts.base.html</file>
      <file type="M">docs..includes.latex.commands.html</file>
      <file type="M">docs.libs.ml.optimization.md</file>
      <file type="M">docs.libs.ml.multiple.linear.regression.md</file>
      <file type="M">docs.libs.ml.distance.metrics.md</file>
      <file type="M">docs.libs.ml.als.md</file>
    </fixedFiles>
  </bug>
  <bug id="20830" opendate="2021-1-3 00:00:00" fixdate="2021-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a type of HEADLESS_CLUSTER_IP for rest service type</summary>
      <description>Now we can choose ClusterIP or NodePort or LoadBalancer as rest service type. But in our internal kubernetes cluster, there is no kube-proxy, and ClusterIP mode rely on kube-proxy. So I think can we support another type of HEADLESS_CLUSTER_IP to directly talk to jobmanager pod? cc fly_in_gis</description>
      <version>None</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesClientTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.decorators.ExternalServiceDecoratorTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesClusterDescriptor.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.InternalServiceDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.ExternalServiceDecorator.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.kubernetes.config.configuration.html</file>
    </fixedFiles>
  </bug>
</bugrepository>
