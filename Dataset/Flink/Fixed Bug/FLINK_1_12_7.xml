<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="FLINK">
  <bug id="21384" opendate="2021-2-16 00:00:00" fixdate="2021-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Automatically copy maven dependencies to clipboard on click</summary>
      <description>Flink has a number of optional dependencies users may need to copy into their pom files to use, such as connectors and formats. The docs should automatically copy the maven dependency to the users' clipboard when clicked.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.static.js.flink.js</file>
      <file type="M">docs.README.md</file>
      <file type="M">docs.layouts.shortcodes.sql.download.table.html</file>
      <file type="M">docs.layouts.shortcodes.artifact.html</file>
    </fixedFiles>
  </bug>
  <bug id="25362" opendate="2021-12-17 00:00:00" fixdate="2021-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect dependencies in Table Confluent/Avro docs</summary>
      <description>"Confluent Avro Format" is missing an explanation to also add the dependency to flink-avro have the confluent repository defined"Avro Format" should not show the maven dependency to flink-sql-avro but instead flink-avro</description>
      <version>1.12.7,1.13.5,1.14.2</version>
      <fixedVersion>1.13.6,1.14.4,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.layouts.shortcodes.sql.download.table.html</file>
      <file type="M">docs.data.sql.connectors.yml</file>
      <file type="M">docs.content.docs.connectors.table.formats.avro-confluent.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.formats.avro-confluent.md</file>
    </fixedFiles>
  </bug>
  <bug id="25366" opendate="2021-12-17 00:00:00" fixdate="2021-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enforce BINARY/VARBINARY precision when outputing to a Sink</summary>
      <description>When a column is declared with BINARY/VARBINARY with a specific length, i.e. BINARY(10) a sink that would require following this precision strictly (like a relational DB) would throw errors if the records received exceed this limit. </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.sink.ConstraintEnforcer.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSinkITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.ExecutionConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug id="25375" opendate="2021-12-19 00:00:00" fixdate="2021-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Log4j to 2.17.0</summary>
      <description>Log4j 2.17.0 has been released &amp;#91;1&amp;#93; This release contains the changes noted below: Address CVE-2021-45105. Require components that use JNDI to be enabled individually via system properties. Remove LDAP and LDAPS as supported protocols from JNDI.&amp;#91;1&amp;#93; https://github.com/apache/logging-log4j2/blob/6b1581901ba7a107cdc4a2208ecec03655722b44/RELEASE-NOTES.md#apache-log4j-2170-release-notes</description>
      <version>1.11.6,1.12.7,1.13.5,1.14.2</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.NOTICE-binary.PREAMBLE.txt</file>
      <file type="M">pom.xml</file>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.project-configuration.md</file>
    </fixedFiles>
  </bug>
  <bug id="25445" opendate="2021-12-27 00:00:00" fixdate="2021-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TaskExecutor always creates local file for task even when local state store is not used</summary>
      <description>`TaskExecutor` will create file, check `localRecoveryEnabled` and load local state store for each task submission in method `localStateStoreForSubtask`. For batch jobs, the `localRecoveryEnabled` is always false, and there's no need to create these local files for task in `TaskExecutor`</description>
      <version>1.12.7,1.13.5,1.14.2</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.StreamOperatorSnapshotRestoreTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.LocalStateForwardingTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.benchmark.StateBackendBenchmarkUtils.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.snapshot.RocksFullSnapshotStrategy.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestLocalRecoveryConfig.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskStateManagerImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskLocalStateStoreImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskLocalStateStoreImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.LocalRecoveryConfig.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapSnapshotStrategy.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointTaskStateManager.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.SavepointLocalRecoveryProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="25960" opendate="2022-2-4 00:00:00" fixdate="2022-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Distribute the data read buffers more fairly among result partitions for sort-shuffle</summary>
      <description>Currently, the data read buffers for sort-shuffle are allocated in a random way and some result partitions may occupy too many buffers which leads to the starvation of other result partitions. This ticket aims to improve the scenario by not reading data for those result partitions which already occupy more than the average number of read buffers per result partition.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.BatchShuffleReadBufferPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="25961" opendate="2022-2-4 00:00:00" fixdate="2022-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove transport client from Elasticsearch 6/7 connectors (tests only)</summary>
      <description>The Elasticsearch 6/7 connectors still use deprecated transport client for integration tests. This is not really necessary and brings a lot of dependencies, the HighLevelRestClient is fully sufficient and could be used as drop-in replacement.The change affects only tests.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.testutils.SourceSinkDataTestKit.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="26961" opendate="2022-3-31 00:00:00" fixdate="2022-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update multiple Jackson dependencies to v2.13.2 and v2.13.2.1</summary>
      <description>There is a High CVE-2020-36518, https://github.com/advisories/GHSA-57j2-w4cx-62h2which was fixed with 2.13.2.1</description>
      <version>None</version>
      <fixedVersion>1.14.5,1.15.0,elasticsearch-3.0.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-gs-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug id="27059" opendate="2022-4-5 00:00:00" fixdate="2022-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-compress</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-compress.src.test.java.org.apache.flink.formats.compress.CompressWriterFactoryTest.java</file>
      <file type="M">flink-formats.flink-compress.src.test.java.org.apache.flink.formats.compress.CompressionFactoryITCase.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
