<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  
  <bug fixdate="2016-10-1 01:00:00" id="4546" opendate="2016-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove STREAM keyword in Stream SQL</summary>
      <description>It is about to unify Batch SQL and Stream SQL grammar, esp. removing STREAM keyword in Stream SQL. detailed discuss mailing list: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Some-thoughts-about-unify-Stream-SQL-and-Batch-SQL-grammer-td13060.html</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.stream.sql.SqlITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.schema.TransStreamTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.schema.StreamableTableSourceTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.schema.DataStreamTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.datastream.StreamTableSourceScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.datastream.RemoveDeltaRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.datastream.StreamTableSourceScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.StreamTableEnvironment.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-9-1 01:00:00" id="4549" opendate="2016-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test and document implicitly supported SQL functions</summary>
      <description>Calcite supports many SQL functions by translating them into RexNode s. However, SQL functions like NULLIF, OVERLAPS are neither tested nor document although supported.These functions should be tested and added to the documentation. We could adopt parts from the Calcite documentation.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.expression.TemporalTypesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-1 01:00:00" id="4554" opendate="2016-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for array types</summary>
      <description>Support creating arrays:ARRAY[1, 2, 3]Access array values:myArray[3]And operations like:UNNEST, UNNEST WITH ORDINALITY, CARDINALITY</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.TypeCheckUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.ProjectionTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.FlinkTypeFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.comparison.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.calls.ScalarOperators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-1-11 01:00:00" id="4611" opendate="2016-9-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make "AUTO" credential provider as default for Kinesis Connector</summary>
      <description>Right now, the Kinesis Consumer / Producer by default directly expects the access key id and secret access key to be given in the config properties.This isn't a good practice for accessing AWS services, and usually Kinesis users would most likely be running their Flink application in AWS instances that have embedded credentials that can be access via the default credential provider chain. Therefore, it makes sense to change the default AWS_CREDENTIALS_PROVIDER to AUTO instead of BASIC.To avoid breaking user code, we only use directly supplied AWS credentials if both access key and secret key is given through AWS_ACCESS_KEY and AWS_SECRET_KEY. Otherwise, the default credential provider chain is used.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-9-12 01:00:00" id="4612" opendate="2016-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Close FileWriter using try with resources</summary>
      <description>FileWriter is not closed properly in many places in the project modules</description>
      <version>1.1.2</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.aggregators.AggregatorsITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.clients.examples.LocalExecutorITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.WriteFormatAsText.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.WriteFormatAsCsv.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.DataSourceTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.disk.FileChannelStreamsTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.PrimitiveInputFormatTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CsvInputFormatTest.java</file>
      <file type="M">flink-examples.flink-examples-batch.src.main.java.org.apache.flink.examples.java.relational.util.WebLogDataGenerator.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.testutils.TestFileUtils.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-10-20 01:00:00" id="4639" opendate="2016-9-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Calcite features more pluggable</summary>
      <description>Some users might want to extend the feature set of the Table API by adding or replacing Calcite optimizer rules, modifying the parser etc. It would be good to have means to hook into the Table API and change Calcite behavior. We should implement something like a CalciteConfigBuilder.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.TableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.TableEnvironmentITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableConfig.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.BatchTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-9-27 01:00:00" id="4702" opendate="2016-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka consumer must commit offsets asynchronously</summary>
      <description>The offset commit calls to Kafka may occasionally take very long.In that case, the notifyCheckpointComplete() method blocks for long and the KafkaConsumer cannot make progress and cannot perform checkpoints.Kafka 0.9+ have methods to commit asynchronously.We should use those and make sure no more than one commit is concurrently in progress, to that commit requests do not pile up.</description>
      <version>1.1.2</version>
      <fixedVersion>1.1.3,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-9-29 01:00:00" id="4710" opendate="2016-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove transitive Guice dependency from Hadoop</summary>
      <description>This transitive dependency is not relevant for the parts of the Hadoop code invoked by Flink (Yarn client, Hdfs). Removing it clears dependency conflicts for users.</description>
      <version>1.1.2</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-3 01:00:00" id="4727" opendate="2016-10-3 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Kafka 0.9 Consumer should also checkpoint auto retrieved offsets even when no data is read</summary>
      <description>This is basically the 0.9 version counterpart for FLINK-3440.When the 0.9 consumer fetches initial offsets from Kafka on startup, but does not have any data to read, it should also checkpoint &amp; commit these initial offsets.</description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09ITCase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010ITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-4 01:00:00" id="4739" opendate="2016-10-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding packaging details for the Elasticsearch connector</summary>
      <description>When an uber-jar containing an Elasticsearch sink is executed, an IllegalArgumentException may occur, which is caused by conflicting files of Elasticsearch and it's dependencies in META-INF/services.As agreed in http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/NoClassDefFoundError-with-ElasticsearchSink-on-Yarn-tt8822.html#none, the documentation should point out how to build a sound uber-jar containing an Elasticsearch sink.</description>
      <version>1.1.2</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.elasticsearch2.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-10-7 01:00:00" id="4768" opendate="2016-10-7 00:00:00" resolution="Done">
    <buginformation>
      <summary>Migrate High Availability configuration options</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnApplicationMasterRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNHighAvailabilityITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.CliFrontendYarnAddressConfigurationTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAProcessFailureBatchRecoveryITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.ChaosMonkeyITCase.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.SecureTestEnvironment.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.RollingSinkSecuredITCase.java</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingUtils.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperTestEnvironment.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ZooKeeperUtilTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.ZooKeeperTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderRetrievalTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerHARecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.HighAvailabilityModeTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.FlinkZooKeeperQuorumPeer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.SecurityContext.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.HighAvailabilityMode.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.FileSystemBlobStore.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitorITCase.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.DefaultCLI.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-11 01:00:00" id="4794" opendate="2016-10-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>partition_by_hash() crashes if no parameter is provided</summary>
      <description>partition_by_hash() crashes if no parameter is provided.Looks like a line of code was missed, check distinct()def distinct(self, *fields): f = None if len(fields) == 0: f = lambda x: (x,) fields = (0,)</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.DataSet.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-11 01:00:00" id="4795" opendate="2016-10-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CsvStringify crashes in case of tuple in tuple, t.e. ("a", True, (1,5))</summary>
      <description>CsvStringify crashes in case of tuple in tuple, t.e. ("a", True, (1,5))Looks like, mistyping in CsvStringify._map()def _map(self, value): if isinstance(value, (tuple, list)): return "(" + b", ".join([self.map(x) for x in value]) + ")" else: return str(value) self._map() should be calledBut this will affect write_csv() and read_csv().write_csv() will work automaticallyand read_csv() should be implemented to be able to read Tuple type.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.main.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.DataSet.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-10-14 01:00:00" id="4827" opendate="2016-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The scala example of SQL on Streaming Tables with wrong variable name in flink document</summary>
      <description>val env = StreamExecutionEnvironment.getExecutionEnvironmentval tEnv = TableEnvironment.getTableEnvironment(env)// read a DataStream from an external sourceval ds: DataStream&amp;#91;(Long, String, Integer)&amp;#93; = env.addSource(...)// register the DataStream under the name "Orders"tableEnv.registerDataStream("Orders", ds, 'user, 'product, 'amount)// run a SQL query on the Table and retrieve the result as a new Tableval result = tableEnv.sql( "SELECT product, amount FROM Orders WHERE product LIKE '%Rubber%'")There is no variable named tableEnv had defined ,Only tEnv defined here</description>
      <version>1.1.0,1.1.2</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-14 01:00:00" id="4832" opendate="2016-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Count/Sum 0 elements</summary>
      <description>Currently, the Table API is unable to count or sum up 0 elements. We should improve DataSet aggregations for this. Maybe by union the original DataSet with a dummy record or by using a MapPartition function. Coming up with a good design for this is also part of this issue.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.DataSetAggregateRule.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-15 01:00:00" id="4833" opendate="2016-10-15 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Unstable test OperatorStatsAccumulatorTest.testAccumulatorHeavyHitterCountMinSketch</summary>
      <description>Some instances:view-source:https://s3.amazonaws.com/archive.travis-ci.org/jobs/167801187/log.txtview-source:https://s3.amazonaws.com/archive.travis-ci.org/jobs/167801191/log.txtview-source:https://s3.amazonaws.com/archive.travis-ci.org/jobs/167801193/log.txtview-source:https://s3.amazonaws.com/archive.travis-ci.org/jobs/167801195/log.txt</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-operator-stats.src.main.java.org.apache.flink.contrib.operatorstatistics.heavyhitters.HeavyHitterMergeException.java</file>
      <file type="M">flink-contrib.flink-operator-stats.src.main.java.org.apache.flink.contrib.operatorstatistics.heavyhitters.CountMinHeavyHitter.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-10-17 01:00:00" id="4843" opendate="2016-10-17 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce Test for FsCheckpointStateOutputStream::getPos</summary>
      <description>Introduce a test for FsCheckpointStateOutputStream::getPos, which is currently not included in the tests.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStateOutputStreamTest.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-10-21 01:00:00" id="4875" opendate="2016-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>operator name not correctly inferred</summary>
      <description/>
      <version>1.1.2</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-21 01:00:00" id="4876" opendate="2016-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow web interface to be bound to a specific ip/interface/inetHost</summary>
      <description>Currently the web interface automatically binds to all interfaces on 0.0.0.0. IMHO there are some use cases to only bind to a specific ipadress, (e.g. access through an authenticated proxy, not binding on the management or backup interface)</description>
      <version>1.1.2,1.1.3,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorConfig.java</file>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-11-23 01:00:00" id="5143" opendate="2016-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add EXISTS to list of supported operators</summary>
      <description>EXISTS is supported in certain cases. We should add it so that e.g. TPC-H query 4 runs properly.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-23 01:00:00" id="5144" opendate="2016-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error while applying rule AggregateJoinTransposeRule</summary>
      <description>AggregateJoinTransposeRule seems to cause errors. We have to investigate if this is a Flink or Calcite error. Here a simplified example:select sum(l_extendedprice)from lineitem, partwhere p_partkey = l_partkey and l_quantity &lt; ( select avg(l_quantity) from lineitem where l_partkey = p_partkey )Exception:Exception in thread "main" java.lang.AssertionError: Internal error: Error occurred while applying rule AggregateJoinTransposeRule at org.apache.calcite.util.Util.newInternal(Util.java:792) at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:148) at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:225) at org.apache.calcite.rel.rules.AggregateJoinTransposeRule.onMatch(AggregateJoinTransposeRule.java:342) at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:213) at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:819) at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:334) at org.apache.flink.api.table.BatchTableEnvironment.optimize(BatchTableEnvironment.scala:251) at org.apache.flink.api.table.BatchTableEnvironment.translate(BatchTableEnvironment.scala:286) at org.apache.flink.api.scala.table.BatchTableEnvironment.toDataSet(BatchTableEnvironment.scala:139) at org.apache.flink.api.scala.table.package$.table2RowDataSet(package.scala:77) at org.apache.flink.api.scala.sql.tpch.TPCHQueries$.runQ17(TPCHQueries.scala:826) at org.apache.flink.api.scala.sql.tpch.TPCHQueries$.main(TPCHQueries.scala:57) at org.apache.flink.api.scala.sql.tpch.TPCHQueries.main(TPCHQueries.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)Caused by: java.lang.AssertionError: Type mismatch:rowtype of new rel:RecordType(BIGINT l_partkey, BIGINT p_partkey) NOT NULLrowtype of set:RecordType(BIGINT p_partkey) NOT NULL at org.apache.calcite.util.Litmus$1.fail(Litmus.java:31) at org.apache.calcite.plan.RelOptUtil.equal(RelOptUtil.java:1838) at org.apache.calcite.plan.volcano.RelSubset.add(RelSubset.java:273) at org.apache.calcite.plan.volcano.RelSet.add(RelSet.java:148) at org.apache.calcite.plan.volcano.VolcanoPlanner.addRelToSet(VolcanoPlanner.java:1820) at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1766) at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:1032) at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:1052) at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:1942) at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:136) ... 17 more</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
    </fixedFiles>
  </bug>
  
</bugrepository>