<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  
  <bug fixdate="2023-3-25 01:00:00" id="31222" opendate="2023-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove usage of deprecated ConverterUtils.toApplicationId</summary>
      <description>When reading the code, I found that we use ConverterUtils.toApplicationId to convert applicationId, this method is deprecated, we should use ApplicationId.fromString</description>
      <version>1.17.1</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterClientFactory.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-4-27 01:00:00" id="31962" opendate="2023-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>libssl not found when running CI</summary>
      <description>Installed Maven 3.2.5 to /home/vsts/maven_cache/apache-maven-3.2.5Installing required softwareReading package lists...Building dependency tree...Reading state information...bc is already the newest version (1.07.1-2build1).bc set to manually installed.libapr1 is already the newest version (1.6.5-1ubuntu1).libapr1 set to manually installed.0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.--2023-04-27 11:42:53-- http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.11_amd64.debResolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 185.125.190.36, 185.125.190.39, ...Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.HTTP request sent, awaiting response... 404 Not Found2023-04-27 11:42:53 ERROR 404: Not Found.</description>
      <version>1.16.2,1.18.0,1.17.1</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-12 01:00:00" id="3224" opendate="2016-1-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The Streaming API does not call setInputType if a format implements InputTypeConfigurable</summary>
      <description>Per parent JIRA.</description>
      <version>None</version>
      <fixedVersion>0.10.2,1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.FileSinkFunction.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-12 01:00:00" id="3225" opendate="2016-1-12 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Optimize logical Table API plans in Calcite</summary>
      <description>This task implements the optimization of logical Table API plans with Apache Calcite. The input of the optimization process is a logical query plan consisting of Calcite RelNodes. FLINK-3223 translates Table API queries into this representation.The result of this issue is an optimized logical plan.Calcite's rule-based optimizer applies query rewriting and optimization rules. For Batch SQL, we can use (a subset of) Calcite’s default optimization rules. For this issue we have to add the Calcite optimizer to the translation process select an appropriate set of batch optimization rules from Calcite’s default rules. We can reuse the rules selected by Timo’s first SQL implementation.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.UnionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.StringExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.SelectITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.GroupedAggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.ExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.CastingITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.AsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.UnionITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.StringExpressionsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.SelectITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.PojoGroupingITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.JoinITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.GroupedAggregationsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.FilterITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.ExpressionsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.CastingITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.AsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.table.test.AggregationsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.operators.DataSetTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.api.table.sql.calcite.node.DataSetUnion.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.api.table.sql.calcite.node.DataSetSource.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.api.table.sql.calcite.node.DataSetSort.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.api.table.sql.calcite.node.DataSetReduceGroup.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.api.table.sql.calcite.node.DataSetReduce.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.api.table.sql.calcite.node.DataSetMap.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.api.table.sql.calcite.node.DataSetJoin.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.api.table.sql.calcite.node.DataSetFlatMap.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.api.table.sql.calcite.node.DataSetExchange.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.api.table.sql.calcite.DataSetRelNode.java</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.api.table.package-info.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.JavaBatchTranslator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-7-7 01:00:00" id="32277" opendate="2023-6-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce operator fusion codegen basic framework</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.multipleinput.input.InputSelectionHandlerTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.input.InputSpec.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.input.InputSelectionHandler.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.multipleinput.BatchMultipleInputStreamOperator.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.TestingBatchExecNode.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNode.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-7-7 01:00:00" id="32279" opendate="2023-6-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shuffle HashJoin support spill to disk when enable operator fusion codegen</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.hashtable.LongHybridHashTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.OperatorFusionCodegenITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.fusion.spec.HashJoinFusionCodegenSpec.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.fusion.spec.CalcFusionCodegenSpec.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-6-9 01:00:00" id="32304" opendate="2023-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce rpc-akka jar size</summary>
      <description>We bundle unnecessary dependencies in the rpc-akka jar; we can easily shave of 15mb of dependencies.</description>
      <version>None</version>
      <fixedVersion>1.18.0,1.17.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-rpc.flink-rpc-akka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-7-15 01:00:00" id="32349" opendate="2023-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support atomic for CREATE TABLE AS SELECT(CTAS) statement</summary>
      <description>For detailed information, see FLIP-305https://cwiki.apache.org/confluence/display/FLINK/FLIP-305%3A+Support+atomic+for+CREATE+TABLE+AS+SELECT%28CTAS%29+statement</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.delegation.DefaultExecutor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.ExecutorMock.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.CreateTableASOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.delegation.Executor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.TableConfigOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.executor.python.ChainingOptimizingExecutor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.execution.JobStatusHook.java</file>
      <file type="M">docs.layouts.shortcodes.generated.table.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-6-15 01:00:00" id="32351" opendate="2023-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce base interfaces for call procedure</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.Catalog.java</file>
      <file type="M">flink-python.pyflink.table.catalog.py</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2023-6-16 01:00:00" id="32369" opendate="2023-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setup cron build</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.compile.sh</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-6-25 01:00:00" id="32426" opendate="2023-6-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix adaptive local hash agg can&amp;#39;t work when auxGrouping exist</summary>
      <description>For the following case, the field `a` is primary key,  we select from `AuxGroupingTable` and group by a, b. Since a is primary key, it also guarantee the unique, so planner will extract b as auxGrouping field.registerCollection( "AuxGroupingTable", data2, type2, "a, b, c, d, e", nullablesOfData2, FlinkStatistic.builder().uniqueKeys(Set(Set("a").asJava).asJava).build())checkResult( "SELECT a, b, COUNT(c) FROM AuxGroupingTable GROUP BY a, b", Seq( row(1, 1, 1), row(2, 3, 2), row(3, 4, 3), row(4, 10, 4), row(5, 11, 5) ))  Due to the generated code doesn't get auxGrouping fields from input RowData and then setting it to aggBuffer, the aggBuffer RowData loses some fields, and it will throw an index Exception when get the field from it. As following:Caused by: java.lang.AssertionError: index (1) should &lt; 1    at org.apache.flink.table.data.binary.BinaryRowData.assertIndexIsValid(BinaryRowData.java:127)    at org.apache.flink.table.data.binary.BinaryRowData.isNullAt(BinaryRowData.java:156)    at org.apache.flink.table.data.utils.JoinedRowData.isNullAt(JoinedRowData.java:113)    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.toBinaryRow(RowDataSerializer.java:201)    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.serialize(RowDataSerializer.java:103)    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.serialize(RowDataSerializer.java:48)    at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.serialize(StreamElementSerializer.java:165)    at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.serialize(StreamElementSerializer.java:43)    at org.apache.flink.runtime.plugable.SerializationDelegate.write(SerializationDelegate.java:54)    at org.apache.flink.runtime.io.network.api.writer.RecordWriter.serializeRecord(RecordWriter.java:141)    at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:107)    at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:55)    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:134)    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collectAndCheckIfChained(RecordWriterOutput.java:114)    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:95)    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:48)    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:59)    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:31)    at LocalHashAggregateWithKeys$39.processElement_split2(Unknown Source)    at LocalHashAggregateWithKeys$39.processElement(Unknown Source)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:108)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:77)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)    at BatchExecCalc$10.processElement(Unknown Source)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:108)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:77)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)    at SourceConversion$6.processElement(Unknown Source)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:108)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:77)    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)    at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)    at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)    at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)    at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)</description>
      <version>1.18.0,1.17.1</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.HashAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ProjectionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-7-25 01:00:00" id="32428" opendate="2023-6-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce base interfaces for CatalogStore</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.FactoryUtilTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FactoryUtil.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-7-28 01:00:00" id="32457" opendate="2023-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update current documentation of JSON_OBJECTAGG/JSON_ARRAYAGG to clarify the limitation</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0,1.17.2</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.WrapJsonAggFunctionArgumentsRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.logical.WrapJsonAggFunctionArgumentsRuleTest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-7-30 01:00:00" id="32501" opendate="2023-6-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong execution plan of a proctime window aggregation generated due to incorrect cost evaluation</summary>
      <description>Currently when uses window aggregation referring a windowing tvf with a filter condition, may encounter wrong plan which may hang forever in runtime(the window aggregate operator never output)for such a case:insert into sink select window_start, window_end, b, COALESCE(sum(case when a = 11 then 1 end), 0) c from TABLE( TUMBLE(TABLE source, DESCRIPTOR(proctime), INTERVAL '10' SECONDS) ) where a in (1, 5, 7, 9, 11) GROUP BY window_start, window_end, bgenerate wrong plan which didn't combine the proctime WindowTableFunction into WindowAggregate (so when translate to execution plan the WindowAggregate will wrongly recognize the window as an event-time window, then the WindowAggregateOperator will not receive watermark nor setup timers to fire any windows in runtime)Sink(table=[default_catalog.default_database.sink], fields=[ws, we, b, c])+- Calc(select=[CAST(window_start AS TIMESTAMP(6)) AS ws, CAST(window_end AS TIMESTAMP(6)) AS we, b, CAST(COALESCE($f1, 0) AS BIGINT) AS c]) +- WindowAggregate(groupBy=[b], window=[TUMBLE(win_start=[window_start], win_end=[window_end], size=[10 s])], select=[b, SUM($f3) AS $f1, start('w$) AS window_start, end('w$) AS window_end]) +- Exchange(distribution=[hash[b]]) +- Calc(select=[window_start, window_end, b, CASE((a = 11), 1, null:INTEGER) AS $f3], where=[SEARCH(a, Sarg[1, 5, 7, 9, 11])]) +- WindowTableFunction(window=[TUMBLE(time_col=[proctime], size=[10 s])]) +- Calc(select=[a, b, PROCTIME() AS proctime]) +- TableSourceScan(table=[[default_catalog, default_database, source, project=[a, b], metadata=[]]], fields=[a, b])expected plan:Sink(table=[default_catalog.default_database.sink], fields=[ws, we, b, c])+- Calc(select=[CAST(window_start AS TIMESTAMP(6)) AS ws, CAST(window_end AS TIMESTAMP(6)) AS we, b, CAST(COALESCE($f1, 0) AS BIGINT) AS c]) +- WindowAggregate(groupBy=[b], window=[TUMBLE(time_col=[proctime], size=[10 s])], select=[b, SUM($f3) AS $f1, start('w$) AS window_start, end('w$) AS window_end]) +- Exchange(distribution=[hash[b]]) +- Calc(select=[b, CASE((a = 11), 1, null:INTEGER) AS $f3, PROCTIME() AS proctime], where=[SEARCH(a, Sarg[1, 5, 7, 9, 11])]) +- TableSourceScan(table=[[default_catalog, default_database, source, project=[a, b], metadata=[]]], fields=[a, b])</description>
      <version>1.16.2,1.17.1</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdSelectivityTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdSelectivity.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-7-3 01:00:00" id="32516" opendate="2023-7-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to parse [CREATE OR ] REPLACE TABLE AS statement</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-7-3 01:00:00" id="32517" opendate="2023-7-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to execute [CREATE OR] REPLACE TABLE AS statement</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.utils.OperationConverterUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlCreateTableConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConverters.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ModifyOperationVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlReplaceTableAs.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-7-3 01:00:00" id="32519" opendate="2023-7-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add doc for [CREATE OR] REPLACE TABLE AS statement</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.create.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-7-6 01:00:00" id="32547" opendate="2023-7-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add missing doc for Timestamp support in ProtoBuf format</summary>
      <description>In FLINK-30093, we have support Timestamp type, and added the doc for it, but missed to updating the English version.</description>
      <version>1.17.1</version>
      <fixedVersion>1.18.0,1.17.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.formats.protobuf.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-7-11 01:00:00" id="32578" opendate="2023-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cascaded group by window time columns on a proctime window aggregate may result hang for ever</summary>
      <description>Currently when group by window time columns on a proctime window aggregate result will get a wrong plan which may result hang for ever in runtime.For such a query:insert into s1SELECT window_start, window_end, sum(cnt), count(*)FROM ( SELECT a, b, window_start, window_end, count(*) as cnt, sum(d) as sum_d, max(d) as max_d FROM TABLE(TUMBLE(TABLE src1, DESCRIPTOR(proctime), INTERVAL '5' MINUTE)) GROUP BY a, window_start, window_end, b)GROUP BY a, window_start, window_endthe inner proctime window works fine, but the outer one doesn't work due to a wrong plan which will translate to a unexpected event mode window operator:Sink(table=[default_catalog.default_database.s1], fields=[ws, we, b, c])+- Calc(select=[CAST(window_start AS TIMESTAMP(6)) AS ws, CAST(window_end AS TIMESTAMP(6)) AS we, CAST(EXPR$2 AS BIGINT) AS b, CAST(EXPR$3 AS BIGINT) AS c]) +- WindowAggregate(groupBy=[a], window=[TUMBLE(win_start=[window_start], win_end=[window_end], size=[5 min])], select=[a, SUM(cnt) AS EXPR$2, COUNT(*) AS EXPR$3, start('w$) AS window_start, end('w$) AS window_end]) +- Exchange(distribution=[hash[a]]) +- Calc(select=[a, window_start, window_end, cnt]) +- WindowAggregate(groupBy=[a, b], window=[TUMBLE(time_col=[proctime], size=[5 min])], select=[a, b, COUNT(*) AS cnt, start('w$) AS window_start, end('w$) AS window_end]) +- Exchange(distribution=[hash[a, b]]) +- Calc(select=[a, b, d, PROCTIME() AS proctime]) +- TableSourceScan(table=[[default_catalog, default_database, src1, project=[a, b, d], metadata=[]]], fields=[a, b, d])</description>
      <version>1.17.1</version>
      <fixedVersion>1.18.0,1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.WindowAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalGroupAggregateRule.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-8-12 01:00:00" id="32581" opendate="2023-7-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for atomic CTAS</summary>
      <description>add docs for atomic CTAS</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.docs.dev.table.sourcesSinks.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sourcesSinks.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2023-7-27 01:00:00" id="32703" opendate="2023-7-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hotfix] flink-python POM has a typo for protobuf-java in shading config</summary>
      <description>Fix typo. `inculde` -&gt; `include`                                  &lt;includes combine.children="append"&gt;                                    &lt;include&gt;net.razorvine:*&lt;/include&gt;                                    &lt;include&gt;net.sf.py4j:*&lt;/include&gt;                                    &lt;include&gt;org.apache.beam:*&lt;/include&gt;                                    &lt;include&gt;com.fasterxml.jackson.core:*&lt;/include&gt;                                    &lt;include&gt;joda-time:*&lt;/include&gt;                                    &lt;inculde&gt;com.google.protobuf:*&lt;/inculde&gt;                                    &lt;include&gt;org.apache.arrow:*&lt;/include&gt;                                    &lt;include&gt;io.netty:*&lt;/include&gt;                                    &lt;include&gt;com.google.flatbuffers:*&lt;/include&gt;                                    &lt;include&gt;com.alibaba:pemja&lt;/include&gt;                                &lt;/includes&gt;</description>
      <version>1.16.2,1.18.0,1.17.1</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-9-4 01:00:00" id="32755" opendate="2023-8-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add quick start guide for Flink OLAP</summary>
      <description>I propose to add a new QUICKSTART.md guide that provides instructions for beginner to build a production ready Flink OLAP Service by using flink-jdbc-driver, flink-sql-gateway and flink session cluster.</description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.overview.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-8-5 01:00:00" id="32759" opendate="2023-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the removed config in the doc</summary>
      <description>The cluster.declarative-resource-management.enabled was removed at FLINK-21095(https://github.com/apache/flink/pull/15838/files), so it doesn't work now.However, the flink doc still have it.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.elastic.scaling.md</file>
      <file type="M">docs.content.zh.docs.deployment.elastic.scaling.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-8-5 01:00:00" id="32760" opendate="2023-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Version Conflict in flink-sql-connector-hive for shaded.parquet prefix packages</summary>
      <description>SummaryIn https://issues.apache.org/jira/browse/FLINK-23074 it seems like shading parquet dependency from hive-exec is done. But I think this is not enough and causing errors like below when I try to read parquet file using sql-gateway which requires both flink-parquet and flink-sql-connector-hive dependencies. CauseParquet dependency not only includes org.apache.parquet but also shaded.parquet prefix dependencies. (ref)So we need to shade both.- flink-parquet depends on Parquet 1.12.3 with shaded Thrift 0.16.0 (prefix: shaded.parquet)- flink-sql-connector-hive depends on hive-exec 3.1.3 with Parquet 1.10.0 and shaded Thrift 0.9.3 (prefix: shaded.parquet)- Code compiled against Thrift 0.16.0 attempts to run against 0.9.3, causing the error.Proposed solutionAdding new shading rule to flink-sql-connector-hive project.I have confirmed that adding this rule could resolve the above error.&lt;relocation&gt; &lt;pattern&gt;shaded.parquet&lt;/pattern&gt; &lt;shadedPattern&gt;shaded.parquet.flink.hive.shaded&lt;/shadedPattern&gt;&lt;/relocation&gt; I would be happy to implement it if the proposal is accepted. Thanks </description>
      <version>1.17.1</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.3.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.9.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2023-9-14 01:00:00" id="32863" opendate="2023-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Flink UI&amp;#39;s time precision from second level to millisecond level</summary>
      <description>This an UI improvement for OLAP jobs.OLAP queries are generally small queries which will finish at the seconds or milliseconds, but currently the time precision displayed is second level and not enough for OLAP queries. Millisecond part of time is very important for users and developers, to see accurate time, for performance measurement and optimization. The displayed time includes job duration, task duration, task start time, end time and so on.It would be nice to improve this for better OLAP user experience.</description>
      <version>1.17.1</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.log-list.task-manager-log-list.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.timeline.job-timeline.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.taskmanagers.job-overview-drawer-taskmanagers.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.list.job-overview-list.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.detail.job-overview-drawer-detail.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.job-detail.status.job-status.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.subtask.job-checkpoints-subtask.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.detail.job-checkpoints-detail.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.log-list.job-manager-log-list.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.components.job-list.job-list.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.components.humanize-duration.pipe.ts</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2023-9-7 01:00:00" id="33053" opendate="2023-9-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Watcher leak in Zookeeper HA mode</summary>
      <description>We observe a watcher leak in our OLAP stress test when enabling Zookeeper HA mode. TM's watches on the leader of JobMaster has not been stopped after job finished.Here is how we re-produce this issue: Start a session cluster and enable Zookeeper HA mode. Continuously and concurrently submit short queries, e.g. WordCount to the cluster. echo -n wchp | nc {zk host} {zk port} to get current watches.We can see a lot of watches on /flink/{cluster_name}/leader/{job_id}/connection_info.</description>
      <version>1.17.0,1.18.0,1.17.1</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-9-13 01:00:00" id="33083" opendate="2023-9-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SupportsReadingMetadata is not applied when loading a CompiledPlan</summary>
      <description>If a few conditions are met, we can not apply ReadingMetadata interface: source overwrites: @Override public boolean supportsMetadataProjection() { return false; } source does not implement SupportsProjectionPushDown table has metadata columns e.g.CREATE TABLE src ( physical_name STRING, physical_sum INT, timestamp TIMESTAMP_LTZ(3) NOT NULL METADATA VIRTUAL) we query the table SELECT * FROM srcIt fails with:Caused by: java.lang.IllegalArgumentException: Row arity: 1, but serializer arity: 2 at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:124)The reason is SupportsReadingMetadataSpec is created only in the PushProjectIntoTableSourceScanRule, but the rule is not applied when 1 &amp; 2</description>
      <version>1.16.2,1.17.1</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.TableSourceJsonPlanITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSourceUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSinkUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.NonDeterministicDagTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.WatermarkAssignerChangelogNormalizeTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.PushLocalAggIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.optimize.ScanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.jsonplan.testReuseSourceWithoutProjectionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.connector.file.table.FileSystemTableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.spec.DynamicTableSourceSpec.java</file>
    </fixedFiles>
  </bug>
</bugrepository>