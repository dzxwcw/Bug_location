<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2021-2-16 01:00:00" id="21384" opendate="2021-2-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Automatically copy maven dependencies to clipboard on click</summary>
      <description>Flink has a number of optional dependencies users may need to copy into their pom files to use, such as connectors and formats. The docs should automatically copy the maven dependency to the users' clipboard when clicked.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.static.js.flink.js</file>
      <file type="M">docs.README.md</file>
      <file type="M">docs.layouts.shortcodes.sql.download.table.html</file>
      <file type="M">docs.layouts.shortcodes.artifact.html</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-12-29 01:00:00" id="25472" opendate="2021-12-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update to Log4j 2.17.1</summary>
      <description>We should update from Log4j 2.17.0 to 2.17.1 to address CVE-2021-44832: Apache Log4j2 vulnerable to RCE via JDBC Appender when attacker controls configuration.</description>
      <version>1.12.8,1.13.6,1.14.3,1.15.0</version>
      <fixedVersion>1.12.8,1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.NOTICE-binary.PREAMBLE.txt</file>
      <file type="M">pom.xml</file>
      <file type="M">docs.content.docs.dev.datastream.project-configuration.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.project-configuration.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-6-19 01:00:00" id="25699" opendate="2022-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use HashMap for MAP value constructors</summary>
      <description>Currently, the usage of maps is inconsistent. It is not ensured that duplicate keys get eliminated. For CAST and output conversion this is solved. However, we should have a similar implementation in MAP value constructor like in CAST.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.MapTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.functions.CastFunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-24 01:00:00" id="25785" opendate="2022-1-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update com.h2database:h2 to 2.0.210</summary>
      <description>Two security vulnerabilities in H2 Console (CVE-2022-23221 and possible DNS rebinding attack) are fixed in 2.0.120. Flink is currently on 2.0.206 since https://issues.apache.org/jira/browse/FLINK-25576Note: Flink is using this dependency only for testing, so it's not directly impacted by the CVE. We just want to be good citizens and update our dependencies</description>
      <version>1.13.5,1.14.3,1.15.0</version>
      <fixedVersion>1.14.4,1.15.0,1.13.7</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-1-25 01:00:00" id="25805" opendate="2022-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use compact DataType serialization for default classes instead of internal ones</summary>
      <description>It is more likely that default conversion classes spam the plan than internal classes. In most cases when internal classes are used, they usually also use logical type instead of data type. So it should be safer to skip default conversion classes. This also reduces the plan size for serializing `ResolvedSchema`.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowJoinJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testDistinctSplitEnabled.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregateWithSumCountDistinctAndRetraction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.jsonplan.testIncrementalAggregate.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.jsonplan.testDistinctAggCalls[isMiniBatchEnabled=true].out</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DataTypeJsonSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalTypeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DataTypeJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DataTypeJsonDeserializer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-27 01:00:00" id="25856" opendate="2022-1-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix use of UserDefinedType in from_elements</summary>
      <description>If we define a new UserDefinedType, and use it in `from_elements`, it will failed.class VectorUDT(UserDefinedType): @classmethod def sql_type(cls): return DataTypes.ROW( [ DataTypes.FIELD("type", DataTypes.TINYINT()), DataTypes.FIELD("size", DataTypes.INT()), DataTypes.FIELD("indices", DataTypes.ARRAY(DataTypes.INT())), DataTypes.FIELD("values", DataTypes.ARRAY(DataTypes.DOUBLE())), ] ) @classmethod def module(cls): return "pyflink.ml.core.linalg" def serialize(self, obj): if isinstance(obj, SparseVector): indices = [int(i) for i in obj._indices] values = [float(v) for v in obj._values] return 0, obj.size(), indices, values elif isinstance(obj, DenseVector): values = [float(v) for v in obj._values] return 1, None, None, values else: raise TypeError("Cannot serialize %r of type %r".format(obj, type(obj)))self.t_env.from_elements([ (Vectors.dense([1, 2, 3, 4]), 0., 1.), (Vectors.dense([2, 2, 3, 4]), 0., 2.), (Vectors.dense([3, 2, 3, 4]), 0., 3.), (Vectors.dense([4, 2, 3, 4]), 0., 4.), (Vectors.dense([5, 2, 3, 4]), 0., 5.), (Vectors.dense([11, 2, 3, 4]), 1., 1.), (Vectors.dense([12, 2, 3, 4]), 1., 2.), (Vectors.dense([13, 2, 3, 4]), 1., 3.), (Vectors.dense([14, 2, 3, 4]), 1., 4.), (Vectors.dense([15, 2, 3, 4]), 1., 5.), ], DataTypes.ROW([ DataTypes.FIELD("features", VectorUDT()), DataTypes.FIELD("label", DataTypes.DOUBLE()), DataTypes.FIELD("weight", DataTypes.DOUBLE())]))</description>
      <version>1.14.3,1.15.0</version>
      <fixedVersion>1.14.4,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.utils.python.PythonTableUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.api.common.python.PythonBridgeUtils.java</file>
      <file type="M">flink-python.pyflink.table.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-30 01:00:00" id="25879" opendate="2022-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Track used search terms in Matomo</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.4,1.15.0,1.13.7</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.layouts.partials.docs.inject.menu-after.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-30 01:00:00" id="25883" opendate="2022-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The value of DEFAULT_BUNDLE_PROCESSOR_CACHE_SHUTDOWN_THRESHOLD_S is too large</summary>
      <description>InÂ this line, the value of DEFAULT_BUNDLE_PROCESSOR_CACHE_SHUTDOWN_THRESHOLD_S is set to 3153600000. This is more than the default value of threading.TIMEOUT_MAX on Windows Python, which is 4294967. Due to this, "OverflowError: timeout value is too large" error is produced.Full traceback: File "G:\PycharmProjects\PyFlink\venv_from_scratch\lib\site-packages\apache_beam\runners\worker\data_plane.py", line 218, in run while not self._finished.wait(next_call - time.time()): File "C:\Python38\lib\threading.py", line 558, in wait signaled = self._cond.wait(timeout) File "C:\Python38\lib\threading.py", line 306, in wait gotit = waiter.acquire(True, timeout)OverflowError: timeout value is too large</description>
      <version>None</version>
      <fixedVersion>1.12.8,1.14.4,1.15.0,1.13.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.sdk.worker.main.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-31 01:00:00" id="25897" opendate="2022-1-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update project configuration gradle doc to 7.x version</summary>
      <description>Update the gradle build script and its doc page to 7.x</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.configuration.overview.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-4-2 01:00:00" id="25926" opendate="2022-2-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update org.postgresql:postgresql to 42.3.3</summary>
      <description>Security vulnerability CVE-2022-21724 is fixed in 42.2.25. Flink is currently on 42.2.10.Note: Flink uses this dependency in a Provided scope only.</description>
      <version>1.13.5,1.14.3,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.internal.converter.PostgresRowConverter.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.dialect.psql.PostgresTypeMapper.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2022-2-8 01:00:00" id="26017" opendate="2022-2-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add debug log message when marking a job result as dirty</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-3-13 01:00:00" id="26099" opendate="2022-2-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table connector proctime attributes has syntax error</summary>
      <description>The example for proctime attributes has syntax error (missing comma after 3rd column) table proctime:Â CREATE TABLE MyTable ( MyField1 INT, MyField2 STRING, MyField3 BOOLEAN MyField4 AS PROCTIME() -- declares a proctime attribute) WITH (...)Â Â Â </description>
      <version>1.14.3</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.overview.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-3-15 01:00:00" id="26164" opendate="2022-2-15 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Document watermark alignment</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.event-time.generating.watermarks.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.event-time.generating.watermarks.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-17 01:00:00" id="26223" opendate="2022-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Making ZK-related logs available in tests</summary>
      <description>Recently, we had a few incidents where it appears that ZooKeeper wasn't behaving as expected. It might help to have to the ZooKeeper logs available in these cases.We have multiple options: Introduce an extension to change the ZK log level for specific tests Lower the ZK log level again and make the logs being written to the standard log files Lower the ZK log level again and move the ZK logs into a dedicated file to avoid spoiling the Flink logs</description>
      <version>1.13.6,1.14.3,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0,1.13.7</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.log4j.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-10-9 01:00:00" id="2642" opendate="2015-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala Table API crashes when executing word count example</summary>
      <description>I tried to run the examples provided in the documentation of Flink's Table API. Unfortunately, the Scala word count example provided in the documentation doesn't work and does not give a meaningful exception.(Other examples work fine)Here my code:package org.apache.flink.examples.scalaimport org.apache.flink.api.scala._import org.apache.flink.api.scala.table._object WordCount { def main(args: Array[String]): Unit = { // set up execution environment val env = ExecutionEnvironment.getExecutionEnvironment case class WC(word: String, count: Int) val input = env.fromElements(WC("hello", 1), WC("hello", 1), WC("ciao", 1)) val expr = input.toTable val result = expr.groupBy('word).select('word, 'count.sum as 'count).toDataSet[WC] result.print() }}Here the thrown exception:Exception in thread "main" org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1.applyOrElse(JobManager.scala:414) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.LeaderSessionMessageFilter$$anonfun$receive$1.applyOrElse(LeaderSessionMessageFilter.scala:36) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:33) at org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:28) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118) at org.apache.flink.runtime.LogMessages$$anon$1.applyOrElse(LogMessages.scala:28) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:104) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.Exception: The user defined 'open(Configuration)' method in class org.apache.flink.api.table.runtime.ExpressionSelectFunction caused an exception: null at org.apache.flink.runtime.operators.RegularPactTask.openUserCode(RegularPactTask.java:1368) at org.apache.flink.runtime.operators.chaining.ChainedMapDriver.openTask(ChainedMapDriver.java:47) at org.apache.flink.runtime.operators.RegularPactTask.openChainedTasks(RegularPactTask.java:1408) at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:142) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:581) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.NullPointerException at org.apache.flink.api.table.codegen.IndentStringContext$$anonfun$j$2.apply(Indenter.scala:30) at org.apache.flink.api.table.codegen.IndentStringContext$$anonfun$j$2.apply(Indenter.scala:23) at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771) at org.apache.flink.api.table.codegen.IndentStringContext.j(Indenter.scala:23) at org.apache.flink.api.table.codegen.GenerateSelect.generateInternal(GenerateSelect.scala:55) at org.apache.flink.api.table.codegen.GenerateSelect.generateInternal(GenerateSelect.scala:32) at org.apache.flink.api.table.codegen.ExpressionCodeGenerator.generate(ExpressionCodeGenerator.scala:66) at org.apache.flink.api.table.runtime.ExpressionSelectFunction.open(ExpressionSelectFunction.scala:46) at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:33) at org.apache.flink.runtime.operators.RegularPactTask.openUserCode(RegularPactTask.java:1366) ... 5 more</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.plan.PlanTranslator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-1 01:00:00" id="26429" opendate="2022-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump karma from 6.3.11 to 6.3.14</summary>
      <description>We should bump Karma from 6.3.11 to 6.3.14. Karma prior to version 6.3.14 contains a cross-site scripting vulnerability.This doesn't directly affect Flink.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package.json</file>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-9 01:00:00" id="2643" opendate="2015-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change Travis Build Profile to Exclude Hadoop 2.0.0-alpha, Include 2.7.0</summary>
      <description>In discussion on the mailing list we reached consensus to change the Hadoop versions that we build Flink with on Travis.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.deploy.to.maven.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-include-yarn.pom.xml</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2.pom.xml</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-1 01:00:00" id="26430" opendate="2022-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump follow-redirects from 1.14.7 to 1.14.8</summary>
      <description>We should bump follow-redirects from 1.14.7 to 1.14.8. Version prior to 1.14.8 could expose sensitive information to an unauthorized actor.This doesn't directly affect Flink.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-3 01:00:00" id="26467" opendate="2022-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compile RowDataToStringConverter lazily</summary>
      <description>Currently, we prepare for `print()` whenever `sqlQuery` is called. However, we could postpone the compilation until it is really needed.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.casting.RowDataToStringConverterImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-6 01:00:00" id="26501" opendate="2022-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Quickstarts Scala nightly end-to-end test failed on azure due to checkponts failed and logs contains exceptions</summary>
      <description>2022-03-05T02:35:36.4040037Z Mar 05 02:35:36 2022-03-05 02:35:34,334 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Triggering checkpoint 1 (type=CHECKPOINT) @ 1646447734295 for job b236087395260dc34648b84c2b86d6e8.2022-03-05T02:35:36.4041701Z Mar 05 02:35:36 2022-03-05 02:35:34,387 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Decline checkpoint 1 by task e8a324cae6bf452d32db6797bbbafad0 of job b236087395260dc34648b84c2b86d6e8 at 127.0.0.1:45911-0a50f5 @ localhost (dataPort=44047).2022-03-05T02:35:36.4043279Z Mar 05 02:35:36 org.apache.flink.util.SerializedThrowable: Task name with subtask : Source: Sequence Source (Deprecated) -&gt; Map -&gt; Sink: Unnamed (1/1)#0 Failure reason: Checkpoint was declined (task is closing)2022-03-05T02:35:36.4044531Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1389) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4045729Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1382) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4047172Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.triggerCheckpointBarrier(Task.java:1348) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4049092Z Mar 05 02:35:36 at org.apache.flink.runtime.taskexecutor.TaskExecutor.triggerCheckpoint(TaskExecutor.java:956) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4050158Z Mar 05 02:35:36 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_322]2022-03-05T02:35:36.4050929Z Mar 05 02:35:36 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_322]2022-03-05T02:35:36.4051776Z Mar 05 02:35:36 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_322]2022-03-05T02:35:36.4052559Z Mar 05 02:35:36 at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_322]2022-03-05T02:35:36.4053373Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[?:?]2022-03-05T02:35:36.4054849Z Mar 05 02:35:36 at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]2022-03-05T02:35:36.4055685Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[?:?]2022-03-05T02:35:36.4056461Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[?:?]2022-03-05T02:35:36.4057219Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[?:?]2022-03-05T02:35:36.4057899Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]2022-03-05T02:35:36.4059666Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]2022-03-05T02:35:36.4061005Z Mar 05 02:35:36 at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4062324Z Mar 05 02:35:36 at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4063941Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]2022-03-05T02:35:36.4065009Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4066205Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4067514Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4068255Z Mar 05 02:35:36 at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]2022-03-05T02:35:36.4069019Z Mar 05 02:35:36 at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]2022-03-05T02:35:36.4069638Z Mar 05 02:35:36 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]2022-03-05T02:35:36.4070271Z Mar 05 02:35:36 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) ~[?:?]2022-03-05T02:35:36.4070862Z Mar 05 02:35:36 at akka.actor.ActorCell.invoke(ActorCell.scala:548) ~[?:?]2022-03-05T02:35:36.4071453Z Mar 05 02:35:36 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]2022-03-05T02:35:36.4072430Z Mar 05 02:35:36 at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]2022-03-05T02:35:36.4073023Z Mar 05 02:35:36 at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]2022-03-05T02:35:36.4073687Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_322]2022-03-05T02:35:36.4074596Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_322]2022-03-05T02:35:36.4075712Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_322]2022-03-05T02:35:36.4076437Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) ~[?:1.8.0_322]2022-03-05T02:35:36.4077754Z Mar 05 02:35:36 2022-03-05 02:35:34,410 WARN org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger checkpoint 1 for job b236087395260dc34648b84c2b86d6e8. (0 consecutive failed attempts so far)2022-03-05T02:35:36.4078865Z Mar 05 02:35:36 org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint was declined (task is closing)2022-03-05T02:35:36.4080161Z Mar 05 02:35:36 at org.apache.flink.runtime.messages.checkpoint.SerializedCheckpointException.unwrap(SerializedCheckpointException.java:51) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4081619Z Mar 05 02:35:36 at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage(CheckpointCoordinator.java:988) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4083063Z Mar 05 02:35:36 at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$declineCheckpoint$2(ExecutionGraphHandler.java:103) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4085407Z Mar 05 02:35:36 at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4086635Z Mar 05 02:35:36 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_322]2022-03-05T02:35:36.4087419Z Mar 05 02:35:36 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_322]2022-03-05T02:35:36.4088438Z Mar 05 02:35:36 at java.lang.Thread.run(Thread.java:750) [?:1.8.0_322]2022-03-05T02:35:36.4089614Z Mar 05 02:35:36 Caused by: org.apache.flink.util.SerializedThrowable: Task name with subtask : Source: Sequence Source (Deprecated) -&gt; Map -&gt; Sink: Unnamed (1/1)#0 Failure reason: Checkpoint was declined (task is closing)2022-03-05T02:35:36.4090937Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1389) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4092177Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1382) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4093430Z Mar 05 02:35:36 at org.apache.flink.runtime.taskmanager.Task.triggerCheckpointBarrier(Task.java:1348) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4094740Z Mar 05 02:35:36 at org.apache.flink.runtime.taskexecutor.TaskExecutor.triggerCheckpoint(TaskExecutor.java:956) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4095836Z Mar 05 02:35:36 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_322]2022-03-05T02:35:36.4096579Z Mar 05 02:35:36 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_322]2022-03-05T02:35:36.4097766Z Mar 05 02:35:36 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_322]2022-03-05T02:35:36.4098684Z Mar 05 02:35:36 at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_322]2022-03-05T02:35:36.4101381Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[?:?]2022-03-05T02:35:36.4102353Z Mar 05 02:35:36 at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]2022-03-05T02:35:36.4103218Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[?:?]2022-03-05T02:35:36.4104019Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[?:?]2022-03-05T02:35:36.4104801Z Mar 05 02:35:36 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[?:?]2022-03-05T02:35:36.4105719Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]2022-03-05T02:35:36.4108356Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]2022-03-05T02:35:36.4110333Z Mar 05 02:35:36 at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4112523Z Mar 05 02:35:36 at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4113601Z Mar 05 02:35:36 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]2022-03-05T02:35:36.4114790Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4116110Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4117636Z Mar 05 02:35:36 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]2022-03-05T02:35:36.4118641Z Mar 05 02:35:36 at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]2022-03-05T02:35:36.4119307Z Mar 05 02:35:36 at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]2022-03-05T02:35:36.4120161Z Mar 05 02:35:36 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]2022-03-05T02:35:36.4120842Z Mar 05 02:35:36 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) ~[?:?]2022-03-05T02:35:36.4121482Z Mar 05 02:35:36 at akka.actor.ActorCell.invoke(ActorCell.scala:548) ~[?:?]2022-03-05T02:35:36.4122113Z Mar 05 02:35:36 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]2022-03-05T02:35:36.4122736Z Mar 05 02:35:36 at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]2022-03-05T02:35:36.4123332Z Mar 05 02:35:36 at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]2022-03-05T02:35:36.4123984Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_322]2022-03-05T02:35:36.4124749Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_322]2022-03-05T02:35:36.4125750Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_322]2022-03-05T02:35:36.4126591Z Mar 05 02:35:36 at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) ~[?:1.8.0_322]2022-03-05T02:35:36.4128133Z Mar 05 02:35:36 2022-03-05 02:35:34,430 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Sequence Source (Deprecated) -&gt; Map -&gt; Sink: Unnamed (1/1) (e8a324cae6bf452d32db6797bbbafad0) switched from RUNNING to FINISHED. https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32553&amp;view=logs&amp;j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&amp;t=cc5499f8-bdde-5157-0d76-b6528ecd808e&amp;l=18735</description>
      <version>1.14.3,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-9 01:00:00" id="26543" opendate="2022-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the issue that exceptions generated in startup are missed in Python loopback mode</summary>
      <description/>
      <version>1.14.3,1.15.0</version>
      <fixedVersion>1.14.5,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.boot.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-11 01:00:00" id="26609" opendate="2022-3-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support sum operation in KeyedStream</summary>
      <description>In Java, KeyedStream has a "sum" operator, But when i using in PyFlink. that operator is not found.Â So i implemented it in Python Flink KeyedStream And test has passed.Â But i don't know if this feature is necessary in PyFlink.so i send a jira task in here, we can discuss it.Â If that feature is necessary, i have already implemented that feature so i can summit a PR in github.Â Thansks.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-13 01:00:00" id="26618" opendate="2022-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove jar statement not aligned with pipleline.jars</summary>
      <description>Currently, `remove jar` statement doesn't remove the corresponding jars in pipeline.jars.</description>
      <version>1.14.3</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.set.q</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.context.SessionContext.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-3-13 01:00:00" id="26619" opendate="2022-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for the window assigners in Python DataStream API</summary>
      <description>Write the window allocator usage document of pyflink data flow API</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.datastream.operators.windows.md</file>
      <file type="M">docs.content.zh.docs.dev.datastream.operators.windows.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-15 01:00:00" id="2673" opendate="2015-9-15 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Scala API does not support Option type as key</summary>
      <description>The Scala API does not support the Option type as a key. It could be useful to allow grouping on a field with this type.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.JoinITCase.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.OptionTypeInfo.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2022-4-21 01:00:00" id="27339" opendate="2022-4-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some classes don&amp;#39;t have a package</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-test-utils.src.test.java.TableAssertionTest.java</file>
      <file type="M">flink-end-to-end-tests.flink-file-sink-test.src.main.java.FileSinkProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-file-sink-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-6-26 01:00:00" id="27418" opendate="2022-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink SQL TopN result is wrong</summary>
      <description>Flink SQL TopN is executed multiple times with different results, sometimes with correct results and sometimes with incorrect results.Example:@Test public void flinkSqlJoinRetract() { EnvironmentSettings settings = EnvironmentSettings.newInstance() .useBlinkPlanner() .inStreamingMode() .build(); StreamExecutionEnvironment streamEnv = StreamExecutionEnvironment.getExecutionEnvironment(); streamEnv.setParallelism(1); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(streamEnv, settings); tableEnv.getConfig().setIdleStateRetention(Duration.ofSeconds(10000)); RowTypeInfo waybillTableTypeInfo = buildWaybillTableTypeInfo(); RowTypeInfo itemTableTypeInfo = buildItemTableTypeInfo(); SourceFunction&lt;Row&gt; waybillSourceFunction = buildWaybillStreamSource(waybillTableTypeInfo); SourceFunction&lt;Row&gt; itemSourceFunction = buildItemStreamSource(itemTableTypeInfo); String waybillTable = "waybill"; String itemTable = "item"; DataStreamSource&lt;Row&gt; waybillStream = streamEnv.addSource( waybillSourceFunction, waybillTable, waybillTableTypeInfo); DataStreamSource&lt;Row&gt; itemStream = streamEnv.addSource( itemSourceFunction, itemTable, itemTableTypeInfo); Expression[] waybillFields = ExpressionParser .parseExpressionList(String.join(",", waybillTableTypeInfo.getFieldNames()) + ",proctime.proctime").toArray(new Expression[0]); Expression[] itemFields = ExpressionParser .parseExpressionList( String.join(",", itemTableTypeInfo.getFieldNames()) + ",proctime.proctime") .toArray(new Expression[0]); tableEnv.createTemporaryView(waybillTable, waybillStream, waybillFields); tableEnv.createTemporaryView(itemTable, itemStream, itemFields); String sql = "select \n" + " city_id, \n" + " count(*) as cnt\n" + "from (\n" + " select id,city_id\n" + " from (\n" + " select \n" + " id,\n" + " city_id,\n" + " row_number() over(partition by id order by utime desc ) as rno \n" + " from (\n" + " select \n" + " waybill.id as id,\n" + " coalesce(item.city_id, waybill.city_id) as city_id,\n" + " waybill.utime as utime \n" + " from waybill left join item \n" + " on waybill.id = item.id \n" + " ) \n" + " )\n" + " where rno =1\n" + ")\n" + "group by city_id"; StatementSet statementSet = tableEnv.createStatementSet(); Table table = tableEnv.sqlQuery(sql); DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; rowDataStream = tableEnv.toRetractStream(table, Row.class); rowDataStream.printToErr(); try { streamEnv.execute(); } catch (Exception e) { e.printStackTrace(); } } private static RowTypeInfo buildWaybillTableTypeInfo() { TypeInformation[] types = new TypeInformation[]{Types.INT(), Types.STRING(), Types.LONG(), Types.LONG()}; String[] fields = new String[]{"id", "city_id", "rider_id", "utime"}; return new RowTypeInfo(types, fields); } private static RowTypeInfo buildItemTableTypeInfo() { TypeInformation[] types = new TypeInformation[]{Types.INT(), Types.STRING(), Types.LONG()}; String[] fields = new String[]{"id", "city_id", "utime"}; return new RowTypeInfo(types, fields); } //id,rider_id,city_id,utime private static SourceFunction&lt;Row&gt; buildWaybillStreamSource(RowTypeInfo rowTypeInfo) { return new SourceFunction&lt;Row&gt;() { private volatile boolean stopped = false; int count = 0; int[] ids = {111, 222, 333, 111}; String[] cityIds = {"A", "A", "B", "A"}; @Override public void run(SourceContext&lt;Row&gt; ctx) throws Exception { while (!stopped) { int id = ids[count % ids.length]; String cityId = cityIds[count % cityIds.length]; Row row = new Row(4); row.setField(0, id); row.setField(1, cityId); row.setField(2, (long) RandomUtils.nextInt(1000, 2000)); row.setField(3, System.currentTimeMillis()); printRow(rowTypeInfo, row); ctx.collect(row); if (++count &gt; 3) { stopped = true; } } } @Override public void cancel() { stopped = true; } }; } //id,city_id,utime private static SourceFunction&lt;Row&gt; buildItemStreamSource(RowTypeInfo rowTypeInfo) { return new SourceFunction&lt;Row&gt;() { private volatile boolean stopped = false; int count = 0; int[] ids = {111, 333}; String[] cityIds = {"C", "D"}; @Override public void run(SourceContext&lt;Row&gt; ctx) throws Exception { while (!stopped) { Thread.sleep(RandomUtils.nextInt(1000, 2000)); int id = ids[count % ids.length]; String cityId = cityIds[count % cityIds.length]; Row row = new Row(3); row.setField(0, id); row.setField(1, cityId); //row.setField(2, System.currentTimeMillis()); printRow(rowTypeInfo, row); ctx.collect(row); if (++count &gt;= 2) { stopped = true; } } } @Override public void cancel() { stopped = true; } }; } public static void printRow(RowTypeInfo rowTypeInfo, Row row) { String prefix = ""; for (int i = 0; i &lt; rowTypeInfo.getArity(); ++i) { prefix = i &gt; 0 ? "," : ""; System.out.print(prefix + rowTypeInfo.getFieldNames()[i] + ":" + row.getField(i)); } System.out.println(); }------------------------------------------------------------wrong resultright resultid:111,city_id:A,rider_id:1137,utime:1650979957702id:222,city_id:A,rider_id:1976,utime:1650979957725id:333,city_id:B,rider_id:1916,utime:1650979957725id:111,city_id:A,rider_id:1345,utime:1650979957725(true,A,1)(false,A,1)(true,A,2)(true,B,1)(false,A,2)(true,A,1)(false,A,1)(true,A,2)id:111,city_id:C,utime:null(false,A,2)(true,A,1)(true,C,1)(false,A,1)(false,C,1)(true,C,2)id:333,city_id: D,utime:null(false,B,1)(true,D,1)The final result:C,2D,1is wrong.Â id:111,city_id:A,rider_id:1155,utime:1650980662019id:222,city_id:A,rider_id:1875,utime:1650980662042id:333,city_id:B,rider_id:1430,utime:1650980662042id:111,city_id:A,rider_id:1308,utime:1650980662042(true,A,1)(false,A,1)(true,A,2)(true,B,1)(false,A,2)(true,A,1)(false,A,1)(true,A,2)id:111,city_id:C,utime:null(false,A,2)(true,A,1)(false,A,1)(true,A,2)(false,A,2)(true,A,1)(true,C,1)id:333,city_id: D,utime:null(false,B,1)(true,D,1)The final result:A,1C,1D,1is right.Â Â </description>
      <version>1.12.2,1.14.3</version>
      <fixedVersion>1.15.1,1.16.0,1.14.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.rank.RetractableTopNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.rank.RetractableTopNFunction.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-5-28 01:00:00" id="27441" opendate="2022-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scrollbar is missing for particular UI elements (Accumulators, Backpressure, Watermarks)</summary>
      <description>The angular version bump introduced a bug, where for nzScroll does not support percentage in CSS calc, so the scrollbar will be invisible. There is an easy workaround, the linked Angular discussion covers it.Angular issue: https://github.com/NG-ZORRO/ng-zorro-antd/issues/3090</description>
      <version>1.14.3,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.watermarks.job-overview-drawer-watermarks.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.taskmanagers.job-overview-drawer-taskmanagers.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.subtasks.job-overview-drawer-subtasks.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.accumulators.job-overview-drawer-accumulators.component.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-6-9 01:00:00" id="27544" opendate="2022-5-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Example code in &amp;#39;Structure of Table API and SQL Programs&amp;#39; is out of date and cannot run</summary>
      <description>The example code in Structure of Table API and SQL Programs of 'Concepts &amp; Common API' is out of date and when user run this piece of code, they will get the following result:Exception in thread "main" org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.SinkTable'.Table options are:'connector'='blackhole''rows-per-second'='1' at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:262) at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:421) at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:222) at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:178) at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) at scala.collection.Iterator.foreach(Iterator.scala:937) at scala.collection.Iterator.foreach$(Iterator.scala:937) at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) at scala.collection.IterableLike.foreach(IterableLike.scala:70) at scala.collection.IterableLike.foreach$(IterableLike.scala:69) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike.map(TraversableLike.scala:233) at scala.collection.TraversableLike.map$(TraversableLike.scala:226) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:178) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1656) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:782) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:861) at org.apache.flink.table.api.internal.TablePipelineImpl.execute(TablePipelineImpl.java:56) at com.yck.TestTableAPI.main(TestTableAPI.java:43)Caused by: org.apache.flink.table.api.ValidationException: Unsupported options found for 'blackhole'.Unsupported options:rows-per-secondSupported options:connectorproperty-version at org.apache.flink.table.factories.FactoryUtil.validateUnconsumedKeys(FactoryUtil.java:624) at org.apache.flink.table.factories.FactoryUtil$FactoryHelper.validate(FactoryUtil.java:914) at org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.validate(FactoryUtil.java:978) at org.apache.flink.connector.blackhole.table.BlackHoleTableSinkFactory.createDynamicTableSink(BlackHoleTableSinkFactory.java:64) at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:259) ... 19 moreI think this mistake would drive users crazy when they first fry Table API &amp; Flink SQL since this is the very first code they see.Overall this code is outdated in two places:1. The Query creating temporary table should be CREATE TEMPORARY TABLE SinkTable WITH ('connector' = 'blackhole') LIKE SourceTable (EXCLUDING OPTIONS) instead of CREATE TEMPORARY TABLE SinkTable WITH ('connector' = 'blackhole') LIKE SourceTable which missed (EXCLUDING OPTIONS) sql_like_pattern2. The part creating a source table should be tableEnv.createTemporaryTable("SourceTable", TableDescriptor.forConnector("datagen") .schema(Schema.newBuilder() .column("f0", DataTypes.STRING()) .build()) .option(DataGenConnectorOptions.ROWS_PER_SECOND, 1L) .build());instead of tableEnv.createTemporaryTable("SourceTable", TableDescriptor.forConnector("datagen") .schema(Schema.newBuilder() .column("f0", DataTypes.STRING()) .build()) .option(DataGenOptions.ROWS_PER_SECOND, 100) .build());since the class DataGenOptions was replaced by class DataGenConnectorOptions in this commitThe test code is in my github Repository(version 1.15) and version 1.14The affected versions are 1.15 and 1.14.</description>
      <version>1.14.0,1.14.2,1.14.3,1.14.4,1.15.0</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.common.md</file>
      <file type="M">docs.content.zh.docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-7-16 01:00:00" id="27618" opendate="2022-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support CumeDist</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.test.java.org.apache.flink.table.runtime.operators.over.SumAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.over.frame.UnboundedPrecedingOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.over.frame.UnboundedOverWindowFrame.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.over.frame.UnboundedFollowingOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.over.frame.SlidingOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.over.frame.OffsetOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.operators.over.frame.InsensitiveOverFrame.java</file>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.generated.AggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.OverAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.ImperativeAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.DistinctAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.DeclarativeAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.AggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecOverAggregateBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecOverAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-5-25 01:00:00" id="27763" opendate="2022-5-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove netty bundling&amp;relocation in flink-streaming-kinesis-tests</summary>
      <description>flink-streaming-kinesis-tests bundles and relocates netty (and no other dependency). We can safely remove this because no class within this module references netty, nor any other module relies on the relocated versions.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-streaming-kinesis-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-29 01:00:00" id="2779" opendate="2015-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation to reflect new Stream/Window API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.navbar.html</file>
      <file type="M">docs.internals.general.arch.md</file>
      <file type="M">docs.index.md</file>
      <file type="M">docs.apis.streaming.guide.md</file>
      <file type="M">docs.apis.programming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-16 01:00:00" id="28094" opendate="2022-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade AWS SDK to support ap-southeast-3</summary>
      <description>The AWS base module pulls AWS SDK v2.17.52 which does not support ap-southeast-3. Update to the latest version. Ensure to cover connectors (KDS/KDF/DDB) and formats (avro-glue-schema-registry and json-glue-schema-registry)</description>
      <version>None</version>
      <fixedVersion>1.16.0,1.15.2,1.14.6</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-json-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-glue-schema-registry-avro-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-aws-kinesis-streams.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-aws-kinesis-firehose.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.JobManagerWatermarkTrackerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-streams.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-aws-base.src.test.java.org.apache.flink.connector.aws.util.AWSAsyncSinkUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-aws-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.DynamoDBStreamsProxy.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-6-16 01:00:00" id="28095" opendate="2022-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace IOUtils dependency on oss filesystem</summary>
      <description>The oss fs has an undeclared dependency on commons-io for a single call to IOUtils.We can make our lives a little bit easier by using the Flink IOUtils instead.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.java.org.apache.flink.fs.osshadoop.writer.OSSRecoverableFsDataOutputStream.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-16 01:00:00" id="28096" opendate="2022-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive dialect support set variable</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveOperationExecutor.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveDialectFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2022-1-24 01:00:00" id="28655" opendate="2022-7-24 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support show jobs statement in SqlGatewayService</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.utils.Constants.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-24 01:00:00" id="28658" opendate="2022-7-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for job lifecycle statements</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.17.0,1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.show.md</file>
      <file type="M">docs.content.docs.dev.table.sql.jar.md</file>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.docs.dev.table.sql-gateway.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.show.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.jar.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-27 01:00:00" id="2933" opendate="2015-10-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink scala libraries exposed with maven should carry scala version</summary>
      <description>&amp;#91;If I put this on the wrong component, can someone please update?&amp;#93;Major versions of scala are not forward nor backwards compatible. Libraries build for 2.10 will not work with 2.11 or vice versa.In order to avoid build related problems, it is strongly recommended to append the scala version it is compatible within the artifact id. This ensures the correct version of the library is pulled in rather than deferring the problem to a future build or runtime error.For example, akka exposes the following packages for the same version:&lt;dependency&gt; &lt;groupId&gt;com.typesafe.akka&lt;/groupId&gt; &lt;artifactId&gt;akka-actor_2.10&lt;/artifactId&gt; &lt;version&gt;2.3.14&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.typesafe.akka&lt;/groupId&gt; &lt;artifactId&gt;akka-actor_2.11&lt;/artifactId&gt; &lt;version&gt;2.3.14&lt;/version&gt;&lt;/dependency&gt;</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.change-scala-version.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-test-utils.pom.xml</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-streaming-java.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-twitter.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-rabbitmq.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-nifi.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-flume.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-elasticsearch.pom.xml</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop1.pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-scala-shell.pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-runtime-web.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-optimizer.pom.xml</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
      <file type="M">flink-libraries.flink-python.pom.xml</file>
      <file type="M">flink-libraries.flink-ml.pom.xml</file>
      <file type="M">flink-libraries.flink-gelly.pom.xml</file>
      <file type="M">flink-libraries.flink-gelly-scala.pom.xml</file>
      <file type="M">flink-java8.pom.xml</file>
      <file type="M">flink-fs-tests.pom.xml</file>
      <file type="M">flink-examples.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-examples.flink-examples-batch.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-contrib.flink-tweet-inputformat.pom.xml</file>
      <file type="M">flink-contrib.flink-streaming-contrib.pom.xml</file>
      <file type="M">flink-contrib.flink-storm.pom.xml</file>
      <file type="M">flink-contrib.flink-storm-examples.pom.xml</file>
      <file type="M">flink-contrib.flink-operator-stats.pom.xml</file>
      <file type="M">flink-contrib.flink-connector-wikiedits.pom.xml</file>
      <file type="M">flink-clients.pom.xml</file>
      <file type="M">flink-batch-connectors.flink-hbase.pom.xml</file>
      <file type="M">flink-batch-connectors.flink-hadoop-compatibility.pom.xml</file>
      <file type="M">flink-batch-connectors.flink-avro.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-1 01:00:00" id="3310" opendate="2016-2-1 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add back pressure statistics to web frontend</summary>
      <description>When a task is receiving data at a higher rate than it can process, the task is back pressuring preceding tasks. Currently, there is no way to tell whether this is the case or not. An indicator for back pressure is tasks being stuck in buffer requests on the network stack. This means that they have filled all their buffers with data, but the following tasks/network are not consuming them fast enough.A simple way to measure back pressure is to sample running tasks and report back pressure if they are stuck in the blocking buffers calls.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.plan.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.vendor.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.fonts.FontAwesome.otf</file>
      <file type="M">flink-runtime-web.web-dashboard.web.fonts.fontawesome-webfont.woff2</file>
      <file type="M">flink-runtime-web.web-dashboard.web.fonts.fontawesome-webfont.woff</file>
      <file type="M">flink-runtime-web.web-dashboard.web.fonts.fontawesome-webfont.ttf</file>
      <file type="M">flink-runtime-web.web-dashboard.web.fonts.fontawesome-webfont.svg</file>
      <file type="M">flink-runtime-web.web-dashboard.web.fonts.fontawesome-webfont.eot</file>
      <file type="M">flink-runtime-web.web-dashboard.web.css.vendor.css</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobs.jobs.svc.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobs.jobs.ctrl.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.index.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.common.filters.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.common.directives.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.jade</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
</bugrepository>