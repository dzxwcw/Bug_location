<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2018-4-9 01:00:00" id="10114" opendate="2018-8-9 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support Orc for StreamingFileSink</summary>
      <description/>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-orc.pom.xml</file>
      <file type="M">docs.dev.connectors.streamfile.sink.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-12-19 01:00:00" id="10935" opendate="2018-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement KubeClient with Faric8 Kubernetes clients</summary>
      <description>Implement KubeClient with Faric8 Kubernetes clients and add tests</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-docs.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-12-19 01:00:00" id="10937" opendate="2018-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add entrypoint scripts for k8s</summary>
      <description>Flink official docker image could be used to active kubernetes integration. An entrypoint script for k8s should be added.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-16 01:00:00" id="13745" opendate="2019-8-16 00:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Flink cache on Travis does not exist</summary>
      <description>More and more often I observe that Flink builds fail on Travis because of missing Flink caches:Cached flink dir /home/travis/flink_cache/40072/flink does not exist. Exiting build.It seems as if Travis cannot guarantee that a cache survives as long as the different profiles of a build are running. It would be good to solve this problem because now we have regularly failing builds:https://travis-ci.org/apache/flink/builds/572559629https://travis-ci.org/apache/flink/builds/572523730https://travis-ci.org/apache/flink/builds/571576734</description>
      <version>1.9.0,1.10.0,1.11.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.controller.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-4-2 01:00:00" id="14311" opendate="2019-10-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming File Sink end-to-end test failed on Travis</summary>
      <description>The Streaming File Sink end-to-end test fails on Travis because it does not produce output for 10 minutes.https://api.travis-ci.org/v3/job/591992274/log.txt</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-10 01:00:00" id="15177" opendate="2019-12-10 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Migrate RocksDB Configurable Options to new type safe config options</summary>
      <description>The RocksDB config options are currently all types to String and are manually parsed and validated. This can be simplified using the new config option classes.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBConfigurableOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.DefaultConfigurableOptionsFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2019-12-24 01:00:00" id="15373" opendate="2019-12-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update descriptions for framework / task off-heap memory config options</summary>
      <description>Update descriptions for "taskmanager.memory.framework.off-heap.size" and "taskmanager.memory.task.off-heap.size" to explicitly state that: Both direct and native memory are accounted Will be fully counted into MaxDirectMemorySizeDetailed discussion can be found in this ML thread.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs..includes.generated.task.manager.memory.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-24 01:00:00" id="15374" opendate="2019-12-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update descriptions for jvm overhead config options</summary>
      <description>Update descriptions for "taskmanager.memory.jvm-overhead.&amp;#91;min|max|fraction&amp;#93;" to remove "I/O direct memory" and explicitly state that it's not counted into MaxDirectMemorySize.Detailed discussion can be found in this ML thread.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs..includes.generated.task.manager.memory.configuration.html</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-3-14 01:00:00" id="15584" opendate="2020-1-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Give nested data type of ROWs in ValidationException</summary>
      <description>In INSERT INTO baz_sinkSELECT  a, ROW(b, c)FROM foo_sourceSchema mismatch mistakes will not get proper detail level, yielding the following:Caused by: org.apache.flink.table.api.ValidationException: Field types of query result and registered TableSink &amp;#91;baz_sink&amp;#93; do not match. Query result schema: &amp;#91;a: Integer, EXPR$2: Row&amp;#93; TableSink schema: &amp;#91;a: Integer, payload: Row&amp;#93;Leaving the user with an opaque 'Row' type to debug. </description>
      <version>1.9.1,1.10.0,1.11.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.validation.InsertIntoValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sinks.TableSinkUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-23 01:00:00" id="15738" opendate="2020-1-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump powermock to 2.0.4</summary>
      <description>Kinesis tests are failing with a NullPointerException due to their Whitebox usages.We have to bump powermock from 2.0.2 to at least 2.0.4.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-5 01:00:00" id="15921" opendate="2020-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Travis-ci error: PYTHON exited with EXIT CODE: 143</summary>
      <description>Currently, some Travis CI failures occur, such as: &amp;#91;1&amp;#93;,&amp;#91;2&amp;#93;. The reason for the failure is that the python dependent `grpcio` released the latest version 1.27.0 &amp;#91;3&amp;#93; today, which resulted in the test cache not having the latest dependency, and the timeout of downloading in the repo. If the problem will be fixed after the first download when the network is in good condition. I am still watching the latest build &amp;#91;4&amp;#93;. If it fails for a long time, we will try to set a lower version of `grpcio` or optimize the current test case. I would like to watch for a while . What do you think?  &amp;#91;1&amp;#93;https://travis-ci.org/apache/flink/builds/646250268 &amp;#91;2&amp;#93;https://travis-ci.org/apache/flink/jobs/646281060&amp;#91;3&amp;#93; https://pypi.org/project/grpcio/#files&amp;#91;4&amp;#93;https://travis-ci.org/apache/flink/builds/646355253</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.tox.ini</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-11 01:00:00" id="15982" opendate="2020-2-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>&amp;#39;Quickstarts Java nightly end-to-end test&amp;#39; is failed on travis</summary>
      <description>==============================================================================Running 'Quickstarts Java nightly end-to-end test'==============================================================================TEST_DATA_DIR: /home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-42718423491Flink dist directory: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT22:16:44.021 [INFO] Scanning for projects...22:16:44.095 [INFO] ------------------------------------------------------------------------22:16:44.095 [INFO] BUILD FAILURE22:16:44.095 [INFO] ------------------------------------------------------------------------22:16:44.098 [INFO] Total time: 0.095 s22:16:44.099 [INFO] Finished at: 2020-02-10T22:16:44+00:0022:16:44.143 [INFO] Final Memory: 5M/153M22:16:44.143 [INFO] ------------------------------------------------------------------------22:16:44.144 [ERROR] The goal you specified requires a project to execute but there is no POM in this directory (/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-42718423491). Please verify you invoked Maven from the correct directory. -&gt; [Help 1]22:16:44.144 [ERROR] 22:16:44.145 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.22:16:44.145 [ERROR] Re-run Maven using the -X switch to enable full debug logging.22:16:44.145 [ERROR] 22:16:44.145 [ERROR] For more information about the errors and possible solutions, please read the following articles:22:16:44.145 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MissingProjectException/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/test_quickstarts.sh: line 57: cd: flink-quickstart-java: No such file or directorycp: cannot create regular file '/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-42718423491/flink-quickstart-java/src/main/java/org/apache/flink/quickstart/Elasticsearch5SinkExample.java': No such file or directorysed: can't read /home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-42718423491/flink-quickstart-java/src/main/java/org/apache/flink/quickstart/Elasticsearch5SinkExample.java: No such file or directoryawk: fatal: cannot open file `pom.xml' for reading (No such file or directory)sed: can't read pom.xml: No such file or directorysed: can't read pom.xml: No such file or directory22:16:45.312 [INFO] Scanning for projects...22:16:45.386 [INFO] ------------------------------------------------------------------------22:16:45.386 [INFO] BUILD FAILURE22:16:45.386 [INFO] ------------------------------------------------------------------------22:16:45.391 [INFO] Total time: 0.097 s22:16:45.391 [INFO] Finished at: 2020-02-10T22:16:45+00:0022:16:45.438 [INFO] Final Memory: 5M/153M22:16:45.438 [INFO] ------------------------------------------------------------------------22:16:45.440 [ERROR] The goal you specified requires a project to execute but there is no POM in this directory (/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-42718423491). Please verify you invoked Maven from the correct directory. -&gt; [Help 1]22:16:45.440 [ERROR] 22:16:45.440 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.22:16:45.440 [ERROR] Re-run Maven using the -X switch to enable full debug logging.22:16:45.440 [ERROR] 22:16:45.440 [ERROR] For more information about the errors and possible solutions, please read the following articles:22:16:45.440 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MissingProjectException/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/test_quickstarts.sh: line 73: cd: target: No such file or directoryjava.io.FileNotFoundException: flink-quickstart-java-0.1.jar (No such file or directory) at java.util.zip.ZipFile.open(Native Method) at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:225) at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:155) at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:126) at sun.tools.jar.Main.list(Main.java:1115) at sun.tools.jar.Main.run(Main.java:293) at sun.tools.jar.Main.main(Main.java:1288)Success: There are no flink core classes are contained in the jar.Failure: Since Elasticsearch5SinkExample.class and other user classes are not included in the jar. [FAIL] Test script contains errors.Here are some instances: https://api.travis-ci.org/v3/job/648404584/log.txt https://api.travis-ci.org/v3/job/648404591/log.txt https://api.travis-ci.org/v3/job/648404598/log.txt</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-11 01:00:00" id="15999" opendate="2020-2-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extract “Concepts” material from API/Library sections and start proper concepts section</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.fig.times.clocks.svg</file>
      <file type="M">docs.internals.stream.checkpointing.zh.md</file>
      <file type="M">docs.internals.stream.checkpointing.md</file>
      <file type="M">docs.fig.event.ingestion.processing.time.svg</file>
      <file type="M">docs.fig.processes.svg</file>
      <file type="M">docs.concepts.index.zh.md</file>
      <file type="M">docs.concepts.index.md</file>
      <file type="M">docs.concepts.stream-processing.md</file>
      <file type="M">docs.concepts.programming-model.zh.md</file>
      <file type="M">docs.concepts.glossary.zh.md</file>
      <file type="M">docs.concepts.glossary.md</file>
      <file type="M">docs.dev.event.time.md</file>
      <file type="M">docs.concepts.timely-stream-processing.md</file>
      <file type="M">docs.dev.stream.operators.process.function.zh.md</file>
      <file type="M">docs.dev.stream.operators.process.function.md</file>
      <file type="M">docs.concepts.runtime.zh.md</file>
      <file type="M">docs.concepts.runtime.md</file>
      <file type="M">docs.concepts.flink-architecture.md</file>
      <file type="M">docs.dev.stream.state.state.md</file>
      <file type="M">docs.dev.stream.state.index.md</file>
      <file type="M">docs.dev.stream.state.broadcast.state.md</file>
      <file type="M">docs.concepts.stateful-stream-processing.md</file>
      <file type="M">docs.concepts.programming-model.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-12 01:00:00" id="16014" opendate="2020-2-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>S3 plugin ClassNotFoundException SAXParser</summary>
      <description>While stress-testing s3 plugin on EMR. org.apache.flink.util.FlinkRuntimeException: Could not perform checkpoint 2 for operator Map (114/160). at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:839) at org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:104) at org.apache.flink.streaming.runtime.io.CheckpointBarrierUnaligner.notifyBarrierReceived(CheckpointBarrierUnaligner.java:149) at org.apache.flink.streaming.runtime.io.InputProcessorUtil$1.lambda$notifyBarrierReceived$0(InputProcessorUtil.java:80) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.run(StreamTaskActionExecutor.java:87) at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:255) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:508) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:492) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.RuntimeException: org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3://emr-unaligned-checkpoints-testing-eu-central-1/inflight/9ae223e41008b17568d7f63c12360268_output/part-file.-1: com.amazonaws.SdkClientException: Couldn't initialize a SAX driver to create an XMLReader: Couldn't initialize a SAX driver to create an XMLReader at org.apache.flink.runtime.io.network.BufferPersisterImpl$Writer.checkErroneousUnsafe(BufferPersisterImpl.java:262) at org.apache.flink.runtime.io.network.BufferPersisterImpl$Writer.add(BufferPersisterImpl.java:137) at org.apache.flink.runtime.io.network.BufferPersisterImpl.addBuffers(BufferPersisterImpl.java:66) at org.apache.flink.streaming.runtime.tasks.StreamTask.prepareInflightDataSnapshot(StreamTask.java:935) at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:898) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94) at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:870) at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:826) ... 12 moreCaused by: org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3://emr-unaligned-checkpoints-testing-eu-central-1/inflight/9ae223e41008b17568d7f63c12360268_output/part-file.-1: com.amazonaws.SdkClientException: Couldn't initialize a SAX driver to create an XMLReader: Couldn't initialize a SAX driver to create an XMLReader at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:177) at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:145) at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2251) at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2149) at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2088) at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1734) at org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:2970) at org.apache.flink.fs.s3hadoop.common.HadoopFileSystem.exists(HadoopFileSystem.java:152) at org.apache.flink.core.fs.PluginFileSystemFactory$ClassLoaderFixingFileSystem.exists(PluginFileSystemFactory.java:143) at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.exists(SafetyNetWrapperFileSystem.java:102) at org.apache.flink.runtime.io.network.BufferPersisterImpl$Writer.get(BufferPersisterImpl.java:213) at org.apache.flink.runtime.io.network.BufferPersisterImpl$Writer.run(BufferPersisterImpl.java:167)Caused by: com.amazonaws.SdkClientException: Couldn't initialize a SAX driver to create an XMLReader at com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.&lt;init&gt;(XmlResponsesSaxParser.java:118) at com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsV2Unmarshaller.unmarshall(Unmarshallers.java:87) at com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsV2Unmarshaller.unmarshall(Unmarshallers.java:77) at com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:62) at com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:31) at com.amazonaws.http.response.AwsResponseHandlerAdapter.handle(AwsResponseHandlerAdapter.java:70) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleResponse(AmazonHttpClient.java:1554) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1272) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1056) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:743) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:717) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667) at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649) at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4325) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4272) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4266) at com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:876) at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$5(S3AFileSystem.java:1262) at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:317) at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:280) at org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:1255) at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2223) ... 9 moreCaused by: org.xml.sax.SAXException: SAX2 driver class org.apache.xerces.parsers.SAXParser not foundjava.lang.ClassNotFoundException: org.apache.xerces.parsers.SAXParser at org.xml.sax.helpers.XMLReaderFactory.loadClass(XMLReaderFactory.java:230) at org.xml.sax.helpers.XMLReaderFactory.createXMLReader(XMLReaderFactory.java:191) at com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.&lt;init&gt;(XmlResponsesSaxParser.java:115) ... 32 moreCaused by: java.lang.ClassNotFoundException: org.apache.xerces.parsers.SAXParser at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at org.apache.flink.core.plugin.PluginLoader$PluginClassLoader.loadClass(PluginLoader.java:149) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at org.xml.sax.helpers.NewInstance.newInstance(NewInstance.java:82) at org.xml.sax.helpers.XMLReaderFactory.loadClass(XMLReaderFactory.java:228) ... 34 more</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-2-13 01:00:00" id="16049" opendate="2020-2-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove outdated "Best Practices" section from Application Development Section</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.best.practices.zh.md</file>
      <file type="M">docs.dev.best.practices.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-3-17 01:00:00" id="16122" opendate="2020-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>build system: transfer.sh uploads are unstable (February 2020)</summary>
      <description>This issue has been brought up on the dev@ list: https://lists.apache.org/thread.html/rb6661e419b869f040e66a4dd46022fd11961e8e5aebe646b2260f6f8%40%3Cdev.flink.apache.org%3EIssues: timeouts logs not available</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.watchdog.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-19 01:00:00" id="16166" opendate="2020-2-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Javadoc doclint option not working on Java 11</summary>
      <description>The doclint option is overridden in the java 11 profile.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-3-2 01:00:00" id="1619" opendate="2015-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add pre-aggregator for time windows</summary>
      <description>Currently there is only support for pre-aggregators for tumbling policies.A pre-aggregator should be added for time policies.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.windowbuffer.TumblingPreReducerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.windowbuffer.TumblingGroupedPreReducerTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.windowing.windowbuffer.BasicWindowBufferTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.WindowBuffer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.TumblingPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.TumblingGroupedPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.BasicWindowBuffer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.TimeTriggerPolicy.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.TimeEvictionPolicy.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.CountTriggerPolicy.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.policy.CountEvictionPolicy.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.windowing.WindowIntegrationTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.WindowUtils.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.windowing.windowbuffer.SlidingPreReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.WindowedDataStream.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStreamSource.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-22 01:00:00" id="16233" opendate="2020-2-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive connector missing log4j1 exclusions against certain hive versions</summary>
      <description>Click to add description</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-8-23 01:00:00" id="16245" opendate="2020-2-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use a delegating classloader as the user code classloader to prevent class leaks.</summary>
      <description>As reported in FLINK-11205, a reference to the user-code ClassLoader can be held by some libraries, causing class leaks.One way to circumvent this class leak is if the ClassLoader that we set as the user-code ClassLoader is a delegating ClassLoader to the real class loader, and when closing the user code ClassLoader we null out the reference.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.FlinkUserCodeClassLoader.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoadersTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.ChildFirstClassLoader.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.BatchFineGrainedRecoveryITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.ClientUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-24 01:00:00" id="16263" opendate="2020-2-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>BaseRowArrowReaderWriterTest/RowArrowReaderWriterTest sun.misc.Unsafe or java.nio.DirectByteBuffer.&lt;init&gt;(long, int) not available</summary>
      <description>https://travis-ci.org/apache/flink/jobs/65440936418:17:45.003 [INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.924 s - in org.apache.flink.table.runtime.arrow.ArrowUtilsTest18:17:45.019 [INFO] Running org.apache.flink.table.runtime.arrow.BaseRowArrowReaderWriterTestsun.misc.Unsafe or java.nio.DirectByteBuffer.&lt;init&gt;(long, int) not availablejava.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.&lt;init&gt;(long, int) not available at io.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:399) at io.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:257) at io.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:247) at io.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:248) at org.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:228) at org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:242) at org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:132) at org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:120) at org.apache.flink.table.runtime.arrow.ArrowReaderWriterTestBase.testBasicFunctionality(ArrowReaderWriterTestBase.java:68) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)18:17:45.128 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.102 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.table.runtime.arrow.BaseRowArrowReaderWriterTest18:17:45.128 [ERROR] testBasicFunctionality(org.apache.flink.table.runtime.arrow.BaseRowArrowReaderWriterTest) Time elapsed: 0.097 s &lt;&lt;&lt; FAILURE!java.lang.AssertionError: Exception in test: sun.misc.Unsafe or java.nio.DirectByteBuffer.&lt;init&gt;(long, int) not available18:17:45.143 [INFO] Running org.apache.flink.table.runtime.arrow.RowArrowReaderWriterTestsun.misc.Unsafe or java.nio.DirectByteBuffer.&lt;init&gt;(long, int) not availablejava.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.&lt;init&gt;(long, int) not available at io.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:399) at io.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:257) at io.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:247) at io.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:248) at org.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:228) at org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:242) at org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:132) at org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:120) at org.apache.flink.table.runtime.arrow.ArrowReaderWriterTestBase.testBasicFunctionality(ArrowReaderWriterTestBase.java:68) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)18:17:45.209 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.057 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.table.runtime.arrow.RowArrowReaderWriterTest18:17:45.209 [ERROR] testBasicFunctionality(org.apache.flink.table.runtime.arrow.RowArrowReaderWriterTest) Time elapsed: 0.056 s &lt;&lt;&lt; FAILURE!java.lang.AssertionError: Exception in test: sun.misc.Unsafe or java.nio.DirectByteBuffer.&lt;init&gt;(long, int) not available</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-26 01:00:00" id="16287" opendate="2020-2-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ES6 sql jar relocates log4j2</summary>
      <description>flink-sql-connector-elasticsearch6 still defines a relocation rule for log4j2, but this dependency is no longer bundled and instead provided by flink-dist.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-3-28 01:00:00" id="16337" opendate="2020-2-28 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add RelNodes and Rules for vectorized Python UDF execution</summary>
      <description>As the title describes, the aim of this JIRA is to add RelNodes and Rules for vectorized Python UDF execution. </description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.PythonCalcSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.util.python.PythonTableUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.PythonUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.SplitPythonConditionFromJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.SplitPythonConditionFromCorrelateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.PythonCalcSplitRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamPythonCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetPythonCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamMatch.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.PythonFunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.rules.logical.PythonCorrelateSplitRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.rules.datastream.DataStreamPythonCorrelateRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.ExpressionReductionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.ExpressionReductionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.utils.python.PythonTableUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.PythonUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecPythonCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecPythonCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecCalcRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SplitPythonConditionFromJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SplitPythonConditionFromCorrelateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMatch.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.PythonFunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecPythonCorrelateRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecPythonCorrelateRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PythonCorrelateSplitRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.python.SimplePythonFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.python.PythonFunction.java</file>
      <file type="M">flink-python.pyflink.table.udf.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-3-28 01:00:00" id="16348" opendate="2020-2-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add commas to large numeric accumulators</summary>
      <description>Make large numeric accumulator values easier to read.Ex 273232 -&gt; 273,232</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.accumulators.job-overview-drawer-accumulators.component.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-2 01:00:00" id="16370" opendate="2020-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink-dist bundles ZK 3.5 as JDK11-exclusive dependency</summary>
      <description>This is the output from the CI system (https://travis-ci.org/apache/flink/jobs/656931001)16:35:30.798 [ERROR] testKillYarnSessionClusterEntrypoint(org.apache.flink.yarn.YARNHighAvailabilityITCase) Time elapsed: 10.363 s &lt;&lt;&lt; ERROR!org.apache.flink.client.deployment.ClusterDeploymentException: Couldn't deploy Yarn session cluster at org.apache.flink.yarn.YARNHighAvailabilityITCase.deploySessionCluster(YARNHighAvailabilityITCase.java:296) at org.apache.flink.yarn.YARNHighAvailabilityITCase.lambda$testKillYarnSessionClusterEntrypoint$0(YARNHighAvailabilityITCase.java:165) at org.apache.flink.yarn.YARNHighAvailabilityITCase.testKillYarnSessionClusterEntrypoint(YARNHighAvailabilityITCase.java:157)Caused by: org.apache.flink.yarn.YarnClusterDescriptor$YarnDeploymentException: The YARN application unexpectedly switched to state FAILED during deployment. Diagnostics from YARN: Application application_1583080501498_0002 failed 2 times in previous 10000 milliseconds due to AM Container for appattempt_1583080501498_0002_000002 exited with exitCode: 1Failing this attempt.Diagnostics: Exception from container-launch.Container id: container_1583080501498_0002_02_000001Exit code: 1Stack trace: ExitCodeException exitCode=1: at org.apache.hadoop.util.Shell.runCommand(Shell.java:972) at org.apache.hadoop.util.Shell.run(Shell.java:869) at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170) at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236) at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305) at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84) at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at java.base/java.lang.Thread.run(Thread.java:834)... snip ...16:44:14.840 [INFO] Results:16:44:14.840 [INFO] 16:44:14.840 [ERROR] Errors: 16:44:14.840 [ERROR] YARNHighAvailabilityITCase.testJobRecoversAfterKillingTaskManager:187-&gt;YarnTestBase.runTest:242-&gt;lambda$testJobRecoversAfterKillingTaskManager$1:191-&gt;deploySessionCluster:296 Â» ClusterDeployment16:44:14.840 [ERROR] YARNHighAvailabilityITCase.testKillYarnSessionClusterEntrypoint:157-&gt;YarnTestBase.runTest:242-&gt;lambda$testKillYarnSessionClusterEntrypoint$0:165-&gt;deploySessionCluster:296 Â» ClusterDeployment16:44:14.840 [INFO] 16:44:14.840 [ERROR] Tests run: 25, Failures: 0, Errors: 2, Skipped: 4Digging deeper into the problem, this seems to be the root cause:2020-03-01 16:35:14,444 INFO org.apache.flink.shaded.curator4.org.apache.curator.utils.Compatibility [] - Using emulated InjectSessionExpiration2020-03-01 16:35:14,466 WARN org.apache.flink.shaded.curator4.org.apache.curator.CuratorZookeeperClient [] - session timeout [1000] is less than connection timeout [15000]2020-03-01 16:35:14,491 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Shutting YarnSessionClusterEntrypoint down with application status FAILED. Diagnostics java.lang.NoSuchMethodError: org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.quorum.flexible.QuorumMaj.&lt;init&gt;(Ljava/util/Map;)V at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.EnsembleTracker.&lt;init&gt;(EnsembleTracker.java:57) at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.&lt;init&gt;(CuratorFrameworkImpl.java:159) at org.apache.flink.shaded.curator4.org.apache.curator.framework.CuratorFrameworkFactory$Builder.build(CuratorFrameworkFactory.java:165) at org.apache.flink.runtime.util.ZooKeeperUtils.startCuratorFramework(ZooKeeperUtils.java:138) at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:128) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:305) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:263) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:207) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:169) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:168) at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:518) at org.apache.flink.yarn.entrypoint.YarnSessionClusterEntrypoint.main(YarnSessionClusterEntrypoint.java:80).2020-03-01 16:35:14,502 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Stopping Akka RPC service.2020-03-01 16:35:14,512 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator [] - Shutting down remote daemon.2020-03-01 16:35:14,514 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator [] - Remote daemon shut down; proceeding with flushing remote transports.2020-03-01 16:35:14,548 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator [] - Remoting shut down.2020-03-01 16:35:14,604 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Stopped Akka RPC service.2020-03-01 16:35:14,592 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Could not start cluster entrypoint YarnSessionClusterEntrypoint.org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint YarnSessionClusterEntrypoint. at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:187) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:518) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.yarn.entrypoint.YarnSessionClusterEntrypoint.main(YarnSessionClusterEntrypoint.java:80) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]Caused by: java.lang.NoSuchMethodError: org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.quorum.flexible.QuorumMaj.&lt;init&gt;(Ljava/util/Map;)V at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.EnsembleTracker.&lt;init&gt;(EnsembleTracker.java:57) ~[flink-shaded-zookeeper-3.4.10.jar:3.4.10-10.0] at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.&lt;init&gt;(CuratorFrameworkImpl.java:159) ~[flink-shaded-zookeeper-3.4.10.jar:3.4.10-10.0] at org.apache.flink.shaded.curator4.org.apache.curator.framework.CuratorFrameworkFactory$Builder.build(CuratorFrameworkFactory.java:165) ~[flink-shaded-zookeeper-3.4.10.jar:3.4.10-10.0] at org.apache.flink.runtime.util.ZooKeeperUtils.startCuratorFramework(ZooKeeperUtils.java:138) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:128) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:305) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:263) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:207) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:169) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at java.security.AccessController.doPrivileged(Native Method) ~[?:?] at javax.security.auth.Subject.doAs(Subject.java:423) ~[?:?] at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0] at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:168) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] ... 2 more</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-2 01:00:00" id="16374" opendate="2020-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>StreamingKafkaITCase: IOException: error=13, Permission denied</summary>
      <description>Build: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5792&amp;view=logs&amp;j=25197a20-5964-5b06-5716-045f87dc0ea9&amp;t=0c53f4dc-c81e-5ebb-13b2-08f1994a2d322020-03-02T05:13:23.4758068Z [INFO] Running org.apache.flink.tests.util.kafka.StreamingKafkaITCase2020-03-02T05:13:55.8260013Z [ERROR] Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 32.346 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.tests.util.kafka.StreamingKafkaITCase2020-03-02T05:13:55.8262664Z [ERROR] testKafka[0: kafka-version:0.10.2.0](org.apache.flink.tests.util.kafka.StreamingKafkaITCase) Time elapsed: 9.217 s &lt;&lt;&lt; ERROR!2020-03-02T05:13:55.8264067Z java.io.IOException: Cannot run program "/tmp/junit5236495846374568650/junit4714535957173883866/bin/start-cluster.sh": error=13, Permission denied2020-03-02T05:13:55.8264733Z at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.testKafka(StreamingKafkaITCase.java:85)2020-03-02T05:13:55.8265242Z Caused by: java.io.IOException: error=13, Permission denied2020-03-02T05:13:55.8265717Z at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.testKafka(StreamingKafkaITCase.java:85)2020-03-02T05:13:55.8266083Z 2020-03-02T05:13:55.8271420Z [ERROR] testKafka[1: kafka-version:0.11.0.2](org.apache.flink.tests.util.kafka.StreamingKafkaITCase) Time elapsed: 11.228 s &lt;&lt;&lt; ERROR!2020-03-02T05:13:55.8272670Z java.io.IOException: Cannot run program "/tmp/junit8038960384540194088/junit1280636219654303027/bin/start-cluster.sh": error=13, Permission denied2020-03-02T05:13:55.8273343Z at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.testKafka(StreamingKafkaITCase.java:85)2020-03-02T05:13:55.8273847Z Caused by: java.io.IOException: error=13, Permission denied2020-03-02T05:13:55.8274418Z at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.testKafka(StreamingKafkaITCase.java:85)2020-03-02T05:13:55.8274768Z 2020-03-02T05:13:55.8275429Z [ERROR] testKafka[2: kafka-version:2.2.0](org.apache.flink.tests.util.kafka.StreamingKafkaITCase) Time elapsed: 11.89 s &lt;&lt;&lt; ERROR!2020-03-02T05:13:55.8276386Z java.io.IOException: Cannot run program "/tmp/junit5500905670445852005/junit4695208010500962520/bin/start-cluster.sh": error=13, Permission denied2020-03-02T05:13:55.8277257Z at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.testKafka(StreamingKafkaITCase.java:85)2020-03-02T05:13:55.8277760Z Caused by: java.io.IOException: error=13, Permission denied2020-03-02T05:13:55.8278228Z at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.testKafka(StreamingKafkaITCase.java:85)</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.watchdog.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-5 01:00:00" id="16431" opendate="2020-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pass build profile into end to end test script on Azure</summary>
      <description>The nightly tests scripts assumes that it has access to $PROFILE, which does not seem to be true.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-5 01:00:00" id="16432" opendate="2020-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Building Hive connector gives problems</summary>
      <description>When building the current Flink source I keep running to problems with the hive connector.The problems focus around dependencies that are not available by default: org.pentaho:pentaho-aggdesigner-algorithm javax.jms:jms</description>
      <version>1.10.2,1.11.0</version>
      <fixedVersion>1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-6 01:00:00" id="16464" opendate="2020-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>result-mode tableau may shift when content contains Chinese String in SQL CLI</summary>
      <description> result-mode tableau may shift when column content contains Chinese String in SQL CLI  as following: Flink SQL&gt; select * from user_ino;+-----+----------------------+--------+----------------------+| +/- | user_name | is_new | content_col |+-----+----------------------+--------+----------------------+| + | sam | true | content || + | 中文名 | false | content || + | leonard | true | content |We can calculate column widths with UDTF-8 format bytes not length of string to avoid this.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliUtilsTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliTableauResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliTableauResultView.java</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-7 01:00:00" id="16480" opendate="2020-3-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve e2e test failure error reporting</summary>
      <description>The purpose of this change is to improve the error reporting for e2e tests: The log upload for e2e tests fails if the bash e2e tests fail coredumps, dumpstreams etc. are not included into the log upload Logs are not scanned for exceptions when exception checking is turned off</description>
      <version>1.11.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-4 01:00:00" id="1652" opendate="2015-3-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong superstep number in VertexCentricIteration in Collection mode</summary>
      <description>When in collection execution mode, the superstep number is not correctly updated for Spargel's and Gelly's VertexCentricIteration. There seems to be to problem with DeltaIteration.See also relevant discussion in dev@ .</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-gelly.src.test.java.org.apache.flink.graph.test.operations.DegreesWithExceptionITCase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.CollectionExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-11 01:00:00" id="16550" opendate="2020-3-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HadoopS3* tests fail with NullPointerException exceptions</summary>
      <description>Logs: https://travis-ci.org/github/apache/flink/jobs/660975486?utm_medium=notificationAll subsequent builds failed as well. It is likely that this commit / FLINK-16014 introduced the issue, as these tests depend on S3 credentials to be available.09:38:48.022 [INFO] -------------------------------------------------------09:38:48.025 [INFO] T E S T S09:38:48.026 [INFO] -------------------------------------------------------09:38:48.657 [INFO] Running org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase09:38:48.669 [INFO] Running org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase09:38:54.541 [ERROR] Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 5.88 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase09:38:54.542 [ERROR] testResumeAfterCommit(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase) Time elapsed: 3.592 s &lt;&lt;&lt; ERROR!java.lang.Exception: Unexpected exception, expected&lt;java.io.IOException&gt; but was&lt;java.lang.NullPointerException&gt; at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeAfterCommit(HadoopS3RecoverableWriterExceptionITCase.java:162)09:38:54.542 [ERROR] testResumeWithWrongOffset(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase) Time elapsed: 0.24 s &lt;&lt;&lt; ERROR!java.lang.Exception: Unexpected exception, expected&lt;java.io.IOException&gt; but was&lt;java.lang.NullPointerException&gt; at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeWithWrongOffset(HadoopS3RecoverableWriterExceptionITCase.java:182)09:38:54.542 [ERROR] testExceptionWritingAfterCloseForCommit(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase) Time elapsed: 0.448 s &lt;&lt;&lt; ERROR!java.lang.Exception: Unexpected exception, expected&lt;java.io.IOException&gt; but was&lt;java.lang.NullPointerException&gt; at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testExceptionWritingAfterCloseForCommit(HadoopS3RecoverableWriterExceptionITCase.java:144)09:38:55.173 [INFO] Running org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase09:38:58.737 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 10.066 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase09:38:58.737 [ERROR] testDirectoryListing(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase) Time elapsed: 3.448 s &lt;&lt;&lt; ERROR!java.io.FileNotFoundException: No such file or directory: s3://[secure]/temp/tests-f37db36e-c116-4c58-a16b-8ca241baae4b/testdir09:38:59.447 [INFO] Running org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase09:39:01.791 [ERROR] Tests run: 13, Failures: 0, Errors: 13, Skipped: 0, Time elapsed: 6.611 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase09:39:01.797 [ERROR] testCloseWithNoData(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 2.394 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testCloseWithNoData(HadoopS3RecoverableWriterITCase.java:186)09:39:01.798 [ERROR] testCommitAfterPersist(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.191 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testCommitAfterPersist(HadoopS3RecoverableWriterITCase.java:208)09:39:01.799 [ERROR] testRecoverWithEmptyState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.235 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:384) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithSmallData(HadoopS3RecoverableWriterITCase.java:352) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverWithEmptyState(HadoopS3RecoverableWriterITCase.java:302)09:39:01.799 [ERROR] testRecoverFromIntermWithoutAdditionalState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.181 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:384) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithSmallData(HadoopS3RecoverableWriterITCase.java:352) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverFromIntermWithoutAdditionalState(HadoopS3RecoverableWriterITCase.java:316)09:39:01.799 [ERROR] testCallingDeleteObjectTwiceDoesNotThroughException(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.181 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testCallingDeleteObjectTwiceDoesNotThroughException(HadoopS3RecoverableWriterITCase.java:245)09:39:01.801 [ERROR] testCommitAfterNormalClose(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.174 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testCommitAfterNormalClose(HadoopS3RecoverableWriterITCase.java:196)09:39:01.802 [ERROR] testRecoverWithStateWithMultiPart(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.338 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:384) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithMultiPartUploads(HadoopS3RecoverableWriterITCase.java:364) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverWithStateWithMultiPart(HadoopS3RecoverableWriterITCase.java:330)09:39:01.803 [ERROR] testRecoverFromIntermWithoutAdditionalStateWithMultiPart(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.486 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:384) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithMultiPartUploads(HadoopS3RecoverableWriterITCase.java:364) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverFromIntermWithoutAdditionalStateWithMultiPart(HadoopS3RecoverableWriterITCase.java:337)09:39:01.810 [ERROR] testRecoverWithState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.199 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:384) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithSmallData(HadoopS3RecoverableWriterITCase.java:352) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverWithState(HadoopS3RecoverableWriterITCase.java:309)09:39:01.810 [ERROR] testCleanupRecoverableState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.202 s &lt;&lt;&lt; ERROR!java.lang.Exception: Unexpected exception, expected&lt;java.io.FileNotFoundException&gt; but was&lt;java.lang.NullPointerException&gt; at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testCleanupRecoverableState(HadoopS3RecoverableWriterITCase.java:223)09:39:01.810 [ERROR] testCommitAfterRecovery(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.26 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testCommitAfterRecovery(HadoopS3RecoverableWriterITCase.java:270)09:39:01.810 [ERROR] testRecoverAfterMultiplePersistsState(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.165 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:384) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithSmallData(HadoopS3RecoverableWriterITCase.java:352) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverAfterMultiplePersistsState(HadoopS3RecoverableWriterITCase.java:323)09:39:01.810 [ERROR] testRecoverAfterMultiplePersistsStateWithMultiPart(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase) Time elapsed: 0.735 s &lt;&lt;&lt; ERROR!java.lang.NullPointerException at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:384) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithMultiPartUploads(HadoopS3RecoverableWriterITCase.java:364) at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverAfterMultiplePersistsStateWithMultiPart(HadoopS3RecoverableWriterITCase.java:344)09:39:14.711 [WARNING] Tests run: 8, Failures: 0, Errors: 0, Skipped: 2, Time elapsed: 15.262 s - in org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase09:39:15.047 [INFO] 09:39:15.047 [INFO] Results:09:39:15.047 [INFO] 09:39:15.047 [ERROR] Errors: 09:39:15.047 [ERROR] HadoopS3FileSystemITCase&gt;AbstractHadoopFileSystemITTest.testDirectoryListing:127 Â» FileNotFound09:39:15.047 [ERROR] HadoopS3RecoverableWriterExceptionITCase.testExceptionWritingAfterCloseForCommit Â» 09:39:15.047 [ERROR] HadoopS3RecoverableWriterExceptionITCase.testResumeAfterCommit Â» Unexpected e...09:39:15.047 [ERROR] HadoopS3RecoverableWriterExceptionITCase.testResumeWithWrongOffset Â» Unexpect...09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testCallingDeleteObjectTwiceDoesNotThroughException:245 Â» NullPointer09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testCleanupRecoverableState Â» Unexpected exce...09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testCloseWithNoData:186 Â» NullPointer09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testCommitAfterNormalClose:196 Â» NullPointer09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testCommitAfterPersist:208 Â» NullPointer09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testCommitAfterRecovery:270 Â» NullPointer09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testRecoverAfterMultiplePersistsState:323-&gt;testResumeAfterMultiplePersistWithSmallData:352-&gt;testResumeAfterMultiplePersist:384 Â» NullPointer09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testRecoverAfterMultiplePersistsStateWithMultiPart:344-&gt;testResumeAfterMultiplePersistWithMultiPartUploads:364-&gt;testResumeAfterMultiplePersist:384 Â» NullPointer09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testRecoverFromIntermWithoutAdditionalState:316-&gt;testResumeAfterMultiplePersistWithSmallData:352-&gt;testResumeAfterMultiplePersist:384 Â» NullPointer09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testRecoverFromIntermWithoutAdditionalStateWithMultiPart:337-&gt;testResumeAfterMultiplePersistWithMultiPartUploads:364-&gt;testResumeAfterMultiplePersist:384 Â» NullPointer09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testRecoverWithEmptyState:302-&gt;testResumeAfterMultiplePersistWithSmallData:352-&gt;testResumeAfterMultiplePersist:384 Â» NullPointer09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testRecoverWithState:309-&gt;testResumeAfterMultiplePersistWithSmallData:352-&gt;testResumeAfterMultiplePersist:384 Â» NullPointer09:39:15.047 [ERROR] HadoopS3RecoverableWriterITCase.testRecoverWithStateWithMultiPart:330-&gt;testResumeAfterMultiplePersistWithMultiPartUploads:364-&gt;testResumeAfterMultiplePersist:384 Â» NullPointer09:39:15.047 [INFO] 09:39:15.047 [ERROR] Tests run: 26, Failures: 0, Errors: 17, Skipped: 2</description>
      <version>1.11.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.main.java.com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-12 01:00:00" id="16565" opendate="2020-3-12 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Make Pipeline Json compitable between Java and Python if all Pipelinestage are Java ones</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.ml.tests.test.pipeline.it.case.py</file>
      <file type="M">flink-python.pyflink.ml.tests.test.pipeline.py</file>
      <file type="M">flink-python.pyflink.ml.api.base.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-13 01:00:00" id="16590" opendate="2020-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-oss-fs-hadoop: Not all dependencies in NOTICE file are bundled</summary>
      <description>NOTICE file in flink-oss-fs-hadoop lists org.apache.commons:commons-compress as a bundled dependency which is not correct. There are likely other dependencies that are wrongly listed in the NOTICE file.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.licenses.LICENSE.stax2api</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.licenses.LICENSE.re2j</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.licenses.LICENSE.protobuf</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.licenses.LICENSE.paranamer</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.licenses.LICENSE.jzlib</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.licenses.LICENSE.cddlv1.1</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.licenses.LICENSE.cddlv1.0</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.licenses.LICENSE.asm</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-17 01:00:00" id="16629" opendate="2020-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming bucketing end-to-end test output hash mismatch</summary>
      <description>https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/6298/logs/722Some of the output mismatch failures were reported in another ticket: https://issues.apache.org/jira/browse/FLINK-162272020-03-17T02:04:19.9176915Z Number of produced values 30618/600002020-03-17T02:04:19.9202731Z Truncating buckets2020-03-17T02:04:25.0504959Z Truncating buckets2020-03-17T02:04:30.1731295Z Truncating buckets2020-03-17T02:04:35.3190114Z Truncating buckets2020-03-17T02:04:40.4723887Z Truncating buckets2020-03-17T02:04:45.5984655Z Truncating buckets2020-03-17T02:04:50.7185356Z Truncating buckets2020-03-17T02:04:55.8627129Z Truncating buckets2020-03-17T02:05:01.0715985Z Number of produced values 74008/600002020-03-17T02:05:02.3976850Z Cancelling job dba2fdb79579158295db27d0214fc2ff.2020-03-17T02:05:03.4633541Z Cancelled job dba2fdb79579158295db27d0214fc2ff.2020-03-17T02:05:03.4738270Z Waiting for job (dba2fdb79579158295db27d0214fc2ff) to reach terminal state CANCELED ...2020-03-17T02:05:03.5149228Z Job (dba2fdb79579158295db27d0214fc2ff) reached terminal state CANCELED2020-03-17T02:05:03.5150587Z Job dba2fdb79579158295db27d0214fc2ff was cancelled, time to verify2020-03-17T02:05:03.5590118Z FAIL Bucketing Sink: Output hash mismatch. Got c3787e7a52d913675e620837a7531742, expected 01aba5ff77a0ef5e5cf6a727c248bdc3.2020-03-17T02:05:03.5591888Z head hexdump of actual:2020-03-17T02:05:03.5989908Z 0000000 ( 7 , 1 0 , 0 , S o m e p a y2020-03-17T02:05:03.5991252Z 0000010 l o a d . . . ) \n ( 7 , 1 0 , 12020-03-17T02:05:03.5991923Z 0000020 , S o m e p a y l o a d . . .2020-03-17T02:05:03.5993055Z 0000030 ) \n ( 7 , 1 0 , 2 , S o m e p2020-03-17T02:05:03.5993690Z 0000040 a y l o a d . . . ) \n ( 7 , 1 02020-03-17T02:05:03.5994332Z 0000050 , 3 , S o m e p a y l o a d .2020-03-17T02:05:03.5994967Z 0000060 . . ) \n ( 7 , 1 0 , 4 , S o m e2020-03-17T02:05:03.5995744Z 0000070 p a y l o a d . . . ) \n ( 7 ,2020-03-17T02:05:03.5996359Z 0000080 1 0 , 5 , S o m e p a y l o a2020-03-17T02:05:03.5997133Z 0000090 d . . . ) \n ( 7 , 1 0 , 6 , S o2020-03-17T02:05:03.5997704Z 00000a0 m e p a y l o a d . . . ) \n (2020-03-17T02:05:03.5998295Z 00000b0 7 , 1 0 , 7 , S o m e p a y l2020-03-17T02:05:03.5999087Z 00000c0 o a d . . . ) \n ( 7 , 1 0 , 8 ,2020-03-17T02:05:03.6000243Z 00000d0 S o m e p a y l o a d . . . )2020-03-17T02:05:03.6000880Z 00000e0 \n ( 7 , 1 0 , 9 , S o m e p a2020-03-17T02:05:03.6001494Z 00000f0 y l o a d . . . ) \n 2020-03-17T02:05:03.6001999Z 00000fa2020-03-17T02:05:03.9875220Z Stopping taskexecutor daemon (pid: 49278) on host fv-az668.2020-03-17T02:05:04.2569285Z Stopping standalonesession daemon (pid: 46323) on host fv-az668.2020-03-17T02:05:04.7664418Z Stopping taskexecutor daemon (pid: 46615) on host fv-az668.2020-03-17T02:05:04.7674722Z Skipping taskexecutor daemon (pid: 47009), because it is not running anymore on fv-az668.2020-03-17T02:05:04.7687383Z Skipping taskexecutor daemon (pid: 47299), because it is not running anymore on fv-az668.2020-03-17T02:05:04.7689091Z Skipping taskexecutor daemon (pid: 47619), because it is not running anymore on fv-az668.2020-03-17T02:05:04.7690289Z Stopping taskexecutor daemon (pid: 48538) on host fv-az668.2020-03-17T02:05:04.7691796Z Stopping taskexecutor daemon (pid: 48988) on host fv-az668.2020-03-17T02:05:04.7692365Z [FAIL] Test script contains errors.2020-03-17T02:05:04.7713750Z Checking of logs skipped.2020-03-17T02:05:04.7714249Z 2020-03-17T02:05:04.7715316Z [FAIL] 'Streaming bucketing end-to-end test' failed after 2 minutes and 43 seconds! Test exited with exit code 1</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-3-18 01:00:00" id="16663" opendate="2020-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Docs version 1.10 missing from version picker dropdown</summary>
      <description>When going to the latest master docs &amp;#91;1&amp;#93;, the documentation version 1.10 is not shown in the "Pick Docs Version" dropdown menu.&amp;#91;1&amp;#93; https://ci.apache.org/projects/flink/flink-docs-master/</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..config.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-20 01:00:00" id="16702" opendate="2020-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>develop JDBCCatalogFactory, descriptor, and validator for service discovery</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.catalog.AbstractJDBCCatalog.java</file>
      <file type="M">flink-connectors.flink-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-24 01:00:00" id="16741" opendate="2020-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web UI: Enable listing TM Logs and displaying Logs by Filename</summary>
      <description>add log list and read log by name for taskmanager in the web</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.refresh-download.refresh-download.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.refresh-download.refresh-download.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.refresh-download.refresh-download.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.navigation.navigation.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.task-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.task-manager.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.task-manager-routing.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.stdout.task-manager-stdout.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.stdout.task-manager-stdout.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.stdout.task-manager-stdout.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.status.task-manager-status.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.logs.task-manager-logs.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.logs.task-manager-logs.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.logs.task-manager-logs.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.stdout.job-manager-stdout.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.logs.job-manager-logs.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.task-manager.ts</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-24 01:00:00" id="16748" opendate="2020-3-24 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add Python UDTF doc</summary>
      <description>Currently Python UDTF has been supported in coming release 1.11, so we should add relevant docs.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.python.python.udfs.zh.md</file>
      <file type="M">docs.dev.table.python.python.udfs.md</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-9-25 01:00:00" id="16768" opendate="2020-3-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HadoopS3RecoverableWriterITCase.testRecoverWithStateWithMultiPart hangs</summary>
      <description>Logs: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6584&amp;view=logs&amp;j=d44f43ce-542c-597d-bf94-b0718c71e5e8&amp;t=d26b3528-38b0-53d2-05f7-37557c2405e42020-03-24T15:52:18.9196862Z "main" #1 prio=5 os_prio=0 tid=0x00007fd36c00b800 nid=0xc21 runnable [0x00007fd3743ce000]2020-03-24T15:52:18.9197235Z java.lang.Thread.State: RUNNABLE2020-03-24T15:52:18.9197536Z at java.net.SocketInputStream.socketRead0(Native Method)2020-03-24T15:52:18.9197931Z at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)2020-03-24T15:52:18.9198340Z at java.net.SocketInputStream.read(SocketInputStream.java:171)2020-03-24T15:52:18.9198749Z at java.net.SocketInputStream.read(SocketInputStream.java:141)2020-03-24T15:52:18.9199171Z at sun.security.ssl.InputRecord.readFully(InputRecord.java:465)2020-03-24T15:52:18.9199840Z at sun.security.ssl.InputRecord.readV3Record(InputRecord.java:593)2020-03-24T15:52:18.9200265Z at sun.security.ssl.InputRecord.read(InputRecord.java:532)2020-03-24T15:52:18.9200663Z at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975)2020-03-24T15:52:18.9201213Z - locked &lt;0x00000000927583d8&gt; (a java.lang.Object)2020-03-24T15:52:18.9201589Z at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933)2020-03-24T15:52:18.9202026Z at sun.security.ssl.AppInputStream.read(AppInputStream.java:105)2020-03-24T15:52:18.9202583Z - locked &lt;0x0000000092758c00&gt; (a sun.security.ssl.AppInputStream)2020-03-24T15:52:18.9203029Z at org.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:137)2020-03-24T15:52:18.9203558Z at org.apache.http.impl.io.SessionInputBufferImpl.read(SessionInputBufferImpl.java:198)2020-03-24T15:52:18.9204121Z at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:176)2020-03-24T15:52:18.9204626Z at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:135)2020-03-24T15:52:18.9205121Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9205679Z at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)2020-03-24T15:52:18.9206164Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9206786Z at com.amazonaws.services.s3.internal.S3AbortableInputStream.read(S3AbortableInputStream.java:125)2020-03-24T15:52:18.9207361Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9207839Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9208327Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9208809Z at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)2020-03-24T15:52:18.9209273Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9210003Z at com.amazonaws.util.LengthCheckInputStream.read(LengthCheckInputStream.java:107)2020-03-24T15:52:18.9210658Z at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)2020-03-24T15:52:18.9211154Z at org.apache.hadoop.fs.s3a.S3AInputStream.lambda$read$3(S3AInputStream.java:445)2020-03-24T15:52:18.9211631Z at org.apache.hadoop.fs.s3a.S3AInputStream$$Lambda$42/1936375962.execute(Unknown Source)2020-03-24T15:52:18.9212044Z at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)2020-03-24T15:52:18.9212553Z at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:260)2020-03-24T15:52:18.9212972Z at org.apache.hadoop.fs.s3a.Invoker$$Lambda$23/1457226878.execute(Unknown Source)2020-03-24T15:52:18.9213408Z at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:317)2020-03-24T15:52:18.9213866Z at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:256)2020-03-24T15:52:18.9214273Z at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:231)2020-03-24T15:52:18.9214701Z at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:441)2020-03-24T15:52:18.9215443Z - locked &lt;0x00000000926e88b0&gt; (a org.apache.hadoop.fs.s3a.S3AInputStream)2020-03-24T15:52:18.9215852Z at java.io.DataInputStream.read(DataInputStream.java:149)2020-03-24T15:52:18.9216305Z at org.apache.flink.runtime.fs.hdfs.HadoopDataInputStream.read(HadoopDataInputStream.java:94)2020-03-24T15:52:18.9216781Z at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)2020-03-24T15:52:18.9217187Z at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)2020-03-24T15:52:18.9217571Z at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)2020-03-24T15:52:18.9218108Z - locked &lt;0x00000000926ea000&gt; (a java.io.InputStreamReader)2020-03-24T15:52:18.9218475Z at java.io.InputStreamReader.read(InputStreamReader.java:184)2020-03-24T15:52:18.9218876Z at java.io.BufferedReader.fill(BufferedReader.java:161)2020-03-24T15:52:18.9219261Z at java.io.BufferedReader.readLine(BufferedReader.java:324)2020-03-24T15:52:18.9219890Z - locked &lt;0x00000000926ea000&gt; (a java.io.InputStreamReader)2020-03-24T15:52:18.9220256Z at java.io.BufferedReader.readLine(BufferedReader.java:389)2020-03-24T15:52:18.9220914Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getContentsOfFile(HadoopS3RecoverableWriterITCase.java:423)2020-03-24T15:52:18.9221704Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:411)2020-03-24T15:52:18.9222457Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithMultiPartUploads(HadoopS3RecoverableWriterITCase.java:364)2020-03-24T15:52:18.9223222Z at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverWithStateWithMultiPart(HadoopS3RecoverableWriterITCase.java:330)2020-03-24T15:52:18.9223817Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)2020-03-24T15:52:18.9224232Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)2020-03-24T15:52:18.9224729Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)2020-03-24T15:52:18.9225160Z at java.lang.reflect.Method.invoke(Method.java:498)2020-03-24T15:52:18.9225675Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)2020-03-24T15:52:18.9226171Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)2020-03-24T15:52:18.9226682Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)2020-03-24T15:52:18.9227187Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)2020-03-24T15:52:18.9227661Z at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2020-03-24T15:52:18.9228145Z at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2020-03-24T15:52:18.9228718Z at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)2020-03-24T15:52:18.9229112Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-03-24T15:52:18.9229582Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)2020-03-24T15:52:18.9230029Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)2020-03-24T15:52:18.9230525Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)2020-03-24T15:52:18.9230963Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)2020-03-24T15:52:18.9231546Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)2020-03-24T15:52:18.9231999Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)2020-03-24T15:52:18.9232432Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)2020-03-24T15:52:18.9232862Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)2020-03-24T15:52:18.9233307Z at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)2020-03-24T15:52:18.9233833Z at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)2020-03-24T15:52:18.9234284Z at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)2020-03-24T15:52:18.9234700Z at org.junit.rules.RunRules.evaluate(RunRules.java:20)2020-03-24T15:52:18.9235076Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363)2020-03-24T15:52:18.9235599Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)2020-03-24T15:52:18.9236124Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)2020-03-24T15:52:18.9236648Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)2020-03-24T15:52:18.9237167Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)2020-03-24T15:52:18.9237688Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)2020-03-24T15:52:18.9238244Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)2020-03-24T15:52:18.9238745Z at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)2020-03-24T15:52:18.9239202Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)2020-03-24T15:52:18.9239549Z 2020-03-24T15:52:18.9239794Z "VM Thread" os_prio=0 tid=0x00007fd36c260800 nid=0xc58 runnable  </description>
      <version>1.10.0,1.11.0,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.watchdog.sh</file>
      <file type="M">tools.ci.test.controller.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-25 01:00:00" id="16778" opendate="2020-3-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>the java e2e profile isn&amp;#39;t setting the hadoop switch on Azure</summary>
      <description>Context: https://lists.apache.org/thread.html/r06e597b3dadfee00593989b1cfae0f2b83548f412c8fdca6d4bc3dbe%40%3Cdev.flink.apache.org%3E the azure setup doesn't appear to be equivalent yet since the java e2e profile isn't setting the hadoop switch (-Pe2e-hadoop), as a result of which SQLClientKafkaITCase isn't run</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-7-26 01:00:00" id="16795" opendate="2020-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>End to end tests timeout on Azure</summary>
      <description>Example: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6650&amp;view=logs&amp;j=08866332-78f7-59e4-4f7e-49a56faa3179 or https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6637&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5##[error]The job running on agent Azure Pipelines 6 ran longer than the maximum time of 200 minutes. For more information, see https://go.microsoft.com/fwlink/?linkid=2077134and ##[error]The operation was canceled.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-26 01:00:00" id="16796" opendate="2020-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix The Bug of Python UDTF in SQL Query</summary>
      <description>When executes Python UDTF in sql query, it will cause some problem.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonCorrelate.scala</file>
      <file type="M">flink-python.pyflink.table.tests.test.udtf.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-26 01:00:00" id="16798" opendate="2020-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Logs from BashJavaUtils are not properly preserved and passed into TM logs.</summary>
      <description>With FLINK-15519, in the TM start-up scripts, we have captured logs from BashJavaUtils and passed into the TM JVM process via environment variable. These logs will be merged with other TM logs, writing to same places respecting user's log configurations.This effort was broken in FLINK-15727, where the outputs from BashJavaUtils are thrown away, except for the result JVM parameters and dynamic configurations</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.taskmanager.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-26 01:00:00" id="16805" opendate="2020-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>StreamingKafkaITCase fails with "Could not instantiate instance using default factory."</summary>
      <description>CI: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6654&amp;view=logs&amp;j=fc5181b0-e452-5c8f-68de-1097947f6483&amp;t=27d1d645-cbce-54e2-51c4-d8b45fe246072020-03-26T08:17:42.8881925Z [INFO] T E S T S2020-03-26T08:17:42.8882791Z [INFO] -------------------------------------------------------2020-03-26T08:17:43.6840472Z [INFO] Running org.apache.flink.tests.util.kafka.StreamingKafkaITCase2020-03-26T08:17:43.6933052Z [ERROR] Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 0.006 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.tests.util.kafka.StreamingKafkaITCase2020-03-26T08:17:43.6934567Z [ERROR] testKafka[0: kafka-version:0.10.2.0](org.apache.flink.tests.util.kafka.StreamingKafkaITCase) Time elapsed: 0.004 s &lt;&lt;&lt; ERROR!2020-03-26T08:17:43.6935170Z java.lang.RuntimeException: Could not instantiate instance using default factory.2020-03-26T08:17:43.6935702Z at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.&lt;init&gt;(StreamingKafkaITCase.java:72)2020-03-26T08:17:43.6936024Z 2020-03-26T08:17:43.6936691Z [ERROR] testKafka[1: kafka-version:0.11.0.2](org.apache.flink.tests.util.kafka.StreamingKafkaITCase) Time elapsed: 0 s &lt;&lt;&lt; ERROR!2020-03-26T08:17:43.6937288Z java.lang.RuntimeException: Could not instantiate instance using default factory.2020-03-26T08:17:43.6937789Z at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.&lt;init&gt;(StreamingKafkaITCase.java:72)2020-03-26T08:17:43.6938113Z 2020-03-26T08:17:43.6938890Z [ERROR] testKafka[2: kafka-version:2.2.0](org.apache.flink.tests.util.kafka.StreamingKafkaITCase) Time elapsed: 0 s &lt;&lt;&lt; ERROR!2020-03-26T08:17:43.6939646Z java.lang.RuntimeException: Could not instantiate instance using default factory.2020-03-26T08:17:43.6940153Z at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.&lt;init&gt;(StreamingKafkaITCase.java:72)2020-03-26T08:17:43.6940485Z 2020-03-26T08:17:44.0270048Z [INFO] 2020-03-26T08:17:44.0270457Z [INFO] Results:2020-03-26T08:17:44.0270649Z [INFO] 2020-03-26T08:17:44.0270863Z [ERROR] Errors: 2020-03-26T08:17:44.0271847Z [ERROR] StreamingKafkaITCase.&lt;init&gt;:72 Â» Runtime Could not instantiate instance using ...2020-03-26T08:17:44.0272651Z [ERROR] StreamingKafkaITCase.&lt;init&gt;:72 Â» Runtime Could not instantiate instance using ...2020-03-26T08:17:44.0273487Z [ERROR] StreamingKafkaITCase.&lt;init&gt;:72 Â» Runtime Could not instantiate instance using ...2020-03-26T08:17:44.0274218Z [INFO] 2020-03-26T08:17:44.0274517Z [ERROR] Tests run: 3, Failures: 0, Errors: 3, Skipped: 0</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-27 01:00:00" id="16834" opendate="2020-3-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Examples cannot be run from IDE</summary>
      <description>Due to removing the dependency flink-clients from flink-streaming-java, the examples can no longer be executed from the IDE. The problem is that the flink-clients dependency is missing.In order to solve this problem, we need to add the flink-clients dependency to all modules which need it and previously obtained it transitively from flink-streaming-java.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-table-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-table-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-27 01:00:00" id="16837" opendate="2020-3-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable trimStackTrace in surefire plugin</summary>
      <description>Surefire has a trimStackTrace option (enabled by default) which is supposed to cut off the junit parts of stacktraces.However this has various unfortunate side-effects, such as being overzealous and hiding important bits of the stacktrace and hiding suppressed exceptions.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-30 01:00:00" id="16858" opendate="2020-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose partitioned by grammar</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.resources.org.apache.flink.sql.parser.utils.ParserResource.properties</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.validate.FlinkSqlConformance.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.utils.ParserResource.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-30 01:00:00" id="16859" opendate="2020-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce FileSystemTableFactory, FileSystemTableSource, FileSystemTableSink</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TestSinkUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-31 01:00:00" id="16885" opendate="2020-3-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL hive-connector wilcard excludes don&amp;#39;t work on maven 3.1.X</summary>
      <description>The sql-connector-hive modules added in FLINK-16455 use wildcards imports to exclude all transitive dependencies from hive.This is a maven 3.2.1+ feature. This may imply that Flink cannot be properly built anymore with maven 3.1 .</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-31 01:00:00" id="16887" opendate="2020-3-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor retraction rules to support inferring ChangelogMode</summary>
      <description>Current retraction machanism only support 2 message kinds (+ and -). However, since FLIP-95, we will introduce more message kinds to users (insert/delete/update_before/update_after). In order to support that, we should first refactor current retraction rules to support ChangelogMode inference. In previous, every node will be attached with a AccMode trait after retraction rule. In the proposed design, we will infer ChangelogMode trait for every node. Design documentation: https://docs.google.com/document/d/1n_iXIQsKT3uiBqENR8j8RdjRhZfzMhhB66QZvx2rFjE/edit?ts=5e8419c1#</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.trait.retractionTraitDefs.scala</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.UpdatableTopNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.TopNFunctionTestBase.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.RetractableTopNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.rank.AppendOnlyTopNFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepLastRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepFirstRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepLastRowFunctionTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.UpdatableTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.RetractableTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.AppendOnlyTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.AbstractTopNFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.MiniBatchDeduplicateKeepLastRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateKeepLastRowFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.deduplicate.DeduplicateFunctionHelper.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.MiniBatchGroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.MiniBatchGlobalGroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.GroupTableAggFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.validation.OverWindowValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.DeduplicateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.physical.stream.RetractionRulesWithTwoStageAggTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.physical.stream.RetractionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.stream.sql.validation.MatchRecognizeValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SortLimitTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.ModifiedMonotonicityTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.RetractionRulesWithTwoStageAggTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.RetractionRulesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.sinks.DataStreamTableSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.UpdatingPlanChecker.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelTreeWriterImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RankProcessStrategy.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.ExecNodePlanDumper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.trait.TraitUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.trait.retractionTraits.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.DataStreamQueryOperation.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicity.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonIntermediateTableScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalcBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCorrelateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDeduplicate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGlobalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecIncrementalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecIntermediateTableScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLocalGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLookupJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMatch.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMiniBatchAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTemporalJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTemporalSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecValues.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecWatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecWindowJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalRel.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkUpdateAsRetractionTraitInitProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.StreamOptimizeContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.RelNodeBlock.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.FlinkExpandConversionRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecRankRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecRetractionRules.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecSortLimitRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.TwoStageOptimizedAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.DataStreamTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.IntermediateRelTable.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-3 01:00:00" id="16951" opendate="2020-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate parquet to file system connector</summary>
      <description>Implement ParquetFileSystemFormatFactory</description>
      <version>1.11.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.RowPartitionComputer.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-3 01:00:00" id="16961" opendate="2020-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Netty 4 to 4.1.44</summary>
      <description>https://nvd.nist.gov/vuln/detail/CVE-2019-20444https://nvd.nist.gov/vuln/detail/CVE-2019-20445</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
      <file type="M">flink-connectors.flink-hbase.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-3 01:00:00" id="16962" opendate="2020-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Convert DatadogReporter to plugin</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DatadogHttpReporter.java</file>
      <file type="M">docs.monitoring.metrics.zh.md</file>
      <file type="M">docs.monitoring.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-3 01:00:00" id="16965" opendate="2020-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Convert Graphite reporter to plugin</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-graphite.src.main.java.org.apache.flink.metrics.graphite.GraphiteReporter.java</file>
      <file type="M">docs.monitoring.metrics.zh.md</file>
      <file type="M">docs.monitoring.metrics.md</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2020-6-3 01:00:00" id="16975" opendate="2020-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add docs for FileSystem connector</summary>
      <description/>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemOptions.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-3 01:00:00" id="16976" opendate="2020-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update chinese documentation for ListCheckpointed deprecation</summary>
      <description>The change for the english documentation is in https://github.com/apache/flink/commit/10aadfc6906a1629f7e60eacf087e351ba40d517The original Jira issue is FLINK-6258.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.state.state.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-4 01:00:00" id="16979" opendate="2020-4-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-sql-connector-hive-1.2.2_2.11 doesn&amp;#39;t compile on JDK11</summary>
      <description>Both the jdk11 compile and e2e test failed in this nightly: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7052&amp;view=logs&amp;j=946871de-358d-5815-3994-8175615bc253&amp;t=a4536961-0635-5533-730b-7dc5e128220e[ERROR] Failed to execute goal on project flink-sql-connector-hive-1.2.2_2.11: Could not resolve dependencies for project org.apache.flink:flink-sql-connector-hive-1.2.2_2.11:jar:1.11-SNAPSHOT: Could not find artifact jdk.tools:jdk.tools:jar:1.6 at specified path /usr/lib/jvm/adoptopenjdk-11-hotspot-amd64/../lib/tools.jar -&gt; [Help 1]</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-13 01:00:00" id="1698" opendate="2015-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add polynomial base feature mapper to ML library</summary>
      <description>Add feature mapper which maps a vector into the polynomial feature space. This can be used as a preprocessing step prior to applying a Learner of Flink's ML library.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.regression.RegressionData.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.regression.MultipleLinearRegressionSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.package.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseMatrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.Transformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.ParameterMap.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.FlinkTools.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  <bug fixdate="2020-5-6 01:00:00" id="16997" opendate="2020-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add new factory interfaces and utilities</summary>
      <description>Adds the factory interfaces and necessary utilities for discovering and configuring connectors.This issue will provide a reference implementation how factories, connectors, and formats play together.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.Catalog.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.catalog.completeness.py</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-5-6 01:00:00" id="17004" opendate="2020-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document the CREATE TABLE ... LIKE syntax in english</summary>
      <description>Document the CREATE TABLE ... LIKE syntax in the Flink's documentation.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-6 01:00:00" id="17005" opendate="2020-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate the CREATE TABLE ... LIKE syntax documentation to Chinese</summary>
      <description>Translate the page created in FLINK-17004</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-6 01:00:00" id="17009" opendate="2020-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fold API-agnostic documentation into DataStream documentation</summary>
      <description>As per FLIP-42, we want to move most cross-API documentation to the DataStream section and deprecate the DataSet API in the future.We want to go from Project Build Setup Basic API Concepts Streaming (DataStream API) Batch (DataSet API) Table API &amp; SQL Data Types &amp; Serialization Managing Execution Libraries Best Practices API Migration GuidesTo DataStream API Table API / SQL DataSet API</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.scala.api.extensions.zh.md</file>
      <file type="M">docs.dev.scala.api.extensions.md</file>
      <file type="M">docs.dev.java.lambdas.zh.md</file>
      <file type="M">docs.dev.datastream.api.md</file>
      <file type="M">docs.tutorials.datastream.api.md</file>
      <file type="M">docs.redirects.basic.api.concepts.md</file>
      <file type="M">docs.monitoring.metrics.md</file>
      <file type="M">docs.index.md</file>
      <file type="M">docs.dev.table.common.md</file>
      <file type="M">docs.dev.stream.operators.windows.md</file>
      <file type="M">docs.dev.stream.operators.index.md</file>
      <file type="M">docs.dev.parallel.md</file>
      <file type="M">docs.dev.java.lambdas.md</file>
      <file type="M">docs.dev.connectors.cassandra.md</file>
      <file type="M">docs.dev.batch.index.md</file>
      <file type="M">docs.dev.batch.hadoop.compatibility.md</file>
      <file type="M">docs.dev.stream.state.state.md</file>
      <file type="M">docs.dev.user.defined.functions.md</file>
      <file type="M">docs.dev.types.serialization.md</file>
      <file type="M">docs.dev.api.concepts.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-6 01:00:00" id="17010" opendate="2020-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming File Sink s3 end-to-end test fails with "Output hash mismatch"</summary>
      <description>CI: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7099&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f52020-04-06T13:17:38.2460013Z Digest: sha256:a61ed0bca213081b64be94c5e1b402ea58bc549f457c2682a86704dd55231e092020-04-06T13:17:38.2475230Z Status: Downloaded newer image for stedolan/jq:latest2020-04-06T13:18:00.4459693Z Number of produced values 13124/600002020-04-06T13:18:25.3214772Z Number of produced values 18300/600002020-04-06T13:19:06.9767370Z Number of produced values 45366/600002020-04-06T13:20:01.2846102Z Number of produced values 60000/600002020-04-06T13:20:02.5940091Z Cancelling job ff95cd4fd52d10b6540c03cf72b33111.2020-04-06T13:20:03.7862792Z Cancelled job ff95cd4fd52d10b6540c03cf72b33111.2020-04-06T13:20:03.8343709Z Waiting for job (ff95cd4fd52d10b6540c03cf72b33111) to reach terminal state CANCELED ...2020-04-06T13:20:05.8474817Z Job (ff95cd4fd52d10b6540c03cf72b33111) reached terminal state CANCELED2020-04-06T13:20:08.6987955Z FAIL File Streaming Sink: Output hash mismatch. Got 61bb5f161b859759a9829516d96e2bbc, expected 6727342fdd3aae2129e61fc8f433fb6f.2020-04-06T13:20:08.6989364Z head hexdump of actual:2020-04-06T13:20:08.7288989Z 0000000 C o m p l e t e d 2 . 0 K i2020-04-06T13:20:08.7289917Z 0000010 B / 3 4 0 . 7 K i B ( 5 . 32020-04-06T13:20:08.7293001Z 0000020 K i B / s ) w i t h 1 1 02020-04-06T13:20:08.7298661Z 0000030 f i l e ( s ) r e m a i n i2020-04-06T13:20:08.7299371Z 0000040 n g \r d o w n l o a d : s 3 :2020-04-06T13:20:08.7301377Z 0000050 / / f l i n k - i n t e g r a t2020-04-06T13:20:08.7302336Z 0000060 i o n - t e s t s / t e m p / t2020-04-06T13:20:08.7303021Z 0000070 e s t _ s t r e a m i n g _ f i2020-04-06T13:20:08.7303968Z 0000080 l e _ s i n k - 7 b 0 7 7 2 1 22020-04-06T13:20:08.7304790Z 0000090 - d 9 f 8 - 4 0 d 8 - 9 d 0 a -2020-04-06T13:20:08.7305297Z 00000a0 f f 9 f 7 e 9 6 d 7 b d / 0 / p2020-04-06T13:20:08.7306285Z 00000b0 a r t - 2 - 1 t o h o s t d2020-04-06T13:20:08.7307138Z 00000c0 i r / t e m p - t e s t - d i r2020-04-06T13:20:08.7307891Z 00000d0 e c t o r y - 3 5 0 6 5 0 6 7 22020-04-06T13:20:08.7308402Z 00000e0 8 9 / t e m p / t e s t _ s t r2020-04-06T13:20:08.7308870Z 00000f0 e a m i n g _ f i l e _ s i n k2020-04-06T13:20:08.7309579Z 0000100 - 7 b 0 7 7 2 1 2 - d 9 f 8 - 42020-04-06T13:20:08.7310295Z 0000110 0 d 8 - 9 d 0 a - f f 9 f 7 e 92020-04-06T13:20:08.7311022Z 0000120 6 d 7 b d / 0 / p a r t - 2 - 12020-04-06T13:20:08.7311537Z 0000130 \n C o m p l e t e d 2 . 0 K2020-04-06T13:20:08.7312010Z 0000140 i B / 3 4 0 . 7 K i B ( 5 .2020-04-06T13:20:08.7312461Z 0000150 3 K i B / s ) w i t h 1 02020-04-06T13:20:08.7312930Z 0000160 9 f i l e ( s ) r e m a i n2020-04-06T13:20:08.7313393Z 0000170 i n g \r C o m p l e t e d 4 .2020-04-06T13:20:08.7313844Z 0000180 4 K i B / 3 4 0 . 7 K i B 2020-04-06T13:20:08.7314332Z 0000190 ( 9 . 8 K i B / s ) w i t h2020-04-06T13:20:08.7314785Z 00001a0 1 0 9 f i l e ( s ) r e m2020-04-06T13:20:08.7315236Z 00001b0 a i n i n g \r d o w n l o a d :2020-04-06T13:20:08.7315957Z 00001c0 s 3 : / / f l i n k - i n t e2020-04-06T13:20:08.7316672Z 00001d0 g r a t i o n - t e s t s / t e2020-04-06T13:20:08.7317163Z 00001e0 m p / t e s t _ s t r e a m i n2020-04-06T13:20:08.7317869Z 00001f0 g _ f i l e _ s i n k - 7 b 0 72020-04-06T13:20:08.7318579Z 0000200 7 2 1 2 - d 9 f 8 - 4 0 d 8 - 92020-04-06T13:20:08.7319283Z 0000210 d 0 a - f f 9 f 7 e 9 6 d 7 b d2020-04-06T13:20:08.7320032Z 0000220 / 0 / p a r t - 2 - 0 t o h2020-04-06T13:20:08.7320747Z 0000230 o s t d i r / t e m p - t e s t2020-04-06T13:20:08.7321447Z 0000240 - d i r e c t o r y - 3 5 0 6 52020-04-06T13:20:08.7321955Z 0000250 0 6 7 2 8 9 / t e m p / t e s t2020-04-06T13:20:08.7322758Z 0000260 _ s t r e a m i n g _ f i l e _2020-04-06T13:20:08.7323476Z 0000270 s i n k - 7 b 0 7 7 2 1 2 - d 92020-04-06T13:20:08.7324210Z 0000280 f 8 - 4 0 d 8 - 9 d 0 a - f f 92020-04-06T13:20:08.7324690Z 0000290 f 7 e 9 6 d 7 b d / 0 / p a r t2020-04-06T13:20:08.7325360Z 00002a0 - 2 - 0 \n C o m p l e t e d 42020-04-06T13:20:08.7325861Z 00002b0 . 4 K i B / 3 4 0 . 7 K i B2020-04-06T13:20:08.7326493Z 00002c0 ( 9 . 8 K i B / s ) w i t2020-04-06T13:20:08.7327076Z 00002d0 h 1 0 8 f i l e ( s ) r e2020-04-06T13:20:08.7327550Z 00002e0 m a i n i n g \r C o m p l e t e2020-04-06T13:20:08.7328001Z 00002f0 d 8 . 1 K i B / 3 4 0 . 7 2020-04-06T13:20:08.7328465Z 0000300 K i B ( 1 7 . 6 K i B / s )2020-04-06T13:20:08.7328933Z 0000310 w i t h 1 0 8 f i l e ( s2020-04-06T13:20:08.7329401Z 0000320 ) r e m a i n i n g \r d o w n2020-04-06T13:20:08.7329889Z 0000330 l o a d : s 3 : / / f l i n k2020-04-06T13:20:08.7330629Z 0000340 - i n t e g r a t i o n - t e s2020-04-06T13:20:08.7331129Z 0000350 t s / t e m p / t e s t _ s t r2020-04-06T13:20:08.7331608Z 0000360 e a m i n g _ f i l e _ s i n k2020-04-06T13:20:08.7332294Z 0000370 - 7 b 0 7 7 2 1 2 - d 9 f 8 - 42020-04-06T13:20:08.7333106Z 0000380 0 d 8 - 9 d 0 a - f f 9 f 7 e 92020-04-06T13:20:08.7333822Z 0000390 6 d 7 b d / 1 / p a r t - 2 - 12020-04-06T13:20:08.7334310Z 00003a0 2 t o h o s t d i r / t e m2020-04-06T13:20:08.7334982Z 00003b0 p - t e s t - d i r e c t o r y2020-04-06T13:20:08.7335680Z 00003c0 - 3 5 0 6 5 0 6 7 2 8 9 / t e m2020-04-06T13:20:08.7336186Z 00003d0 p / t e s t _ s t r e a m i n g2020-04-06T13:20:08.7336967Z 00003e0 _ f i l e _ s i n k - 7 b 0 7 72020-04-06T13:20:08.7338105Z 00003f0 2 1 2 - d 9 f 8 - 4 0 d 8 - 9 d2020-04-06T13:20:08.7338895Z 0000400 0 a - f f 9 f 7 e 9 6 d 7 b d /2020-04-06T13:20:08.7339640Z 0000410 1 / p a r t - 2 - 1 2 \n C o m p2020-04-06T13:20:08.7340137Z 0000420 l e t e d 8 . 1 K i B / 3 42020-04-06T13:20:08.7340611Z 0000430 0 . 7 K i B ( 1 7 . 6 K i2020-04-06T13:20:08.7341079Z 0000440 B / s ) w i t h 1 0 7 f i2020-04-06T13:20:08.7341534Z 0000450 l e ( s ) r e m a i n i n g \r2020-04-06T13:20:08.7341997Z 0000460 C o m p l e t e d 1 1 . 1 K2020-04-06T13:20:08.7342446Z 0000470 i B / 3 4 0 . 7 K i B ( 2 22020-04-06T13:20:08.7342912Z 0000480 . 9 K i B / s ) w i t h 12020-04-06T13:20:08.7343382Z 0000490 0 7 f i l e ( s ) r e m a i2020-04-06T13:20:08.7343869Z 00004a0 n i n g \r d o w n l o a d : s2020-04-06T13:20:08.7344593Z 00004b0 3 : / / f l i n k - i n t e g r2020-04-06T13:20:08.7345322Z 00004c0 a t i o n - t e s t s / t e m p2020-04-06T13:20:08.7345834Z 00004d0 / t e s t _ s t r e a m i n g _2020-04-06T13:20:08.7346517Z 00004e0 f i l e _ s i n k - 7 b 0 7 7 22020-04-06T13:20:08.7347225Z 00004f0 1 2 - d 9 f 8 - 4 0 d 8 - 9 d 02020-04-06T13:20:08.7348173Z 0000500 a - f f 9 f 7 e 9 6 d 7 b d / 02020-04-06T13:20:08.7348985Z 0000510 / p a r t - 2 - 1 0 t o h o2020-04-06T13:20:08.7349719Z 0000520 s t d i r / t e m p - t e s t -2020-04-06T13:20:08.7350442Z 0000530 d i r e c t o r y - 3 5 0 6 5 02020-04-06T13:20:08.7351115Z 0000540 6 7 2 8 9 / t e m p / t e s t _2020-04-06T13:20:08.7352710Z 0000550 s t r e a m i n g _ f i l e _ s2020-04-06T13:20:08.7353469Z 0000560 i n k - 7 b 0 7 7 2 1 2 - d 9 f2020-04-06T13:20:08.7354013Z 0000570 8 - 4 0 d 8 - 9 d 0 a - f f 9 f2020-04-06T13:20:08.7360064Z 0000580 7 e 9 6 d 7 b d / 0 / p a r t -2020-04-06T13:20:08.7360656Z 0000590 2 - 1 0 \n C o m p l e t e d 12020-04-06T13:20:08.7361032Z 00005a0 1 . 1 K i B / 3 4 0 . 7 K i2020-04-06T13:20:08.7361380Z 00005b0 B ( 2 2 . 9 K i B / s ) w2020-04-06T13:20:08.7361741Z 00005c0 i t h 1 0 6 f i l e ( s ) 2020-04-06T13:20:08.7362090Z 00005d0 r e m a i n i n g \r C o m p l e2020-04-06T13:20:08.7362599Z 00005e0 t e d 1 4 . 1 K i B / 3 4 02020-04-06T13:20:08.7362959Z 00005f0 . 7 K i B ( 2 8 . 9 K i B2020-04-06T13:20:08.7363310Z 0000600 / s ) w i t h 1 0 6 f i l2020-04-06T13:20:08.7363671Z 0000610 e ( s ) r e m a i n i n g \r d2020-04-06T13:20:08.7364043Z 0000620 o w n l o a d : s 3 : / / f l2020-04-06T13:20:08.7364639Z 0000630 i n k - i n t e g r a t i o n -2020-04-06T13:20:08.7364990Z 0000640 t e s t s / t e m p / t e s t _2020-04-06T13:20:08.7365351Z 0000650 s t r e a m i n g _ f i l e _ s2020-04-06T13:20:08.7365884Z 0000660 i n k - 7 b 0 7 7 2 1 2 - d 9 f2020-04-06T13:20:08.7366403Z 0000670 8 - 4 0 d 8 - 9 d 0 a - f f 9 f2020-04-06T13:20:08.7366931Z 0000680 7 e 9 6 d 7 b d / 0 / p a r t -2020-04-06T13:20:08.7367456Z 0000690 2 - 9 t o h o s t d i r / t2020-04-06T13:20:08.7367990Z 00006a0 e m p - t e s t - d i r e c t o2020-04-06T13:20:08.7368507Z 00006b0 r y - 3 5 0 6 5 0 6 7 2 8 9 / t2020-04-06T13:20:08.7368870Z 00006c0 e m p / t e s t _ s t r e a m i2020-04-06T13:20:08.7369399Z 00006d0 n g _ f i l e _ s i n k - 7 b 02020-04-06T13:20:08.7369919Z 00006e0 7 7 2 1 2 - d 9 f 8 - 4 0 d 8 -2020-04-06T13:20:08.7370453Z 00006f0 9 d 0 a - f f 9 f 7 e 9 6 d 7 b2020-04-06T13:20:08.7370969Z 0000700 d / 0 / p a r t - 2 - 9 \n C o m2020-04-06T13:20:08.7371334Z 0000710 p l e t e d 1 4 . 1 K i B /2020-04-06T13:20:08.7371679Z 0000720 3 4 0 . 7 K i B ( 2 8 . 9 2020-04-06T13:20:08.7372043Z 0000730 K i B / s ) w i t h 1 0 5 2020-04-06T13:20:08.7372406Z 0000740 f i l e ( s ) r e m a i n i n2020-04-06T13:20:08.7372751Z 0000750 g \r C o m p l e t e d 1 5 . 12020-04-06T13:20:08.7373109Z 0000760 K i B / 3 4 0 . 7 K i B (2020-04-06T13:20:08.7373603Z 0000770 3 0 . 2 K i B / s ) w i t h2020-04-06T13:20:08.7373961Z 0000780 1 0 5 f i l e ( s ) r e m2020-04-06T13:20:08.7374320Z 0000790 a i n i n g \r C o m p l e t e d2020-04-06T13:20:08.7374664Z 00007a0 1 7 . 5 K i B / 3 4 0 . 7 2020-04-06T13:20:08.7375021Z 00007b0 K i B ( 3 4 . 9 K i B / s )2020-04-06T13:20:08.7375363Z 00007c0 w i t h 1 0 5 f i l e ( s2020-04-06T13:20:08.7375855Z 00007d0 ) r e m a i n i n g \r d o w n2020-04-06T13:20:08.7376278Z 00007e0 l o a d : s 3 : / / f l i n k2020-04-06T13:20:08.7376875Z 00007f0 - i n t e g r a t i o n - t e s2020-04-06T13:20:08.7377242Z 0000800 t s / t e m p / t e s t _ s t r2020-04-06T13:20:08.7377588Z 0000810 e a m i n g _ f i l e _ s i n k2020-04-06T13:20:08.7378125Z 0000820 - 7 b 0 7 7 2 1 2 - d 9 f 8 - 42020-04-06T13:20:08.7378641Z 0000830 0 d 8 - 9 d 0 a - f f 9 f 7 e 92020-04-06T13:20:08.7379170Z 0000840 6 d 7 b d / 0 / p a r t - 2 - 12020-04-06T13:20:08.7379517Z 0000850 3 t o h o s t d i r / t e m2020-04-06T13:20:08.7380042Z 0000860 p - t e s t - d i r e c t o r y2020-04-06T13:20:08.7380579Z 0000870 - 3 5 0 6 5 0 6 7 2 8 9 / t e m2020-04-06T13:20:08.7380931Z 0000880 p / t e s t _ s t r e a m i n g2020-04-06T13:20:08.7381460Z 0000890 _ f i l e _ s i n k - 7 b 0 7 72020-04-06T13:20:08.7381975Z 00008a0 2 1 2 - d 9 f 8 - 4 0 d 8 - 9 d2020-04-06T13:20:08.7382506Z 00008b0 0 a - f f 9 f 7 e 9 6 d 7 b d /2020-04-06T13:20:08.7383035Z 00008c0 0 / p a r t - 2 - 1 3 \n C o m p2020-04-06T13:20:08.7383382Z 00008d0 l e t e d 1 7 . 5 K i B / 32020-04-06T13:20:08.7383740Z 00008e0 4 0 . 7 K i B ( 3 4 . 9 K2020-04-06T13:20:08.7384084Z 00008f0 i B / s ) w i t h 1 0 4 f2020-04-06T13:20:08.7384443Z 0000900 i l e ( s ) r e m a i n i n g2020-04-06T13:20:08.7384817Z 0000910 \r d o w n l o a d : s 3 : / /2020-04-06T13:20:08.7385378Z 0000920 f l i n k - i n t e g r a t i o2020-04-06T13:20:08.7385915Z 0000930 n - t e s t s / t e m p / t e s2020-04-06T13:20:08.7387030Z 0000940 t _ s t r e a m i n g _ f i l e2020-04-06T13:20:08.7387666Z 0000950 _ s i n k - 7 b 0 7 7 2 1 2 - d2020-04-06T13:20:08.7388178Z 0000960 9 f 8 - 4 0 d 8 - 9 d 0 a - f f2020-04-06T13:20:08.7388542Z 0000970 9 f 7 e 9 6 d 7 b d / 1 / p a r2020-04-06T13:20:08.7389057Z 0000980 t - 2 - 1 t o h o s t d i r2020-04-06T13:20:08.7389585Z 0000990 / t e m p - t e s t - d i r e c2020-04-06T13:20:08.7390117Z 00009a0 t o r y - 3 5 0 6 5 0 6 7 2 8 92020-04-06T13:20:08.7390476Z 00009b0 / t e m p / t e s t _ s t r e a2020-04-06T13:20:08.7391006Z 00009c0 m i n g _ f i l e _ s i n k - 72020-04-06T13:20:08.7391591Z 00009d0 b 0 7 7 2 1 2 - d 9 f 8 - 4 0 d2020-04-06T13:20:08.7392129Z 00009e0 8 - 9 d 0 a - f f 9 f 7 e 9 6 d2020-04-06T13:20:08.7392662Z 00009f0 7 b d / 1 / p a r t - 2 - 1 \n C2020-04-06T13:20:08.7393012Z 0000a00 o m p l e t e d 1 7 . 5 K i2020-04-06T13:20:08.7393372Z 0000a10 B / 3 4 0 . 7 K i B ( 3 4 .2020-04-06T13:20:08.7393716Z 0000a20 9 K i B / s ) w i t h 1 02020-04-06T13:20:08.7394073Z 0000a30 3 f i l e ( s ) r e m a i n2020-04-06T13:20:08.7394418Z 0000a40 i n g \r C o m p l e t e d 2 12020-04-06T13:20:08.7394895Z 0000a50 . 3 K i B / 3 4 0 . 7 K i B2020-04-06T13:20:08.7395257Z 0000a60 ( 4 1 . 9 K i B / s ) w i2020-04-06T13:20:08.7395655Z 0000a70 t h 1 0 3 f i l e ( s ) r2020-04-06T13:20:08.7396013Z 0000a80 e m a i n i n g \r d o w n l o a2020-04-06T13:20:08.7396587Z 0000a90 d : s 3 : / / f l i n k - i n2020-04-06T13:20:08.7397265Z 0000aa0 t e g r a t i o n - t e s t s /2020-04-06T13:20:08.7397945Z 0000ab0 t e m p / t e s t _ s t r e a m2020-04-06T13:20:08.7398579Z 0000ac0 i n g _ f i l e _ s i n k - 7 b2020-04-06T13:20:08.7399155Z 0000ad0 0 7 7 2 1 2 - d 9 f 8 - 4 0 d 82020-04-06T13:20:08.7399709Z 0000ae0 - 9 d 0 a - f f 9 f 7 e 9 6 d 72020-04-06T13:20:08.7400276Z 0000af0 b d / 0 / p a r t - 2 - 8 t o2020-04-06T13:20:08.7400835Z 0000b00 h o s t d i r / t e m p - t e2020-04-06T13:20:08.7401405Z 0000b10 s t - d i r e c t o r y - 3 5 02020-04-06T13:20:08.7401876Z 0000b20 6 5 0 6 7 2 8 9 / t e m p / t e2020-04-06T13:20:08.7402338Z 0000b30 s t _ s t r e a m i n g _ f i l2020-04-06T13:20:08.7402888Z 0000b40 e _ s i n k - 7 b 0 7 7 2 1 2 -2020-04-06T13:20:08.7403408Z 0000b50 d 9 f 8 - 4 0 d 8 - 9 d 0 a - f2020-04-06T13:20:08.7403772Z 0000b60 f 9 f 7 e 9 6 d 7 b d / 0 / p a2020-04-06T13:20:08.7404284Z 0000b70 r t - 2 - 8 \n C o m p l e t e d2020-04-06T13:20:08.7404647Z 0000b80 2 1 . 3 K i B / 3 4 0 . 7 2020-04-06T13:20:08.7405008Z 0000b90 K i B ( 4 1 . 9 K i B / s )2020-04-06T13:20:08.7405358Z 0000ba0 w i t h 1 0 2 f i l e ( s2020-04-06T13:20:08.7405721Z 0000bb0 ) r e m a i n i n g \r C o m p2020-04-06T13:20:08.7406065Z 0000bc0 l e t e d 2 5 . 0 K i B / 32020-04-06T13:20:08.7406673Z 0000bd0 4 0 . 7 K i B ( 4 8 . 9 K2020-04-06T13:20:08.7407014Z 0000be0 i B / s ) w i t h 1 0 2 f2020-04-06T13:20:08.7407371Z 0000bf0 i l e ( s ) r e m a i n i n g2020-04-06T13:20:08.7407757Z 0000c00 \r d o w n l o a d : s 3 : / /2020-04-06T13:20:08.7408367Z 0000c10 f l i n k - i n t e g r a t i o2020-04-06T13:20:08.7408893Z 0000c20 n - t e s t s / t e m p / t e s2020-04-06T13:20:08.7409226Z 0000c30 t _ s t r e a m i n g _ f i l e2020-04-06T13:20:08.7409750Z 0000c40 _ s i n k - 7 b 0 7 7 2 1 2 - d2020-04-06T13:20:08.7410268Z 0000c50 9 f 8 - 4 0 d 8 - 9 d 0 a - f f2020-04-06T13:20:08.7410630Z 0000c60 9 f 7 e 9 6 d 7 b d / 0 / p a r2020-04-06T13:20:08.7411158Z 0000c70 t - 2 - 1 1 t o h o s t d i2020-04-06T13:20:08.7411673Z 0000c80 r / t e m p - t e s t - d i r e2020-04-06T13:20:08.7412206Z 0000c90 c t o r y - 3 5 0 6 5 0 6 7 2 82020-04-06T13:20:08.7412553Z 0000ca0 9 / t e m p / t e s t _ s t r e2020-04-06T13:20:08.7413082Z 0000cb0 a m i n g _ f i l e _ s i n k -2020-04-06T13:20:08.7413613Z 0000cc0 7 b 0 7 7 2 1 2 - d 9 f 8 - 4 02020-04-06T13:20:08.7414131Z 0000cd0 d 8 - 9 d 0 a - f f 9 f 7 e 9 62020-04-06T13:20:08.7414815Z 0000ce0 d 7 b d / 0 / p a r t - 2 - 1 12020-04-06T13:20:08.7415245Z 0000cf0 \n C o m p l e t e d 2 5 . 0 2020-04-06T13:20:08.7415604Z 0000d00 K i B / 3 4 0 . 7 K i B ( 42020-04-06T13:20:08.7415949Z 0000d10 8 . 9 K i B / s ) w i t h 2020-04-06T13:20:08.7416309Z 0000d20 1 0 1 f i l e ( s ) r e m a2020-04-06T13:20:08.7416668Z 0000d30 i n i n g \r C o m p l e t e d 2020-04-06T13:20:08.7417013Z 0000d40 2 8 . 6 K i B / 3 4 0 . 7 K2020-04-06T13:20:08.7417370Z 0000d50 i B ( 5 4 . 4 K i B / s ) 2020-04-06T13:20:08.7417714Z 0000d60 w i t h 1 0 1 f i l e ( s )2020-04-06T13:20:08.7418076Z 0000d70 r e m a i n i n g \r d o w n l2020-04-06T13:20:08.7418648Z 0000d80 o a d : s 3 : / / f l i n k -2020-04-06T13:20:08.7419198Z 0000d90 i n t e g r a t i o n - t e s t2020-04-06T13:20:08.7419567Z 0000da0 s / t e m p / t e s t _ s t r e2020-04-06T13:20:08.7420079Z 0000db0 a m i n g _ f i l e _ s i n k -2020-04-06T13:20:08.7420614Z 0000dc0 7 b 0 7 7 2 1 2 - d 9 f 8 - 4 02020-04-06T13:20:08.7421247Z 0000dd0 d 8 - 9 d 0 a - f f 9 f 7 e 9 62020-04-06T13:20:08.7421942Z 0000de0 d 7 b d / 1 / p a r t - 2 - 1 32020-04-06T13:20:08.7422343Z 0000df0 t o h o s t d i r / t e m p2020-04-06T13:20:08.7422945Z 0000e00 - t e s t - d i r e c t o r y -2020-04-06T13:20:08.7423359Z 0000e10 3 5 0 6 5 0 6 7 2 8 9 / t e m p2020-04-06T13:20:08.7423759Z 0000e20 / t e s t _ s t r e a m i n g _2020-04-06T13:20:08.7424370Z 0000e30 f i l e _ s i n k - 7 b 0 7 7 22020-04-06T13:20:08.7424968Z 0000e40 1 2 - d 9 f 8 - 4 0 d 8 - 9 d 02020-04-06T13:20:08.7425577Z 0000e50 a - f f 9 f 7 e 9 6 d 7 b d / 12020-04-06T13:20:08.7426180Z 0000e60 / p a r t - 2 - 1 3 \n 2020-04-06T13:20:08.7426460Z 0000e6b2020-04-06T13:20:13.1615895Z rm: cannot remove '/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/lib/flink-shaded-netty-tcnative-static-*.jar': No such file or directory2020-04-06T13:20:13.3069822Z 5c0f753ccbc8092ee92b422b9efc0e2f8b4895f4512cd28ab6e59b995cabacad2020-04-06T13:20:13.3541676Z 5c0f753ccbc8092ee92b422b9efc0e2f8b4895f4512cd28ab6e59b995cabacad2020-04-06T13:20:13.3570646Z [FAIL] Test script contains errors.2020-04-06T13:20:13.3577141Z Checking of logs skipped.2020-04-06T13:20:13.3577527Z 2020-04-06T13:20:13.3578665Z [FAIL] 'Streaming File Sink s3 end-to-end test' failed after 4 minutes and 38 seconds! Test exited with exit code 1</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.s3.operations.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-7 01:00:00" id="17014" opendate="2020-4-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement PipelinedRegionSchedulingStrategy</summary>
      <description>The PipelinedRegionSchedulingStrategy submits one pipelined region to the DefaultScheduler each time. The PipelinedRegionSchedulingStrategy must be aware of the inputs of each pipelined region. It should schedule a region if and only if all the inputs of that region become consumable.PipelinedRegionSchedulingStrategy can implement as below: startScheduling() : schedule all source regions one by one. onPartitionConsumable(partition) : Check all the consumer regions of the notified partition, if all the inputs of a region have turned to be consumable, schedule the region restartTasks(tasksToRestart) : find out all regions which contain the tasks to restart, reschedule those whose inputs are all consumable</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulingResultPartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.SchedulingStrategyUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulingTopology.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-7 01:00:00" id="17026" opendate="2020-4-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce a new Kafka connector with new property keys</summary>
      <description>This new Kafka connector should use new interfaces proposed by FLIP-95, e.g. DynamicTableSource, DynamicTableSink, and Factory.The new proposed keys :Old keyNew keyNoteconnector.typeconnector connector.versionN/Amerged into 'connector' keyconnector.topictopic connector.properties.zookeeper.connectproperties.zookeeper.connect connector.properties.bootstrap.serversproperties.bootstrap.servers connector.properties.group.idproperties.group.id connector.startup-modescan.startup.mode connector.specific-offsetsscan.startup.specific-offsets connector.startup-timestamp-millisscan.startup.timestamp-millis connector.sink-partitionersink.partitioner"fixed", or "round-robin", or a class name "org.mycompany.MyPartitioner"connector.sink-partitioner-classN/Amerged into 'sink.partitioner', not needed anymoreformat.typeformat   </description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.TestFormatFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka011TableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010TableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-5-7 01:00:00" id="17030" opendate="2020-4-7 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add primary key syntax to DDL</summary>
      <description>FLIP-87 defines the concept of primary keys and syntax for defining them in the DDL. The new syntax needs to be supported in the DDL.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlTableColumn.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.api.TableSchemaTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.TableSchemaUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.TableSchema.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-8 01:00:00" id="17049" opendate="2020-4-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support implicit conversions of rows</summary>
      <description>@Test public void testCastRowTableFunction() throws Exception { final List&lt;Row&gt; sourceData = Arrays.asList( Row.of(Row.of(1)) ); final List&lt;Row&gt; sinkData = Arrays.asList( Row.of(Row.of(new BigDecimal("1"))) ); TestCollectionTableFactory.reset(); TestCollectionTableFactory.initData(sourceData); tEnv().sqlUpdate("CREATE TABLE SourceTable(s Row(a INT)) WITH ('connector' = 'COLLECTION')"); tEnv().sqlUpdate("CREATE TABLE SinkTable(s ROW(a DECIMAL(10, 2))) WITH ('connector' = 'COLLECTION')"); tEnv().createTemporarySystemFunction("func", RowCastScalarFunction.class); tEnv().sqlUpdate("INSERT INTO SinkTable SELECT func(s) FROM SourceTable"); tEnv().execute("Test Job"); assertThat(TestCollectionTableFactory.getResult(), equalTo(sinkData)); } public static class RowCastScalarFunction extends ScalarFunction { public @DataTypeHint("ROW&lt;f0 INT&gt;") Row eval( @DataTypeHint("ROW&lt;f0 DECIMAL(10, 2)&gt;") Row row) { return Row.of(1); } }fails with:java.lang.AssertionError: use createStructType() instead at org.apache.calcite.sql.type.SqlTypeFactoryImpl.assertBasic(SqlTypeFactoryImpl.java:225) at org.apache.calcite.sql.type.SqlTypeFactoryImpl.createSqlType(SqlTypeFactoryImpl.java:48) at org.apache.flink.table.planner.calcite.FlinkTypeFactory.createSqlType(FlinkTypeFactory.scala:275) at org.apache.calcite.sql.SqlBasicTypeNameSpec.deriveType(SqlBasicTypeNameSpec.java:205) at org.apache.calcite.sql.SqlDataTypeSpec.deriveType(SqlDataTypeSpec.java:222) at org.apache.calcite.sql.SqlDataTypeSpec.deriveType(SqlDataTypeSpec.java:209) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5960) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5845) at org.apache.calcite.sql.SqlDataTypeSpec.accept(SqlDataTypeSpec.java:186) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1800) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1785) at org.apache.calcite.sql.SqlNode.validateExpr(SqlNode.java:260) at org.apache.calcite.sql.SqlOperator.validateCall(SqlOperator.java:423) at org.apache.calcite.sql.SqlFunction.validateCall(SqlFunction.java:199) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateCall(SqlValidatorImpl.java:5552) at org.apache.calcite.sql.SqlCall.validate(SqlCall.java:116) at org.apache.calcite.sql.SqlNode.validateExpr(SqlNode.java:259) at org.apache.calcite.sql.SqlOperator.validateCall(SqlOperator.java:423) at org.apache.calcite.sql.SqlFunction.validateCall(SqlFunction.java:199) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateCall(SqlValidatorImpl.java:5552) at org.apache.calcite.sql.SqlCall.validate(SqlCall.java:116) at org.apache.calcite.sql.SqlNode.validateExpr(SqlNode.java:259) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateExpr(SqlValidatorImpl.java:4306) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelectList(SqlValidatorImpl.java:4284) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3523) at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084) at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059) at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:128) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:107) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:134) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:351) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:149) at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:66) at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:596) at org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.testCastRowTableFunction(FunctionITCase.java:632) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.intellij.rt.execution.application.AppMainV2.main(AppMainV2.java:131)I think the problem is in org.apache.flink.table.planner.functions.inference.TypeInferenceOperandChecker#castTo</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.RowTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-4-10 01:00:00" id="17081" opendate="2020-4-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Batch test classes in Blink planner does not extend TestLogger</summary>
      <description>Currently streaming test classes in Blink planner has extended TestLogger while batch test classes hasn't. They should also extend TestLogger for better debugging.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.BatchAbstractTestBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.api.TableUtilsStreamingITCase.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2020-5-14 01:00:00" id="17130" opendate="2020-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web UI: Enable listing JM Logs and displaying Logs by Filename</summary>
      <description>add log list and read log by name for jobmanager in the web</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.log-detail.task-manager-log-detail.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager-routing.module.ts</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-14 01:00:00" id="17131" opendate="2020-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Javadocs broken for master</summary>
      <description>The javadocs for the master branch aren't being displayed on the website for some reason.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-14 01:00:00" id="17136" opendate="2020-4-14 00:00:00" resolution="Done">
    <buginformation>
      <summary>Rename toplevel DataSet/DataStream section titles</summary>
      <description>According to FLIP-42: Streaming (DataStream API) -&gt; DataStream API Batch (DataSet API) -&gt; DataSet API</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.datastream.api.zh.md</file>
      <file type="M">docs.dev.datastream.api.md</file>
      <file type="M">docs.dev.batch.index.zh.md</file>
      <file type="M">docs.dev.batch.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-14 01:00:00" id="17142" opendate="2020-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump ORC version</summary>
      <description>The current dependency version of ORC is 1.4.3 which is little outdated so bump the version to 1.5.6.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-formats.flink-orc.pom.xml</file>
      <file type="M">flink-formats.flink-orc-nohive.src.main.java.org.apache.flink.orc.nohive.vector.AbstractOrcNoHiveVector.java</file>
      <file type="M">flink-formats.flink-orc-nohive.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-15 01:00:00" id="17148" opendate="2020-4-15 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support converting pandas dataframe to flink table</summary>
      <description>As the title described, the aim of this Jira is to add support of converting a pandas dataframe to a flink table in PyFlink.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.RowDataArrowReaderWriterTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.RowArrowReaderWriterTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.arrow.ArrowUtilsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.arrow.AbstractArrowPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.RowDataArrowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.ArrowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.arrow.ArrowPythonScalarFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.ArrowUtils.java</file>
      <file type="M">flink-python.pyflink.table.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.types.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.py</file>
      <file type="M">docs.dev.table.python.index.zh.md</file>
      <file type="M">docs.dev.table.python.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-15 01:00:00" id="17149" opendate="2020-4-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce Debezium format to support reading debezium changelogs</summary>
      <description>Introduce DebeziumFormatFactory and DebeziumRowDeserializationSchema to read debezium changelogs.CREATE TABLE my_table ( ...) WITH ( 'connector'='...', -- e.g. 'kafka' 'format'='debezium-json', 'debezium-json.schema-include'='true' -- default false, Debeizum can be configured to include or exclude the message schema 'debezium-json.ignore-parse-errors'='true' -- default false);</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-formats.flink-json.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-18 01:00:00" id="1716" opendate="2015-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CoCoA algorithm to flink-ml</summary>
      <description>Add the communication efficient distributed dual coordinate ascent algorithm to the flink machine learning library. See CoCoA for the implementation details.I propose to first implement it with hinge loss and l2-norm. This way, it will allow us to train SVMs in parallel.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.recommendation.ALSITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.SparseVectorSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.SparseMatrixSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.regression.MultipleLinearRegression.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.recommendation.ALS.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Vector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.SparseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.SparseMatrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Matrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseMatrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.Transformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.Learner.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.LabeledVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.FlinkTools.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.ChainedTransformer.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.ChainedLearner.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-18 01:00:00" id="1717" opendate="2015-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support to read libSVM and SVMLight input files</summary>
      <description>In order to train SVMs, the machine learning library should be able to read standard SVM input file formats. A widespread format is used by libSVM and SMVLight which has the following format:&lt;line&gt; .=. &lt;target&gt; &lt;feature&gt;:&lt;value&gt; &lt;feature&gt;:&lt;value&gt; ... &lt;feature&gt;:&lt;value&gt; # &lt;info&gt;&lt;target&gt; .=. +1 | -1 | 0 | &lt;float&gt; &lt;feature&gt; .=. &lt;integer&gt; | "qid"&lt;value&gt; .=. &lt;float&gt;&lt;info&gt; .=. &lt;string&gt;Details can be found here and here</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.regression.RegressionData.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.feature.PolynomialBaseITSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.regression.MultipleLinearRegression.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.SparseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.SparseMatrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseMatrix.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.feature.PolynomialBase.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.common.LabeledVector.scala</file>
      <file type="M">flink-staging.flink-ml.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-15 01:00:00" id="17172" opendate="2020-4-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-enable debug level logging in Jepsen Tests</summary>
      <description>Since log4j2 was enabled, logs in Jepsen tests are on INFO level. We should re-enable debug level logging in Jepsen Tests.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.db.clj</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-17 01:00:00" id="17209" opendate="2020-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow users to specify dialect in sql-client yaml</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableConfig.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.TableConfigOptions.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.ExecutionEntry.java</file>
      <file type="M">docs..includes.generated.table.config.configuration.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-17 01:00:00" id="17223" opendate="2020-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>System.IO.IOException: No space left on device in misc profile on free Azure builders</summary>
      <description>Builds on the free Azure builders are failing with##[error]Unhandled exception. System.IO.IOException: No space left on device at System.IO.FileStream.WriteNative(ReadOnlySpan`1 source) at System.IO.FileStream.FlushWriteBuffer() at System.IO.FileStream.Flush(Boolean flushToDisk) at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder) at System.Diagnostics.TextWriterTraceListener.Flush() at System.Diagnostics.TraceSource.Flush() at Microsoft.VisualStudio.Services.Agent.TraceManager.Dispose(Boolean disposing) at Microsoft.VisualStudio.Services.Agent.TraceManager.Dispose() at Microsoft.VisualStudio.Services.Agent.HostContext.Dispose(Boolean disposing) at Microsoft.VisualStudio.Services.Agent.HostContext.Dispose() at Microsoft.VisualStudio.Services.Agent.Worker.Program.Main(String[] args)Error reported in diagnostic logs. Please examine the log for more details. - /home/vsts/agents/2.165.2/_diag/Worker_20200414-093250-utc.logSystem.IO.IOException: No space left on device at System.IO.FileStream.WriteNative(ReadOnlySpan`1 source) at System.IO.FileStream.FlushWriteBuffer() at System.IO.FileStream.Flush(Boolean flushToDisk) at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder) at System.Diagnostics.TextWriterTraceListener.Flush() at Microsoft.VisualStudio.Services.Agent.HostTraceListener.WriteHeader(String source, TraceEventType eventType, Int32 id) at Microsoft.VisualStudio.Services.Agent.HostTraceListener.TraceEvent(TraceEventCache eventCache, String source, TraceEventType eventType, Int32 id, String message) at System.Diagnostics.TraceSource.TraceEvent(TraceEventType eventType, Int32 id, String message) at Microsoft.VisualStudio.Services.Agent.Worker.Worker.RunAsync(String pipeIn, String pipeOut) at Microsoft.VisualStudio.Services.Agent.Worker.Program.MainAsync(IHostContext context, String[] args)System.IO.IOException: No space left on device at System.IO.FileStream.WriteNative(ReadOnlySpan`1 source) at System.IO.FileStream.FlushWriteBuffer() at System.IO.FileStream.Flush(Boolean flushToDisk) at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder) at System.Diagnostics.TextWriterTraceListener.Flush() at Microsoft.VisualStudio.Services.Agent.HostTraceListener.WriteHeader(String source, TraceEventType eventType, Int32 id) at Microsoft.VisualStudio.Services.Agent.HostTraceListener.TraceEvent(TraceEventCache eventCache, String source, TraceEventType eventType, Int32 id, String message) at System.Diagnostics.TraceSource.TraceEvent(TraceEventType eventType, Int32 id, String message) at Microsoft.VisualStudio.Services.Agent.Tracing.Error(Exception exception) at Microsoft.VisualStudio.Services.Agent.Worker.Program.MainAsync(IHostContext context, String[] args),##[error]The job running on agent Azure Pipelines 9 ran longer than the maximum time of 240 minutes. For more information, see https://go.microsoft.com/fwlink/?linkid=2077134,##[warning]Agent Azure Pipelines 9 did not respond to a cancelation request with 00:01:00.CI run: https://dev.azure.com/chesnay/flink/_build/results?buildId=205&amp;view=logs&amp;j=764762df-f65b-572b-3d5c-65518c777be4</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-17 01:00:00" id="17226" opendate="2020-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Prometheus relocations</summary>
      <description>Now that we load the Prometheus reporters as plugins we should remove the shade-plugin configuration/relocations.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-prometheus.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-17 01:00:00" id="17227" opendate="2020-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Datadog relocations</summary>
      <description>Now that we load the Datadog reporter as a plugin we should remove the shade-plugin configuration/relocations.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-datadog.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-19 01:00:00" id="17236" opendate="2020-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add new Training section to Documentation</summary>
      <description>This section will contain pages of content contributed from Ververica's Flink training website.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.concepts.stream-processing.md</file>
      <file type="M">docs.concepts.index.zh.md</file>
      <file type="M">docs.concepts.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-19 01:00:00" id="17237" opendate="2020-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Intro to DataStream API page to Training section</summary>
      <description>This page should contain a basic introduction, a complete example, and a pointer to the RideCleansing exercise.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.redirects.tutorials.datastream.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-21 01:00:00" id="17286" opendate="2020-4-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate json to file system connector</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.filesystem.RowPartitionComputerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.SingleDirectoryWriter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.PartitionTempFileManager.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.PartitionPathUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.PartitionLoader.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.GroupedPartitionWriter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSource.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableSink.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.DynamicPartitionWriter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.FileSystemITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.utils.TestRowDataCsvInputFormat.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.utils.TestCsvFileSystemFormatFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.FileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.ParquetFileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcFileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-formats.flink-json.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.DelimitedInputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-21 01:00:00" id="17287" opendate="2020-4-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable merge commit button</summary>
      <description>Make use of the .asf.yaml feature to disable the GitHub merge commit button.Ideally we just drop this into all repos for consistency.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.create.source.release.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-4-22 01:00:00" id="17325" opendate="2020-4-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate orc to file system connector</summary>
      <description>Integrate orc to file system connector, so in the sql world, users can create file system table with orc format by DDL, do some reading, writing and streaming writing. And the RowData is the sql data format. The works are: Introduce OrcRowDataInputFormat with partition support. Introduce RowDataVectorizer. Introduce OrcFileSystemFormatFactory.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-orc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-23 01:00:00" id="17330" opendate="2020-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid scheduling deadlocks caused by cyclic input dependencies between regions</summary>
      <description>Imagine a job like this:A &amp;#8211; (pipelined FORWARD) --&gt; B &amp;#8211; (blocking ALL-to-ALL) --&gt; DA &amp;#8211; (pipelined FORWARD) --&gt; C &amp;#8211; (pipelined FORWARD) --&gt; Dparallelism=2 for all vertices.We will have 2 execution pipelined regions:R1 = {A1, B1, C1, D1}R2 = {A2, B2, C2, D2}R1 has a cross-region input edge (B2-&gt;D1).R2 has a cross-region input edge (B1-&gt;D2).Scheduling deadlock will happen since we schedule a region only when all its inputs are consumable (i.e. blocking partitions to be finished). This is because R1 can be scheduled only if R2 finishes, while R2 can be scheduled only if R1 finishes.To avoid this, one solution is to force a logical pipelined region with intra-region ALL-to-ALL blocking edges to form one only execution pipelined region, so that there would not be cyclic input dependency between regions.Besides that, we should also pay attention to avoid cyclic cross-region POINTWISE blocking edges.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.PipelinedRegionComputeUtilTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.PipelinedRegionComputeUtil.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-23 01:00:00" id="17352" opendate="2020-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>All doc links w/ site.baseurl &amp; link tag are broken</summary>
      <description>Using {{ site.baseurl }}{% link foo.md %}creates a link containing something likehttps://ci.apache.org/projects/flink/flink-docs-master//ci.apache.org/projects/flink/flink-docs-master/foo.htmlThe link tag includes site.baseurl, so no need to include it again.</description>
      <version>1.11.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.tutorials.streaming.analytics.zh.md</file>
      <file type="M">docs.tutorials.streaming.analytics.md</file>
      <file type="M">docs.tutorials.fault.tolerance.zh.md</file>
      <file type="M">docs.tutorials.fault.tolerance.md</file>
      <file type="M">docs.tutorials.event.driven.zh.md</file>
      <file type="M">docs.tutorials.event.driven.md</file>
      <file type="M">docs.tutorials.etl.zh.md</file>
      <file type="M">docs.tutorials.etl.md</file>
      <file type="M">docs.tutorials.datastream.api.zh.md</file>
      <file type="M">docs.tutorials.datastream.api.md</file>
      <file type="M">docs.dev.stream.state.state.md</file>
      <file type="M">docs.dev.stream.state.index.md</file>
      <file type="M">docs.dev.stream.state.broadcast.state.md</file>
      <file type="M">docs.dev.event.time.md</file>
      <file type="M">docs.concepts.timely-stream-processing.zh.md</file>
      <file type="M">docs.concepts.timely-stream-processing.md</file>
      <file type="M">docs.concepts.stateful-stream-processing.zh.md</file>
      <file type="M">docs.concepts.stateful-stream-processing.md</file>
      <file type="M">docs.concepts.index.zh.md</file>
      <file type="M">docs.concepts.index.md</file>
      <file type="M">docs.concepts.flink-architecture.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-18 01:00:00" id="1737" opendate="2015-3-18 00:00:00" resolution="Done">
    <buginformation>
      <summary>Add statistical whitening transformation to machine learning library</summary>
      <description>The statistical whitening transformation &amp;#91;1&amp;#93; is a preprocessing step for different ML algorithms. It decorrelates the individual dimensions and sets its variance to 1.Statistical whitening should be implemented as a Transfomer.Resources:&amp;#91;1&amp;#93; http://en.wikipedia.org/wiki/Whitening_transformation</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.SparseVectorSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.math.DenseVectorSuite.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.Vector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.SparseVector.scala</file>
      <file type="M">flink-staging.flink-ml.src.main.scala.org.apache.flink.ml.math.DenseVector.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-6-26 01:00:00" id="17383" opendate="2020-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink legacy planner should not use CollectionEnvironment any more</summary>
      <description>As discussed in https://github.com/apache/flink/pull/11794，CollectionEnvironment is not a good practice, as it is not going through all the steps that a regular user program would go. We should change the tests to use LocalEnvironment. commit "Introduce CollectionPipelineExecutor for CollectionEnvironment (c983ac9)" should also be reverted at that moment.</description>
      <version>1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.utils.TableProgramsCollectionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.TableEnvironmentITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-26 01:00:00" id="17387" opendate="2020-4-26 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement LookupableTableSource for Hive connector</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemOptions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-26 01:00:00" id="17391" opendate="2020-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>sink.rolling-policy.time.interval default value should be bigger</summary>
      <description>Otherwise there is a lot of small files.We should also consider sin.rolling-policy.file.size</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-27 01:00:00" id="17395" opendate="2020-4-27 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add the sign and shasum logic for PyFlink wheel packages</summary>
      <description>Add the sign and sha logic for PyFlink wheel packages</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.create.binary.release.sh</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-3-18 01:00:00" id="1740" opendate="2015-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>StreamExecutionEnvironment is not respecting the setNumberOfExecutionRetries()</summary>
      <description>The value is not passed to the runtime.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.StreamGraph.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-27 01:00:00" id="17401" opendate="2020-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Labels to Mesos TM taskinfo object</summary>
      <description>Currently labels are not set in the task info object. A lot of companies can have logic specific to labels on TM. For example in criteo, based on labels we set kerberos env variables and mesos debug capabilities. It is critical for us to be able to pass these label values to the task manager.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParametersTest.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerDriverTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.LaunchableMesosWorker.java</file>
      <file type="M">docs.layouts.shortcodes.generated.mesos.task.manager.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-27 01:00:00" id="17404" opendate="2020-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Running HA per-job cluster (rocks, non-incremental) gets stuck killing a non-existing pid in Hadoop 3 build profile</summary>
      <description>CI log: https://api.travis-ci.org/v3/job/678609505/log.txtWaiting for text Completed checkpoint [1-9]* for job 00000000000000000000000000000000 to appear 2 of times in logs...grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directorygrep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directoryStarting standalonejob daemon on host travis-job-e606668f-b674-49c0-8590-e3508e22b99d.grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directorygrep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directoryKilled TM @ 18864kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]Killed TM @ No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#build-times-out-because-no-output-was-receivedThe build has been terminated</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-27 01:00:00" id="17406" opendate="2020-4-27 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add documentation about dynamic table options</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
      <file type="M">docs.dev.table.sql.index.zh.md</file>
      <file type="M">docs.dev.table.sql.index.md</file>
      <file type="M">docs.dev.table.config.zh.md</file>
      <file type="M">docs.dev.table.config.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-27 01:00:00" id="17408" opendate="2020-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce GPUDriver</summary>
      <description>Introduce GPUDriver for GPU resource.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-28 01:00:00" id="17423" opendate="2020-4-28 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Python UDTF in blink planner under batch mode</summary>
      <description>Currently, Python UDTF has been supported under flink planner and blink planner(only stream). This jira dedicates to add Python UDTF support for blink planner under batch mode.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-python.pyflink.table.tests.test.udtf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">docs.dev.table.python.python.udfs.zh.md</file>
      <file type="M">docs.dev.table.python.python.udfs.md</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-28 01:00:00" id="17424" opendate="2020-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client end-to-end test (Old planner) Elasticsearch (v7.5.1) failed due to download error</summary>
      <description>`SQL Client end-to-end test (Old planner) Elasticsearch (v7.5.1)` failed in release-1.10 crone job with below error:Preparing Elasticsearch(version=7)...Downloading Elasticsearch from https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.5.1-linux-x86_64.tar.gz ... % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 4 276M 4 13.3M 0 0 28.8M 0 0:00:09 --:--:-- 0:00:09 28.8M 42 276M 42 117M 0 0 80.7M 0 0:00:03 0:00:01 0:00:02 80.7M 70 276M 70 196M 0 0 79.9M 0 0:00:03 0:00:02 0:00:01 79.9M 89 276M 89 248M 0 0 82.3M 0 0:00:03 0:00:03 --:--:-- 82.4Mcurl: (56) GnuTLS recv error (-54): Error in the pull function. % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0curl: (7) Failed to connect to localhost port 9200: Connection refused[FAIL] Test script contains errors.https://api.travis-ci.org/v3/job/680222168/log.txt</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.elasticsearch-common.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-5-28 01:00:00" id="17428" opendate="2020-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support SupportsProjectionPushDown in planner</summary>
      <description>Support the SupportsProjectionPushDown interface for ScanTableSource.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.testTableSourceSinks.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamingTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.ScanTableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.ScanTableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoLegacyTableSourceScanRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-28 01:00:00" id="17431" opendate="2020-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement table DDLs for Hive dialect part 1</summary>
      <description>Will cover CREATE, DROP, DESCRIBE, SHOW table in this ticket.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.type.ExtendedSqlRowTypeNameSpec.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.type.ExtendedSqlCollectionTypeNameSpec.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.dql.SqlRichDescribeTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.constraint.SqlTableConstraint.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlCreateHiveDatabase.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlAlterHiveDatabaseOwner.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.SqlAlterHiveDatabase.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.HiveDDLUtils.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-28 01:00:00" id="17432" opendate="2020-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename Tutorials to Training</summary>
      <description>Change Tutorials to Training in the sidebar navigation and headings, and change the URL path as well. The motivation for this change is SEO – folks looking for this kind of content are more likely to be searching for training. </description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.tutorials.streaming.analytics.zh.md</file>
      <file type="M">docs.tutorials.streaming.analytics.md</file>
      <file type="M">docs.tutorials.index.zh.md</file>
      <file type="M">docs.tutorials.index.md</file>
      <file type="M">docs.tutorials.fault.tolerance.zh.md</file>
      <file type="M">docs.tutorials.fault.tolerance.md</file>
      <file type="M">docs.tutorials.event.driven.zh.md</file>
      <file type="M">docs.tutorials.event.driven.md</file>
      <file type="M">docs.tutorials.etl.zh.md</file>
      <file type="M">docs.tutorials.etl.md</file>
      <file type="M">docs.tutorials.datastream.api.zh.md</file>
      <file type="M">docs.tutorials.datastream.api.md</file>
      <file type="M">docs.concepts.index.zh.md</file>
      <file type="M">docs.concepts.index.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-6-29 01:00:00" id="17465" opendate="2020-4-29 00:00:00" resolution="Done">
    <buginformation>
      <summary>Update Chinese user documentation for job manager memory model</summary>
      <description>This is a follow-up for FLINK-16946.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.memory.mem.tuning.zh.md</file>
      <file type="M">docs.ops.memory.mem.trouble.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.tm.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.jobmanager.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.zh.md</file>
      <file type="M">docs.ops.memory.mem.migration.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-29 01:00:00" id="17467" opendate="2020-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement aligned savepoint in UC mode</summary>
      <description/>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-stream-stateful-job-upgrade-test.src.main.java.org.apache.flink.streaming.tests.StatefulStreamJobUpgradeTestProgram.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.MockSubtaskCheckpointCoordinatorBuilder.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.InputProcessorUtil.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierTracker.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierAligner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.TestInputChannel.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.serialization.EventSerializerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointOptionsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.Buffer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.CheckpointBarrier.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointOptions.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointedInputGate.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierUnaligner.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-30 01:00:00" id="17471" opendate="2020-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move LICENSE and NOTICE files to root directory of python distribution</summary>
      <description>This is observed and proposed by Robert during 1.10.1 RC1 check:Another question that I had while checking the release was the"apache-flink-1.10.1.tar.gz" binary, which I suppose is the pythondistribution.It does not contain a LICENSE and NOTICE file at the root level (which isokay [1] for binary releases), but in the "pyflink/" directory. There isalso a "deps/" directory, which contains a full distribution of Flink,without any license files.I believe it would be a little bit nicer to have the LICENSE and NOTICEfile in the root directory (if the python wheels format permits) to makesure it is obvious that all binary release contents are covered by thesefiles.http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/VOTE-Release-1-10-1-release-candidate-1-tp40724p40910.html</description>
      <version>1.9.3,1.10.0,1.11.0</version>
      <fixedVersion>1.9.4,1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.MANIFEST.in</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-30 01:00:00" id="17483" opendate="2020-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update flink-sql-connector-elasticsearch7 NOTICE file to correctly reflect bundled dependencies</summary>
      <description>This issue is found during 1.10.1 RC1 check by Robert, that `com.carrotsearch:hppc` and `com.github:mustachejava` were included into the shaded binary to fix FLINK-16170 but not added into the NOTICE file of flink-sql-connector-elasticsearch7 module. More details please refer to the ML discussion thread.</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-3 01:00:00" id="17497" opendate="2020-5-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Quickstarts Java nightly end-to-end test fails with "class file has wrong version 55.0, should be 52.0"</summary>
      <description>CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=540&amp;view=logs&amp;j=08866332-78f7-59e4-4f7e-49a56faa3179&amp;t=931b3127-d6ee-5f94-e204-48d51cd1c334[ERROR] -&gt; [Help 1][ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureExceptionjava.io.FileNotFoundException: flink-quickstart-java-0.1.jar (No such file or directory) at java.util.zip.ZipFile.open(Native Method) at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:230) at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:160) at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:131) at sun.tools.jar.Main.list(Main.java:1115) at sun.tools.jar.Main.run(Main.java:293) at sun.tools.jar.Main.main(Main.java:1288)Success: There are no flink core classes are contained in the jar.Failure: Since Elasticsearch5SinkExample.class and other user classes are not included in the jar. [FAIL] Test script contains errors.Checking for errors...No errors in log files.Checking for exceptions...No exceptions in log files.Checking for non-empty .out files...grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*.out: No such file or directoryNo non-empty .out files.[FAIL] 'Quickstarts Java nightly end-to-end test' failed after 0 minutes and 6 seconds! Test exited with exit code 1</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.quickstarts.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-5 01:00:00" id="17526" opendate="2020-5-5 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support AVRO serialization and deseriazation schema for RowData type</summary>
      <description>Add support AvroRowDataDeserializationSchema and AvroRowDataSerializationSchema for the new data structure RowData.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-avro.src.test.resources.avro.user.avsc</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.typeutils.AvroSchemaConverter.java</file>
      <file type="M">flink-formats.flink-avro.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-6 01:00:00" id="17536" opendate="2020-5-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change the config option of slot max limitation to "slotmanager.number-of-slots.max"</summary>
      <description/>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ResourceManagerOptions.java</file>
      <file type="M">docs..includes.generated.resource.manager.configuration.html</file>
      <file type="M">docs..includes.generated.expert.scheduling.section.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-6 01:00:00" id="17543" opendate="2020-5-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rerunning failed azure jobs fails when uploading logs</summary>
      <description>No LastRequestResponse on exception ArtifactExistsException: Artifact logs-ci-tests already exists for build 677.</description>
      <version>None</version>
      <fixedVersion>1.11.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.watchdog.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-6-8 01:00:00" id="17581" opendate="2020-5-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update translation of S3 documentation</summary>
      <description>The change in https://github.com/apache/flink/commit/7c5ac3584e42a0e7ebc5e78c532887bf4d383d9d needs to be added to the Chinese variant of the documentation page.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.filesystems.s3.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-8 01:00:00" id="17582" opendate="2020-5-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update quickstarts to use universal Kafka connector</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-6-10 01:00:00" id="17599" opendate="2020-5-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documents due to FLIP-84</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.index.md</file>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.streaming.query.configuration.zh.md</file>
      <file type="M">docs.dev.table.streaming.query.configuration.md</file>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
      <file type="M">docs.dev.table.sql.insert.zh.md</file>
      <file type="M">docs.dev.table.sql.insert.md</file>
      <file type="M">docs.dev.table.sql.index.zh.md</file>
      <file type="M">docs.dev.table.sql.drop.zh.md</file>
      <file type="M">docs.dev.table.sql.drop.md</file>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.sql.alter.zh.md</file>
      <file type="M">docs.dev.table.sql.alter.md</file>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
      <file type="M">docs.dev.table.common.zh.md</file>
      <file type="M">docs.dev.table.common.md</file>
      <file type="M">docs.dev.table.catalogs.zh.md</file>
      <file type="M">docs.dev.table.catalogs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-11 01:00:00" id="17604" opendate="2020-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement format factory for CSV serialization and deseriazation schema of RowData type</summary>
      <description/>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-csv.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-11 01:00:00" id="17606" opendate="2020-5-11 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce DataGen connector in table</summary>
      <description>CREATE TABLE user (    id BIGINT,    age INT,    description STRING) WITH (    'connector' = 'datagen',    'rows-per-second'='100',    ' fields.id.kind' = 'sequence',    'fields.id.start' = '1',    'fields.age.kind' = 'random',    'fields.age.min' = '0',    'fields.age.max' = '100',    'fields.description.kind' = 'random',    'fields.description.length' = '100')-- Default is random generator.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.pom.xml</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.StatefulSequenceSourceTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-11 01:00:00" id="17608" opendate="2020-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add TM log and stdout page back</summary>
      <description>According to the discussion in https://github.com/apache/flink/pull/11731#issuecomment-620048458TM log and stdout page should be added in order not to break the previous user experience.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.task-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.thread-dump.task-manager-thread-dump.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.task-manager.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.task-manager-routing.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.status.task-manager-status.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.log-detail.task-manager-log-detail.component.ts</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-11 01:00:00" id="17614" opendate="2020-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add project ITCase for partition table in filesystem connector</summary>
      <description>create table partitionedTable ( x string, y int, a int, b bigint ) partitioned by (a, b) with ( 'connector' = 'filesystem', ... ）Add ITCase that project field from general field(x, y) and partition key field(a, b) to validate project and default partition value works well like:check( "select y, b, x from partitionedTable where a=3", Seq( row(17, 1, "x17"), row(18, 2, "x18"), row(19, 3, "x19") ))</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.FileSystemITCaseBase.scala</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-11 01:00:00" id="17616" opendate="2020-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Temporarily increase akka.ask.timeout in TPC-DS e2e test</summary>
      <description>Until FLINK-17558 is fixed, we should increase the akka.ask.timeout in the e2e test to mitigate FLINK-17194</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-8-11 01:00:00" id="17621" opendate="2020-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use default akka.ask.timeout in TPC-DS e2e test</summary>
      <description>Revert the changes in FLINK-17616</description>
      <version>1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-11 01:00:00" id="17622" opendate="2020-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove useless switch for decimal in PostresCatalog</summary>
      <description>Remove the useless switch for decimal fields. The Postgres JDBC connector translate them to numeric</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalogTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalog.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-12 01:00:00" id="17628" opendate="2020-5-12 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Remove the unnecessary py4j log information</summary>
      <description>Currently the py4j will print the INFO level logging information to the console. It is unnecessary for users. We should set the level to WARN. </description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.java.gateway.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-12 01:00:00" id="17629" opendate="2020-5-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement format factory for JSON serialization and deserialization schema</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDataSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDataDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-12 01:00:00" id="17634" opendate="2020-5-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reject multiple handler registrations under the same URL</summary>
      <description>In FLINK-11853 a handler was added the is being registered under the same URL as another handler. This should never happen, and we should add a check to ensure this doesn't happen again.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-12 01:00:00" id="17635" opendate="2020-5-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about view support</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
      <file type="M">docs.dev.table.sql.index.zh.md</file>
      <file type="M">docs.dev.table.sql.index.md</file>
      <file type="M">docs.dev.table.sql.drop.zh.md</file>
      <file type="M">docs.dev.table.sql.drop.md</file>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-13 01:00:00" id="17646" opendate="2020-5-13 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Reduce the python package size of PyFlink</summary>
      <description>Currently the python package size of PyFlink has increased to about 320MB, which exceeds the size limit of pypi.org (300MB). We need to remove unnecessary jars to reduce the package size.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-5-13 01:00:00" id="17667" opendate="2020-5-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement INSERT for Hive dialect</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.calcite.CalciteParser.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-13 01:00:00" id="17669" opendate="2020-5-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use new WatermarkStrategy/WatermarkGenerator in Kafka connector</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionStateWithPunctuatedWatermarks.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionStateWithPeriodicWatermarks.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThreadTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka010FetcherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer010.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-5-14 01:00:00" id="17675" opendate="2020-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Resolve CVE-2019-11358 from jquery</summary>
      <description>https://nvd.nist.gov/vuln/detail/CVE-2019-11358</description>
      <version>None</version>
      <fixedVersion>1.11.0,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">NOTICE</file>
      <file type="M">docs.page.js.jquery.min.js</file>
      <file type="M">docs.page.js.flink.js</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-14 01:00:00" id="17679" opendate="2020-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the bug of encoding bytes in cython coder</summary>
      <description>The python bytes b'x\x00\x00\x00' will be transposed to b'\x'. If we use strlen() of c function to compute the length of char*, we will get wrong length. So we need to use the Python function of len() to compute the length. </description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.fast.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.fast.coder.impl.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.fast.coder.impl.pxd</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-14 01:00:00" id="17692" opendate="2020-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"flink-end-to-end-tests/test-scripts/hadoop/yarn.classpath" present after building Flink</summary>
      <description>Some changes introduced in FLINK-11086 cause the "flink-end-to-end-tests/test-scripts/hadoop/yarn.classpath" file to be generated and present in the source tree after building Flink.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-15 01:00:00" id="17707" opendate="2020-5-15 00:00:00" resolution="Done">
    <buginformation>
      <summary>Support configuring replica of Deployment based HA setups</summary>
      <description>At the moment, in the native K8s setups, we hard code the replica of Deployment to 1. However, when users enable the ZooKeeper HighAvailabilityServices, they would like to configure the replica of Deployment also for faster failover.This ticket proposes to make replica of Deployment configurable in the ZooKeeper based HA setups.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParametersTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParameters.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.kubernetes.config.configuration.html</file>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
      <file type="M">docs.content.zh.docs.deployment.resource-providers.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-5-15 01:00:00" id="17722" opendate="2020-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Failed to find the file" in "build_wheels" stage</summary>
      <description>CI https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1343&amp;view=logs&amp;j=fe7ebddc-3e2f-5c50-79ee-226c8653f218&amp;t=b2830442-93c7-50ff-36f4-5b3e2dca8c83Successfully built dill crcmod httplib2 hdfs oauth2client future avro-python3Installing collected packages: six, pbr, mock, dill, typing, crcmod, numpy, pyarrow, python-dateutil, typing-extensions, fastavro, httplib2, protobuf, pymongo, docopt, idna, chardet, urllib3, requests, hdfs, pyparsing, pydot, pyasn1, pyasn1-modules, rsa, oauth2client, grpcio, future, avro-python3, pytz, apache-beam, cythonSuccessfully installed apache-beam-2.19.0 avro-python3-1.9.2.1 chardet-3.0.4 crcmod-1.7 cython-0.29.16 dill-0.3.1.1 docopt-0.6.2 fastavro-0.21.24 future-0.18.2 grpcio-1.29.0 hdfs-2.5.8 httplib2-0.12.0 idna-2.9 mock-2.0.0 numpy-1.18.4 oauth2client-3.0.0 pbr-5.4.5 protobuf-3.11.3 pyarrow-0.15.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydot-1.4.1 pymongo-3.10.1 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2020.1 requests-2.23.0 rsa-4.0 six-1.14.0 typing-3.7.4.1 typing-extensions-3.7.4.2 urllib3-1.25.9+ (( i++ ))+ (( i&lt;3 ))+ (( i=0 ))+ (( i&lt;3 ))+ /home/vsts/work/1/s/flink-python/dev/.conda/envs/3.5/bin/python setup.py bdist_wheelCompiling pyflink/fn_execution/fast_coder_impl.pyx because it changed.Compiling pyflink/fn_execution/fast_operations.pyx because it changed.[1/2] Cythonizing pyflink/fn_execution/fast_coder_impl.pyx[2/2] Cythonizing pyflink/fn_execution/fast_operations.pyxFailed to find the file /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/opt/flink-sql-client_*.jar.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure.controller.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2020-5-15 01:00:00" id="17737" opendate="2020-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>KeyedStateCheckpointingITCase fails in UnalignedCheckpoint mode</summary>
      <description/>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.io.AlternatingCheckpointBarrierHandlerTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.io.AlternatingCheckpointBarrierHandler.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-5-16 01:00:00" id="17764" opendate="2020-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update tips about the default planner when the planner parameter value is not recognized</summary>
      <description>This default planner has been set to blink in the code.However, when the planner parameter value is not recognized, the default planner is prompted to be flink.  </description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.StreamSQLExample.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.StreamSQLExample.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-24 01:00:00" id="1777" opendate="2015-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Java 8 Lambdas with Eclipse documentation</summary>
      <description>The Eclipse JDT compiler team has introduced a compiler flag for us, which is not covered in the Flink documentation yet.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">docs.java8.programming.guide.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-17 01:00:00" id="17771" opendate="2020-5-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"PyFlink end-to-end test" fails with "The output result: [] is not as expected: [2, 3, 4]!" on Java11</summary>
      <description>Java 11 nightly profile: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1579&amp;view=logs&amp;j=6caf31d6-847a-526e-9624-468e053467d6&amp;t=679407b1-ea2c-5965-2c8d-1467777fff88Job has been submitted with JobID ef78030becb3bfd6415d3de2e06420b4java.lang.AssertionError: The output result: [] is not as expected: [2, 3, 4]! at org.apache.flink.python.tests.FlinkStreamPythonUdfSqlJob.main(FlinkStreamPythonUdfSqlJob.java:55) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288) at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198) at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:148) at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:689) at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:227) at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:906) at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:982) at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30) at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:982)Stopping taskexecutor daemon (pid: 2705) on host fv-az670.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-17 01:00:00" id="17773" opendate="2020-5-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation for new WatermarkGenerator/WatermarkStrategies</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.event.timestamp.extractors.md</file>
      <file type="M">docs.dev.event.timestamps.watermarks.md</file>
      <file type="M">docs.dev.event.time.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-17 01:00:00" id="17774" opendate="2020-5-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>supports all kinds of changes for select result</summary>
      <description>FLINK-17252 has supported select query, however only append change is supported. because FLINK-16998 is not finished. This issue aims to support all kinds of changes based on FLINK-16998.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamingRuntimeContext.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.collect.utils.TestCoordinationRequestHandler.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.collect.CollectSinkFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorCoordinator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectSinkOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectSinkFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.collect.CollectCoordinationResponse.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sinks.StreamSelectTableSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sinks.BatchSelectTableSink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.AggregationITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.sinks.StreamSelectTableSink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.sinks.SelectTableSinkSchemaConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.sinks.SelectTableSinkBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.sinks.BatchSelectTableSink.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.utils.PrintUtilsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.utils.PrintUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.PlannerMock.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ModifyOperationVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.delegation.Planner.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableResultImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.SelectTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-17 01:00:00" id="17777" opendate="2020-5-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Mesos Jepsen Tests pass with Hadoop-free Flink</summary>
      <description>Since FLINK-11086, we can no longer build a Flink distribution with Hadoop. Therefore, we need to set the HADOOP_CLASSPATH environment variable for the TM processes.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.db.clj</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-17 01:00:00" id="17779" opendate="2020-5-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Orc file format support filter push down</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcFileSystemFormatFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-18 01:00:00" id="17788" opendate="2020-5-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>scala shell in yarn mode is broken</summary>
      <description>When I start scala shell in yarn mode, one yarn app will be launched, and after I write some flink code and trigger a flink job, another yarn app will be launched but would failed to launch due to some conflicts.</description>
      <version>1.10.1,1.11.0</version>
      <fixedVersion>1.10.2,1.11.0,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-5-18 01:00:00" id="17792" opendate="2020-5-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failing to invoking jstack on TM processes should not fail Jepsen Tests</summary>
      <description>jstack can fail if the JVM process exits prematurely while or before we invoke jstack. If jstack fails, the exception propagates and exits the Jepsen Tests prematurely.</description>
      <version>1.10.1,1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.utils.clj</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-18 01:00:00" id="17795" opendate="2020-5-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an example to show how to leverage GPU resources</summary>
      <description>Add an example to show how to leverage GPU resources.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-5-18 01:00:00" id="17799" opendate="2020-5-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression in all network benchmarks</summary>
      <description>Performance regression took place between May 2nd (commit 18af2a1) and May 15th (commit 282da0dd3e). Unfortunately in this period benchmarking infrastructure was broken, so we do not know on which day has it happened.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-19 01:00:00" id="17802" opendate="2020-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set offset commit only if group id is configured for new Kafka Table source</summary>
      <description>As https://issues.apache.org/jira/browse/FLINK-17619 described,the new Kafka Table source exits same problem and should fix too.note: this  fix both for master and release-1.11 </description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSourceBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-19 01:00:00" id="17809" opendate="2020-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>BashJavaUtil script logic does not work for paths with spaces</summary>
      <description>Multiple paths aren't quoted (class path, conf_dir) resulting in errors if they contain spaces.</description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.taskmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-19 01:00:00" id="17810" opendate="2020-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for K8s application mode</summary>
      <description>Add document for how to start/stop K8s application cluster.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.0,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-19 01:00:00" id="17814" opendate="2020-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate native kubernetes document to Chinese</summary>
      <description>https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/native_kubernetes.html Translate the native kubernetes document to Chinese.English updated in 7723774a0402e10bc914b1fa6128e3c80678dafe</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-19 01:00:00" id="17816" opendate="2020-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change Latency Marker to work with "scheduleAtFixedDelay" instead of "scheduleAtFixedRate"</summary>
      <description>Latency Markers and other periodic timers are scheduled with scheduleAtFixedRate. That means every X time the callable is called. If it blocks (backpressure) is can be called immediately again.I would suggest to switch this to scheduleAtFixedDelay to avoid calling for a lot of latency marker injections when there is no way to actually execute the injection call.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamSource.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-1-20 01:00:00" id="17827" opendate="2020-5-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>scala-shell.sh should fail early if no mode is specified, or have default logging settings</summary>
      <description>The scala-shell has multiple modes it can run in: local, remote and yarn.It is mandatory to specify such a mode, but this is only enforced on the scala side, not in the bash script.The problem is that the scala-shell script derives the log4j properties from the mode, and if no mode is set, then the log4j properties are empty.This leads to a warning from slf4j that no logger was defined and all that.Either scala-shell.sh should fail early if no mode is specified, or it should have some default logging settings (e.g., the ones for local/remote).</description>
      <version>1.11.0</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.start-script.start-scala-shell.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-20 01:00:00" id="17836" opendate="2020-5-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for Hive dim join</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.hive.streaming.zh.md</file>
      <file type="M">docs.dev.table.hive.hive.streaming.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-20 01:00:00" id="17842" opendate="2020-5-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression on 19.05.2020</summary>
      <description>There is a noticeable performance regression in many benchmarks:http://codespeed.dak8s.net:8000/timeline/?ben=serializerHeavyString&amp;env=2http://codespeed.dak8s.net:8000/timeline/?ben=networkThroughput.1000,1ms&amp;env=2http://codespeed.dak8s.net:8000/timeline/?ben=networkThroughput.100,100ms&amp;env=2http://codespeed.dak8s.net:8000/timeline/?ben=globalWindow&amp;env=2that happened on May 19th, probably between 260ef2c and 2f18138</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.NonSpanningWrapper.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.SpanningWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-20 01:00:00" id="17843" opendate="2020-5-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check for RowKind when converting Row to expression</summary>
      <description>A row ctor does not allow for a rowKind thus we should check if the rowKind is set when converting from Row to expression.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.expressions.ObjectToExpressionTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.ApiExpressionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-20 01:00:00" id="17847" opendate="2020-5-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ArrayIndexOutOfBoundsException happens when codegen StreamExec operator</summary>
      <description>user case://source table create table json_table( w_es BIGINT, w_type STRING, w_isDdl BOOLEAN, w_data ARRAY&lt;ROW&lt;pay_info STRING, online_fee DOUBLE, sign STRING, account_pay_fee DOUBLE&gt;&gt;, w_ts TIMESTAMP(3), w_table STRING) WITH ( 'connector.type' = 'kafka', 'connector.version' = '0.10', 'connector.topic' = 'json-test2', 'connector.properties.zookeeper.connect' = 'localhost:2181', 'connector.properties.bootstrap.servers' = 'localhost:9092', 'connector.properties.group.id' = 'test-jdbc', 'connector.startup-mode' = 'earliest-offset', 'format.type' = 'json', 'format.derive-schema' = 'true')// real data:{"w_es":1589870637000,"w_type":"INSERT","w_isDdl":false,"w_data":[{"pay_info":"channelId=82&amp;onlineFee=89.0&amp;outTradeNo=0&amp;payId=0&amp;payType=02&amp;rechargeId=4&amp;totalFee=89.0&amp;tradeStatus=success&amp;userId=32590183789575&amp;sign=00","online_fee":"89.0","sign":"00","account_pay_fee":"0.0"}],"w_ts":"2020-05-20T13:58:37.131Z","w_table":"cccc111"}//queryselect w_ts, 'test' as city1_id, w_data[0].pay_info AS cate3_id, w_data as pay_order_id from json_tableexception://Caused by: java.lang.ArrayIndexOutOfBoundsException: 1427848Caused by: java.lang.ArrayIndexOutOfBoundsException: 1427848 at org.apache.flink.table.runtime.util.SegmentsUtil.getByteMultiSegments(SegmentsUtil.java:598) at org.apache.flink.table.runtime.util.SegmentsUtil.getByte(SegmentsUtil.java:590) at org.apache.flink.table.runtime.util.SegmentsUtil.bitGet(SegmentsUtil.java:534) at org.apache.flink.table.dataformat.BinaryArray.isNullAt(BinaryArray.java:117) at StreamExecCalc$10.processElement(Unknown Source) Looks like in the codegen StreamExecCalc$10 operator some operation visit a '-1' index which should be wrong, this bug exits both in 1.10 and 1.11 public class StreamExecCalc$10 extends org.apache.flink.table.runtime.operators.AbstractProcessStreamOperator implements org.apache.flink.streaming.api.operators.OneInputStreamOperator { private final Object[] references; private final org.apache.flink.table.dataformat.BinaryString str$3 = org.apache.flink.table.dataformat.BinaryString.fromString("test"); private transient org.apache.flink.table.runtime.typeutils.BaseArraySerializer typeSerializer$5; final org.apache.flink.table.dataformat.BoxedWrapperRow out = new org.apache.flink.table.dataformat.BoxedWrapperRow(4); private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null); public StreamExecCalc$10( Object[] references, org.apache.flink.streaming.runtime.tasks.StreamTask task, org.apache.flink.streaming.api.graph.StreamConfig config, org.apache.flink.streaming.api.operators.Output output) throws Exception { this.references = references; typeSerializer$5 = (((org.apache.flink.table.runtime.typeutils.BaseArraySerializer) references[0])); this.setup(task, config, output); } @Override public void open() throws Exception { super.open(); } @Override public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception { org.apache.flink.table.dataformat.BaseRow in1 = (org.apache.flink.table.dataformat.BaseRow) element.getValue(); org.apache.flink.table.dataformat.SqlTimestamp field$2; boolean isNull$2; org.apache.flink.table.dataformat.BaseArray field$4; boolean isNull$4; org.apache.flink.table.dataformat.BaseArray field$6; org.apache.flink.table.dataformat.BinaryString field$8; boolean isNull$8; org.apache.flink.table.dataformat.BinaryString result$9; boolean isNull$9; isNull$2 = in1.isNullAt(4); field$2 = null; if (!isNull$2) { field$2 = in1.getTimestamp(4, 3); } isNull$4 = in1.isNullAt(3); field$4 = null; if (!isNull$4) { field$4 = in1.getArray(3); } field$6 = field$4; if (!isNull$4) { field$6 = (org.apache.flink.table.dataformat.BaseArray) (typeSerializer$5.copy(field$6)); } out.setHeader(in1.getHeader()); if (isNull$2) { out.setNullAt(0); } else { out.setNonPrimitiveValue(0, field$2); } if (false) { out.setNullAt(1); } else { out.setNonPrimitiveValue(1, ((org.apache.flink.table.dataformat.BinaryString) str$3)); } boolean isNull$7 = isNull$4 || false || field$6.isNullAt(((int) 0) - 1); org.apache.flink.table.dataformat.BaseRow result$7 = isNull$7 ? null : field$6.getRow(((int) 0) - 1, 4); if (isNull$7) { result$9 = org.apache.flink.table.dataformat.BinaryString.EMPTY_UTF8; isNull$9 = true; } else { isNull$8 = result$7.isNullAt(0); field$8 = org.apache.flink.table.dataformat.BinaryString.EMPTY_UTF8; if (!isNull$8) { field$8 = result$7.getString(0); } result$9 = field$8; isNull$9 = isNull$8; } if (isNull$9) { out.setNullAt(2); } else { out.setNonPrimitiveValue(2, result$9); } if (isNull$4) { out.setNullAt(3); } else { out.setNonPrimitiveValue(3, field$6); } output.collect(outElement.replace(out)); } @Override public void close() throws Exception { super.close(); }}     </description>
      <version>1.10.0,1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ArrayTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-26 01:00:00" id="1785" opendate="2015-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Master tests in flink-tachyon fail with java.lang.NoSuchFieldError: IBM_JAVA</summary>
      <description>The master fail in flink-tachyon test when running mvn test:------------------------------------------------------- T E S T S-------------------------------------------------------------------------------------------------------------- T E S T S-------------------------------------------------------Running org.apache.flink.tachyon.HDFSTestRunning org.apache.flink.tachyon.TachyonFileSystemWrapperTestjava.lang.NoSuchFieldError: IBM_JAVAat org.apache.hadoop.security.UserGroupInformation.getOSLoginModuleName(UserGroupInformation.java:303)at org.apache.hadoop.security.UserGroupInformation.&lt;clinit&gt;(UserGroupInformation.java:348)at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:807)at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:266)at org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:122)at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:775)at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:642)at org.apache.hadoop.hdfs.MiniDFSCluster.&lt;init&gt;(MiniDFSCluster.java:334)at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:316)at org.apache.flink.tachyon.HDFSTest.createHDFS(HDFSTest.java:62)at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)at java.lang.reflect.Method.invoke(Method.java:606)at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)...Results :Failed tests: HDFSTest.createHDFS:76 Test failed IBM_JAVA HDFSTest.createHDFS:76 Test failed Could not initialize classorg.apache.hadoop.security.UserGroupInformationTests in error: HDFSTest.destroyHDFS:83 NullPointer HDFSTest.destroyHDFS:83 NullPointer TachyonFileSystemWrapperTest.testHadoopLoadability:116 »NoClassDefFound Could...Tests run: 6, Failures: 3, Errors: 3, Skipped: 0</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-tachyon.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-21 01:00:00" id="17867" opendate="2020-5-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Hive-3.1.1 test</summary>
      <description>TableEnvHiveConnectorTest::testOrcSchemaEvol fails due to "ClassNotFoundException: org.apache.hadoop.hdfs.client.HdfsDataOutputStream$SyncFlag"</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-22 01:00:00" id="17870" opendate="2020-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>dependent jars are missing to be shipped to cluster in scala shell</summary>
      <description/>
      <version>1.11.0</version>
      <fixedVersion>1.10.2,1.11.0,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.main.java.org.apache.flink.api.java.ScalaShellStreamEnvironment.java</file>
      <file type="M">flink-scala-shell.src.main.java.org.apache.flink.api.java.ScalaShellEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-22 01:00:00" id="17872" opendate="2020-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update StreamingFileSink documents to add avro formats</summary>
      <description>We added Avro-format for StreamingFileSink in FLINK-11395 , but did not update the document to reflect that.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.streamfile.sink.zh.md</file>
      <file type="M">docs.dev.connectors.streamfile.sink.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-22 01:00:00" id="17881" opendate="2020-5-22 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add documentation for PyFlink&amp;#39;s Windows support</summary>
      <description>Currently PyFlink already supports running on windows in Flink 1.11. But as we drop the bat script in Flink 1.11, submitting Python job on windows is not supported. We should add documentation for this.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.python.installation.zh.md</file>
      <file type="M">docs.dev.table.python.installation.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-7-22 01:00:00" id="17886" opendate="2020-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Chinese documentation for new WatermarkGenerator/WatermarkStrategies</summary>
      <description>We need to update the Chinese documentation according to FLINK-17773.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.event.timestamp.extractors.zh.md</file>
      <file type="M">docs.dev.event.timestamps.watermarks.zh.md</file>
      <file type="M">docs.dev.event.time.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-22 01:00:00" id="17889" opendate="2020-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-connector-hive jar contains wrong class in its SPI config file org.apache.flink.table.factories.TableFactory</summary>
      <description>These 2 classes are in flink-connector-hive jar's SPI config fileorg.apache.flink.orc.OrcFileSystemFormatFactoryLicense.org.apache.flink.formats.parquet.ParquetFileSystemFormatFactory Due to this issue, I get the following exception in zeppelin side.Caused by: java.util.ServiceConfigurationError: org.apache.flink.table.factories.TableFactory: Provider org.apache.flink.orc.OrcFileSystemFormatFactory not a subtypeCaused by: java.util.ServiceConfigurationError: org.apache.flink.table.factories.TableFactory: Provider org.apache.flink.orc.OrcFileSystemFormatFactory not a subtype at java.util.ServiceLoader.fail(ServiceLoader.java:239) at java.util.ServiceLoader.access$300(ServiceLoader.java:185) at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:376) at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) at java.util.ServiceLoader$1.next(ServiceLoader.java:480) at java.util.Iterator.forEachRemaining(Iterator.java:116) at org.apache.flink.table.factories.TableFactoryService.discoverFactories(TableFactoryService.java:214) ... 35 more</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-6-23 01:00:00" id="17895" opendate="2020-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Default value of rows-per-second in datagen can be limited</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.factories.DataGenTableSourceFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-24 01:00:00" id="17905" opendate="2020-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Docs for JDBC connector show licence and markup</summary>
      <description/>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.jdbc.zh.md</file>
      <file type="M">docs.dev.connectors.jdbc.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-6-25 01:00:00" id="17931" opendate="2020-5-25 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Document fromValues clause</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-26 01:00:00" id="17936" opendate="2020-5-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement type inference for AS</summary>
      <description>Type information gets lost due to the legacy planner expressions. The user might experience unexpected exceptions.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.fieldExpression.scala</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.BuiltInFunctionDefinitions.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-3-27 01:00:00" id="1794" opendate="2015-3-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add test base for scalatest and adapt flink-ml test cases</summary>
      <description>Currently, the flink-ml test cases use the standard ExecutionEnvironment which can cause problems in parallel test executions as they happen on Travis. For these tests it would be helpful to have an appropriate Scala test base which instantiates a ForkableFlinkMiniCluster and sets the ExecutionEnvironment appropriately.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils.pom.xml</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.regression.MultipleLinearRegressionITCase.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.recommendation.ALSITCase.scala</file>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.feature.PolynomialBaseITCase.scala</file>
      <file type="M">flink-staging.flink-ml.pom.xml</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.Client.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-26 01:00:00" id="17944" opendate="2020-5-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong output in sql client&amp;#39;s table mode</summary>
      <description>When I run the following sql example, I get the wrong outputSELECT name, COUNT(*) AS cnt FROM (VALUES ('Bob'), ('Alice'), ('Greg'), ('Bob')) AS NameTable(name) GROUP BY name;   Bob 1 Alice 1 Greg 1 Bob 2 This is due to we add kind in Row, so the sematics of equals method changes</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResult.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-6-27 01:00:00" id="17960" opendate="2020-5-27 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Improve commands in the "Common Questions" document for PyFlink</summary>
      <description>Currently, in the "Common Questions" document, we have the command `$ setup-pyflink-virtual-env.sh` to run the script. However, the script is not executable. It would be better to replace the command with `$ sh setup-pyflink-virtual-env.sh` and add download command.$ curl -O https://ci.apache.org/projects/flink/flink-docs-master/downloads/setup-pyflink-virtual-env.sh$ sh setup-pyflink-virtual-env.sh</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.python.common.questions.zh.md</file>
      <file type="M">docs.dev.table.python.common.questions.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-27 01:00:00" id="17974" opendate="2020-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test new Flink Docker image</summary>
      <description>Test Flink's new Docker image and the corresponding Dockerfile: Try to build custom image Try to run different Flink processes (Master (session, per-job), TaskManager) Try custom configuration and log properties</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.docker.zh.md</file>
      <file type="M">docs.ops.deployment.docker.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-27 01:00:00" id="17976" opendate="2020-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test native K8s integration</summary>
      <description>Test Flink's native K8s integration: session mode application mode custom Flink image custom configuration and log properties</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
      <file type="M">docs.ops.deployment.index.zh.md</file>
      <file type="M">docs.ops.deployment.index.md</file>
      <file type="M">docs.ops.deployment.docker.zh.md</file>
      <file type="M">docs.ops.deployment.docker.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-27 01:00:00" id="17977" opendate="2020-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check log sanity</summary>
      <description>Run a normal Flink workload (e.g. job with fixed number of failures on session cluster) and check that the produced Flink logs make sense and don't contain confusing statements.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.net.ConnectionUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.SecurityUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.registration.RetryingRegistration.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-27 01:00:00" id="17981" opendate="2020-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add new Flink docs homepage content</summary>
      <description>Flink docs homepage requires a serious redesign to better guide users through the different sections of the documentation. This ticket is focused soley on updating the text.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..config.yml</file>
      <file type="M">docs.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-27 01:00:00" id="17982" opendate="2020-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Resolve TODO&amp;#39;s in concepts documentation</summary>
      <description>The concepts section in the documentation contains several TODO's. These should be replaced with proper content or the TODO's removed for 1.11.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.concepts.stateful-stream-processing.md</file>
      <file type="M">docs.concepts.timely-stream-processing.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-4-30 01:00:00" id="1799" opendate="2015-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala API does not support generic arrays</summary>
      <description>The Scala API does not support generic arrays at the moment. It throws a rather unhelpful error message ```InvalidTypesException: The given type is not a valid object array```.Code to reproduce the problem is given below:def main(args: Array[String]) { foobar[Double]}def foobar[T: ClassTag: TypeInformation]: DataSet[Block[T]] = { val tpe = createTypeInformation[Array[T]] null}</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.types.TypeInformationGenTest.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.codegen.TypeInformationGen.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-30 01:00:00" id="1800" opendate="2015-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a "Beta" badge in the documentation to components in flink-staging</summary>
      <description>As per mailing list discussion: http://apache-flink-incubator-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Add-a-quot-Beta-quot-badge-in-the-documentation-to-components-in-flink-staging-td4801.html</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..layouts.default.html</file>
      <file type="M">docs..includes.sidenav.html</file>
      <file type="M">docs..includes.navbar.html</file>
      <file type="M">docs.table.md</file>
      <file type="M">docs.streaming.guide.md</file>
      <file type="M">docs.hadoop.compatibility.md</file>
      <file type="M">docs.gelly.guide.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-28 01:00:00" id="18004" opendate="2020-5-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update checkpoint UI related pictures in documentation</summary>
      <description>After FLINK-13390 which clarifies what the "state size" means on incremental checkpoint, the checkpoint UI has already changed, and we should also update related documentation to not mislead users.</description>
      <version>1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.fig.checkpoint.monitoring-summary.png</file>
      <file type="M">docs.fig.checkpoint.monitoring-history.png</file>
      <file type="M">docs.fig.checkpoint.monitoring-details.summary.png</file>
      <file type="M">docs.fig.checkpoint.monitoring-details.subtasks.png</file>
      <file type="M">docs.fig.checkpoint.monitoring-details.png</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2020-6-29 01:00:00" id="18029" opendate="2020-5-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more ITCases for Kafka with new formats</summary>
      <description>Add ITCase for Kafka read/write CSV Add ITCase for Kafka read/write Avro Add ITCase for Kafka read/write JSON</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2020-6-1 01:00:00" id="18046" opendate="2020-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Decimal column stats not supported for Hive table</summary>
      <description>For now, we can just return CatalogColumnStatisticsDataDouble for decimal columns.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogHiveMetadataTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveStatsUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-2 01:00:00" id="18065" opendate="2020-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for new scalar/table functions</summary>
      <description>Write documentation for scalar/table functions of FLIP-65.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-2 01:00:00" id="18066" opendate="2020-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for how to develop a new table source/sink</summary>
      <description>Covers how to write a custom source/sink and format using FLIP-95 interfaces.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sourceSinks.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-2-31 01:00:00" id="1807" opendate="2015-3-31 00:00:00" resolution="Won&amp;#39;t Do">
    <buginformation>
      <summary>Stochastic gradient descent optimizer for ML library</summary>
      <description>Stochastic gradient descent (SGD) is a widely used optimization technique in different ML algorithms. Thus, it would be helpful to provide a generalized SGD implementation which can be instantiated with the respective gradient computation. Such a building block would make the development of future algorithms easier.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-ml.src.test.scala.org.apache.flink.ml.regression.RegressionData.scala</file>
      <file type="M">flink-staging.flink-ml.pom.xml</file>
      <file type="M">docs.libs.ml.index.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-8-3 01:00:00" id="18080" opendate="2020-6-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate the "Kerberos Authentication Setup and Configuration" page into Chinese</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.deployment.security.security-kerberos.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-3 01:00:00" id="18081" opendate="2020-6-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix broken links in "Kerberos Authentication Setup and Configuration" doc</summary>
      <description>The config.html#kerberos-based-security is not valid now.</description>
      <version>1.10.1,1.11.0,1.12.0</version>
      <fixedVersion>1.10.3,1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.security-kerberos.zh.md</file>
      <file type="M">docs.ops.security-kerberos.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-6-3 01:00:00" id="18084" opendate="2020-6-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create documentation for the Application Mode</summary>
      <description>This ticket aims at documenting the application mode and its new capabilities. This should also include the documentation for Yarn (for Kubernetes the documentation is tracked by another ticket).</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.FlinkYarnSessionCliTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.cli.KubernetesSessionCli.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.ExecutorCLITest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ExecutorCLI.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.zh.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
      <file type="M">docs.ops.deployment.index.zh.md</file>
      <file type="M">docs.ops.deployment.index.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-6-3 01:00:00" id="18089" opendate="2020-6-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the network zero-copy test into the azure E2E pipeline</summary>
      <description>The zero-copy E2E test added in Flink-10742 is only added to the deprecated travis pipeline previously. It should be added into the Azure test pipeline.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-31 01:00:00" id="1809" opendate="2015-3-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add standard scaler to ML library</summary>
      <description>Many ML algorithms require the input data to have mean 0 and a variance 1 for each individual feature &amp;#91;1&amp;#93;. Therefore, a Transformer which achieves exactly that would be a valuable addition to the machine learning library.Resources:&amp;#91;1&amp;#93; http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.sidenav.html</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-11-4 01:00:00" id="18117" opendate="2020-6-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Kerberized YARN per-job on Docker test" fails with "Could not start hadoop cluster."</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2683&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f52020-06-04T06:03:53.2844296Z Creating slave1 ... [32mdone [0m2020-06-04T06:03:53.4981251Z [1BWaiting for hadoop cluster to come up. We have been trying for 0 seconds, retrying ...2020-06-04T06:03:58.5980181Z Waiting for hadoop cluster to come up. We have been trying for 5 seconds, retrying ...2020-06-04T06:04:03.6997087Z Waiting for hadoop cluster to come up. We have been trying for 10 seconds, retrying ...2020-06-04T06:04:08.7910791Z Waiting for hadoop cluster to come up. We have been trying for 15 seconds, retrying ...2020-06-04T06:04:13.8921621Z Waiting for hadoop cluster to come up. We have been trying for 20 seconds, retrying ...2020-06-04T06:04:18.9648844Z Waiting for hadoop cluster to come up. We have been trying for 25 seconds, retrying ...2020-06-04T06:04:24.0381851Z Waiting for hadoop cluster to come up. We have been trying for 31 seconds, retrying ...2020-06-04T06:04:29.1220264Z Waiting for hadoop cluster to come up. We have been trying for 36 seconds, retrying ...2020-06-04T06:04:34.1882187Z Waiting for hadoop cluster to come up. We have been trying for 41 seconds, retrying ...2020-06-04T06:04:39.2784948Z Waiting for hadoop cluster to come up. We have been trying for 46 seconds, retrying ...2020-06-04T06:04:44.3843337Z Waiting for hadoop cluster to come up. We have been trying for 51 seconds, retrying ...2020-06-04T06:04:49.4703561Z Waiting for hadoop cluster to come up. We have been trying for 56 seconds, retrying ...2020-06-04T06:04:54.5463207Z Waiting for hadoop cluster to come up. We have been trying for 61 seconds, retrying ...2020-06-04T06:04:59.6650405Z Waiting for hadoop cluster to come up. We have been trying for 66 seconds, retrying ...2020-06-04T06:05:04.7500168Z Waiting for hadoop cluster to come up. We have been trying for 71 seconds, retrying ...2020-06-04T06:05:09.8177904Z Waiting for hadoop cluster to come up. We have been trying for 76 seconds, retrying ...2020-06-04T06:05:14.9751297Z Waiting for hadoop cluster to come up. We have been trying for 81 seconds, retrying ...2020-06-04T06:05:20.0336417Z Waiting for hadoop cluster to come up. We have been trying for 87 seconds, retrying ...2020-06-04T06:05:25.1627704Z Waiting for hadoop cluster to come up. We have been trying for 92 seconds, retrying ...2020-06-04T06:05:30.2583315Z Waiting for hadoop cluster to come up. We have been trying for 97 seconds, retrying ...2020-06-04T06:05:35.3283678Z Waiting for hadoop cluster to come up. We have been trying for 102 seconds, retrying ...2020-06-04T06:05:40.4184029Z Waiting for hadoop cluster to come up. We have been trying for 107 seconds, retrying ...2020-06-04T06:05:45.5388372Z Waiting for hadoop cluster to come up. We have been trying for 112 seconds, retrying ...2020-06-04T06:05:50.6155334Z Waiting for hadoop cluster to come up. We have been trying for 117 seconds, retrying ...2020-06-04T06:05:55.7225186Z Command: start_hadoop_cluster failed. Retrying...2020-06-04T06:05:55.7237999Z Starting Hadoop cluster2020-06-04T06:05:56.5188293Z kdc is up-to-date2020-06-04T06:05:56.5292716Z master is up-to-date2020-06-04T06:05:56.5301735Z slave2 is up-to-date2020-06-04T06:05:56.5306179Z slave1 is up-to-date2020-06-04T06:05:56.6800566Z Waiting for hadoop cluster to come up. We have been trying for 0 seconds, retrying ...2020-06-04T06:06:01.7668291Z Waiting for hadoop cluster to come up. We have been trying for 5 seconds, retrying ...2020-06-04T06:06:06.8620265Z Waiting for hadoop cluster to come up. We have been trying for 10 seconds, retrying ...2020-06-04T06:06:11.9753596Z Waiting for hadoop cluster to come up. We have been trying for 15 seconds, retrying ...2020-06-04T06:06:17.0402846Z Waiting for hadoop cluster to come up. We have been trying for 21 seconds, retrying ...2020-06-04T06:06:22.1650005Z Waiting for hadoop cluster to come up. We have been trying for 26 seconds, retrying ...2020-06-04T06:06:27.2500179Z Waiting for hadoop cluster to come up. We have been trying for 31 seconds, retrying ...2020-06-04T06:06:32.3133809Z Waiting for hadoop cluster to come up. We have been trying for 36 seconds, retrying ...2020-06-04T06:06:37.4432923Z Waiting for hadoop cluster to come up. We have been trying for 41 seconds, retrying ...2020-06-04T06:06:42.5658250Z Waiting for hadoop cluster to come up. We have been trying for 46 seconds, retrying ...2020-06-04T06:06:47.6682536Z Waiting for hadoop cluster to come up. We have been trying for 51 seconds, retrying ...2020-06-04T06:06:52.7810371Z Waiting for hadoop cluster to come up. We have been trying for 56 seconds, retrying ...2020-06-04T06:06:57.8860269Z Waiting for hadoop cluster to come up. We have been trying for 61 seconds, retrying ...2020-06-04T06:07:03.0337979Z Waiting for hadoop cluster to come up. We have been trying for 67 seconds, retrying ...2020-06-04T06:07:08.1080310Z Waiting for hadoop cluster to come up. We have been trying for 72 seconds, retrying ...2020-06-04T06:07:13.2297578Z Waiting for hadoop cluster to come up. We have been trying for 77 seconds, retrying ...2020-06-04T06:07:18.3779034Z Waiting for hadoop cluster to come up. We have been trying for 82 seconds, retrying ...2020-06-04T06:07:23.4789495Z Waiting for hadoop cluster to come up. We have been trying for 87 seconds, retrying ...2020-06-04T06:07:28.6063062Z Waiting for hadoop cluster to come up. We have been trying for 92 seconds, retrying ...2020-06-04T06:07:33.8220409Z Waiting for hadoop cluster to come up. We have been trying for 97 seconds, retrying ...2020-06-04T06:07:38.9439231Z Waiting for hadoop cluster to come up. We have been trying for 102 seconds, retrying ...2020-06-04T06:07:44.0193849Z Waiting for hadoop cluster to come up. We have been trying for 108 seconds, retrying ...2020-06-04T06:07:49.1241642Z Waiting for hadoop cluster to come up. We have been trying for 113 seconds, retrying ...2020-06-04T06:07:54.2425087Z Waiting for hadoop cluster to come up. We have been trying for 118 seconds, retrying ...2020-06-04T06:07:59.3835321Z Command: start_hadoop_cluster failed. Retrying...2020-06-04T06:07:59.3847275Z Starting Hadoop cluster2020-06-04T06:08:00.1959109Z kdc is up-to-date2020-06-04T06:08:00.1968717Z master is up-to-date2020-06-04T06:08:00.1982811Z slave1 is up-to-date2020-06-04T06:08:00.1988143Z slave2 is up-to-date2020-06-04T06:08:00.4014781Z Waiting for hadoop cluster to come up. We have been trying for 0 seconds, retrying ...2020-06-04T06:08:05.5168483Z Waiting for hadoop cluster to come up. We have been trying for 5 seconds, retrying ...2020-06-04T06:08:10.6759355Z Waiting for hadoop cluster to come up. We have been trying for 10 seconds, retrying ...2020-06-04T06:08:15.8307550Z Waiting for hadoop cluster to come up. We have been trying for 15 seconds, retrying ...2020-06-04T06:08:21.0143341Z Waiting for hadoop cluster to come up. We have been trying for 21 seconds, retrying ...2020-06-04T06:08:26.0932297Z Waiting for hadoop cluster to come up. We have been trying for 26 seconds, retrying ...2020-06-04T06:08:31.2526775Z Waiting for hadoop cluster to come up. We have been trying for 31 seconds, retrying ...2020-06-04T06:08:36.4356124Z Waiting for hadoop cluster to come up. We have been trying for 36 seconds, retrying ...2020-06-04T06:08:41.5607530Z Waiting for hadoop cluster to come up. We have been trying for 41 seconds, retrying ...2020-06-04T06:08:46.6407963Z Waiting for hadoop cluster to come up. We have been trying for 46 seconds, retrying ...2020-06-04T06:08:51.8464789Z Waiting for hadoop cluster to come up. We have been trying for 51 seconds, retrying ...2020-06-04T06:08:56.9735817Z Waiting for hadoop cluster to come up. We have been trying for 56 seconds, retrying ...2020-06-04T06:09:02.1023842Z Waiting for hadoop cluster to come up. We have been trying for 62 seconds, retrying ...2020-06-04T06:09:07.2390427Z Waiting for hadoop cluster to come up. We have been trying for 67 seconds, retrying ...2020-06-04T06:09:12.4433329Z Waiting for hadoop cluster to come up. We have been trying for 72 seconds, retrying ...2020-06-04T06:09:17.5390800Z Waiting for hadoop cluster to come up. We have been trying for 77 seconds, retrying ...2020-06-04T06:09:22.7020537Z Waiting for hadoop cluster to come up. We have been trying for 82 seconds, retrying ...2020-06-04T06:09:27.8754909Z Waiting for hadoop cluster to come up. We have been trying for 87 seconds, retrying ...2020-06-04T06:09:33.0447274Z Waiting for hadoop cluster to come up. We have been trying for 93 seconds, retrying ...2020-06-04T06:09:38.1804596Z Waiting for hadoop cluster to come up. We have been trying for 98 seconds, retrying ...2020-06-04T06:09:43.3636590Z Waiting for hadoop cluster to come up. We have been trying for 103 seconds, retrying ...2020-06-04T06:09:48.4975410Z Waiting for hadoop cluster to come up. We have been trying for 108 seconds, retrying ...2020-06-04T06:09:53.6117328Z Waiting for hadoop cluster to come up. We have been trying for 113 seconds, retrying ...2020-06-04T06:09:58.7785946Z Waiting for hadoop cluster to come up. We have been trying for 118 seconds, retrying ...2020-06-04T06:10:03.9748663Z Command: start_hadoop_cluster failed. Retrying...2020-06-04T06:10:03.9808244Z Command: start_hadoop_cluster failed 3 times.2020-06-04T06:10:03.9823071Z ERROR: Could not start hadoop cluster. Aborting...Frequent, suspicious logs2020-06-04T06:10:04.5032658Z 20/06/04 06:05:42 WARN ipc.Client: Failed to connect to server: master.docker-hadoop-cluster-network/172.19.0.3:9000: try once and fail.2020-06-04T06:10:04.5033211Z java.net.ConnectException: Connection refused...2020-06-04T06:10:04.6867876Z 20/06/04 06:04:11 ERROR namenode.NameNode: Failed to start namenode.2020-06-04T06:10:04.6868640Z java.net.BindException: Port in use: 0.0.0.0:504702020-06-04T06:10:04.6869062Z at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:998)2020-06-04T06:10:04.6869702Z at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:935)2020-06-04T06:10:04.6870199Z at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:171)2020-06-04T06:10:04.6870740Z at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:842)2020-06-04T06:10:04.6871235Z at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:693)2020-06-04T06:10:04.6871728Z at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:906)2020-06-04T06:10:04.6872202Z at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:885)2020-06-04T06:10:04.6872699Z at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1626)2020-06-04T06:10:04.6873701Z at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1694)2020-06-04T06:10:04.6874100Z Caused by: java.net.BindException: Address already in use2020-06-04T06:10:04.6901805Z at sun.nio.ch.Net.bind0(Native Method)2020-06-04T06:10:04.6902168Z at sun.nio.ch.Net.bind(Net.java:433)2020-06-04T06:10:04.6902478Z at sun.nio.ch.Net.bind(Net.java:425)2020-06-04T06:10:04.6902847Z at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)2020-06-04T06:10:04.6903296Z at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)2020-06-04T06:10:04.6903744Z at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)2020-06-04T06:10:04.6904395Z at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:993)2020-06-04T06:10:04.6904727Z ... 8 more2020-06-04T06:10:04.6905005Z 20/06/04 06:04:11 INFO util.ExitUtil: Exiting with status 12020-06-04T06:10:04.6905401Z 20/06/04 06:04:11 INFO namenode.NameNode: SHUTDOWN_MSG:</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.config.hdfs-site.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.Dockerfile</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.bootstrap.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-6-4 01:00:00" id="18131" opendate="2020-6-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for the new JSON format</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.formats.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-4 01:00:00" id="18132" opendate="2020-6-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for the new CSV format</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.formats.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-4 01:00:00" id="18134" opendate="2020-6-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for the Debezium format</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.kafka.zh.md</file>
      <file type="M">docs.dev.table.connectors.kafka.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-6-5 01:00:00" id="18140" opendate="2020-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for ORC format</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.formats.parquet.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.parquet.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-5 01:00:00" id="18141" opendate="2020-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Parquet format</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.formats.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.index.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-6-5 01:00:00" id="18161" opendate="2020-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Changing parallelism is not possible in sql-client.sh</summary>
      <description>I tried using SET execution.parallelism=12and changing the parallelism in the configuration file.My SQL queries were always running with p=1 for all operators.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-8 01:00:00" id="18173" opendate="2020-6-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bundle flink-csv and flink-json jars in lib</summary>
      <description>The biggest problem for distributions I see is the variety of problems caused by users' lack of format dependency. These three formats are very small and no third party dependence, and they are widely used by table users. Actually, we don't have any other built-in table formats now.. flink-csv-1.10.0.jar flink-json-1.10.0.jar We can just bundle them in "flink/lib/". It not solve all problems and it is independent of "fat" and "slim". But also improve usability.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.release-notes.flink-1.11.zh.md</file>
      <file type="M">docs.release-notes.flink-1.11.md</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">docs.dev.table.connectors.formats.avro.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.avro.md</file>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-8 01:00:00" id="18175" opendate="2020-6-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add human readable summary for configured and derived memory sizes.</summary>
      <description>FLIP-49 &amp; FLIP-116 introduces sophisticated memory configurations for TaskManager and Master processes. Before the JVM processes are started, Flink derives the accurate sizes for all necessary components, based on both explicit user configurations and implicit defaults.To make the configuration results (especially those implicitly derived) clear to users, it would be helpful to print them in the beginning of the process logs. Currently, we only have printed JVM parameters (TM &amp; Master) dynamic memory configurations (TM only). They are incomplete (jvm overhead for both processes and off-heap memory for the master process are not presented) and unfriendly (in bytes).Therefore, I propose to add a human readable summary at the beginning of process logs.See also this PR discussion.</description>
      <version>1.10.1,1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.bash.BashJavaUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-8 01:00:00" id="18176" opendate="2020-6-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add supplement for file system connector document</summary>
      <description>Illustate difference between old file system connector and new file system connector.Add supplement for rolling policy.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.filesystem.zh.md</file>
      <file type="M">docs.dev.table.connectors.filesystem.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-6-9 01:00:00" id="18215" opendate="2020-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>BashJavaUtils logging message should include log level</summary>
      <description>The BashJavaUtils currently log things like this:[] - The derived from fraction jvm overhead memory (160.000mb (167772162 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used insteadThis means that users cannot differentiate between info/warn/error messages.An example where this might be helpful is FLINK-18214.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.resources.log4j-bash-utils.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-9 01:00:00" id="18222" opendate="2020-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Avro Confluent Schema Registry nightly end-to-end test" unstable with "Kafka cluster did not start after 120 seconds"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3045&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f52020-06-09T15:16:48.1427795Z ==============================================================================2020-06-09T15:16:48.1428609Z Running 'Avro Confluent Schema Registry nightly end-to-end test'2020-06-09T15:16:48.1429204Z ==============================================================================2020-06-09T15:16:48.1438117Z TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-481432981702020-06-09T15:16:48.2985167Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT2020-06-09T15:16:48.3157575Z Downloading Kafka from https://archive.apache.org/dist/kafka/0.10.2.0/kafka_2.11-0.10.2.0.tgz2020-06-09T15:16:48.3214487Z % Total % Received % Xferd Average Speed Time Time Time Current2020-06-09T15:16:48.3215154Z Dload Upload Total Spent Left Speed2020-06-09T15:16:48.3215597Z 2020-06-09T15:16:48.3528820Z 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 02020-06-09T15:16:49.3421526Z 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 02020-06-09T15:16:50.3415678Z 8 35.8M 8 2960k 0 0 2896k 0 0:00:12 0:00:01 0:00:11 2896k2020-06-09T15:16:51.3406836Z 23 35.8M 23 8544k 0 0 4226k 0 0:00:08 0:00:02 0:00:06 4225k2020-06-09T15:16:51.6553485Z 70 35.8M 70 25.2M 0 0 8550k 0 0:00:04 0:00:03 0:00:01 8548k2020-06-09T15:16:51.6555606Z 100 35.8M 100 35.8M 0 0 10.7M 0 0:00:03 0:00:03 --:--:-- 10.7M2020-06-09T15:16:51.9818041Z Downloading confluent from http://packages.confluent.io/archive/3.2/confluent-oss-3.2.0-2.11.tar.gz2020-06-09T15:16:51.9880242Z % Total % Received % Xferd Average Speed Time Time Time Current2020-06-09T15:16:51.9880983Z Dload Upload Total Spent Left Speed2020-06-09T15:16:51.9914252Z 2020-06-09T15:16:52.3398614Z 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 02020-06-09T15:16:53.3399552Z 9 398M 9 39.5M 0 0 111M 0 0:00:03 --:--:-- 0:00:03 111M2020-06-09T15:16:53.9149276Z 47 398M 47 188M 0 0 139M 0 0:00:02 0:00:01 0:00:01 138M2020-06-09T15:16:53.9150980Z 100 398M 100 398M 0 0 206M 0 0:00:01 0:00:01 --:--:-- 206M2020-06-09T15:17:04.3565942Z Waiting for broker...2020-06-09T15:17:12.4215170Z Waiting for broker...2020-06-09T15:17:14.3012835Z Waiting for broker...2020-06-09T15:17:16.1965074Z Waiting for broker...2020-06-09T15:17:18.1102274Z Waiting for broker...2020-06-09T15:17:19.9929632Z Waiting for broker...2020-06-09T15:17:21.8607172Z Waiting for broker...2020-06-09T15:17:23.7802949Z Waiting for broker...2020-06-09T15:17:25.6695260Z Waiting for broker...2020-06-09T15:17:27.5536417Z Waiting for broker...2020-06-09T15:17:29.4327778Z Waiting for broker...2020-06-09T15:17:31.3203091Z Waiting for broker...2020-06-09T15:17:33.1987150Z Waiting for broker...2020-06-09T15:17:35.0694860Z Waiting for broker...2020-06-09T15:17:36.9595576Z Waiting for broker...2020-06-09T15:17:38.9243558Z Waiting for broker...2020-06-09T15:17:40.7984064Z Waiting for broker...2020-06-09T15:17:42.6676095Z Waiting for broker...2020-06-09T15:17:44.5628797Z Waiting for broker...2020-06-09T15:17:46.4374532Z Waiting for broker...2020-06-09T15:17:48.3086761Z Waiting for broker...2020-06-09T15:17:50.1574336Z Waiting for broker...2020-06-09T15:17:52.0432952Z Waiting for broker...2020-06-09T15:17:53.9406541Z Waiting for broker...2020-06-09T15:17:55.8162052Z Waiting for broker...2020-06-09T15:17:57.7090015Z Waiting for broker...2020-06-09T15:17:59.5747770Z Waiting for broker...2020-06-09T15:18:01.4601854Z Waiting for broker...2020-06-09T15:18:03.3332039Z Waiting for broker...2020-06-09T15:18:05.2210453Z Waiting for broker...2020-06-09T15:18:07.1133675Z Waiting for broker...2020-06-09T15:18:09.0132417Z Waiting for broker...2020-06-09T15:18:10.8769511Z Waiting for broker...2020-06-09T15:18:12.7601639Z Waiting for broker...2020-06-09T15:18:14.6389770Z Waiting for broker...2020-06-09T15:18:16.5210725Z Waiting for broker...2020-06-09T15:18:18.4088216Z Waiting for broker...2020-06-09T15:18:20.2732225Z Waiting for broker...2020-06-09T15:18:22.1558390Z Waiting for broker...2020-06-09T15:18:24.0400570Z Waiting for broker...2020-06-09T15:18:25.9134038Z Waiting for broker...2020-06-09T15:18:27.7922350Z Waiting for broker...2020-06-09T15:18:29.6748679Z Waiting for broker...2020-06-09T15:18:31.5340996Z Waiting for broker...2020-06-09T15:18:33.3998472Z Waiting for broker...2020-06-09T15:18:35.2718135Z Waiting for broker...2020-06-09T15:18:37.1426082Z Waiting for broker...2020-06-09T15:18:39.1282264Z Waiting for broker...2020-06-09T15:18:41.0029183Z Waiting for broker...2020-06-09T15:18:42.8700037Z Waiting for broker...2020-06-09T15:18:44.7531621Z Waiting for broker...2020-06-09T15:18:46.6465173Z Waiting for broker...2020-06-09T15:18:48.9504192Z Waiting for broker...2020-06-09T15:18:50.4165383Z Waiting for broker...2020-06-09T15:18:52.2931688Z Waiting for broker...2020-06-09T15:18:54.1669857Z Waiting for broker...2020-06-09T15:18:56.0238505Z Waiting for broker...2020-06-09T15:18:57.8931143Z Waiting for broker...2020-06-09T15:18:59.7607751Z Kafka cluster did not start after 120 seconds. Printing Kafka logs:There's a lot of log output I didn't analyze yet.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-10 01:00:00" id="18229" opendate="2020-6-10 00:00:00" resolution="Done">
    <buginformation>
      <summary>Pending worker requests should be properly cleared</summary>
      <description>Currently, if Kubernetes/Yarn does not have enough resources to fulfill Flink's resource requirement, there will be pending pod/container requests on Kubernetes/Yarn. These pending resource requirements are never cleared until either fulfilled or the Flink cluster is shutdown.However, sometimes Flink no longer needs the pending resources. E.g., the slot request is then fulfilled by another slots that become available, or the job failed due to slot request timeout (in a session cluster). In such cases, Flink does not remove the resource request until the resource is allocated, then it discovers that it no longer needs the allocated resource and release them. This would affect the underlying Kubernetes/Yarn cluster, especially when the cluster is under heavy workload.It would be good for Flink to cancel pod/container requests as earlier as possible if it can discover that some of the pending workers are no longer needed.There are several approaches potentially achieve this. We can always check whether there's a pending worker that can be canceled when a PendingTaskManagerSlot is unassigned. We can have a separate timeout for requesting new worker. If the resource cannot be allocated within the given time since requested, we should cancel that resource request and claim a resource allocation failure. We can share the same timeout for starting new worker (proposed in FLINK-13554). This is similar to 2), but it requires the worker to be registered, rather than allocated, before timeout.</description>
      <version>1.9.3,1.10.1,1.11.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerDriverTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManagerDriver.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.TaskExecutorManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedTaskManagerTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.active.ResourceManagerDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriverTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.KubernetesResourceManagerDriver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-10 01:00:00" id="18238" opendate="2020-6-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>RemoteChannelThroughputBenchmark deadlocks</summary>
      <description>In the last couple of days RemoteChannelThroughputBenchmark.remoteRebalance deadlocked for the second time:http://codespeed.dak8s.net:8080/job/flink-master-benchmarks/6019/</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-6-11 01:00:00" id="18248" opendate="2020-6-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update data type documentation for 1.11</summary>
      <description>Update the data type documentation for 1.11.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.types.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-11 01:00:00" id="18258" opendate="2020-6-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement SHOW PARTITIONS for Hive dialect</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ShowOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.utils.SqlParserHelper.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-12 01:00:00" id="18264" opendate="2020-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate the "External Resource Framework" page into Chinese</summary>
      <description/>
      <version>1.11.0</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.external.resources.zh.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-6-12 01:00:00" id="18268" opendate="2020-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct Table API in Temporal table docs</summary>
      <description>see user's feedback&amp;#91;1&amp;#93;:The  getTableEnvironment method has been dropped, but the documentation still use it val tEnv = TableEnvironment.getTableEnvironment(env)&amp;#91;1&amp;#93;http://apache-flink.147419.n8.nabble.com/flink-TableEnvironment-can-not-call-getTableEnvironment-api-tt3871.html </description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.streaming.temporal.tables.zh.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.md</file>
      <file type="M">docs.dev.table.connectors.hbase.zh.md</file>
      <file type="M">docs.dev.table.connectors.hbase.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-12 01:00:00" id="18279" opendate="2020-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Simplify table overview page</summary>
      <description>The table overview page contains an overwhelming amount of information. We should simplify the page so users quickly know: What dependencies they need to add in their user code Which planner to use</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.index.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-6-15 01:00:00" id="18294" opendate="2020-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log java processes and disk space after each e2e test</summary>
      <description>To debug interferences between e2e test it would be helpful to log disk usages and leftover java processes.I've seen instances where, right before the java e2e tests are run, there is still a kafka process running, and in one abnormal case we use 13gb more disk space.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-16 01:00:00" id="18326" opendate="2020-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kubernetes NOTICE issues</summary>
      <description>snakeyaml: 1.23 -&gt; 1.24logging-interceptor: 3.12.0 -&gt; 3.12.6generex is actually excluded</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-16 01:00:00" id="18328" opendate="2020-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Blink planner NOTICE issues</summary>
      <description>not actually bundled: asm, json-smart, accessors-smart, joda-time</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-16 01:00:00" id="18330" opendate="2020-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python NOTICE issues</summary>
      <description>beam-runners-core-java / beam-vendor-bytebuddy bundled but not listedprotobuf-java-util listed but not bundledThe NOTICE file additionally lists various dependencies that are bundled by beam. While this is fine, the lack of separation makes verification difficult.These would be the entries for directly bundled dependencies:This project bundles the following dependencies under the Apache Software License 2.0 (http://www.apache.org/licenses/LICENSE-2.0.txt)- com.fasterxml.jackson.core:jackson-annotations:2.10.1- com.fasterxml.jackson.core:jackson-core:2.10.1- com.fasterxml.jackson.core:jackson-databind:2.10.1- com.google.flatbuffers:flatbuffers-java:1.9.0- io.netty:netty-buffer:4.1.44.Final- io.netty:netty-common:4.1.44.Final- joda-time:joda-time:2.5- org.apache.arrow:arrow-format:0.16.0- org.apache.arrow:arrow-memory:0.16.0- org.apache.arrow:arrow-vector:0.16.0- org.apache.beam:beam-model-fn-execution:2.19.0- org.apache.beam:beam-model-job-management:2.19.0- org.apache.beam:beam-model-pipeline:2.19.0- org.apache.beam:beam-runners-core-construction-java:2.19.0- org.apache.beam:beam-runners-java-fn-execution:2.19.0- org.apache.beam:beam-sdks-java-core:2.19.0- org.apache.beam:beam-sdks-java-fn-execution:2.19.0- org.apache.beam:beam-vendor-grpc-1_21_0:0.1- org.apache.beam:beam-vendor-guava-26_0-jre:0.1- org.apache.beam:beam-vendor-sdks-java-extensions-protobuf:2.19.0This project bundles the following dependencies under the BSD license.See bundled license files for details- net.sf.py4j:py4j:0.10.8.1- com.google.protobuf:protobuf-java:3.7.1This project bundles the following dependencies under the MIT license. (https://opensource.org/licenses/MIT)See bundled license files for details.- net.razorvine:pyrolite:4.13These are the ones that are (supposedly) bundled by beam, which would need additional verification:The bundled Apache Beam dependencies bundle the following dependencies under the Apache Software License 2.0 (http://www.apache.org/licenses/LICENSE-2.0.txt)- com.google.api.grpc:proto-google-common-protos:1.12.0- com.google.code.gson:gson:2.7- com.google.guava:guava:26.0-jre- io.grpc:grpc-auth:1.21.0- io.grpc:grpc-core:1.21.0- io.grpc:grpc-context:1.21.0- io.grpc:grpc-netty:1.21.0- io.grpc:grpc-protobuf:1.21.0- io.grpc:grpc-stub:1.21.0- io.grpc:grpc-testing:1.21.0- io.netty:netty-buffer:4.1.34.Final- io.netty:netty-codec:4.1.34.Final- io.netty:netty-codec-http:4.1.34.Final- io.netty:netty-codec-http2:4.1.34.Final- io.netty:netty-codec-socks:4.1.34.Final- io.netty:netty-common:4.1.34.Final- io.netty:netty-handler:4.1.34.Final- io.netty:netty-handler-proxy:4.1.34.Final- io.netty:netty-resolver:4.1.34.Final- io.netty:netty-transport:4.1.34.Final- io.netty:netty-transport-native-epoll:4.1.34.Final- io.netty:netty-transport-native-unix-common:4.1.34.Final- io.netty:netty-tcnative-boringssl-static:2.0.22.Final- io.opencensus:opencensus-api:0.21.0- io.opencensus:opencensus-contrib-grpc-metrics:0.21.0The bundled Apache Beam dependencies bundle the following dependencies under the BSD license.See bundled license files for details- com.google.protobuf:protobuf-java-util:3.7.1- com.google.auth:google-auth-library-credentials:0.13.0</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-16 01:00:00" id="18331" opendate="2020-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive NOTICE issues</summary>
      <description>1.2/2.2 NOTICE entries are not sorted alphabetically.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.2.0.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-1.2.2.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-19 01:00:00" id="18380" opendate="2020-6-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a table source example</summary>
      <description>We introduce a lot of interfaces in FLIP-95. We should add a little example for demoing.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.WordCountTable.java</file>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.WordCountSQL.java</file>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.StreamWindowSQLExample.java</file>
      <file type="M">flink-examples.flink-examples-table.src.main.java.org.apache.flink.table.examples.java.StreamSQLExample.java</file>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-6-8 01:00:00" id="1844" opendate="2015-4-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Normaliser to ML library</summary>
      <description>In many algorithms in ML, the features' values would be better to lie between a given range of values, usually in the range (0,1) &amp;#91;1&amp;#93;. Therefore, a Transformer could be implemented to achieve that normalisation.Resources: &amp;#91;1&amp;#93;http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.libs.ml.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-29 01:00:00" id="18447" opendate="2020-6-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add &amp;#39;flink-connector-base&amp;#39; to &amp;#39;flink-dist&amp;#39;</summary>
      <description>Source connectors will rely mostly on 'flink-connector-base'. It is like a high-level Source API.Including it in flink-dist would avoid that each connector has to package that into a fat-jar. It would then be used similarly to other APIs.</description>
      <version>1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-7-2 01:00:00" id="18469" opendate="2020-7-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Application Mode to release notes.</summary>
      <description/>
      <version>1.11.0</version>
      <fixedVersion>1.11.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.release-notes.flink-1.11.zh.md</file>
      <file type="M">docs.release-notes.flink-1.11.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-2 01:00:00" id="18471" opendate="2020-7-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-runtime lists "org.uncommons.maths:uncommons-maths:1.2.2a" as a bundled dependency, but it isn&amp;#39;t</summary>
      <description>This is the relevant section in the NOTICE fileThis project bundles the following dependencies under the Apache Software License 2.0. (http://www.apache.org/licenses/LICENSE-2.0.txt)- com.typesafe.akka:akka-remote_2.11:2.5.21- io.netty:netty:3.10.6.Final- org.uncommons.maths:uncommons-maths:1.2.2a.The uncommons-maths dependency is not declared anywhere, nor is it included in the binary file.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-2 01:00:00" id="18472" opendate="2020-7-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Local Installation Getting Started Guide</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.try-flink.table.api.zh.md</file>
      <file type="M">docs.try-flink.table.api.md</file>
      <file type="M">docs.try-flink.python.table.api.zh.md</file>
      <file type="M">docs.try-flink.python.table.api.md</file>
      <file type="M">docs.try-flink.flink-operations-playground.zh.md</file>
      <file type="M">docs.try-flink.flink-operations-playground.md</file>
      <file type="M">docs.try-flink.datastream.api.zh.md</file>
      <file type="M">docs.try-flink.datastream.api.md</file>
      <file type="M">docs.index.zh.md</file>
      <file type="M">docs.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-2 01:00:00" id="18477" opendate="2020-7-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ChangelogSocketExample does not work</summary>
      <description>The example fails on a fresh build with: The program finished with the following exception:org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Unable to create a source for reading table 'default_catalog.default_database.UserScores'.Table options are:'byte-delimiter'='10''changelog-csv.column-delimiter'='|''connector'='socket''format'='changelog-csv''hostname'='localhost''port'='9999' at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302) at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198) at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149) at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:699) at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:232) at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:916) at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992) at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30) at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)Caused by: org.apache.flink.table.api.ValidationException: Unable to create a source for reading table 'default_catalog.default_database.UserScores'.Table options are:'byte-delimiter'='10''changelog-csv.column-delimiter'='|''connector'='socket''format'='changelog-csv''hostname'='localhost''port'='9999' at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:125) at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.buildTableScan(CatalogSourceTable.scala:135) at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.scala:78) at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3492) at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2415) at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2102) at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051) at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661) at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642) at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345) at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151) at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:773) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:745) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:238) at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78) at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:658) at org.apache.flink.table.examples.java.connectors.ChangelogSocketExample.main(ChangelogSocketExample.java:89) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288) ... 8 moreCaused by: org.apache.flink.table.api.TableException: Could not load service provider for factories. at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:346) at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:221) at org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:326) at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:118) ... 31 moreCaused by: java.util.ServiceConfigurationError: org.apache.flink.table.factories.Factory: Provider org.apache.flink.table.examples.java.connectors.SocketDynamicTableFactory not found at java.util.ServiceLoader.fail(ServiceLoader.java:239) at java.util.ServiceLoader.access$300(ServiceLoader.java:185) at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:372) at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) at java.util.ServiceLoader$1.next(ServiceLoader.java:480) at java.util.Iterator.forEachRemaining(Iterator.java:116) at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:342) ... 34 more</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-4 01:00:00" id="18485" opendate="2020-7-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kerberized YARN per-job on Docker test failed during unzip jce_policy-8.zip</summary>
      <description>Instance on 1.11 branch: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4230&amp;view=logs&amp;j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&amp;t=94459a52-42b6-5bfc-5d74-690b5d3c6de8+ curl -LOH Cookie: oraclelicense=accept-securebackup-cookie http://download.oracle.com/otn-pub/java/jce/8/jce_policy-8.zip % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0100 429 100 429 0 0 1616 0 --:--:-- --:--:-- --:--:-- 1616 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0100 7073 100 7073 0 0 20139 0 --:--:-- --:--:-- --:--:-- 20139+ unzip jce_policy-8.zipArchive: jce_policy-8.zip End-of-central-directory signature not found. Either this file is not a zipfile, or it constitutes one disk of a multi-part archive. In the latter case the central directory and zipfile comment will be found on the last disk(s) of this archive.unzip: cannot find zipfile directory in one of jce_policy-8.zip or jce_policy-8.zip.zip, and cannot find jce_policy-8.zip.ZIP, period.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.Dockerfile</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-5 01:00:00" id="18486" opendate="2020-7-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for the &amp;#39;%&amp;#39; modulus function</summary>
      <description>This is a follow-up issue of FLINK-18240 to add documentation for the new system operator %.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.functions.systemFunctions.zh.md</file>
      <file type="M">docs.dev.table.functions.systemFunctions.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-7-7 01:00:00" id="18507" opendate="2020-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move get_config implementation to TableEnvironment to eliminate the duplication</summary>
      <description>Currently, TableEnvironment.get_config is abstract and the implementations in the child classes BatchTableEnvironment/StreamTableEnvironment are duplicate. The implementation could be moved to TableEnvironment to eliminate the duplication.</description>
      <version>1.9.0,1.10.0,1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-7 01:00:00" id="18519" opendate="2020-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Propagate exception to client when execution fails for REST submission</summary>
      <description>Currently when a user submits an application using the REST api and the execution fails, the exception is logged, but not sent back to the client. This issue aims at propagating the reason back to the client so that it is easier for the user to debug his/her application.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-7-8 01:00:00" id="18524" opendate="2020-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala varargs cause exception for new inference</summary>
      <description>Scala varargs are supported but cause an exception currently. Because there are two signatures (a valid and invalid one) in the class file.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.extraction.FunctionMappingExtractor.java</file>
      <file type="M">flink-table.flink-table-api-scala.src.test.scala.org.apache.flink.table.types.extraction.TypeInferenceExtractorScalaTest.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-8 01:00:00" id="18526" opendate="2020-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the configuration of Python UDF using Managed Memory in the doc of Pyflink</summary>
      <description>Add the configuration of Python UDF using Managed Memory in the doc of Pyflink</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.python.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.table.python.vectorized.python.udfs.md</file>
      <file type="M">docs.dev.table.python.python.udfs.zh.md</file>
      <file type="M">docs.dev.table.python.python.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-8 01:00:00" id="18528" opendate="2020-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update UNNEST to new type system</summary>
      <description>Updates the built-in UNNEST function to the new type system.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.LogicalUnnestRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.ExplodeFunctionUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalUnnestRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.bridging.BridgingSqlFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.utils.DataTypeUtils.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.RowType.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-8 01:00:00" id="18529" opendate="2020-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query Hive table and filter by timestamp partition can fail</summary>
      <description>The following examplecreate table foo (x int) partitioned by (ts timestamp);select x from foo where timestamp '2020-07-08 13:08:14' = ts;fails withCatalogException: HiveCatalog currently only supports timestamp of precision 9</description>
      <version>None</version>
      <fixedVersion>1.11.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-9 01:00:00" id="1855" opendate="2015-4-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SocketTextStreamWordCount example cannot be run from the webclient</summary>
      <description>When trying to parameterize(with for instance "localhost 9999") the SocketTextStreamWordCount streaming example from the webclient, I get the following error:HTTP ERROR: 500Problem accessing /runJob. Reason: org/apache/flink/streaming/examples/wordcount/WordCount</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-examples.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-7-13 01:00:00" id="18583" opendate="2020-7-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The _id field is incorrectly set to index in Elasticsearch6 DynamicTableSink</summary>
      <description>The _id field is incorrectly set to index in Elasticsearch6 DynamicTableSink.It is caused by this line</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSink.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-13 01:00:00" id="18585" opendate="2020-7-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dynamic index can not work in new DynamicTableSink</summary>
      <description>because the IndexGenerator.open() was not inited properly.</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.1,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch7DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.RowElasticsearchSinkFunction.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.table.IndexGeneratorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-14 01:00:00" id="18593" opendate="2020-7-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive bundle jar URLs are broken</summary>
      <description>we should use https://repo.maven.apache.org/maven2/ instead</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-9-15 01:00:00" id="18598" opendate="2020-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add instructions for asynchronous execute in PyFlink doc</summary>
      <description>Add instructions for asynchronous execute in PyFlink doc</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.faq.zh.md</file>
      <file type="M">docs.dev.python.faq.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-15 01:00:00" id="18600" opendate="2020-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kerberized YARN per-job on Docker test failed to download JDK 8u251</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4514&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=ff888d9b-cd34-53cc-d90f-3e446d355529+ mkdir -p /usr/java/default+ curl -Ls https://download.oracle.com/otn-pub/java/jdk/8u251-b08/3d5a2bb8f8d4428bbe94aed7ec7ae784/jdk-8u251-linux-x64.tar.gz -H Cookie: oraclelicense=accept-securebackup-cookie+ tar --strip-components=1 -xz -C /usr/java/default/gzip: stdin: not in gzip formattar: Child returned status 1</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.Dockerfile</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.pyflink.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-16 01:00:00" id="18616" opendate="2020-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add SHOW CURRENT DDLs</summary>
      <description>Supports:SHOW CURRENT CATALOG;SHOW CURRENT DATABASE;</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.BatchTableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.catalog.CatalogTableITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.test.java.org.apache.flink.sql.parser.hive.FlinkHiveSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutorBuilder.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">docs.dev.table.sql.show.zh.md</file>
      <file type="M">docs.dev.table.sql.show.md</file>
      <file type="M">docs.dev.table.hive.hive.dialect.zh.md</file>
      <file type="M">docs.dev.table.hive.hive.dialect.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-16 01:00:00" id="18619" opendate="2020-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update training to use WatermarkStrategy</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.learn-flink.streaming.analytics.zh.md</file>
      <file type="M">docs.learn-flink.streaming.analytics.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-20 01:00:00" id="18643" opendate="2020-7-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate Jenkins jobs to ci-builds.apache.org</summary>
      <description>Infra is reworking the Jenkins setup, so we have to migrate our jobs that do the snapshot deployments.Alternatively, find other ways to do this (Azure?) to reduce number of used infrastructure services./cc rmetzger</description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.deploy.staging.jars.sh</file>
      <file type="M">tools.deploy.to.maven.sh</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-21 01:00:00" id="18650" opendate="2020-7-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The description of dispatcher in Flink Architecture document is not accurate</summary>
      <description>The description of dispatcher is like below:The Dispatcher provides a REST interface to submit Flink applications for execution and starts a new JobManager for each submitted job. It also runs the Flink WebUI to provide information about job executions. As I understand it, is it "starts a new JobMaster" rather than JobManager?   </description>
      <version>1.11.0</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.concepts.flink-architecture.zh.md</file>
      <file type="M">docs.concepts.flink-architecture.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-22 01:00:00" id="18672" opendate="2020-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Scala code examples for UDF type inference annotations</summary>
      <description>The Scala code examples for the UDF type inference annotations are not correct.For example: the following FunctionHint annotation @FunctionHint( input = Array(@DataTypeHint("INT"), @DataTypeHint("INT")), output = @DataTypeHint("INT"))needs to be changed to@FunctionHint( input = Array(new DataTypeHint("INT"), new DataTypeHint("INT")), output = new DataTypeHint("INT"))</description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-22 01:00:00" id="18676" opendate="2020-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update version of aws to support use of default constructor of "WebIdentityTokenCredentialsProvider"</summary>
      <description>Background:I am using Flink 1.11.0 on kubernetes platform. To give access of aws services to taskmanager/jobmanager, we are using "IAM Roles for Service Accounts" . I have configured below property in flink-conf.yaml to use credential provider.fs.s3a.aws.credentials.provider: com.amazonaws.auth.WebIdentityTokenCredentialsProvider Issue:When taskmanager/jobmanager is starting up, during this it complains that "WebIdentityTokenCredentialsProvider" doesn't have "public constructor" and container doesn't come up. Solution:Currently the above credential's class is being used from "flink-s3-fs-hadoop" which gets "aws-java-sdk-core" dependency from "flink-s3-fs-base". In "flink-s3-fs-base",  version of aws is 1.11.754 . The support of default constructor for "WebIdentityTokenCredentialsProvider" is provided from aws version 1.11.788 and onward.</description>
      <version>1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-base.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-7-23 01:00:00" id="18697" opendate="2020-7-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding flink-table-api-java-bridge_2.11 to a Flink job kills the IDE logging</summary>
      <description>Steps to reproduce: Set up a Flink project using a Maven archetype Add "flink-table-api-java-bridge_2.11" as a dependency Running Flink won't produce any log outputProbable cause:"flink-table-api-java-bridge_2.11" has a dependency to "org.apache.flink:flink-streaming-java_2.11:test-jar:tests:1.11.0", which contains a "log4j2-test.properties" file.When I disable Log4j2 debugging (with "-Dlog4j2.debug"), I see the following line:DEBUG StatusLogger Reconfiguration complete for context[name=3d4eac69] at URI jar:file:/Users/robert/.m2/repository/org/apache/flink/flink-streaming-java_2.11/1.11.0/flink-streaming-java_2.11-1.11.0-tests.jar!/log4j2-test.properties (org.apache.logging.log4j.core.LoggerContext@568bf312) with optional ClassLoader: null</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-11-26 01:00:00" id="18716" opendate="2020-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the deprecated "execute" and "insert_into" calls in PyFlink Table API docs</summary>
      <description>Currently the TableEnvironment#execute and the Table#insert_into is deprecated, but the docs of PyFlink Table API still use them. We should replace them with the recommended API.</description>
      <version>1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.python.shell.zh.md</file>
      <file type="M">docs.ops.python.shell.md</file>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.streaming.query.configuration.zh.md</file>
      <file type="M">docs.dev.table.streaming.query.configuration.md</file>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.table.api.connectors.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.table.api.connectors.md</file>
      <file type="M">docs.dev.python.table.api.tutorial.zh.md</file>
      <file type="M">docs.dev.python.table.api.tutorial.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-27 01:00:00" id="18725" opendate="2020-7-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Run Kubernetes test" failed with "30025: provided port is already allocated"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4901&amp;view=logs&amp;j=08866332-78f7-59e4-4f7e-49a56faa3179&amp;t=3e8647c1-5a28-5917-dd93-bf78594ea994The Service "flink-job-cluster" is invalid: spec.ports[2].nodePort: Invalid value: 30025: provided port is already allocated</description>
      <version>1.11.0,1.11.1</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.container-scripts.job-cluster-service.yaml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-27 01:00:00" id="18726" opendate="2020-7-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support INSERT INTO specific columns</summary>
      <description>Currently Flink only supports insert into a table without specifying columns, but most database systems support insert into specific columns byINSERT INTO table_name(column1, column2, ...) ...The columns not specified will be filled with default values or NULL if no default value is given when creating the table.As Flink currently does not support default values when creating tables, we can fill the unspecified columns with NULL and throw exceptions if there are columns with NOT NULL constraints.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TestData.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.TableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.PreValidateReWriter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.schema.CatalogSourceTable.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-27 01:00:00" id="18730" opendate="2020-7-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Beta tag from SQL Client docs</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-27 01:00:00" id="18731" opendate="2020-7-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The monotonicity of UNIX_TIMESTAMP function is not correct</summary>
      <description>Currently, the monotonicity of UNIX_TIMESTAMP function is always INCREASING, actually, when it has empty function arguments (UNIX_TIMESTAMP(), is equivalent to NOW()), its monotonicity is INCREASING. otherwise its monotonicity should be NOT_MONOTONIC. (e.g. UNIX_TIMESTAMP(string))</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCollationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-5 01:00:00" id="18831" opendate="2020-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the Python documentation about the operations in Table</summary>
      <description>Currently, there are a few documentation is out of date and should be updated. For example, Python UDTF has been already supported in Python Table API and we could use examples of Python UDTF instead of Java UDTF in the Python doc.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-6 01:00:00" id="18838" opendate="2020-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support JdbcCatalog in Python Table API</summary>
      <description>We should provide built-in support for JdbcCatalog in Python Table API.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.catalog.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.table.catalog.py</file>
      <file type="M">docs.dev.table.connectors.jdbc.zh.md</file>
      <file type="M">docs.dev.table.connectors.jdbc.md</file>
      <file type="M">docs.dev.table.catalogs.zh.md</file>
      <file type="M">docs.dev.table.catalogs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-6 01:00:00" id="18839" opendate="2020-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about how to use catalog in Python Table API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
      <file type="M">docs.dev.table.hive.hive.functions.zh.md</file>
      <file type="M">docs.dev.table.hive.hive.functions.md</file>
      <file type="M">docs.dev.table.hive.hive.dialect.zh.md</file>
      <file type="M">docs.dev.table.hive.hive.dialect.md</file>
      <file type="M">docs.dev.table.connectors.jdbc.zh.md</file>
      <file type="M">docs.dev.table.connectors.jdbc.md</file>
      <file type="M">docs.dev.table.catalogs.zh.md</file>
      <file type="M">docs.dev.table.catalogs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-7 01:00:00" id="18842" opendate="2020-8-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>e2e test failed to download "localhost:9999/flink.tgz" in "Wordcount on Docker test"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5260&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=2b7514ee-e706-5046-657b-3430666e7bd92020-08-06T20:51:31.2499580Z [91m+ wget -nv -O flink.tgz localhost:9999/flink.tgz2020-08-06T20:51:31.2501498Z [0m127.0.0.1 - - [06/Aug/2020 20:51:31] "GET /flink.tgz HTTP/1.1" 200 -2020-08-06T20:51:31.2502214Z [91mfailed: Connection refused.2020-08-06T20:51:31.6885068Z [0m [91m2020-08-06 20:51:31 URL:http://localhost:9999/flink.tgz [322693675/322693675] -&gt; "flink.tgz" [1]2020-08-06T20:51:31.6888547Z [0m [91m+ [ false = true ]2020-08-06T20:51:31.6889384Z [0m [91m+ tar -xf flink.tgz --strip-components=12020-08-06T20:51:34.8125585Z [0m [91m+ rm flink.tgz2020-08-06T20:51:34.8699287Z [0m [91m+ chown -R flink:flink .2020-08-07T00:20:42.7919165Z [0m2020-08-07T00:20:43.0365895Z ##[error]The operation was canceled.</description>
      <version>1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.docker.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-9 01:00:00" id="18861" opendate="2020-8-9 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support add_source() to get a DataStream for Python StreamExecutionEnvironment</summary>
      <description>Support add_source() to get a DataStream for Python StreamExecutionEnvironment. </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-10 01:00:00" id="18874" opendate="2020-8-10 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support conversion between Table and DataStream</summary>
      <description>Support Converting a DataStream into a Table, and a Table to append/retract Stream.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-11 01:00:00" id="18883" opendate="2020-8-11 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support reduce() operation for Python KeyedStream.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.datastream.runtime.operators.python.DataStreamPythonStatelessFunctionOperator.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-11 01:00:00" id="18884" opendate="2020-8-11 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add chaining strategy and slot sharing group interfaces for Python DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-11 01:00:00" id="18885" opendate="2020-8-11 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add partitioning interfaces for Python DataStream API.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-11 01:00:00" id="18886" opendate="2020-8-11 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support Kafka connectors for Python DataStream API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.testing.test.case.utils.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.descriptor.py</file>
      <file type="M">flink-python.pyflink.pyflink.gateway.server.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-11 01:00:00" id="18888" opendate="2020-8-11 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support execute_async for StreamExecutionEnvironment.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.util.DataStreamTestCollectSink.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.util.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-13 01:00:00" id="18916" opendate="2020-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a "Operations" link(linked to dev/table/tableApi.md) under the "Python API" -&gt; "User Guide" -&gt; "Table API" section</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.user-guide.table.sql.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.sql.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.table.api.connectors.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.table.api.connectors.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.index.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-13 01:00:00" id="18917" opendate="2020-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a "Built-in Functions" link (linked to dev/table/functions/systemFunctions.md) under the "Python API" -&gt; "User Guide" -&gt; "Table API" section</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..layouts.base.html</file>
      <file type="M">docs.page.js.flink.js</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-16 01:00:00" id="1892" opendate="2015-4-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Local job execution does not exit.</summary>
      <description>When using the LocalTezEnvironment to run a job from the IDE the job fails to exit after producing data. The following thread seems to run and not allow the job to exit:"Thread-31" #46 prio=5 os_prio=31 tid=0x00007fb5d2c43000 nid=0x5507 runnable &amp;#91;0x0000000127319000&amp;#93; java.lang.Thread.State: RUNNABLE at java.lang.Throwable.fillInStackTrace(Native Method) at java.lang.Throwable.fillInStackTrace(Throwable.java:783) locked &lt;0x000000076dfda130&gt; (a java.lang.InterruptedException) at java.lang.Throwable.&lt;init&gt;(Throwable.java:250) at java.lang.Exception.&lt;init&gt;(Exception.java:54) at java.lang.InterruptedException.&lt;init&gt;(InterruptedException.java:57) at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1220) at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:335) at java.util.concurrent.PriorityBlockingQueue.take(PriorityBlockingQueue.java:545) at org.apache.tez.dag.app.rm.LocalTaskSchedulerService$AsyncDelegateRequestHandler.processRequest(LocalTaskSchedulerService.java:322) at org.apache.tez.dag.app.rm.LocalTaskSchedulerService$AsyncDelegateRequestHandler.run(LocalTaskSchedulerService.java:316) at java.lang.Thread.run(Thread.java:745)</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-13 01:00:00" id="18926" opendate="2020-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a "Environment Variables" document under the "Python API" -&gt; "User Guide" -&gt; "Table API" section</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">docs..includes.generated.python.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-13 01:00:00" id="18930" opendate="2020-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate "Hive Dialect" page of "Hive Integration" into Chinese</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.hive.dialect.zh.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-9-14 01:00:00" id="18957" opendate="2020-8-14 00:00:00" resolution="Done">
    <buginformation>
      <summary>Implement bulk fulfil-ability timeout tracking for shared slots</summary>
      <description>Track fulfil-ability of required physical slots for all SharedSlot(s) (no matter whether they are created at this bulk or not) with timeout. This ensures we will not wait indefinitely if the required slots for this bulk cannot be fully fulfilled at the same time. Create a LogicalSlotRequestBulk to track all physical requests and logical slot requests (logical slot requests only which belong to the bulk) Mark physical slot request fulfilled in LogicalSlotRequestBulk, once its future is done If any physical slot request fails then clear the LogicalSlotRequestBulk to stop the fulfil-ability check Schedule a fulfil-ability check in LogicalSlotRequestBulkChecker for the LogicalSlotRequestBulk In case of timeout: cancel/fail the logical slot futures of the bulk in SharedSlot(s) remove</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkCheckerImpl.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.OneSlotPerExecutionSlotAllocatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkCheckerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.slotpool.BulkSlotProviderImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkChecker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulk.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.BulkSlotProviderImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.BulkSlotProvider.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.MergingSharedSlotProfileRetrieverTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SharedSlotProfileRetriever.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.MergingSharedSlotProfileRetrieverFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocatorFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-8-18 01:00:00" id="18987" opendate="2020-8-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sort global job parameters in WebUI job configuration view</summary>
      <description>The configuration page for a job has various entries for set GlobalJobParameters.It would be nice if these were sorted alphabetically.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.configuration.job-configuration.component.ts</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-8-19 01:00:00" id="18997" opendate="2020-8-19 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Rename type_info to result_type to make it more clear</summary>
      <description>Currently, the name of result type parameter for Python DataStream.map() and flat_map() is type_info. def map(self, func: Union[Callable, MapFunction], type_info: TypeInformation = None) \ -&gt; 'DataStream':I think rename it to result_type would be more clear for users. def map(self, func: Union[Callable, MapFunction], result_type: TypeInformation = None) \ -&gt; 'DataStream':</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-9-28 01:00:00" id="19070" opendate="2020-8-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive connector should throw a meaningful exception if user reads/writes ACID tables</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2020-9-31 01:00:00" id="19097" opendate="2020-8-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support add_jars() for Python StreamExecutionEnvironment</summary>
      <description>Add add_jars() interface in Python StreamExecutionEnvironment to enable users to specify jar dependencies in their Python DataStream Job.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-9-1 01:00:00" id="19109" opendate="2020-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split Reader eats chained periodic watermarks</summary>
      <description>Attempting to generate watermarks chained to the Split Reader / ContinuousFileReaderOperator, as inSingleOutputStreamOperator&lt;Event&gt; results = env .readTextFile(...) .map(...) .assignTimestampsAndWatermarks(bounded) .keyBy(...) .process(...);leads to the Watermarks failing to be produced. Breaking the chain, via disableOperatorChaining() or a rebalance, works around the bug. Using punctuated watermarks also avoids the issue.Looking at this in the debugger reveals that timer service is being prematurely quiesced.In many respects this is FLINK-7666 brought back to life.The problem is not present in 1.9.3.There's a minimal reproducible example in https://github.com/alpinegizmo/flink-question-001/tree/bug.</description>
      <version>1.10.0,1.10.1,1.10.2,1.11.0,1.11.1</version>
      <fixedVersion>1.10.3,1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImplTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-1 01:00:00" id="19110" opendate="2020-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flatten current PyFlink documentation structure</summary>
      <description>The navigation for this entire section is overly complex. I would much rather see something flatter, like this: Python API Installation Table API Tutorial Table API User's Guide DataStream API User's Guide FAQ</description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.try-flink.python.table.api.zh.md</file>
      <file type="M">docs.try-flink.python.table.api.md</file>
      <file type="M">docs.ops.python.shell.zh.md</file>
      <file type="M">docs.ops.python.shell.md</file>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.sql.alter.zh.md</file>
      <file type="M">docs.dev.table.sql.alter.md</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.python.udfs.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.udfs.index.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.types.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.types.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.config.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.python.config.md</file>
      <file type="M">docs.dev.python.user-guide.table.metrics.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.metrics.md</file>
      <file type="M">docs.dev.python.user-guide.table.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.index.md</file>
      <file type="M">docs.dev.python.user-guide.table.dependency.management.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.dependency.management.md</file>
      <file type="M">docs.dev.python.user-guide.table.conversion.of.pandas.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.conversion.of.pandas.md</file>
      <file type="M">docs.dev.python.user-guide.table.built.in.functions.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.built.in.functions.md</file>
      <file type="M">docs.dev.python.user-guide.table.10.minutes.to.table.api.zh.md</file>
      <file type="M">docs.dev.python.user-guide.table.10.minutes.to.table.api.md</file>
      <file type="M">docs.dev.python.user-guide.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.index.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.index.zh.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.index.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.data.types.zh.md</file>
      <file type="M">docs.dev.python.user-guide.datastream.data.types.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.table.api.tutorial.zh.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.table.api.tutorial.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.index.zh.md</file>
      <file type="M">docs.dev.python.getting-started.tutorial.index.md</file>
      <file type="M">docs.dev.python.getting-started.installation.zh.md</file>
      <file type="M">docs.dev.python.getting-started.installation.md</file>
      <file type="M">docs.dev.python.getting-started.index.zh.md</file>
      <file type="M">docs.dev.python.getting-started.index.md</file>
      <file type="M">docs.dev.python.faq.zh.md</file>
      <file type="M">docs.dev.python.faq.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-2 01:00:00" id="19119" opendate="2020-9-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the documentation to use Expression instead of strings in the Python Table API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.expression.py</file>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.common.zh.md</file>
      <file type="M">docs.dev.table.common.md</file>
      <file type="M">docs.dev.python.table.api.tutorial.zh.md</file>
      <file type="M">docs.dev.python.table.api.tutorial.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.intro.to.table.api.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.intro.to.table.api.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.conversion.of.pandas.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.conversion.of.pandas.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-4 01:00:00" id="19138" opendate="2020-9-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python UDF supports directly specifying input_types as DataTypes.ROW</summary>
      <description>Python UDF supports input_types=DataTypes.ROW</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udtf.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-11 01:00:00" id="19201" opendate="2020-9-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink e2e tests is instable and failed with "Connection broken: OSError"</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6452&amp;view=logs&amp;j=ff2e2ea5-07e3-5521-7b04-a4fc3ad765e9&amp;t=6945d9e3-ebef-5993-0c44-838d8ad079c02020-09-10T21:37:42.9988117Z install conda ... [SUCCESS]2020-09-10T21:37:43.0018449Z install miniconda... [SUCCESS]2020-09-10T21:37:43.0082244Z installing python environment...2020-09-10T21:37:43.0100408Z installing python3.5...2020-09-10T21:37:58.7214400Z install python3.5... [SUCCESS]2020-09-10T21:37:58.7253792Z installing python3.6...2020-09-10T21:38:06.5855143Z install python3.6... [SUCCESS]2020-09-10T21:38:06.5903358Z installing python3.7...2020-09-10T21:38:11.5444706Z 2020-09-10T21:38:11.5484852Z ('Connection broken: OSError("(104, \'ECONNRESET\')")', OSError("(104, 'ECONNRESET')"))2020-09-10T21:38:11.5513130Z 2020-09-10T21:38:11.8044086Z conda install 3.7 failed. You can retry to exec the script.</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-18 01:00:00" id="19284" opendate="2020-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about how to use Python UDF in the Java Table API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.python.udfs.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.table.environment.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.table.environment.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.dependency.management.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.dependency.management.md</file>
      <file type="M">docs.dev.python.faq.zh.md</file>
      <file type="M">docs.dev.python.faq.md</file>
      <file type="M">docs.dev.python.datastream-api-users-guide.dependency.management.zh.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-9-23 01:00:00" id="19364" opendate="2020-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Batch Physical Pandas Group Window Aggregate Rule and RelNode</summary>
      <description>Add Batch Physical Pandas Group Window Aggregate Rule and RelNode to support Pandas Batch Group Window Aggregation</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupWindowAggregateFunctionOperator.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udaf.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-1-28 01:00:00" id="19436" opendate="2020-9-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TPC-DS end-to-end test (Blink planner) failed during shutdown</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7009&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=2b7514ee-e706-5046-657b-3430666e7bd92020-09-27T22:37:53.2236467Z Stopping taskexecutor daemon (pid: 2992) on host fv-az655.2020-09-27T22:37:53.4450715Z Stopping standalonesession daemon (pid: 2699) on host fv-az655.2020-09-27T22:37:53.8014537Z Skipping taskexecutor daemon (pid: 11173), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8019740Z Skipping taskexecutor daemon (pid: 11561), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8022857Z Skipping taskexecutor daemon (pid: 11849), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8023616Z Skipping taskexecutor daemon (pid: 12180), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8024327Z Skipping taskexecutor daemon (pid: 12950), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8025027Z Skipping taskexecutor daemon (pid: 13472), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8025727Z Skipping taskexecutor daemon (pid: 16577), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8026417Z Skipping taskexecutor daemon (pid: 16959), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8027086Z Skipping taskexecutor daemon (pid: 17250), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8027770Z Skipping taskexecutor daemon (pid: 17601), because it is not running anymore on fv-az655.2020-09-27T22:37:53.8028400Z Stopping taskexecutor daemon (pid: 18438) on host fv-az655.2020-09-27T22:37:53.8029314Z /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/bin/taskmanager.sh: line 99: 18438 Terminated "${FLINK_BIN_DIR}"/flink-daemon.sh $STARTSTOP $ENTRYPOINT "${ARGS[@]}"2020-09-27T22:37:53.8029895Z [FAIL] Test script contains errors.2020-09-27T22:37:53.8032092Z Checking for errors...2020-09-27T22:37:55.3713368Z No errors in log files.2020-09-27T22:37:55.3713935Z Checking for exceptions...2020-09-27T22:37:56.9046391Z No exceptions in log files.2020-09-27T22:37:56.9047333Z Checking for non-empty .out files...2020-09-27T22:37:56.9064402Z No non-empty .out files.2020-09-27T22:37:56.9064859Z 2020-09-27T22:37:56.9065588Z [FAIL] 'TPC-DS end-to-end test (Blink planner)' failed after 16 minutes and 54 seconds! Test exited with exit code 1</description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.4,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.sql.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.batch.sql.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-10-30 01:00:00" id="19480" opendate="2020-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support RichFunction in Python DataStream API</summary>
      <description>Currently, RichFunction is still not supported in Python DataStream API. It's a prerequisite for several features, such as: Metrics support State support The other functionalities which are available in the Java FunctionContext</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.streaming.api.typeutils.PythonTypeUtilsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.utils.PythonOperatorUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.AbstractPythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.AbstractPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonOverWindowAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.AbstractArrowPythonAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.PythonTableFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.AbstractPythonScalarFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.typeutils.PythonTypeUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamDataStreamStatelessPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.StatelessTwoInputPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.StatelessOneInputPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonReduceOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonPartitionCustomOperator.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pyx</file>
      <file type="M">flink-python.pyflink.datastream.functions.py</file>
      <file type="M">flink-python.pyflink.datastream.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-9 01:00:00" id="19542" opendate="2020-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement LeaderElectionService and LeaderRetrievalService based on Kubernetes API</summary>
      <description>LeaderElectionService Contends for the leadership of a service in JobManager. There are four components in a JobManager instance that use LeaderElectionService: ResourceManager, Dispatcher, JobManager, RestEndpoint(aka WebMonitor).  Once the leader election is finished, the active leader addresses will be stored in the ConfigMap so that other components could retrieve successfully.  LeaderRetrievalService Used by Client to get the RestEndpoint address for the job submission. Used by JobManager to get the ResourceManager address for registration. Used by TaskManagers to retrieve addresses of the corresponding LeaderElectionService(e.g. JobManager address, ResourceManager address) for registration and offering slots</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.FlinkMatchers.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.KubernetesClientTestBase.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClientTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">docs..includes.generated.kubernetes.config.configuration.html</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.TestingFlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.FlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.java</file>
      <file type="M">flink-annotations.src.main.java.org.apache.flink.annotation.docs.Documentation.java</file>
      <file type="M">docs.ops.config.zh.md</file>
      <file type="M">docs.ops.config.md</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.kubernetes.itcases.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-9 01:00:00" id="19543" opendate="2020-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement RunningJobsRegistry, JobGraphStore based on Kubernetes API</summary>
      <description>RunningJobsRegistry Registry for the running jobs. All the jobs in the registry will be recovered when JobManager failover.  JobGraphStore JobGraph instances for running JobManagers. Note that only the meta information(aka location reference, DFS path) will be stored in the Zookeeper/ConfigMap. The real data is stored on the DFS.Each component(Dispatcher, ResourceManager, JobManager, RestEndpoint) will have a dedicated ConfigMap. All the HA information relevant for a specific component will be stored in a single ConfigMap. So the Dispatcher's ConfigMap would then contain the current leader, the running jobs and the pointers to the persisted JobGraphs.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.TestingFlinkKubeClient.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityTestBase.java</file>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.kubernetes.itcases.sh</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServices.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-9 01:00:00" id="19544" opendate="2020-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement CheckpointRecoveryFactory based on Kubernetes API</summary>
      <description>CheckpointRecoveryFactory Stores meta information to Zookeeper/ConfigMap for checkpoint recovery. Stores the latest checkpoint counter.Each component(Dispatcher, ResourceManager, JobManager, RestEndpoint) will have a dedicated ConfigMap. All the HA information relevant for a specific component will be stored in a single ConfigMap. The JobManager's ConfigMap would then contain the current leader, the pointers to the checkpoints and the checkpoint ID counter. Since “Get(check the leader)-and-Update(write back to the ConfigMap)” is a transactional operation, we will completely solved the concurrent modification issues and not using the "lock-and-release" in Zookeeper.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.KubernetesUtils.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.utils.Constants.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServices.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-9 01:00:00" id="19545" opendate="2020-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add e2e test for native Kubernetes HA</summary>
      <description>We could use minikube for the E2E tests. Start a Flink session/application cluster on K8s, kill one TaskManager pod or JobManager Pod and wait for the job recovered from the latest checkpoint successfully.kubectl exec -it {pod_name} -- /bin/sh -c "kill 1"</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.kubernetes.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-9 01:00:00" id="19546" opendate="2020-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for native Kubernetes HA</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.jobmanager.high.availability.zh.md</file>
      <file type="M">docs.ops.jobmanager.high.availability.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-14 01:00:00" id="19643" opendate="2020-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Pandas UDAF Doc</summary>
      <description>Add Pandas UDAF Doc</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-3-15 01:00:00" id="19667" opendate="2020-10-15 00:00:00" resolution="Done">
    <buginformation>
      <summary>Add integration with AWS Glue</summary>
      <description>AWS Glue is releasing new features for the AWS Glue Data Catalog. This request is to add a new format to launch an integration for Apache Flink with AWS Glue Data Catalog</description>
      <version>1.11.0,1.11.1,1.11.2,1.11.3</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-16 01:00:00" id="19675" opendate="2020-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The plan of is incorrect when Calc contains WHERE clause, composite fields access and Python UDF at the same time</summary>
      <description>For the following job:SELECT a, pyFunc1(b, d._1) FROM MyTable WHERE a + 1 &gt; 0The plan is as following:FlinkLogicalCalc(select=[a, pyFunc1(b, f0) AS EXPR$1])+- FlinkLogicalCalc(select=[a, b, d._1 AS f0]) +- FlinkLogicalLegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c, d)]]], fields=[a, b, c, d])It's incorrect as the where condition is missing.</description>
      <version>1.10.1,1.11.0</version>
      <fixedVersion>1.10.3,1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.PythonCalcSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.PythonCalcSplitRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRule.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-21 01:00:00" id="19749" opendate="2020-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve documentation in Table API page</summary>
      <description>in this link there's a "description":Performs a field add operation. Existing fields will be replaced if add columns name is the same as the existing column name. Moreover, if the added fields have duplicate field name, then the last one is used.Two point:①" add columns name "-&gt;" added column's name"②"if the added fields have duplicate field name",what's the example that has two duplicate field name?Could you modify them?They're a little misleading/unclear.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-21 01:00:00" id="19750" opendate="2020-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deserializer is not opened in Kafka consumer when restoring from state</summary>
      <description>When a job using Kafka consumer is recovered from a checkpoint or savepoint, the open method of the record deserializer is not called. This is possibly because this.deserializer.open is put into the else clause by mistake, which will only be called if the job has a clean start. </description>
      <version>1.11.0,1.11.1,1.11.2</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-22 01:00:00" id="19755" opendate="2020-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix CEP documentation error of the example in &amp;#39;After Match Strategy&amp;#39; section</summary>
      <description>symbol tax price rowtime======== ===== ======= ===================== XYZ 1 7 2018-09-17 10:00:01 XYZ 2 9 2018-09-17 10:00:02 XYZ 1 10 2018-09-17 10:00:03 XYZ 2 5 2018-09-17 10:00:04 XYZ 2 17 2018-09-17 10:00:05 XYZ 2 14 2018-09-17 10:00:06SELECT *FROM Ticker MATCH_RECOGNIZE( PARTITION BY symbol ORDER BY rowtime MEASURES SUM(A.price) AS sumPrice, FIRST(rowtime) AS startTime, LAST(rowtime) AS endTime ONE ROW PER MATCH [AFTER MATCH STRATEGY] PATTERN (A+ C) DEFINE A AS SUM(A.price) &lt; 30 ) AFTER MATCH SKIP TO LAST A symbol sumPrice startTime endTime======== ========== ===================== ===================== XYZ 26 2018-09-17 10:00:01 2018-09-17 10:00:04 XYZ 15 2018-09-17 10:00:03 2018-09-17 10:00:05 XYZ 22 2018-09-17 10:00:04 2018-09-17 10:00:06 XYZ 17 2018-09-17 10:00:05 2018-09-17 10:00:06Again, the first result matched against the rows #1, #2, #3, #4.Compared to the previous strategy, the next match includes only row #3 (mapped to A) again for the next matching.Therefore, the second result matched against the rows #3, #4, #5.The third result matched against the rows #4, #5, #6.The last result matched against the rows #5, #6.i think it will exist looping match when coming to 17, 14 using "AFTER MATCH SKIP TO LAST A "</description>
      <version>1.11.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.streaming.match.recognize.zh.md</file>
      <file type="M">docs.dev.table.streaming.match.recognize.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-11-30 01:00:00" id="19894" opendate="2020-10-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use iloc for positional slicing instead of direct slicing in from_pandas</summary>
      <description>When you use floats are index of pandas, it produces a wrong results: &gt;&gt;&gt; import pandas as pd&gt;&gt;&gt; t_env.from_pandas(pd.DataFrame({'a': [1, 2, 3]}, index=[2., 3., 4.])).to_pandas() a0 11 2 This is because direct slicing uses the value as index when the index contains floats: &gt;&gt;&gt; pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.])[2:] a2.0 13.0 24.0 3&gt;&gt;&gt; pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.]).iloc[2:] a4.0 3&gt;&gt;&gt; pd.DataFrame({'a': [1,2,3]}, index=[2, 3, 4])[2:] a4 3 </description>
      <version>1.11.0,1.12.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.conversion.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-30 01:00:00" id="19896" opendate="2020-10-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve first-n-row fetching in the rank operator</summary>
      <description>Currently Deduplicate operator only supports first-row deduplication (ordered by proc-time). In scenario of first-n-rows deduplication, the planner has to resort to Rank operator. However, Rank operator is less efficient than Deduplicate due to larger state and more state access.This issue proposes to extend DeduplicateKeepFirstRowFunction to support first-n-rows deduplication. And the original first-row deduplication would be a special case of first-n-rows deduplication.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.RankJsonPlanITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecRank.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-30 01:00:00" id="19897" opendate="2020-10-30 00:00:00" resolution="Done">
    <buginformation>
      <summary>Improve UI related to FLIP-102</summary>
      <description>This ticket collects issues that came up after merging FLIP-102 related changes into master. The following issues should be fixed. Add Tooltip to Heap metrics cell pointing out that the max metrics might differ from the configured maximum value. This tooltip could be made optional and only appears if heap max is different from the configured value. Here's a proposal for the tooltip text: The maximum heap displayed might differ from the configured values depending on the used GC algorithm for this process. Rename "Network Memory Segments" into "Netty Shuffle Buffers" Rename "Network Garbage Collection" into "Garbage Collection"</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.styles.rewrite.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-30 01:00:00" id="19899" opendate="2020-10-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Kinesis][EFO] Optimise error handling to use a separate exception delivery mechanism</summary>
      <description>BackgroundThere is a queue used to pass events between the network client and consumer application. When an error is thrown in the network thread, the queue is cleared to make space for the error event. This means that records will be thrown away to make space for errors (the records would be subsequently reloaded from the shard).ScopeAdd a new mechanism to pass exceptions between threads, meaning data does not need to be discarded. When an error is thrown, the error event will be processed by the consumer once all of the records have been processed.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.FakeKinesisFanOutBehavioursFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxyV2Test.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-11-19 01:00:00" id="20243" opendate="2020-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove useless words in documents</summary>
      <description/>
      <version>1.11.0</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-15 01:00:00" id="2025" opendate="2015-5-15 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support booleans in CSV reader</summary>
      <description>It would be great if Flink allowed to read booleans from CSV files, e.g. 1 for true and 0 for false.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.parser.StringParser.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.types.parser.FieldParser.java</file>
      <file type="M">docs.apis.programming.guide.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-12-3 01:00:00" id="20456" opendate="2020-12-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make streaming SQL concepts more approachable</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.streaming.dynamic.tables.md</file>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.streaming.index.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
      <file type="M">docs.dev.table.sql.gettingStarted.zh.md</file>
      <file type="M">docs.dev.table.streaming.index.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.zh.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.md</file>
      <file type="M">docs.dev.table.streaming.joins.zh.md</file>
      <file type="M">docs.dev.table.streaming.joins.md</file>
      <file type="M">docs.dev.table.streaming.time.attributes.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-10 01:00:00" id="20562" opendate="2020-12-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support ExplainDetails for EXPLAIN sytnax</summary>
      <description>Currently, EXPLAIN syntax only supports to print the default AST, logical plan, and physical plan. However, it doesn't support to print detailed information such as CHANGELOG_MODE, ESTIMATED_COST, JSON_EXECUTION_PLAN which are defined in ExplainDetail.Allow users to specify the ExplainDetails in statement.EXPLAIN [ExplainDetail[, ExplainDetail]*] &lt;statement&gt;ExplainDetail: { ESTIMATED_COST, CHANGELOG_MODE, JSON_EXECUTION_PLAN}Print the plan for the statement with specified ExplainDetails. ESTIMATED_COSTgenerates cost information on physical node estimated by optimizer, e.g. TableSourceScan(..., cumulative cost = {1.0E8 rows, 1.0E8 cpu, 2.4E9 io, 0.0 network, 0.0 memory})CHANGELOG_MODEgenerates changelog mode for every physical rel node. e.g. GroupAggregate(..., changelogMode=&amp;#91;I,UA,D&amp;#93;)JSON_EXECUTION_PLANgenerates the execution plan in json format of the program.Flink SQL&gt; EXPLAIN ESTIMATED_COST, CHANGELOG SELECT * FROM MyTable;...</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ExplainOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.resources.org.apache.flink.sql.parser.utils.ParserResource.properties</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.utils.ParserResource.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.dql.SqlRichExplain.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.table.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-10 01:00:00" id="20563" opendate="2020-12-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support built-in functions for Hive versions prior to 1.2.0</summary>
      <description>Currently Hive built-in functions are supported only for Hive-1.2.0 and later. We should investigate how to lift this limitation.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.module.hive.HiveModuleTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV120.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV100.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-2-8 01:00:00" id="20894" opendate="2021-1-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce SupportsAggregatePushDown interface</summary>
      <description>Will introduce the SupportsAggregatePushDown interface for local agg pushdown</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.ScanTableSource.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-4-21 01:00:00" id="21073" opendate="2021-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mention that RocksDB ignores equals/hashCode because it works on binary data</summary>
      <description>See https://lists.apache.org/thread.html/ra43e2b5d388831290c293b9daf0eee0b0a5d9712543b62c83234a829%40%3Cuser.flink.apache.org%3E</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.state.backends.md</file>
      <file type="M">docs.content.zh.docs.ops.state.state.backends.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-25 01:00:00" id="21123" opendate="2021-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Beanutils 1.9.x to 1.9.4</summary>
      <description>CVE-2019-10086</description>
      <version>None</version>
      <fixedVersion>1.11.4,1.12.2,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-5-14 01:00:00" id="22666" opendate="2021-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make structured type&amp;#39;s fields more lenient during casting</summary>
      <description>While writing documentation in FLINK-22537, I found some issues when using the Scala DataStream API. We should add more tests to identify those.</description>
      <version>None</version>
      <fixedVersion>1.14.0,1.13.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastsTest.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.types.LogicalTypeCastAvoidanceTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeCasts.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-17 01:00:00" id="22673" opendate="2021-5-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document about add jar related commands</summary>
      <description>Including ADD JAR, SHOW JAR, REMOVE JAR.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.show.md</file>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.show.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-8-24 01:00:00" id="23936" opendate="2021-8-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python UDFs instances are reinitialized if there is no input for more than 1 minute</summary>
      <description>We receive this feedback from some PyFlink users. After some investigation, we find out that the root case is that there is a mechanism in Beam that it will released the BundleProcessors which are inactive for more than 1 minute: https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/worker/sdk_worker.py#L90</description>
      <version>1.10.0,1.11.0,1.12.0,1.13.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.sdk.worker.main.py</file>
    </fixedFiles>
  </bug>
  
</bugrepository>