<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2016-6-27 01:00:00" id="3838" opendate="2016-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CLI parameter parser is munging application params</summary>
      <description>If parameters for an application use a single '-' (e.g. -maxtasks) then the CLI argument parser will munge these, and the app gets passed either just the parameter name (e.g. 'maxtask') if the start of the parameter doesn't match a Flink parameter, or you get two values, with the first value being the part that matched (e.g. '-m') and the second value being the rest (e.g. 'axtasks').The parser should ignore everything after the jar path parameter.Note that using --&lt;parameter name&gt; does seem to work.</description>
      <version>1.0.2,1.0.3</version>
      <fixedVersion>1.0.4,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.CliFrontendRunTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-29 01:00:00" id="3854" opendate="2016-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Avro key-value rolling sink writer</summary>
      <description>Support rolling sink writer in avro key value format.preferably without additional classpath dependenciespreferable in same format as M/R jobs for backward compatibility</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.RollingSinkITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-30 01:00:00" id="3855" opendate="2016-4-30 00:00:00" resolution="Done">
    <buginformation>
      <summary>Upgrade Jackson version</summary>
      <description>Jackson version in use (2.4.2) is rather old (and not even the latest patch from minor version), so it'd be make sense to upgrade to bit newer. Latest would be 2.7.4, but at first I propose going to 2.5.5.All tests pass, but if there are issues I'd be happy to help; I'm author of Jackson project.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-streaming-connectors.flink-connector-elasticsearch2.pom.xml</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-6-31 01:00:00" id="3993" opendate="2016-5-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Add generateSequence() support to Python API</summary>
      <description>Right now, I believe that there is only from_elements() support in order to create a sequence of numbers. It is interesting to be able to create a list of numbers from the Python API also, apart from the Java API. It would not be complicated, since we already have generateSequence(). I am already working on this, and will create a pull request shortly in Github.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.main.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.OperationInfo.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Environment.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Constants.py</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonPlanBinder.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonOperationInfo.java</file>
      <file type="M">docs.apis.batch.python.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-1 01:00:00" id="4002" opendate="2016-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Improve testing infraestructure</summary>
      <description>The Verify() test function errors out when array elements are missing:env.generate_sequence(1, 5)\ .map(Id()).map_partition(Verify([1,2,3,4], "Sequence")).output()IndexError: list index out of rangeThere should also be more documentation in test functions.I am already working on a pull request to fix this.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.main2.py</file>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.main.py</file>
      <file type="M">flink-libraries.flink-python.src.test.java.org.apache.flink.python.api.PythonPlanBinderTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-3 01:00:00" id="4017" opendate="2016-6-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Add Aggregation support to Python API</summary>
      <description>Aggregations are not currently supported in the Python API.I was getting started with setting up and working with Flink and figured this would be a relatively simple task for me to get started with. Currently working on this at https://github.com/geofbot/flink</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.main2.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.DataSet.py</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonPlanBinder.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonOperationInfo.java</file>
      <file type="M">docs.apis.batch.python.md</file>
      <file type="M">docs.apis.batch.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-8 01:00:00" id="4035" opendate="2016-6-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Apache Kafka 0.10 connector</summary>
      <description>Kafka 0.10.0.0 introduced protocol changes related to the producer. Published messages now include timestamps and compressed messages now include relative offsets. As it is now, brokers must decompress publisher compressed messages, assign offset to them, and recompress them, which is wasteful and makes it less likely that compression will be used at all.</description>
      <version>1.0.3</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.JobManagerCommunicationUtils.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.testutils.DataGenerators.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaShortRetentionTestBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcherTimestampsTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionStateWithPunctuatedWatermarks.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionStateWithPeriodicWatermarks.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer09.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.SimpleConsumerThread.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010ITCase.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010TableSource.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010JsonTableSource.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer010.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer010.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-0.10.pom.xml</file>
      <file type="M">docs.page.js.flink.js</file>
      <file type="M">docs.dev.connectors.kafka.md</file>
      <file type="M">flink-streaming-connectors.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-7-15 01:00:00" id="4070" opendate="2016-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support literals on left side of binary expressions</summary>
      <description>The Table API does not support binary expressions like 12 &lt; 'f0 in Scala DSL where the left side is a literal. Maybe this can be solved by implicits or at least by a 12.toExpr method.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.ExpressionsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2016-7-1 01:00:00" id="4139" opendate="2016-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn: Adjust parallelism and task slots correctly</summary>
      <description>The Yarn CLI should handle the following situations correctly: The user specifies no parallelism -&gt; parallelism is adjusted to #taskSlots * #nodes. The user specifies parallelism but no #taskSlots or too few slots -&gt; #taskSlots are set such that they meet the parallelismThese functionality has been present in Flink 1.0.x but there were some glitches in the implementation.</description>
      <version>1.0.3,1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.FlinkYarnSessionCliTest.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.CliFrontendYarnAddressConfigurationTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.CliFrontendAddressConfigurationTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-1 01:00:00" id="4141" opendate="2016-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TaskManager failures not always recover when killed during an ApplicationMaster failure in HA mode on Yarn</summary>
      <description>High availability on Yarn often fails to recover in the following test scenario:1. Kill application master process.2. Then, while application master is recovering, randomly kill several task managers (with some delay).After the application master recovered, not all the killed task manager are brought back and no further attempts are made the restart them.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManagerCallbackHandler.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnFlinkResourceManager.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.messages.ContainersComplete.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.messages.ContainersAllocated.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.standalone.StandaloneResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.FlinkResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-1 01:00:00" id="4142" opendate="2016-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Recovery problem in HA on Hadoop Yarn 2.4.1</summary>
      <description>On Hadoop Yarn 2.4.1, recovery in HA fails in the following scenario:1) Kill application master, let it recover normally.2) After that, kill a task manager.Now, Yarn tries to restart the killed task manager in an endless loop.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.jobmanager.high.availability.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-1 01:00:00" id="4144" opendate="2016-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn properties file: replace hostname/port with Yarn application id</summary>
      <description>We should use the application id instead of the host/port. The hostname and port of the JobManager can change (HA). Also, it is not unique depending on the network configuration.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.CliFrontendYarnAddressConfigurationTest.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-7-5 01:00:00" id="4151" opendate="2016-7-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Address Travis CI build time: We are exceeding the 2 hours limit</summary>
      <description>We've recently started hitting the two hours limit for Travis CI.I'll look into some approaches to get our build stable again.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-7-7 01:00:00" id="4166" opendate="2016-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate automatic different namespaces in Zookeeper for Flink applications</summary>
      <description>We should automatically generate different namespaces per Flink application in Zookeeper to avoid interference between different applications that refer to the same Zookeeper entries.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnConfigKeys.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnApplicationMasterRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.FlinkYarnSessionCliTest.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.CliFrontendYarnAddressConfigurationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.DefaultCLI.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">docs.setup.yarn.setup.md</file>
      <file type="M">docs.setup.jobmanager.high.availability.md</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-7-8 01:00:00" id="4185" opendate="2016-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reflecting rename from Tachyon to Alluxio</summary>
      <description>The Tachyon project has been renamed to Alluxio earlier this year. The goal of this issue is to reflect this in the Flink documentation.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.batch.connectors.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-7-14 01:00:00" id="4216" opendate="2016-7-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WordWithCount example with Java has wrong generics type</summary>
      <description>The Java example of the POJOs results in the:Exception in thread "main" java.lang.Error: Unresolved compilation problem: due to the wrong type of the generics of the DataStream.Currently it is DataStream&lt;Tuple2&lt;String,Integer&gt;&gt;but should be DataSource&lt;WordWithCount&gt;.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.common.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-17 01:00:00" id="4226" opendate="2016-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo: Define Keys using Field Expressions example should use window and not reduce</summary>
      <description>...val words: DataStream[WC] = // [...]val wordCounts = words.keyBy("word").window(/*window specification*/)// or, as a case class, which is less typingcase class WC(word: String, count: Int)val words: DataStream[WC] = // [...]val wordCounts = words.keyBy("word").reduce(/*window specification*/)Should be: val wordCounts = words.keyBy("word").reduce window(/window specification/)</description>
      <version>1.0.3</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.common.index.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-7-18 01:00:00" id="4229" opendate="2016-7-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not start Metrics Reporter by default</summary>
      <description>By default, we start a JMX reported that binds to a port and comes with extra threads. We should not start any reported by default to keep the overhead to a minimum.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerMetricTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.metrics.reporter.JMXReporterTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.reporter.JMXReporter.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.metrics.MetricRegistry.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.apis.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-18 01:00:00" id="4230" opendate="2016-7-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Session Windowing IT Case</summary>
      <description>An ITCase for Session Windows is missing that tests correct behavior under several parallel sessions, with timely events, late events within and after the lateness interval.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.TestEventPayload.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.StreamEventFactory.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.StreamConfiguration.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.SessionWindowITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.SessionStreamConfiguration.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.SessionEventGeneratorImpl.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.SessionEvent.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.SessionConfiguration.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.ParallelSessionsEventGenerator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.LongRandomGenerator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.EventGeneratorFactory.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.windowing.sessionwindows.EventGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-1 01:00:00" id="4288" opendate="2016-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make it possible to unregister tables</summary>
      <description>Table names can not be changed yet. After registration you can not modify the table behind a table name. Maybe this behavior is too restrictive.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.TableEnvironmentITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-1 01:00:00" id="4292" opendate="2016-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HCatalog project incorrectly set up</summary>
      <description>The HCatalog project is erroneous in IntelliJ, because it misses the Scala SDK dependency.</description>
      <version>1.0.3</version>
      <fixedVersion>1.1.1,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-batch-connectors.flink-hcatalog.src.main.scala.org.apache.flink.hcatalog.scala.HCatInputFormat.scala</file>
      <file type="M">flink-batch-connectors.flink-hcatalog.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-11-1 01:00:00" id="4294" opendate="2016-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow access of composite type fields</summary>
      <description>Currently all Flink CompositeTypes are treated as GenericRelDataTypes. It would be better to access individual fields of composite types, too. e.g.SELECT composite.name FROM compositesSELECT tuple.f0 FROM tuples'f0.getField(0)</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.TableWithSQLITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.sql.SqlITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.ProjectionTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.FlinkRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.logical.LogicalNode.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.FlinkTypeFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.FlinkPlannerImpl.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.fieldExpression.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-8-2 01:00:00" id="4306" opendate="2016-8-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Flink and Storm dependencies in flink-storm and flink-storm-examples</summary>
      <description>Flink dependencies should be in scope provided, like in the other libraries. flink-storm-examples should not draw storm-core directly, but only via flink-storm, so it gets the proper transitive dependency exclusions flink-storm-examples should have the clojure jar repository as an additional maven repository</description>
      <version>1.0.3,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm.pom.xml</file>
      <file type="M">flink-contrib.flink-storm-examples.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-2 01:00:00" id="4307" opendate="2016-8-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Broken user-facing API for ListState</summary>
      <description>The user-facing ListState is supposed to return an empty list when no element is contained in the state.A previous change altered that behavior to make it in the runtime classes accessible whether a ListState is empty.To not break the user-facing API, we need to restore the behavior for ListState exposed to the users via the RuntimeContext.</description>
      <version>None</version>
      <fixedVersion>1.1.0,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContextTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamingRuntimeContext.java</file>
    </fixedFiles>
  </bug>
  
</bugrepository>