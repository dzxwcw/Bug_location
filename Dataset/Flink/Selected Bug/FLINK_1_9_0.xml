<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  
  <bug fixdate="2019-2-25 01:00:00" id="11745" opendate="2019-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TTL end-to-end test restores from the savepoint after the job cancelation</summary>
      <description>The state TTL end-to-end test currently cancels the first running job, takes savepoint and starts the job again from stratch without using the savepoint. The second job should start from the previously taken savepoint.</description>
      <version>1.6.4,1.7.2,1.8.0,1.9.0</version>
      <fixedVersion>1.6.5,1.7.3,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.stream.state.ttl.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-3-1 01:00:00" id="11791" opendate="2019-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Describe how to build Flink with Hadoop in build guide</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.flinkDev.building.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-13 01:00:00" id="11892" opendate="2019-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port conflict when running nightly end-to-end tests</summary>
      <description>When I do the end-to-end check according to `https://github.com/apache/flink/tree/master/flink-end-to-end-tests`. I got the follows problem:1. Executed command， and the message of console as follows:FLINK_DIR=/Users/jincheng/work/FlinkRelease/1.8/flink-1.8.0/flink-dist/target/flink-1.8.0-bin/flink-1.8.0export FLINK_DIRsh flink-end-to-end-tests/run-nightly-tests.sh......Starting taskexecutor daemon on host jinchengsunjcs-iMac.local.Dispatcher REST endpoint is up.Job (180a3cfc35d549417e5807520d7402f9) is running.Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...2. Log info:2019-03-13 07:56:33,670 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator         - Remoting shut down.2019-03-13 07:56:33,673 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint         - Could not start cluster entrypoint StandaloneSessionClusterEntrypoint.org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint StandaloneSessionClusterEntrypoint.at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:190)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:535)at org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint.main(StandaloneSessionClusterEntrypoint.java:65)Caused by: java.net.BindException: Could not start actor system on any port in port range 6123at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:172)at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:112)at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:87)at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils.createRpcService(AkkaRpcServiceUtils.java:84)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createRpcService(ClusterEntrypoint.java:296)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:264)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:216)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:172)at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:171)3. environmentMacOS: 10.14.3 Java version "1.8.0_151"jinchengsunjcs-iMac:~ jincheng$ echo $0-bash</description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.README.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-13 01:00:00" id="11896" opendate="2019-3-13 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce stream physical nodes</summary>
      <description>This issues aims to introduce flink stream physical RelNode, such as StreamExecCalc, StreamExecExchange, StreamExecExpand etc.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.api.TableConfigOptions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkOptimizeContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamPhysicalRel.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecDataStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecJoinBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecGroupAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalWatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalRank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.common.CommonExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.WatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.Rank.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.LogicalWatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableConfig.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-3-18 01:00:00" id="11950" opendate="2019-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add missing dependencies in NOTICE file of flink-dist.</summary>
      <description>Add Missing dependencies in NOTICE file of flink-dist. </description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">NOTICE-binary</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-18 01:00:00" id="11956" opendate="2019-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove shading from S3 filesystems build</summary>
      <description/>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.java.org.apache.flink.fs.s3presto.PrestoS3FileSystemTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.java.org.apache.flink.fs.s3presto.S3FileSystemFactory.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3FileSystemTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.java.org.apache.flink.fs.s3hadoop.S3FileSystemFactory.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">docs.ops.filesystems.s3.zh.md</file>
      <file type="M">docs.ops.filesystems.s3.md</file>
      <file type="M">docs.ops.filesystems.index.zh.md</file>
      <file type="M">docs.ops.filesystems.index.md</file>
      <file type="M">tools.travis.shade.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-1-28 01:00:00" id="1197" opendate="2014-10-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Special page on website that describes issues that occur together with types</summary>
      <description>Describe topics like: issues, restrictions etc. when using Java 8 Lambdas general TypeExtractor functionalities like simple input type inferences and known issues</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.sidenav.html</file>
      <file type="M">docs.internal.job.scheduling.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-21 01:00:00" id="11989" opendate="2019-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable metric reporter modules in jdk9 runs</summary>
      <description>The Reporter modules are currently disabled on the travis jdk9 jobs as we ran into some issues in the MetricRegistry that prevented them from suceeding.It appears that this issue no longer exists, so let's enable them again.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.stage.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-4-29 01:00:00" id="12065" opendate="2019-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>E2E tests fail due to illegal-access warning on Java 9</summary>
      <description>When accessing inaccessible fields via reflection a warning is printed on Java 9 like below:WARNING: An illegal reflective access operation has occurredWARNING: All illegal access operations will be denied in a future releaseWARNING: An illegal reflective access operation has occurredWARNING: Illegal reflective access by org.apache.flink.core.memory.HybridMemorySegment (file:/home/travis/build/zentol/flink/flink-dist/target/flink-1.9-SNAPSHOT-bin/flink-1.9-SNAPSHOT/lib/flink-dist_2.11-1.9-SNAPSHOT.jar) to field java.nio.Buffer.addressThese are printed into the .out file of the processes, and cause e2e tests to fail since we check for empty .out files.From what I've gathered we cannot disable these warnings, so we'll have to adapt the check to ignore these.We can't just fix these accesses since they also occur in libraries (like akka's netty)</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-4-5 01:00:00" id="12117" opendate="2019-4-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CassandraConnectorITCase fails on Java 9</summary>
      <description>From what I found cassandra never really supported Java 9, so we will likely have to disable the tests in the jdk9 profile.java.lang.ExceptionInInitializerError at org.github.jamm.MemoryMeter.measure(MemoryMeter.java:178) at org.apache.cassandra.utils.ObjectSizes.measure(ObjectSizes.java:162) at org.apache.cassandra.utils.ObjectSizes.&lt;clinit&gt;(ObjectSizes.java:39) at org.apache.cassandra.dht.RandomPartitioner.&lt;clinit&gt;(RandomPartitioner.java:47) at java.base/java.lang.Class.forName0(Native Method) at java.base/java.lang.Class.forName(Class.java:292) at org.apache.cassandra.utils.FBUtilities.classForName(FBUtilities.java:434) at org.apache.cassandra.utils.FBUtilities.instanceOrConstruct(FBUtilities.java:450) at org.apache.cassandra.utils.FBUtilities.newPartitioner(FBUtilities.java:400) at org.apache.cassandra.config.DatabaseDescriptor.applyConfig(DatabaseDescriptor.java:353) at org.apache.cassandra.config.DatabaseDescriptor.&lt;clinit&gt;(DatabaseDescriptor.java:119) at org.apache.cassandra.service.StartupChecks$4.execute(StartupChecks.java:167) at org.apache.cassandra.service.StartupChecks.verify(StartupChecks.java:107) at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:162) at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:416) at org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase$EmbeddedCassandraService.start(CassandraConnectorITCase.java:147) at org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.startCassandra(CassandraConnectorITCase.java:186) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47) at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242) at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)Caused by: java.lang.StringIndexOutOfBoundsException: begin 0, end -1, length 5 at java.base/java.lang.String.checkBoundsBeginEnd(String.java:3116) at java.base/java.lang.String.substring(String.java:1885) at org.github.jamm.MemoryLayoutSpecification.getEffectiveMemoryLayoutSpecification(MemoryLayoutSpecification.java:190) at org.github.jamm.MemoryLayoutSpecification.&lt;clinit&gt;(MemoryLayoutSpecification.java:31) ... 34 more</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.stage.sh</file>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-6 01:00:00" id="12119" opendate="2019-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add OWASP Dependency Check</summary>
      <description>In order to obtain some visibility on the current known security vulnerabilities in Flink's dependencies. It would be useful to include the OWASP dependency check plugin &amp;#91;1&amp;#93; into our Maven build.By including it into flink-parent, we can get summary of all dependencies of all child projects by runningmvn clean org.owasp:dependency-check-maven:5.0.0-M2:aggregateWe should probably exclude some modules from the dependency-check. These could be: flink-docs flink-fs-tests flink-yarn-tests flink-contribAnything else? What about flink-python/flink-streaming-python?**In addition I propose to exclude all dependencies in the system or provided scope.At least initially, the build would never fails because of vulnerabilities. &amp;#91;1&amp;#93; https://jeremylong.github.io/DependencyCheck/dependency-check-maven/index.html</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-4-9 01:00:00" id="12137" opendate="2019-4-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more proper explanation on flink streaming connectors</summary>
      <description>When I write code using flink streaming connector, IntelliJ shows alert message.I want to suggest more clear guidance message without warning message.Actually, AWS_CREDENTIALS_PROVIDER is a constant of AWSConfigConstants class.When documentation shows AWSConfigConstants class instead of ConsumerConfigConstants which is extended class of it, developers do not need to take their times to figure out warning message. </description>
      <version>None</version>
      <fixedVersion>1.8.1,1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.kinesis.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-4-15 01:00:00" id="12192" opendate="2019-4-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for generating optimized logical plan for grouping sets and distinct aggregate</summary>
      <description>This issue aims to supports generating optimized logical plan for grouping sets and distinct aggregate. (mentioned in FLINK-12076 and FLINK-12098)for batch, query with distinct aggregate will be rewritten into two non-distinct aggregates by extended AggregateExpandDistinctAggregatesRule, the first aggregate computes the distinct key and non-distinct aggregate function, and the second aggregate computes the distinct aggregate function based on first aggregate result.  The first aggregate has grouping sets if there are more than one distinct aggregate functions on different fields.for stream, query with distinct aggregate is handled by SplitAggregateRule in FLINK-12161.query with grouping sets (or cube, rollup) will be rewritten into a regular aggregate with expand, and the expand node will duplicates the input data for each simple group. e.g.schema:MyTable: a: INT, b: BIGINT, c: VARCHAR(32), d: VARCHAR(32) Original records:+-----+-----+-----+-----+| a | b | c | d |+-----+-----+-----+-----+| 1 | 1 | c1 | d1 |+-----+-----+-----+-----+| 1 | 2 | c1 | d2 |+-----+-----+-----+-----+| 2 | 1 | c1 | d1 |+-----+-----+-----+-----+SELECT a, c, SUM(b) as b FROM MyTable GROUP BY GROUPING SETS (a, c)logical plan after expanded:LogicalCalc(expr#0..3=[{inputs}], proj#0..1=[{exprs}], b=[$t3]) LogicalAggregate(group=[{0, 2, 3}], groups=[[]], b=[SUM($1)]) LogicalExpand(projects=[{a=[$0], b=[$1], c=[null], $e=[1]}, {a=[null], b=[$1], c=[$2], $e=[2]}]) LogicalNativeTableScan(table=[[builtin, default, MyTable]])notes:'$e = 1' is equivalent to 'group by a''$e = 2' is equivalent to 'group by c'expanded records:+-----+-----+-----+-----+| a | b | c | $e |+-----+-----+-----+-----+ ---+---| 1 | 1 | null| 1 | |+-----+-----+-----+-----+ records expanded by record1| null| 1 | c1 | 2 | |+-----+-----+-----+-----+ ---+---| 1 | 2 | null| 1 | |+-----+-----+-----+-----+ records expanded by record2| null| 2 | c1 | 2 | |+-----+-----+-----+-----+ ---+---| 2 | 1 | null| 1 | |+-----+-----+-----+-----+ records expanded by record3| null| 1 | c1 | 2 | |+-----+-----+-----+-----+ ---+---</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.MiscITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.SortUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.ExpandUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecSortLimitRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecLimitRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortMergeJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecLimit.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashJoin.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecExpand.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecExchange.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalSort.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.common.CommonPhysicalJoin.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-15 01:00:00" id="12193" opendate="2019-4-15 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Send TM "can be released status" with RM heartbeat</summary>
      <description>We introduced a conditional release of Task Executor in Resource Manager in FLINK-10941. At the moment RM directly asks TE every release timeout whether it can be released (all depending consumers are done). We can piggyback TE/RM heartbeats for this purpose. In this case, we do not need additional RPC call to TE gateway and could potentially release TE quicker.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestJobFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-17 01:00:00" id="12226" opendate="2019-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update CLI docs about SUSPEND/TERMINATE</summary>
      <description/>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.cli.md</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2019-5-20 01:00:00" id="12269" opendate="2019-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Temporal Table Join in blink planner</summary>
      <description>Support translate following "FOR SYSTEM_TIME AS OF" query into StreamExecTemporalTableJoin.SELECT o.amout, o.currency, r.rate, o.amount * r.rateFROM Orders AS o JOIN LatestRates FOR SYSTEM_TIME AS OF o.proctime AS r ON r.currency = o.currencyThis is an extension to current temporal join (FLINK-9738) using a standard syntax introduced in Calcite 1.19.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedNamespaceAggsHandleFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedInput.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedHashFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedCollector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.generated.GeneratedClass.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.UserDefinedFunctionTestUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.SortMergeJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.SingleRowJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.ShuffledHashJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.BroadcastHashJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.stream.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.batch.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.SortMergeJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.ShuffledHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.NestedLoopJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.BroadcastHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.DeadlockBreakupTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.batch.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.runtime.collector.TableFunctionCollector.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.schema.FlinkTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.stream.StreamExecJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkBatchProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.common.CommonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnInterval.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CorrelateCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CollectorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CalcCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.plan.rules.logical.FlinkJoinPushExpressionsRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.ProctimeSqlFunction.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-20 01:00:00" id="12271" opendate="2019-4-20 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Display input field name list when throw Cannot resolve field exception</summary>
      <description>Currently, when we select a field that does not exist, an cannot solve field exception is thrown. For example,org.apache.flink.table.api.ValidationException: Cannot resolve field [_4]It would be better to also display the input field list to indicate existing fields, such as: org.apache.flink.table.api.ValidationException: Cannot resolve field [_4], input field list:[_1, _2, _3].</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.expressions.rules.ReferenceResolverRule.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-5-25 01:00:00" id="12326" opendate="2019-4-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a basic test framework, just like the existing Java TableAPI, abstract some TestBase.</summary>
      <description>Add a basic test framework, just like the existing Java/Scala TableAPI, abstract some TestBase.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-python.pyflink.table.tests.test.end.to.end.py</file>
      <file type="M">flink-python.pyflink.find.flink.home.py</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.pyflink-gateway-server.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-4-27 01:00:00" id="12346" opendate="2019-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala-suffix check broken on Travis</summary>
      <description>the scala-suffix check currently does not work on travis since the maven output is not what the script expects. On travis we have timestamps in the maven output, which breaks the parsing.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.verify.scala.suffixes.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-27 01:00:00" id="12347" opendate="2019-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-table-runtime-blink is missing scala suffix</summary>
      <description>flink-table-runtime-blink has a dependency on flink-streaming-java and thus requires a scala suffix.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-5-29 01:00:00" id="12366" opendate="2019-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up Catalog APIs to make them more consistent and coherent</summary>
      <description>Some of the APIs seem inconsistent with others in terms of exception thrown and error handling. This is to clean them up to maintain consistency and coherence.</description>
      <version>1.9.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.ReadableWritableCatalog.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.ReadableCatalog.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.exceptions.PartitionNotExistException.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.GenericInMemoryCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.GenericInMemoryCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.GenericHiveMetastoreCatalog.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-5-30 01:00:00" id="12370" opendate="2019-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrated Travis for Python Table API</summary>
      <description>Integrated Travis for Python Table API</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.stage.sh</file>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">tools.travis.controller.sh</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-6-30 01:00:00" id="12372" opendate="2019-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement ExecutionSlotAllocator</summary>
      <description>Add and implement ExecutionSlotAllocator interfaceDesign document: https://docs.google.com/document/d/1fstkML72YBO1tGD_dmG2rwvd9bklhRVauh4FSsDDwXUAcceptance criteria ExecutionSlotAllocator interface is defined and implemented interface implementation is unit tested</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-5-6 01:00:00" id="12413" opendate="2019-5-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement ExecutionFailureHandler</summary>
      <description>Implement ExecutionFailureHandler and related classes.Acceptance criteria Implementation or definitions exist for: ExecutionFailureHandler FailureHandlingResult RestartBackoffTimeStrategy (interface definition is enough) ExecutionFailureHandler is unit tested in isolation FailureHandlingResult is unit tested in isolation</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ThrowableClassifierTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.throwable.ThrowableClassifier.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.FailoverStrategy.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-5-6 01:00:00" id="12414" opendate="2019-5-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement ExecutionGraph to SchedulingTopology Adapter</summary>
      <description>Implement an adapter, which adapts the ExecutionGraph to the SchedulingTopology interface.Acceptance criteria The adapter always reflects an up to date view of the ExecutionGraph state</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.SchedulingResultPartition.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-7 01:00:00" id="12431" opendate="2019-5-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port utility methods for extracting fields information from TypeInformation</summary>
      <description>We need those methods in the api-module in order to create Table out of DataSet/Stream.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.StreamTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.java.StreamTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.BatchTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.CalculatedTableFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.BuiltInFunctionDefinitions.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-6-8 01:00:00" id="12446" opendate="2019-5-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation how to enable openSSL</summary>
      <description>Since we won't include openSSL support by default, we should describe the (two) way of enabling this for Flink.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
      <file type="M">docs..includes.generated.security.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-8 01:00:00" id="12447" opendate="2019-5-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump required Maven version to 3.1.1 (from 3.0.3)</summary>
      <description>See https://lists.apache.org/thread.html/57dec7c338eb95247b7a05ded371f4a78420a964045ea9557d501c3f@%3Cdev.flink.apache.org%3E The frontend-maven-plugin requires at least Maven 3.1.0.I propose to bump the required Maven version to 3.1.1.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">README.md</file>
      <file type="M">pom.xml</file>
      <file type="M">docs.flinkDev.building.zh.md</file>
      <file type="M">docs.flinkDev.building.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-11-17 01:00:00" id="1247" opendate="2014-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Links in documentation broken</summary>
      <description>There seem to be some links in the 0.7-incubating documentation broken. For example, the Aggregator link in the Accumulators &amp; Counters section does not work.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.web.client.md</file>
      <file type="M">docs.streaming.guide.md</file>
      <file type="M">docs.examples.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-5-10 01:00:00" id="12473" opendate="2019-5-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the interface of ML pipeline and ML lib</summary>
      <description>This Jira will introduce the major interfaces for ML pipeline and ML lib.The major interfaces and their relationship diagram is shown as below. For more details, please refer to [FLIP39 design doc|https://docs.google.com/document/d/1StObo1DLp8iiy0rbukx8kwAJb0BwDZrQrMWub3DzsEo] </description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-5-10 01:00:00" id="12485" opendate="2019-5-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a tool to check the user interface of Python Table API aligns with Java Table API.</summary>
      <description>After FLINK-12407 Python Table API will align with current Java Table API. So, In the following Table API development, it's better to add a new user interface to the Java Table API will also need to be added to the Python Table API. So In this Jira, we want dd a tool to check the user interface of Python Table API aligns with Java Table API. I'm not sure if we want to force contributors to contribute to the Python Table API while developing the Java Table API. But at least we should have the ability to automatically check if Python is aligned with Java functionality. What do you think?</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.testing.test.case.utils.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-5-10 01:00:00" id="12487" opendate="2019-5-10 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce planner rules to rewrite expression and merge calc</summary>
      <description>This issue aims to introduce planner rules to rewrite expression and merge calc, rules include:1. ConvertToNotInOrInRule, that converts a cascade of predicates to IN or NOT_IN,e.g. converts predicate (x = 1 OR x = 2 OR x = 3 OR x = 4) AND y = 5 to predicate x IN (1, 2, 3, 4) AND y = 5 converts predicate (x &lt;&gt; 1 AND x &lt;&gt; 2 AND x &lt;&gt; 3 AND x &lt;&gt; 4) AND y = 5 to predicate x NOT IN (1, 2, 3, 4) AND y = 52. RewriteCoalesceRule, that rewrites Coalesce to Case When3. FlinkCalcMergeRule, that is copied from Calcite CalcMergeRule, and it will simplify the merged program</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.SplitAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkAggregateExpandDistinctAggregatesRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGeneratorContext.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-5-14 01:00:00" id="12507" opendate="2019-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix AsyncLookupJoin doesn&amp;#39;t close all generated ResultFutures</summary>
      <description>There is a fragile test in AsyncLookupJoinITCase, that not all the udfs are closed at the end.02:40:48.787 [ERROR] Tests run: 22, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 47.098 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase02:40:48.791 [ERROR] testAsyncJoinTemporalTableWithUdfFilter[StateBackend=HEAP](org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase) Time elapsed: 1.266 s &lt;&lt;&lt; FAILURE!java.lang.AssertionError: expected:&lt;0&gt; but was:&lt;2&gt; at org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase.testAsyncJoinTemporalTableWithUdfFilter(AsyncLookupJoinITCase.scala:268)02:40:48.794 [ERROR] testAsyncJoinTemporalTableWithUdfFilter[StateBackend=ROCKSDB](org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase) Time elapsed: 1.033 s &lt;&lt;&lt; FAILURE!java.lang.AssertionError: expected:&lt;0&gt; but was:&lt;2&gt; at org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase.testAsyncJoinTemporalTableWithUdfFilter(AsyncLookupJoinITCase.scala:268)</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.join.lookup.AsyncLookupJoinRunner.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-5-15 01:00:00" id="12516" opendate="2019-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Travis base image from trusty to xenial</summary>
      <description>Currently, our Travis tests are running with a trusty environment which is officially not supported by Ubuntu anymore. It also brings old system libraries which blocks unit tests for FLINK-11579.I propose to update to xenial (Ubuntu 16.04 LTS) which is still supported until 2021-04.Since Travis doesn't support oraclejdk8 on xenial, however, this also implies a switch to openJDK 8.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-15 01:00:00" id="12517" opendate="2019-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run network tests with dynamically-linked openSSL</summary>
      <description>FLINK-9816 adds the ability to work with Netty's wrapper around native openSSL implementations. We should set up unit tests that verify the artifacts we provide, i.e. the dynamically-linked openSSL one in flink-shaded.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.watchdog.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-15 01:00:00" id="12518" opendate="2019-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run e2e tests with openSSL</summary>
      <description>We should modify one end-to-end test each to run with: Java-based SSL dynamically linked openSSL statically linked openSSL</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.file.sink.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.ssl.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-5-15 01:00:00" id="12519" opendate="2019-5-15 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce planner rules about semi/anti join</summary>
      <description>This issue aims to introduce planner rules about semi/anti join, rules include:1. FlinkSemiAntiJoinFilterTransposeRule that pushes semi/anti join down in a tree past a filter2. FlinkSemiAntiJoinJoinTransposeRule that pushes semi/anti join down in a tree past a non semi/anti join3. FlinkSemiAntiJoinProjectTransposeRule that push semi/anti join down in a tree past a project4. ProjectSemiAntiJoinTransposeRule that pushes a project down in a tree past a semi/anti joinplanner rules about non semi/anti join will be introduced in FLINK-12509.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.SortMergeSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.ShuffledHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.SemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.NestedLoopSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.BroadcastHashSemiAntiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-5-16 01:00:00" id="12524" opendate="2019-5-16 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce planner rules about rank</summary>
      <description>This issue aims to introduce planner rules about rank, rules include:1. CalcRankTransposeRule, that transposes FlinkLogicalCalc past FlinkLogicalRank to reduce rank input fields.2. RankNumberColumnRemoveRule, that emoves the output column of rank number iff there is a equality condition for the rank column.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.rules.logical.FlinkLogicalRankRuleForRangeEndTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RankUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelMdUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.CorrelateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdUniqueKeys.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdUniqueGroups.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdPopulationSize.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdDistinctRowCount.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnUniqueness.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnInterval.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-5-16 01:00:00" id="12534" opendate="2019-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce the test cost for Python API</summary>
      <description>Currently, we add the Python API Travis test for Scala 2.12 / Java 9 / Hadoop 2.4.1. due to Python API using Py4j communicate with JVM, the test for Java 9 is enough, and we can remove the test for Scala 2.12 and  Hadoop 2.4.1.  </description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.controller.sh</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-21 01:00:00" id="12572" opendate="2019-5-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement HiveInputFormat to read Hive tables</summary>
      <description/>
      <version>1.9.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveTableConfig.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTablePartition.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-22 01:00:00" id="12590" opendate="2019-5-22 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Replace http links in documentation</summary>
      <description/>
      <version>1.7.2,1.8.0,1.9.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">README.md</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.StreamingJob.scala</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.scala.BatchJob.scala</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.java.StreamingJob.java</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.java.BatchJob.java</file>
      <file type="M">flink-python.README.md</file>
      <file type="M">flink-examples.flink-examples-batch.src.main.java.org.apache.flink.examples.java.misc.CollectionExecutionExample.java</file>
      <file type="M">flink-end-to-end-tests.README.md</file>
      <file type="M">flink-dist.src.main.flink-bin.README.txt</file>
      <file type="M">docs..config.yml</file>
      <file type="M">docs.README.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.zh.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.md</file>
      <file type="M">docs.getting-started.tutorials.local.setup.zh.md</file>
      <file type="M">docs.getting-started.tutorials.local.setup.md</file>
      <file type="M">docs.getting-started.tutorials.flink.on.windows.zh.md</file>
      <file type="M">docs.getting-started.tutorials.flink.on.windows.md</file>
      <file type="M">docs.getting-started.tutorials.datastream.api.zh.md</file>
      <file type="M">docs.getting-started.tutorials.datastream.api.md</file>
      <file type="M">docs.dev.libs.ml.contribution.guide.zh.md</file>
      <file type="M">docs.dev.libs.ml.contribution.guide.md</file>
      <file type="M">docs.dev.connectors.filesystem.sink.zh.md</file>
      <file type="M">docs.dev.connectors.filesystem.sink.md</file>
      <file type="M">.github.CONTRIBUTING.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-22 01:00:00" id="12592" opendate="2019-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>StreamTableEnvironment object has no attribute connect</summary>
      <description>The Python build module failed on Travis with the following problem: 'StreamTableEnvironment' object has no attribute 'connect'.https://api.travis-ci.org/v3/job/535684431/log.txt</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.tox.ini</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-23 01:00:00" id="12604" opendate="2019-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Register TableSource/Sink as CatalogTable</summary>
      <description/>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.catalog.ExternalCatalogSchemaTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.runtime.batch.table.JavaTableEnvironmentITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.PathResolutionTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.catalog.CatalogStructureBuilder.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.schema.TableSourceSinkTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.schema.TableSinkTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.datastream.StreamTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.dataSet.BatchTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.PhysicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.StreamTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.BatchTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.catalog.ExternalTableUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.catalog.ExternalCatalogSchema.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.StreamTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.BatchTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.plan.TableOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.DatabaseCalciteSchema.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.CalciteCatalogTable.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.TableOperationVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.TableOperationDefaultVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.TableOperationCatalogView.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-24 01:00:00" id="12614" opendate="2019-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor YARN tests to not do assertion in @After methods</summary>
      <description>The YARN are executing assertions in the @After methods, which leads to output that it unnecessarily obfuscated. For example, it is not said explicitly which test has failed.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOSecuredITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNHighAvailabilityITCase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnConfigurationITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-26 01:00:00" id="12627" opendate="2019-5-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document how to configure and use catalogs in SQL CLI</summary>
      <description>Ticket of its corresponding Chinese version is FLINK-12894.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-5-27 01:00:00" id="12635" opendate="2019-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>REST API stability test does not cover jar upload</summary>
      <description>The stability test for the REST API currently resides in flink-runtime, but the jar upload is handled via an extension in runtime-web. Since this extension cannot be loaded in flink-runtime it isn't covered by the test.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.compatibility.RestAPIStabilityTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.compatibility.CompatibilityRoutines.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.compatibility.CompatibilityRoutine.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.compatibility.CompatibilityCheckResult.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.compatibility.Compatibility.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-runtime-web.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-5-27 01:00:00" id="12636" opendate="2019-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>REST API stability test does not fail on compatible modifications</summary>
      <description>The stability test does not fail properly if the API was modified in a compatible way. The test should still fail until the snapshot was regenerated.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgram.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.JobWithJars.java</file>
      <file type="M">flink-runtime.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.compatibility.RestAPIStabilityTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-28 01:00:00" id="12639" opendate="2019-5-28 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>FLIP-42: Rework Documentation</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.tutorials.setup.instructions.zh.md</file>
      <file type="M">docs.tutorials.setup.instructions.md</file>
      <file type="M">docs.tutorials.local.setup.zh.md</file>
      <file type="M">docs.tutorials.local.setup.md</file>
      <file type="M">docs.tutorials.index.zh.md</file>
      <file type="M">docs.tutorials.index.md</file>
      <file type="M">docs.tutorials.flink.on.windows.zh.md</file>
      <file type="M">docs.tutorials.flink.on.windows.md</file>
      <file type="M">docs.tutorials.datastream.api.zh.md</file>
      <file type="M">docs.tutorials.datastream.api.md</file>
      <file type="M">docs.tutorials.api.tutorials.zh.md</file>
      <file type="M">docs.tutorials.api.tutorials.md</file>
      <file type="M">docs.examples.index.zh.md</file>
      <file type="M">docs.examples.index.md</file>
      <file type="M">docs.concepts.index.zh.md</file>
      <file type="M">docs.concepts.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-28 01:00:00" id="12641" opendate="2019-5-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Release all partitions on job termination</summary>
      <description>Introduce general logic on the JobManager to release partitions that are no longer required.In the first version, partitions will be kept until the consumer has finished, similarly to the current behavior.Further optimizations, potentially involving failover-regions, can be tackled in a follow-up.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-28 01:00:00" id="12643" opendate="2019-5-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement ExecutionGraph to FailoverTopology Adapter</summary>
      <description>Implement an adapter, which adapts the ExecutionGraph and its sub-components(ExecutionVertex, ExecutionEdge) to the FailoverTopology interfaces.Core components: 1. DefaultFailoverTopology (reflects ExecutionGraph)2. DefaultFailoverVertex (reflects ExecutionVertex)3. DefaultFailoverEdge (reflects ExecutionEdge) </description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-28 01:00:00" id="12644" opendate="2019-5-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setup Java 9 cron jobs</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-28 01:00:00" id="12649" opendate="2019-5-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a shim layer to support multiple versions of HMS</summary>
      <description>We need a shim layer of HMS client to talk to different versions of HMS</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-6-29 01:00:00" id="12664" opendate="2019-5-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement TableSink to write Hive tables</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveTableOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-5-29 01:00:00" id="12679" opendate="2019-5-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support &amp;#39;default-database&amp;#39; config for catalog entries in SQL CLI yaml file</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.descriptors.CatalogDescriptorValidator.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.descriptors.CatalogDescriptor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-catalogs.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-6-31 01:00:00" id="12685" opendate="2019-5-31 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Supports UNNEST query in blink planner</summary>
      <description>this issue aim to support queries with UNNEST keyword, which relate to nested fields.for example: table name: MyTableschema: a: int, b int, c array&amp;#91;int&amp;#93;sql:SELECT a, b, s FROM MyTable, UNNEST(MyTable.c) AS A (s)</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.AggregateReduceGroupingITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkDecorrelateProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkBatchProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.WindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-31 01:00:00" id="12689" opendate="2019-5-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-dist is missing flink-azure-fs-hadoop dependency</summary>
      <description>Build fails when building with:mvn clean install -pl flink-dist -am -DskipTests -Dfast [INFO] flink-scala-shell .................................. SUCCESS [ 10.989 s][INFO] flink-dist ......................................... FAILURE [ 26.068 s][INFO] ------------------------------------------------------------------------[INFO] BUILD FAILURE[INFO] ------------------------------------------------------------------------[INFO] Total time: 13:24 min[INFO] Finished at: 2019-05-31T09:55:22+02:00[INFO] Final Memory: 313M/1834M[INFO] ------------------------------------------------------------------------[ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:3.0.0:single (opt) on project flink-dist_2.11: Failed to create assembly: Error adding file to archive: /Users/gyao/Documents/work/code/github/flink/flink-dist/../flink-filesystems/flink-azure-fs-hadoop/target/flink-azure-fs-hadoop-1.9-SNAPSHOT.jar -&gt; [Help 1]Azure FS dependency should be added to flink-dist with provided scope.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-3 01:00:00" id="12705" opendate="2019-6-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow user to specify the Hive version in use</summary>
      <description>Follow up of FLINK-12649</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-catalogs.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.factories.HiveCatalogFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveInputFormatTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.factories.HiveCatalogFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.descriptors.HiveCatalogValidator.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.descriptors.HiveCatalogDescriptor.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveMetastoreClientFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableOutputFormat.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-6-3 01:00:00" id="12713" opendate="2019-6-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>deprecate descriptor, validator, and factory of ExternalCatalog</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.ExternalCatalogFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.descriptors.ExternalCatalogDescriptorValidator.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.descriptors.ExternalCatalogDescriptor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.factories.TableFactoryUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-5 01:00:00" id="12736" opendate="2019-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ResourceManager may release TM with allocated slots</summary>
      <description>The ResourceManager looks out for TaskManagers that have not had any slots allocated on them for a while, as these could be released to safe resources. If such a TM is found the RM checks via an RPC call whether the TM still holds any partitions. If no partition is held then the TM is released.However, in the RPC callback no check is made whether the TM is actually still idle. In the meantime a slot could've been allocated on the TM.</description>
      <version>1.7.2,1.8.1,1.9.0</version>
      <fixedVersion>1.7.3,1.8.2,1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGatewayBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-5 01:00:00" id="12742" opendate="2019-6-5 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add insert into partition grammar as hive dialect</summary>
      <description/>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-uber.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.PlanningConfigurationBuilder.java</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.sqlexec.SqlExecutableStatement.java</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.SqlProperty.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlTableColumn.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlRowType.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlMapType.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlDropTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlCreateTable.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlArrayType.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.ExtendedSqlType.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-sql-parser.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-8-5 01:00:00" id="12745" opendate="2019-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sparse and dense vector class, and dense matrix class with basic operations.</summary>
      <description>There are basic vector and matrix library for the machine-learning algorithms.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-ml-parent.flink-ml-lib.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-5 01:00:00" id="12746" opendate="2019-6-5 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Getting Started - DataStream Example Walkthrough</summary>
      <description>The planned structure for the new Getting Started Guide is Flink Overview (~ two pages) Project Setup Java Scala Python Quickstarts Example Walkthrough - Table API / SQL Example Walkthrough - DataStream API Docker Playgrounds Flink Cluster Playground Flink Interactive SQL Playground In this ticket we should add "Project Setup" and "Quickstarts -&gt; Example Walkthrough - DataStream API", which covers everything what we have today. This will replace the current "Tutorials" and "Examples" section, which can be removed as part of this ticket as well.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.redirects.tutorials.datastream.api.md</file>
      <file type="M">docs.redirects.example.quickstart.md</file>
      <file type="M">docs.index.md</file>
      <file type="M">docs.getting-started.walkthroughs.index.zh.md</file>
      <file type="M">docs.getting-started.walkthroughs.index.md</file>
      <file type="M">docs.getting-started.tutorials.index.zh.md</file>
      <file type="M">docs.getting-started.tutorials.index.md</file>
      <file type="M">docs.getting-started.tutorials.datastream.api.zh.md</file>
      <file type="M">docs.getting-started.tutorials.datastream.api.md</file>
      <file type="M">docs.getting-started.index.md</file>
      <file type="M">docs.getting-started.examples.index.zh.md</file>
      <file type="M">docs.getting-started.examples.index.md</file>
      <file type="M">docs.getting-started.docker-playgrounds.index.zh.md</file>
      <file type="M">docs.getting-started.docker-playgrounds.index.md</file>
      <file type="M">docs.dev.projectsetup.scala.api.quickstart.zh.md</file>
      <file type="M">docs.dev.projectsetup.scala.api.quickstart.md</file>
      <file type="M">docs.dev.projectsetup.java.api.quickstart.zh.md</file>
      <file type="M">docs.dev.projectsetup.java.api.quickstart.md</file>
      <file type="M">flink-walkthroughs.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.table.walkthroughs.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.cli.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">docs.getting-started.walkthroughs.table.api.zh.md</file>
      <file type="M">docs.getting-started.walkthroughs.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-5 01:00:00" id="12747" opendate="2019-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Getting Started - Table API Example Walkthrough</summary>
      <description>The planned structure for the new Getting Started Guide is Flink Overview (~ two pages) Project Setup Java Scala Python Quickstarts Example Walkthrough - Table API / SQL Example Walkthrough - DataStream API Docker Playgrounds Flink Cluster Playground Flink Interactive SQL Playground This tickets adds the Example Walkthrough for the Table API, which should follow the same structure as the DataStream Example (FLINK-12746), which needs to be completed first.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">docs.getting-started.tutorials.index.zh.md</file>
      <file type="M">docs.getting-started.tutorials.index.md</file>
      <file type="M">docs.getting-started.examples.index.zh.md</file>
      <file type="M">docs.getting-started.examples.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-5 01:00:00" id="12756" opendate="2019-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>migrate HiveCatalog from TypeInformation-based old type system to DataType-based new type system</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.catalog.CatalogTestBase.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.TableSchema.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogGenericMetadataTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTypeUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-6 01:00:00" id="12758" opendate="2019-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add flink-ml-lib module</summary>
      <description>The Jira introduces a new module "flink-ml-lib" under flink-ml-parent.The flink-ml-lib is planned in the roadmap in FLIP-39, as the code base of library implementations of FlinkML. This Jira only aims to create the module, and algorithms will be added in separate Jira in the future. For more details, please refer to [FLIP39 design doc|https://docs.google.com/document/d/1StObo1DLp8iiy0rbukx8kwAJb0BwDZrQrMWub3DzsEo]</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-ml-parent.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-12-6 01:00:00" id="12766" opendate="2019-6-6 00:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Dynamically allocate TaskExecutor’s managed memory to slots.</summary>
      <description>This step is a temporal workaround for release 1.9 to meet the basic usability requirements of batch functions from Blink.</description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.ResourceSpecTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.ResourceSpec.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.ResourceProfileTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.ResourceProfile.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.resources.Resource.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-6 01:00:00" id="12767" opendate="2019-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support user defined connectors/format</summary>
      <description>Currently, only built-in connectors such as FileSystem/Kafka/ES are supported and only built-in formats such as OldCSV/JSON/Avro/CSV/ are supported. We should also provide a convenient way for the connectors/formats that are not built-in supported.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.descriptor.py</file>
      <file type="M">flink-python.pyflink.table.descriptors.py</file>
      <file type="M">flink-python.pyflink.java.gateway.py</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-6 01:00:00" id="12772" opendate="2019-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support mapping ARRAY, MAP, ROW (STRUCT) between Flink and Hive in HiveCatalog</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogGenericMetadataTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTypeUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-7 01:00:00" id="12774" opendate="2019-6-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pin version of build-helper-maven plugin</summary>
      <description>We currently use two different version of the build-helper-maven-plugin, and for a short time recently had a third. I propose to pin the version in the root plugin management.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-scala-shell.pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-mesos.pom.xml</file>
      <file type="M">flink-libraries.flink-gelly-scala.pom.xml</file>
      <file type="M">flink-libraries.flink-gelly-examples.pom.xml</file>
      <file type="M">flink-libraries.flink-cep-scala.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-examples.flink-examples-batch.pom.xml</file>
      <file type="M">flink-connectors.flink-hcatalog.pom.xml</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-7 01:00:00" id="12775" opendate="2019-6-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump flink-shaded version to 7.0</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.AbstractHandler.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-6-10 01:00:00" id="12795" opendate="2019-6-10 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Extracted creation &amp; configuration of FrameworkConfig &amp; RelBuilder to separate class in blink planner</summary>
      <description>just as commit (e682395a) in flink planner, do similar things in blink planner</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.codegen.agg.AggTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-6-11 01:00:00" id="12806" opendate="2019-6-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove beta feature remark from the Universal Kafka connector</summary>
      <description>I think we can remove this remark from the docs as in the last half year there were no issues reported that would say otherwise.The remark about universal connector being a beta feature was introduced in: https://issues.apache.org/jira/browse/FLINK-10900</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.kafka.zh.md</file>
      <file type="M">docs.dev.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-12 01:00:00" id="12812" opendate="2019-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set resource profiles for task slots</summary>
      <description/>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.ResourceProfileTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.ResourceProfile.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.ResourceSpec.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServices.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManager.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-6-13 01:00:00" id="12834" opendate="2019-6-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support CharType and BinaryType in blink runner</summary>
      <description>1.Now we use LogicalType VarcharType to support calcite char type.2.Subsequent TableApi also generates LogicalType's CharType.We need real support CharType in internal code gen and computation.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.types.TypeInfoDataTypeConverter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.types.PlannerTypeUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.types.InternalSerializers.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.types.ClassLogicalTypeConverter.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.vector.heap.AbstractHeapVector.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.TypeGetterSetters.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.DataFormatConverters.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryWriter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.typeutils.TypeCoercion.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.typeutils.TypeCheckUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.sort.SortCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.calls.PrintCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.calls.BinaryStringCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.AggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.RankLikeAggFunctionBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.RexNodeConverter.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-6-14 01:00:00" id="12850" opendate="2019-6-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce TypeInfo for LocalDate/LocalTime/LocalDateTime</summary>
      <description>Now in the new type system of table, the default class of time type is LocalDate and so on.There are some situations that need to be converted to TypeInformation, such as toDataStream, so we need to provide TypeInformation support such as LocalDate.Introduce LocalTimeTypeInfoIntroduce LocalDateSerializer, LocalTimeSerializer, LocalDateTimeSerializerIntroduce LocalDateComparator, LocalTimeComparator, LocalDateTimeComparator</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.Types.scala</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeinfo.Types.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-6-15 01:00:00" id="12856" opendate="2019-6-15 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce planner rule to push projection into TableSource</summary>
      <description>This issue aims to support push projection into ProjectableTableSource or NestedFieldsProjectableTableSource to reduce output fields of a TableSource</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.testTableSources.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-6-16 01:00:00" id="12864" opendate="2019-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improves the performance of Python Table API test cases</summary>
      <description>Most of the Python Table API test cases can be unit test instead of integration test. This can shorten the test time of Python Table API.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.testing.test.case.utils.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.window.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.schema.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.completeness.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.sort.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.set.operation.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.schema.operation.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.join.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.distinct.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.descriptor.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.column.operation.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.catalog.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.calc.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.aggregate.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-17 01:00:00" id="12867" opendate="2019-6-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add insert overwrite grammar as HIVE dialect</summary>
      <description>Support grammar like: insert overwrite tbl1 partition(a=1) select a from tbl2;This overwrite can use whole table or single partition as effective scope. </description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.PreValidateReWriter.scala</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.validate.FlinkSqlConformance.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.dml.RichSqlInsert.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-7-18 01:00:00" id="12883" opendate="2019-6-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add elaborate partition release logic</summary>
      <description>Partitions should be released when all it's consuming pipelined regions are done.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulingTopology.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.PipelinedRegionComputeUtil.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionStrategy.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-7-19 01:00:00" id="12902" opendate="2019-6-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup flink-yarn-tests dependencies</summary>
      <description>The dependency situation in this module is just insane; you have a few unshaded hadoop dependencies from `flink-yarn-tests`, some shaded hadoop dependencies from `flink-shaded-yarn-tests` and yet another set of shaded hadoop dependencies from `flink-shaded-hadoop`. All of these are available on the test classpath, resulting in what is effectively an incomprehensible classpath.This is highlighted in the shading patterns in `flink-shaded-yarn-tests`, which absolutely had to be in sync with `flink-shaded-hadoop` as otherwise you ended up with strange `VerifyErrors`.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-shaded-yarn-tests.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-shaded-yarn-tests.src.main.resources.META-INF.licenses.LICENSE.protobuf</file>
      <file type="M">flink-shaded-yarn-tests.src.main.resources.META-INF.licenses.LICENSE.asm</file>
      <file type="M">flink-shaded-yarn-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-6-21 01:00:00" id="12933" opendate="2019-6-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>support "use catalog" and "use database" in SQL CLI</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-21 01:00:00" id="12934" opendate="2019-6-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add hadoop dependencies as &amp;#39;flink-shaded-hadoop-2-uber&amp;#39; for flink-connector-hive to connect to remote hive metastore service</summary>
      <description>Now, for all hadoop related dependencies, we just use the single jar of flink-shaded-hadoop-2-uber, instead of depending on individual hadoop jars. It's much more convenient for users to use now.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-22 01:00:00" id="12937" opendate="2019-6-22 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce join reorder planner rules in blink planner</summary>
      <description>This issue aims to let blink planner support join reorder. LoptOptimizeJoinRule in Calcite could meet our requirement for now, so we could use directly this rule in blink planner. JoinToMultiJoinRule , ProjectMultiJoinMergeRule and FilterMultiJoinMergeRule should be also introduced to support LoptOptimizeJoinRule.additionally, we add a new rule named RewriteMultiJoinConditionRule which could apply transitive closure on `MultiJoin` for equi-join predicates to create more optimization possibilities.by default, join reorder is disabled, unless sql.optimizer.join-reorder.enabled is set as true.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.rules.logical.FlinkJoinPushExpressionsRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.rules.logical.FlinkAggregateInnerJoinTransposeRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkBatchProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.api.PlannerConfigOptions.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-7-24 01:00:00" id="12955" opendate="2019-6-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add HBase LookupableTableSource and TableFactory</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-hbase.src.main.java.org.apache.flink.addons.hbase.HBaseTableSource.java</file>
      <file type="M">flink-connectors.flink-hbase.src.main.java.org.apache.flink.addons.hbase.HBaseTableSchema.java</file>
      <file type="M">flink-connectors.flink-hbase.src.main.java.org.apache.flink.addons.hbase.HBaseRowInputFormat.java</file>
      <file type="M">flink-connectors.flink-hbase.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-24 01:00:00" id="12957" opendate="2019-6-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix thrift and protobuf dependency examples in documentation</summary>
      <description>The examples in the docs are not up-to-date anymore and should be updated.</description>
      <version>1.7.2,1.8.0,1.9.0</version>
      <fixedVersion>1.7.3,1.8.1,1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.custom.serializers.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-9-2 01:00:00" id="1297" opendate="2014-12-2 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add support for tracking statistics of intermediate results</summary>
      <description>One of the major problems related to the optimizer at the moment is the lack of proper statistics.With the introduction of staged execution, it is possible to instrument the runtime code with a statistics facility that collects the required information for optimizing the next execution stage.I would therefore like to contribute code that can be used to gather basic statistics for the (intermediate) result of dataflows (e.g. min, max, count, count distinct) and make them available to the job manager.Before I start, I would like to hear some feedback form the other users.In particular, to handle skew (e.g. on grouping) it might be good to have some sort of detailed sketch about the key distribution of an intermediate result. I am not sure whether a simple histogram is the most effective way to go. Maybe somebody would propose another lightweight sketch that provides better accuracy.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2019-6-26 01:00:00" id="12990" opendate="2019-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Date type doesn&amp;#39;t consider the local TimeZone</summary>
      <description>Currently, the python DateType is converted by an `int` which indicates the days passed since 1970-1-1 and then the Java side will create a Java Date by call `new Date(days * 86400)`. As we know that the Date constructor expected milliseconds since 1970-1-1 00:00:00 GMT and so we should convert `days * 86400` to GMT milliseconds.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.util.python.PythonTableUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-26 01:00:00" id="12995" opendate="2019-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Hive-1.2.1 build to Travis</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.stage.sh</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-8-26 01:00:00" id="12998" opendate="2019-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document Plugins mechanism</summary>
      <description>Plugins mechanism must be documented before the release. We should write down: benefits how to use existing FileSystem plugins how to implement an own custom FileSystem plugin potential issues of relaying on Thread.currentThread().getContextClassLoader()  (currently it's set only for FileSystemFactory class loading and FileSystemFactory#create() method call - if a FileSystem is accessing getContextClassLoader during runtime (write/reading) it will not work properly as a plugin.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.debugging.classloading.zh.md</file>
      <file type="M">docs.monitoring.debugging.classloading.md</file>
      <file type="M">docs.ops.filesystems.s3.zh.md</file>
      <file type="M">docs.ops.filesystems.s3.md</file>
      <file type="M">docs.ops.filesystems.oss.zh.md</file>
      <file type="M">docs.ops.filesystems.oss.md</file>
      <file type="M">docs.ops.filesystems.index.zh.md</file>
      <file type="M">docs.ops.filesystems.index.md</file>
      <file type="M">docs.ops.filesystems.azure.zh.md</file>
      <file type="M">docs.ops.filesystems.azure.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-6-27 01:00:00" id="13006" opendate="2019-6-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove GenericUDTFReplicateRows from GenericUDTFTest because Hive 1.2.1 doesn&amp;#39;t have it</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDTFTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-27 01:00:00" id="13012" opendate="2019-6-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle default partition name of Hive table</summary>
      <description>When a partition value is null or empty string, Hive assigns a default partition name. We need to be able to handle it.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-27 01:00:00" id="13017" opendate="2019-6-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Broken and irreproducible dockerized docs build</summary>
      <description>The build tools around docs/docker seem broken and (on my machine) give errors like the following while it is working on a colleague's machine:bash: /etc/bash_completion.d/git-prompt.sh: No such file or directorybash: __git_ps1: command not found/usr/bin/env: 'ruby.ruby2.5': No such file or directorybash: __git_ps1: command not foundReason seems to be that your whole user's $HOME is mounted (writable!) into the docker container. We should just mount the docs directory to get builds which are independent from the host system (making them reproducible) not have the commands in the container affect the host</description>
      <version>1.6.4,1.7.2,1.8.0,1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.docker.run.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-28 01:00:00" id="13029" opendate="2019-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove expressionBridge from QueryOperations factories</summary>
      <description>Expression bridge is used to create a schema of QueryOperation. This is no longer necessary with ResolvedExpressions in place.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.TableSourceValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.operations.OperationTreeBuilderImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.ProjectionOperationFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.OperationTreeBuilderFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.JoinOperationFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.operations.AggregateOperationFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.utils.LegacyTypeInfoDataTypeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-1 01:00:00" id="13037" opendate="2019-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate "Concepts -&gt; Glossary" page into Chinese</summary>
      <description>Translate Glossary page into Chinese: https://ci.apache.org/projects/flink/flink-docs-master/concepts/glossary.htmlThe markdown file is located in docs/concepts/glossary.md.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.concepts.glossary.zh.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-7-2 01:00:00" id="13049" opendate="2019-7-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port planner expressions to blink-planner from flink-planner</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sources.tsextractors.ExistingField.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sources.TableSourceUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelMdUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.logical.WindowPropertiesRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.logical.LogicalWindowAggregateRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecSortWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecLocalSortWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecLocalHashWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashWindowAggregateBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.batch.BatchExecHashWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.WindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.LogicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdUniqueKeys.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdUniqueGroups.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnUniqueness.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.logical.groupWindows.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.expressions.windowProperties.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.expressions.PlannerResolvedFieldReference.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.WindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.SortWindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.HashWindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.AggsHandlerCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.util.RexNodeExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RexNodeExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-3 01:00:00" id="13067" opendate="2019-7-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix broken links to contributing docs</summary>
      <description>As contributing links change on https://github.com/apache/flink-web, all links to contributing related docs have become broken. We need to fix these broken links.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.redirects.windows.md</file>
      <file type="M">docs.redirects.setup.quickstart.md</file>
      <file type="M">docs.redirects.filesystems.md</file>
      <file type="M">docs.redirects.example.quickstart.md</file>
      <file type="M">docs.internals.components.zh.md</file>
      <file type="M">docs.internals.components.md</file>
      <file type="M">docs.index.zh.md</file>
      <file type="M">docs.index.md</file>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
      <file type="M">docs.dev.projectsetup.scala.api.quickstart.zh.md</file>
      <file type="M">docs.dev.projectsetup.scala.api.quickstart.md</file>
      <file type="M">docs.dev.projectsetup.java.api.quickstart.zh.md</file>
      <file type="M">docs.dev.projectsetup.java.api.quickstart.md</file>
      <file type="M">docs.dev.libs.ml.contribution.guide.zh.md</file>
      <file type="M">docs.dev.libs.ml.contribution.guide.md</file>
      <file type="M">docs.dev.libs.gelly.index.zh.md</file>
      <file type="M">docs.dev.libs.gelly.index.md</file>
      <file type="M">.github.PULL.REQUEST.TEMPLATE.md</file>
      <file type="M">.github.CONTRIBUTING.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-3 01:00:00" id="13074" opendate="2019-7-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix PartitionableTableSink doesn&amp;#39;t work in flink&amp;blink planner</summary>
      <description/>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sinks.TableSinkUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.PreValidateReWriter.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.BatchTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.DatabaseCalciteSchema.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sinks.TableSinkUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.stream.StreamExecSinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecSinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.PreValidateReWriter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.catalog.DatabaseCalciteSchema.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.CatalogSinkModifyOperation.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-3 01:00:00" id="13082" opendate="2019-7-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support MatchRecognize in blink planner</summary>
      <description>Support MATCH RECOGNIZE in blink planner. This will port the functionality of MATCH RECOGNIZE in flink-planner to blink-planner.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.utils.UserDefinedFunctionTestUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RelExplainUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.ProctimeSqlFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-7-4 01:00:00" id="13094" opendate="2019-7-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide an easy way to read timers using the State Processor API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.TimerSerializer.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.SavepointReaderKeyedStateITCase.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.input.KeyedStateInputFormatTest.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.input.KeyedStateInputFormat.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.functions.KeyedStateReaderFunction.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.TestInternalTimerService.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.InternalTimerServiceImplTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimerService.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2019-7-5 01:00:00" id="13125" opendate="2019-7-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add note to PubSub connector documentation about beta status</summary>
      <description>As part of the review of FLINK-9311, we decided to add a note to the documentation page that the connector is considered beta by the community.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.pubsub.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-7 01:00:00" id="13134" opendate="2019-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>override default hadoop version from 2.4.1 to 2.7.5 in flink-connector-hive</summary>
      <description>Hive 2.3.4 relies on Hadoop 2.7.2 or later version. The default hadoop version in Flink globally is 2.4.1 which misses some classes in 2.7.2 and thus doesn't meet the requirement.Found this bug when running tests locally after merging FLINK-12934. Not sure why Travis CI succeeded for FLINK-12934 though, maybe because the build machine has all versioned hadoops cached locally.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-8 01:00:00" id="13137" opendate="2019-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove legacy python docs</summary>
      <description> Batch (DataSet API)/Transformations and Batch (DataSet API)/Zipping Elements have legacy flink python related documentation</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.batch.zip.elements.guide.zh.md</file>
      <file type="M">docs.dev.batch.zip.elements.guide.md</file>
      <file type="M">docs.dev.batch.dataset.transformations.zh.md</file>
      <file type="M">docs.dev.batch.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-8 01:00:00" id="13155" opendate="2019-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client end-to-end test fails on Travis</summary>
      <description>The SQL Client end-to-end test which executes test-scripts/test_sql_client.sh fails on Travis with non-empty out files.https://api.travis-ci.org/v3/job/554991859/log.txt</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-7-9 01:00:00" id="13173" opendate="2019-7-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Only run openSSL tests if desired</summary>
      <description>Rename flink.tests.force-openssl to flink.tests.with-openssl and only run openSSL-based unit tests if this is set. This way, we avoid systems where the bundled dynamic libraries do not work. Travis seems to run fine and will have this property set.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.watchdog.sh</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.net.SSLUtilsTest.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-12-10 01:00:00" id="13195" opendate="2019-7-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add create table support for SqlClient</summary>
      <description/>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-7-11 01:00:00" id="13208" opendate="2019-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Notice file for upgrading calcite to 1.20</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-11 01:00:00" id="13210" opendate="2019-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive connector test should dependent on blink planner instead of legacy planner</summary>
      <description>Blink planner has more support and more functions, and some ITCase will not be able to measure it without relying on Blink-planner in test.And now, the table env is unified, I think we can use unified table env to it cases.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.batch.connectors.hive.HiveTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-11 01:00:00" id="13214" opendate="2019-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive connector is missing jdk.tools exclusion for Java 9</summary>
      <description>[ERROR] Failed to execute goal on project flink-connector-hive_2.12: Could not resolve dependencies for project org.apache.flink:flink-connector-hive_2.12:jar:1.9-SNAPSHOT: Could not find artifact jdk.tools:jdk.tools:jar:1.7 at specified path C:\Dev\Java\9/../lib/tools.jar -&gt; [Help 1]</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-11 01:00:00" id="13216" opendate="2019-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>AggregateITCase.testNestedGroupByAgg fails on Travis</summary>
      <description>The AggregateITCase.testNestedGroupByAgg fails on Travis withAggregateITCase.testNestedGroupByAgg:472 expected:&lt;List(1,1,1,1,1, 3,1,15,15,3, 4,1,34,34,4, 7,2,23,5,2)&gt; but was:&lt;List(1,1,7,1,1, 3,1,15,15,3, 4,1,34,34,4, 7,2,23,5,2)&gt;https://api.travis-ci.org/v3/job/557214216/log.txt</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.functions.aggfunctions.AggFunctionTestBase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.MinWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.aggfunctions.MaxWithRetractAggFunction.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-12 01:00:00" id="1322" opendate="2014-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala API does not respect WriteMode set by configuration</summary>
      <description>The Scala API does not have output methods which do not take a WriteMode parameter. As default value the NO_OVERWRITE is set. Consequently, a possible global WriteMode set in the configuration is always overwritten. The Java API behaves differently, if no WriteMode is provided. We should sync both APIs to guarantee consistent behaviour.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.DataSet.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-11 01:00:00" id="13221" opendate="2019-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Blink planner should set ScheduleMode to LAZY_FROM_SOURCES_WITH_BATCH_SLOT_REQUEST for batch jobs</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.executor.BatchExecutor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.AdaptedRestartPipelinedRegionStrategyNGFailoverTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-11 01:00:00" id="13222" opendate="2019-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for AdaptedRestartPipelinedRegionStrategyNG</summary>
      <description>It should be documented that if jobmanager.execution.failover-strategy is set to region, the new pipelined region failover strategy (AdaptedRestartPipelinedRegionStrategyNG) will be used. Acceptance Criteria config values region and full are documented to be decided: config values region-legacy and individual remain undocumented</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs..includes.generated.job.manager.configuration.html</file>
      <file type="M">docs.dev.restart.strategies.zh.md</file>
      <file type="M">docs.dev.restart.strategies.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-11 01:00:00" id="13226" opendate="2019-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaProducerExactlyOnceITCase.testMultipleSinkOperators fails on Travis</summary>
      <description>The KafkaProducerExactlyOnceITCase.testMultipleSinkOperators fails on Travis with not producing output for 300 s.https://api.travis-ci.org/v3/job/557290235/log.txt</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaInternalProducerITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaProducer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-11 01:00:00" id="13227" opendate="2019-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate "Asynchronous I/O for External Data Access" page into Chinese</summary>
      <description>The page url is https://ci.apache.org/projects/flink/flink-docs-master/dev/stream/operators/asyncio.htmlThe markdown file is located in flink/docs/dev/stream/operators/asyncio.md</description>
      <version>1.8.0,1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.operators.asyncio.zh.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-11 01:00:00" id="13229" opendate="2019-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ExpressionReducer with udf bug in blink</summary>
      <description>When a udf is reduced by ExpressionReducer, there will be a code gen exception.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.CalcITCase.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-12 01:00:00" id="13243" opendate="2019-7-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>AbstractOperatorRestartTestBase fails on Windows</summary>
      <description>The regular expressions contain new-lines, resulting in no matches being found.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.AbstractOperatorRestoreTestBase.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-4-12 01:00:00" id="13250" opendate="2019-7-12 00:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Blink Planner should assign concrete resources to all nodes</summary>
      <description>Currently some nodes retain a resource spec of "DEFAULT"/ "UNKNOWN" as the result of merging the pre-existing "DEFAULT" with the actual profile.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.executor.BatchExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-15 01:00:00" id="13255" opendate="2019-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip hive connector test for JDK9 profile</summary>
      <description>The Hive binary we depend upon has issues when running with JDK9, e.g. it assumes application class loader is URL class loader, which is no longer true in JDK9.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-15 01:00:00" id="13257" opendate="2019-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>blink runner should avoid stream operator implementing BoundedOneInput</summary>
      <description>According to https://issues.apache.org/jira/browse/FLINK-11879 , BoundedOneInput should not coexist with checkpoint, so we can not use BoundedOneInput in streaming mode.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.sort.StreamSortOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.sort.StreamSortOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.ScanUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.SinkCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.OperatorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.NestedLoopJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.LongHashJoinGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExpandCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CorrelateCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CalcCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.AggCodeGenHelper.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-15 01:00:00" id="13263" opendate="2019-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>supports explain DAG plan in flink-python</summary>
      <description>update existing `explain` to support explain DAG plan in flink-python</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.util.exceptions.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-7-16 01:00:00" id="13289" opendate="2019-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Blink-planner should setKeyFields to upsert table sink</summary>
      <description>Hi , in flink-jdbc connector module, I change the Flink planner to Blink planner to test all test case,because we want to use Blank planner in our program. When I test the JDBCUpsertTableSinkITCase class , the method testUpsert throw the exception:java.lang.UnsupportedOperationException: JDBCUpsertTableSink can not support I saw the src code,in Flink planner , the StreamPlanner set the JDBCUpsertTableSink' keyFields,but in Blink planner , I didn't find anywhere to set JDBCUpsertTableSink' keyFields,so JDBCUpsertTableSink keyFields is null, when execute JDBCUpsertTableSink newFormat(),it thrown the exception.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamTestSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.RankITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.Limit0RemoveITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.UpdatingPlanChecker.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-17 01:00:00" id="13308" opendate="2019-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-python releases 2 jars</summary>
      <description>flink-python uses a classifier to differentiate itseld from the old python API. turns out thsi doesn't work since it still tries to release a normal unshaded flink-python jar.We should drop the classifier, and either stick to flink-python or rename it as proposed in FLINK-12776.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.controller.sh</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.pyflink-shell.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.pyflink-gateway-server.sh</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-container.docker.Dockerfile</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-17 01:00:00" id="13310" opendate="2019-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove shade-plugin configuration in hive-connector</summary>
      <description>The hive connector has a shade plugin configuration but isn't doing anything interesting. We may as well remove it.</description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-17 01:00:00" id="13312" opendate="2019-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>move tests for data type mappings between Flink and Hive into its own test class</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.catalog.CatalogTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogGenericMetadataTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-18 01:00:00" id="13314" opendate="2019-7-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct resultType of some PlannerExpression when operands contains DecimalTypeInfo or BigDecimalTypeInfo in Blink planner</summary>
      <description>Correct resultType of the following PlannerExpression when operands contains DecimalTypeInfo or BigDecimalTypeInfo in Blink planner:Minus/plus/Div/Mul/Ceil/Floor/Round </description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.DecimalITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.expressions.mathExpressions.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.expressions.arithmetic.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.expressions.RexNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-18 01:00:00" id="13315" opendate="2019-7-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port wmstrategies to api-java-bridge</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.DataFormatConverters.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sources.wmstrategies.watermarkStrategies.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sources.wmstrategies.BoundedOutOfOrderTimestamps.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sources.wmstrategies.AscendingTimestamps.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sources.wmstrategies.watermarkStrategies.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.sources.wmstrategies.AscendingTimestamps.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.physical.stream.StreamExecTableSourceScan.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-18 01:00:00" id="13322" opendate="2019-7-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix serializer snapshot recovery in BaseArray and BaseMap serializers.</summary>
      <description>In BaseArray and BaseMap serializers, their element (or key/value) serializers are not stored in the config snapshot. When restoring the BaseArray/BaseMap serializers from the snapshots, their element/key/value serializers might be incorrect. This situation will happen when user uses his custom kryo serializers as element/key/value serializers.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.typeutils.BaseMapSerializerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.typeutils.BaseArraySerializerTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.BinaryArrayTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.BaseRowTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.BaseMapSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.BaseArraySerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.types.InternalSerializers.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-18 01:00:00" id="13323" opendate="2019-7-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests for complex data formats</summary>
      <description>There are currently no tests guarding some complex data formats, for example nested row, generic array and generic map.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.util.SegmentsUtilTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.DecimalTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.DataFormatTestUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.dataformat.BinaryRowTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-18 01:00:00" id="13325" opendate="2019-7-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add test case for FLINK-13249</summary>
      <description/>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-16 01:00:00" id="1333" opendate="2014-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Getter/Setter recognition for POJO fields with generics is not working</summary>
      <description>Fields likeprivate List&lt;Contributors&gt; contributors;Are not recognized correctly, even if they have getters and setters.Workaround: make them public.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.type.extractor.PojoTypeExtractionTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-20 01:00:00" id="13339" opendate="2019-7-20 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add an implementation of pipeline&amp;#39;s api</summary>
      <description>Add an implement PipelineStage, Estimator, Transformer, Model. Add MLSession to hold the execution environment and others session shared variable. Add AlgoOperator for the implementation of algorithms. Add BatchOperator and StreamOperator based on AlgoOperator Add TableSourceBatchOp and TableSourceStreamOp</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-ml-parent.flink-ml-lib.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-21 01:00:00" id="13345" opendate="2019-7-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dump jstack output for Flink JVMs after Jepsen Tests</summary>
      <description>Dump the output of jstack -l &lt;pid&gt; for all Flink JVMs after each Jepsen test. This is helpful for debugging deadlocks.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.utils.clj</file>
      <file type="M">flink-jepsen.src.jepsen.flink.db.clj</file>
      <file type="M">flink-jepsen.README.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-22 01:00:00" id="13353" opendate="2019-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove 2 args constructor in REPLACE expression</summary>
      <description>Replace definition in stringExpression.scala has another constructor with 2 arguments.According to source code, the args' meaning are str, begin. And it call other constructor with 3 args adding the 3rd arg which is the length of str.But its expectTypes is (String, String, String), but actually is (String, int, int).So I think the 2 args defined constructor means search and replacement is "" default, not begin and length of str. </description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.PlannerExpressionConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.stringExpressions.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.PlannerExpressionConverter.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-22 01:00:00" id="13354" opendate="2019-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for how to use blink planner</summary>
      <description>Add documentation for how to use different planner “Overview”: add pom dependency “Concepts &amp; Common API”: add description about how to use different planner in code</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.common.zh.md</file>
      <file type="M">docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-22 01:00:00" id="13355" opendate="2019-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Temporal Table Join in blink planner</summary>
      <description>Add documentation for Temporal Table Join in blink planner “Streaming Concepts / Temporal Tables”: introduce concepts of temporal table in blink planner and the difference and sameness to flink planner temporal table “Joins in Continuous Queries”: how to use temporal join in bink planner “SQL”: join with temporal table in SQL</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.zh.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.md</file>
      <file type="M">docs.dev.table.streaming.joins.zh.md</file>
      <file type="M">docs.dev.table.streaming.joins.md</file>
      <file type="M">docs.dev.table.sql.zh.md</file>
      <file type="M">docs.dev.table.sql.md</file>
      <file type="M">docs.dev.table.sourceSinks.zh.md</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-22 01:00:00" id="13356" opendate="2019-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for TopN and Deduplication in blink planner</summary>
      <description>Add documentation for TopN in blink planner“SQL”: how to write TopN in SQL and some tips</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.zh.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-22 01:00:00" id="13359" opendate="2019-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for DDL introduction</summary>
      <description>Add documentation for DDL introduction “Concepts &amp; Common API”: Add a section to describe how to execute DDL on TableEnvironment. “SQL Client”: Add a section and example in SQL CLI page too?</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sql.zh.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-8-23 01:00:00" id="13374" opendate="2019-7-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala compiler causes StackOverflowError</summary>
      <description>Here is a instance:https://api.travis-ci.org/v3/job/562043336/log.txt</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-23 01:00:00" id="13384" opendate="2019-7-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The back pressure monitoring does not work for StreamSources</summary>
      <description>I think it is caused by: FLINK-12483. The reason is that the BackPressureStatsTrackerImpl samples only the main thread. FLINK-12483 introduced a separate thread for executing the sources in the mailbox model. It is similar to other old bug that concerned only Kafka source: FLINK-3456</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SourceStreamTask.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.backpressure.BackPressureStatsTrackerImplITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskStackTraceSampleableTaskAdapter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-23 01:00:00" id="13386" opendate="2019-7-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix some frictions in the new default Web UI</summary>
      <description>While manually testing the new WebUI I found a few frictions. when using the UI the left panel hides unexpectedly at random moments mouse wheel does not work on the logs (taskmanager, jobmanager) pane the jobmanager configuration is not sorted different sorting of the operators (the old UI showed the sources first) the drop-down list for choosing operator/tasks metrics is not sorted, which makes it super hard to screen through available metrics arrow does not touch the rectangles in Chrome (see attached screenshot)There are also some views missing in the new UI that I personally found useful in the old UI: can't see watermarks for all operators at once no numeric metrics (only graphs)</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.node.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.node.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.dagre.node.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.job-overview.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-detail.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.configuration.job-manager-configuration.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.pipes.pipe.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.pipes.humanize-duration.pipe.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-chart.job-chart.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-chart.job-chart.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-chart.job-chart.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.list.job-overview-list.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.list.job-overview-list.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.metrics.service.ts</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-23 01:00:00" id="13388" opendate="2019-7-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update UI screenshots in the documentation to the new default Web Frontend</summary>
      <description/>
      <version>1.9.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.page.img.quickstart-setup.jobmanager-3.png</file>
      <file type="M">docs.page.img.quickstart-setup.jobmanager-2.png</file>
      <file type="M">docs.page.img.quickstart-setup.jobmanager-1.png</file>
      <file type="M">docs.page.img.quickstart-example.jobmanager-overview.png</file>
      <file type="M">docs.page.img.quickstart-example.jobmanager-job.png</file>
      <file type="M">docs.fig.checkpoint.monitoring-summary.png</file>
      <file type="M">docs.fig.checkpoint.monitoring-history.png</file>
      <file type="M">docs.fig.checkpoint.monitoring-details.summary.png</file>
      <file type="M">docs.fig.checkpoint.monitoring-details.subtasks.png</file>
      <file type="M">docs.fig.checkpoint.monitoring-details.png</file>
      <file type="M">docs.fig.back.pressure.sampling.ok.png</file>
      <file type="M">docs.fig.back.pressure.sampling.in.progress.png</file>
      <file type="M">docs.fig.back.pressure.sampling.high.png</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-24 01:00:00" id="13398" opendate="2019-7-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive connector doesn&amp;#39;t compile on Java 9</summary>
      <description>recent dependency exclusions re-introduced a jdk.tools dependency which cannot be resolved on Java 9.Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.1.1:shade (shade-flink) on project flink-connector-hive_2.11: Error creating shaded jar: Could not resolve following dependencies: [jdk.tools:jdk.tools:jar:1.7 (system)]: Could not resolve dependencies for project org.apache.flink:flink-connector-hive_2.11:jar:1.10-SNAPSHOT: Could not find artifact jdk.tools:jdk.tools:jar:1.7 at specified path /home/travis/openjdk9/../lib/tools.jar -&gt; [Help 1]</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-25 01:00:00" id="13409" opendate="2019-7-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Supported java UDFs in python API</summary>
      <description>It's better to support java UDF in python API.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.Table.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.table.table.py</file>
      <file type="M">docs.dev.table.udfs.zh.md</file>
      <file type="M">docs.dev.table.udfs.md</file>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-25 01:00:00" id="13429" opendate="2019-7-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client end-to-end test fails</summary>
      <description>The SQL Client test does not work on the current master and hangs when executing CEP SQL. We reproduced this on two machines.At commit 475c30cd4064a7bc2e32c963b6ca58e7623251c6 it was working.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.sources.wmstrategies.BoundedOutOfOrderTimestamps.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-8-26 01:00:00" id="13436" opendate="2019-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add TPC-H queries as E2E tests</summary>
      <description>We should add the TPC-H queries as E2E tests in order to verify the blink planner.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-26 01:00:00" id="13440" opendate="2019-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add test that fails job when sync savepoint is discarded.</summary>
      <description>This is a test for FLINK-12858</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStateRestoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointFailureManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorMasterHooksTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.LegacyScheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointFailureManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-26 01:00:00" id="13441" opendate="2019-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add batch sql E2E test which runs with fewer slots than parallelism</summary>
      <description>We should adapt the existing batch E2E test to use the newly introduced ScheduleMode#LAZY_FROM_SOURCES_WITH_BATCH_SLOT_REQUEST and verify that the job runs on a cluster with fewer slots than the job's parallelism. In order to make this work, we need to set the shuffles to be blocking via ExecutionMode#BATCH. As a batch job we should use the DataSetAllroundTestProgram. Update: currently, the ScheduleMode#LAZY_FROM_SOURCES_WITH_BATCH_SLOT_REQUEST option is set only by table planner(s) and cannot be set (configured) for general purpose (batch) job. As agreed offline, this ticket would add a new e2e test for batch sql job instead of modifying DataSetAllroundTestProgram. </description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.splits.split.misc.hadoopfree.sh</file>
      <file type="M">tools.travis.splits.split.misc.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-28 01:00:00" id="13450" opendate="2019-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adjust tests to tolerate arithmetic differences between x86 and ARM</summary>
      <description>Certain arithmetic operations have different precision/rounding on ARM versus x86.Tests using floating point numbers should be changed to tolerate a certain minimal deviation.</description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.runtime.functions.ScalarFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.cost.DataSetCost.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-29 01:00:00" id="13453" opendate="2019-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump shade plugin to 3.2.1</summary>
      <description>The shade plugin fails with an IllegalArgumentException when run on Java 11.</description>
      <version>None</version>
      <fixedVersion>shaded-9.0,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-29 01:00:00" id="13454" opendate="2019-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump japicmp jaxb dependencies</summary>
      <description>The japicmp plugins fails with a ClassNotFoundExceptions with the currently defined jaxb dependencies when run on java 11.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-29 01:00:00" id="13455" opendate="2019-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move jdk.tools exclusions out of dependency management</summary>
      <description>Defining exclusions via dependencyManagement is a bit unreliable, since the shade-plugin ignores them during dependency resolution  if they are defined for transitive dependencies.</description>
      <version>None</version>
      <fixedVersion>shaded-8.0,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-fs-tests.pom.xml</file>
      <file type="M">flink-filesystems.flink-hadoop-fs.pom.xml</file>
      <file type="M">flink-connectors.flink-hbase.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-filesystem.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-29 01:00:00" id="13456" opendate="2019-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump lombok to 1.16.22</summary>
      <description>Compiling the tests for flink-core fails with an ErrorDuringInitialization due to lombok,</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-29 01:00:00" id="13464" opendate="2019-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump powermock to 2.0.2</summary>
      <description>We have to bump powermock to 2.0.2 to resolve this issue in the InitOutputPathTest:java.lang.IllegalStateException: Failed to transform class with name org.apache.flink.core.fs.InitOutputPathTest. Reason: [source error] the called constructor is private in org.apache.flink.core.fs.InitOutputPathTest$NoOpLock    at org.powermock.core.classloader.javassist.JavassistMockClassLoader.defineAndTransformClass(JavassistMockClassLoader.java:119)    at org.powermock.core.classloader.MockClassLoader.loadMockClass(MockClassLoader.java:174)    at org.powermock.core.classloader.MockClassLoader.loadClassByThisClassLoader(MockClassLoader.java:102)    at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass1(DeferSupportingClassLoader.java:147)    at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass(DeferSupportingClassLoader.java:98)    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)    at java.base/java.lang.Class.forName0(Native Method)    at java.base/java.lang.Class.forName(Class.java:398)    at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.createDelegatorFromClassloader(JUnit4TestSuiteChunkerImpl.java:154)    at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.createDelegatorFromClassloader(JUnit4TestSuiteChunkerImpl.java:47)    at org.powermock.tests.utils.impl.AbstractTestSuiteChunkerImpl.createTestDelegators(AbstractTestSuiteChunkerImpl.java:107)    at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.&lt;init&gt;(JUnit4TestSuiteChunkerImpl.java:69)    at org.powermock.modules.junit4.common.internal.impl.AbstractCommonPowerMockRunner.&lt;init&gt;(AbstractCommonPowerMockRunner.java:36)    at org.powermock.modules.junit4.PowerMockRunner.&lt;init&gt;(PowerMockRunner.java:34)    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)    at org.junit.internal.builders.AnnotatedBuilder.buildRunner(AnnotatedBuilder.java:104)    at org.junit.internal.builders.AnnotatedBuilder.runnerForClass(AnnotatedBuilder.java:86)    at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:59)    at org.junit.internal.builders.AllDefaultPossibilitiesBuilder.runnerForClass(AllDefaultPossibilitiesBuilder.java:26)    at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:59)    at org.junit.internal.requests.ClassRequest.getRunner(ClassRequest.java:33)    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:49)    at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)    at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)    at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)Caused by: javassist.CannotCompileException: [source error] the called constructor is private in org.apache.flink.core.fs.InitOutputPathTest$NoOpLock    at javassist.expr.NewExpr.replace(NewExpr.java:214)    at org.powermock.core.transformers.javassist.support.PowerMockExpressionEditor.edit(PowerMockExpressionEditor.java:73)    at javassist.expr.ExprEditor.loopBody(ExprEditor.java:212)    at javassist.expr.ExprEditor.doit(ExprEditor.java:91)    at javassist.CtClassType.instrument(CtClassType.java:1431)    at org.powermock.core.transformers.javassist.InstrumentMockTransformer.transform(InstrumentMockTransformer.java:41)    at org.powermock.core.transformers.javassist.AbstractJavaAssistMockTransformer.transform(AbstractJavaAssistMockTransformer.java:40)    at org.powermock.core.transformers.support.DefaultMockTransformerChain.transform(DefaultMockTransformerChain.java:43)    at org.powermock.core.classloader.MockClassLoader.transformClass(MockClassLoader.java:184)    at org.powermock.core.classloader.javassist.JavassistMockClassLoader.defineAndTransformClass(JavassistMockClassLoader.java:102)    ... 27 moreCaused by: compile error: the called constructor is private in org.apache.flink.core.fs.InitOutputPathTest$NoOpLock    at javassist.compiler.MemberCodeGen.getAccessibleConstructor(MemberCodeGen.java:709)    at javassist.compiler.MemberCodeGen.atMethodCallCore2(MemberCodeGen.java:610)    at javassist.compiler.MemberCodeGen.atMethodCallCore(MemberCodeGen.java:589)    at javassist.expr.NewExpr$ProceedForNew.doit(NewExpr.java:237)    at javassist.compiler.JvstCodeGen.atCallExpr(JvstCodeGen.java:235)    at javassist.compiler.ast.CallExpr.accept(CallExpr.java:46)    at javassist.compiler.CodeGen.atAssignCore(CodeGen.java:877)    at javassist.compiler.CodeGen.atVariableAssign(CodeGen.java:810)    at javassist.compiler.CodeGen.atAssignExpr(CodeGen.java:764)    at javassist.compiler.CodeGen.atStmnt(CodeGen.java:332)    at javassist.compiler.ast.Stmnt.accept(Stmnt.java:50)    at javassist.compiler.CodeGen.atStmnt(CodeGen.java:351)    at javassist.compiler.ast.Stmnt.accept(Stmnt.java:50)    at javassist.compiler.CodeGen.atIfStmnt(CodeGen.java:411)    at javassist.compiler.CodeGen.atStmnt(CodeGen.java:355)    at javassist.compiler.ast.Stmnt.accept(Stmnt.java:50)    at javassist.compiler.Javac.compileStmnt(Javac.java:569)    at javassist.expr.NewExpr.replace(NewExpr.java:208)    ... 36 more</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-29 01:00:00" id="13465" opendate="2019-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump javassist to 3.24.0-GA</summary>
      <description>Bump javassist to resolve this is issue in InitOutputPathTest:java.lang.ClassFormatError: Nest-host class_info_index 47 has bad constant type in class file org/apache/flink/core/fs/InitOutputPathTest$SyncedFileSystem    at java.base/java.lang.ClassLoader.defineClass1(Native Method)    at java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:1016)    at org.powermock.core.classloader.MockClassLoader.defineClass(MockClassLoader.java:180)    at org.powermock.core.classloader.MockClassLoader.loadMockClass(MockClassLoader.java:176)    at org.powermock.core.classloader.MockClassLoader.loadClassByThisClassLoader(MockClassLoader.java:102)    at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass1(DeferSupportingClassLoader.java:147)    at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass(DeferSupportingClassLoader.java:98)    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)    at java.base/java.lang.Class.getDeclaredMethods0(Native Method)    at java.base/java.lang.Class.privateGetDeclaredMethods(Class.java:3167)    at java.base/java.lang.Class.privateGetPublicMethods(Class.java:3192)    at java.base/java.lang.Class.getMethods(Class.java:1905)    at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.getTestMethods(PowerMockJUnit44RunnerDelegateImpl.java:109)    at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.&lt;init&gt;(PowerMockJUnit44RunnerDelegateImpl.java:85)    at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl.&lt;init&gt;(PowerMockJUnit47RunnerDelegateImpl.java:42)    at org.powermock.modules.junit4.internal.impl.PowerMockJUnit49RunnerDelegateImpl.&lt;init&gt;(PowerMockJUnit49RunnerDelegateImpl.java:25)    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)    at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.createDelegatorFromClassloader(JUnit4TestSuiteChunkerImpl.java:165)    at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.createDelegatorFromClassloader(JUnit4TestSuiteChunkerImpl.java:47)    at org.powermock.tests.utils.impl.AbstractTestSuiteChunkerImpl.createTestDelegators(AbstractTestSuiteChunkerImpl.java:107)    at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.&lt;init&gt;(JUnit4TestSuiteChunkerImpl.java:69)    at org.powermock.modules.junit4.common.internal.impl.AbstractCommonPowerMockRunner.&lt;init&gt;(AbstractCommonPowerMockRunner.java:36)    at org.powermock.modules.junit4.PowerMockRunner.&lt;init&gt;(PowerMockRunner.java:34)    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)    at org.junit.internal.builders.AnnotatedBuilder.buildRunner(AnnotatedBuilder.java:104)    at org.junit.internal.builders.AnnotatedBuilder.runnerForClass(AnnotatedBuilder.java:86)    at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:59)    at org.junit.internal.builders.AllDefaultPossibilitiesBuilder.runnerForClass(AllDefaultPossibilitiesBuilder.java:26)    at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:59)    at org.junit.internal.requests.ClassRequest.getRunner(ClassRequest.java:33)    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:49)    at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)    at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)    at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-7-30 01:00:00" id="13499" opendate="2019-7-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove dependency on MapR artifact repository</summary>
      <description>The MapR artifact repository causes some problems. It does not reliably offer a secure (https://) access.We should change the MapR FS connector to work based on reflection and avoid a hard dependency on any of the MapR vendor-specific artifacts. That should allow us to get rid of the dependency without regressing on the support for the file system.</description>
      <version>1.9.0</version>
      <fixedVersion>1.8.2,1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.watchdog.sh</file>
      <file type="M">tools.travis.nightly.sh</file>
      <file type="M">tools.travis.controller.sh</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.main.java.org.apache.flink.runtime.fs.maprfs.MapRFsFactory.java</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.main.java.org.apache.flink.runtime.fs.maprfs.MapRFileSystem.java</file>
      <file type="M">flink-filesystems.flink-mapr-fs.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-8-30 01:00:00" id="13501" opendate="2019-7-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fixes a few issues in documentation for Hive integration</summary>
      <description>Going thru existing Hive doc I found the following issues that should be addressed:1. Section "Hive Integration" should come after "SQL client" (at the same level).2. In Catalog section, there are headers named "Hive Catalog". Also, some information is duplicated with that in "Hive Integration"3. "Data Type Mapping" is Hive specific and should probably move to "Hive integration"</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-31 01:00:00" id="13504" opendate="2019-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NoSuchFieldError when executing DDL via tEnv.sqlUpdate in application project</summary>
      <description>When we create a quickstart project to try flink 1.9/1.10, a NoSuchFieldError is thrown.The dependencies (the flink 1.0 is installed locally for commit 70fe6aa747ad021bbb8dd8cdc0beecc863f010be, flink 1.9 has the same problem): &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-api-java&lt;/artifactId&gt; &lt;version&gt;1.10-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner_2.11&lt;/artifactId&gt; &lt;version&gt;1.10-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;The program code:package com.github.wuchong;import org.apache.flink.table.api.EnvironmentSettings;import org.apache.flink.table.api.TableEnvironment;public class DDLTest { public static void main(String[] args) { EnvironmentSettings settings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build(); TableEnvironment tEnv = TableEnvironment.create(settings); tEnv.sqlUpdate("CREATE TABLE MyTable (\n" + " a int, \n" + " b bigint, \n" + " c varchar \n" + ")\n comment 'table comment'\n" + "partitioned by (b)\n" + "with (\n" + " connector = 'csv', \n" + " csv.path = '/tmp/path'\n" + ")"); }}The exception:Exception in thread "main" java.lang.NoSuchFieldError: names at org.apache.flink.sql.parser.ddl.SqlCreateTable.fullTableName(SqlCreateTable.java:326) at org.apache.flink.table.sqlexec.SqlToOperationConverter.convertCreateTable(SqlToOperationConverter.java:140) at org.apache.flink.table.sqlexec.SqlToOperationConverter.convert(SqlToOperationConverter.java:86) at org.apache.flink.table.planner.StreamPlanner.parse(StreamPlanner.scala:115) at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:335) at com.github.wuchong.DDLTest.main(DDLTest.java:29)</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-uber.pom.xml</file>
      <file type="M">flink-table.flink-table-uber-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-31 01:00:00" id="13518" opendate="2019-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable Hive tests</summary>
      <description>Hive straight up doesn't support Java 11 (or anything above Java 8 really), so we might as well disable all tests on Java 11.15:49:57.131 [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.208 s - in org.apache.flink.batch.connectors.hive.HiveTableFactoryTestSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/home/travis/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.6.2/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/travis/.m2/repository/org/slf4j/slf4j-log4j12/1.7.15/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.15:50:11.610 [INFO] Running org.apache.flink.batch.connectors.hive.TableEnvHiveConnectorTest15:50:11.625 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.009 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.batch.connectors.hive.TableEnvHiveConnectorTest15:50:11.627 [ERROR] org.apache.flink.batch.connectors.hive.TableEnvHiveConnectorTest Time elapsed: 0.007 s &lt;&lt;&lt; ERROR!java.lang.IllegalStateException: Failed to create HiveServer :Error applying authorization policy on hive configuration: class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')Caused by: java.lang.RuntimeException: Error applying authorization policy on hive configuration: class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')Caused by: java.lang.ClassCastException: class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-1 01:00:00" id="13523" opendate="2019-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Verify and correct arithmetic function&amp;#39;s semantic for Blink planner</summary>
      <description/>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.MiniBatchGroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdFilteredColumnIntervalTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.expressions.ReturnTypeInference.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.AvgAggFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.AvgAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.DeclarativeAggregateFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeSystem.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.AggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.DeclarativeAggCodeGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.codegen.agg.TestLongAvgFunc.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateReduceGroupingTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.HashAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.SortAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveShuffleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.AggregateReduceGroupingRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.subquery.SubQuerySemiJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.WindowGroupReorderRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.AggTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.AggWithoutKeysTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.BatchAggTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.SortAggCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.AggCallSelectivityEstimatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.GroupingSetsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.WindowAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.DecimalITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.MiscITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.AggregationITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.MatchRecognizeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.RankITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SplitAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.GroupWindowITCase.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-8-1 01:00:00" id="13529" opendate="2019-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Verify and correct agg function&amp;#39;s semantic for Blink planner</summary>
      <description/>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.validation.AggregateValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.stream.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.RemoveCollationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ConcatWsWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ConcatWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.SqlConcatAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ConcatWsWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ConcatWithRetractAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.aggfunctions.ConcatAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SplitAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.SqlFirstLastValueAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.SortDistinctAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.RankTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicityTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicity.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.SqlIncrSumAggFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-8-2 01:00:00" id="13545" opendate="2019-8-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JoinToMultiJoinRule should not match SEMI/ANTI LogicalJoin</summary>
      <description>run tpcds 14.a on blink planner, an exception will thrownjava.lang.ArrayIndexOutOfBoundsException: 84 at org.apache.calcite.rel.rules.JoinToMultiJoinRule$InputReferenceCounter.visitInputRef(JoinToMultiJoinRule.java:564) at org.apache.calcite.rel.rules.JoinToMultiJoinRule$InputReferenceCounter.visitInputRef(JoinToMultiJoinRule.java:555) at org.apache.calcite.rex.RexInputRef.accept(RexInputRef.java:112) at org.apache.calcite.rex.RexVisitorImpl.visitCall(RexVisitorImpl.java:80) at org.apache.calcite.rex.RexCall.accept(RexCall.java:191) at org.apache.calcite.rel.rules.JoinToMultiJoinRule.addOnJoinFieldRefCounts(JoinToMultiJoinRule.java:481) at org.apache.calcite.rel.rules.JoinToMultiJoinRule.onMatch(JoinToMultiJoinRule.java:166) at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:319) at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:560) at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:419) at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:284) at org.apache.calcite.plan.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:74) at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:215) at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:202)the reason is JoinToMultiJoinRule should match SEMI/ANTI LogicalJoin. before calcite-1.20, SEMI join is represented by SemiJoin which is not matched JoinToMultiJoinRule.</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-8-2 01:00:00" id="13558" opendate="2019-8-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include table examples in flink-dist</summary>
      <description>We want to treat the table api as first-class API. We already included in the lib directory flink.We should also include some examples of the table api in the distribution.Before that we should strip all the dependency and just include the classes from example module.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.pom.xml</file>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-examples.flink-examples-batch.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-3 01:00:00" id="13563" opendate="2019-8-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TumblingGroupWindow should implement toString method</summary>
      <description>@Test def testAllEventTimeTumblingGroupWindowOverTime(): Unit = { val util = streamTestUtil() val table = util.addDataStream[(Long, Int, String)]( "T1", 'long, 'int, 'string, 'rowtime.rowtime) val windowedTable = table .window(Tumble over 5.millis on 'rowtime as 'w) .groupBy('w) .select('int.count) util.verifyPlan(windowedTable) }currently, it's physical plan is HashWindowAggregate(window=[TumblingGroupWindow], select=[Final_COUNT(count$0) AS EXPR$0])+- Exchange(distribution=[single]) +- LocalHashWindowAggregate(window=[TumblingGroupWindow], select=[Partial_COUNT(int) AS count$0]) +- TableSourceScan(table=[[default_catalog, default_database, Table1, source: [TestTableSource(long, int, string)]]], fields=[long, int, string])we know nothing about the TumblingGroupWindow except its name. the expected plan isHashWindowAggregate(window=[TumblingGroupWindow('w, long, 5)], select=[Final_COUNT(count$0) AS EXPR$0])+- Exchange(distribution=[single]) +- LocalHashWindowAggregate(window=[TumblingGroupWindow('w, long, 5)], select=[Partial_COUNT(int) AS count$0]) +- TableSourceScan(table=[[default_catalog, default_database, Table1, source: [TestTableSource(long, int, string)]]], fields=[long, int, string])</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RelTimeIndicatorConverterTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.AggregateReduceGroupingRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.UnnestTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateReduceGroupingTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.api.stream.ExplainTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.logical.groupWindows.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-3 01:00:00" id="13564" opendate="2019-8-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>blink planner should also throw exception if constant with YEAR TO MONTH resolution was used for group windows</summary>
      <description>just as FLINK-11017, blink planner should also throw exception if constant with YEAR TO MONTH resolution was used for group windows</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.StreamLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalWindowAggregateRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.BatchLogicalWindowAggregateRule.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-4 01:00:00" id="13567" opendate="2019-8-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avro Confluent Schema Registry nightly end-to-end test failed on Travis</summary>
      <description>The Avro Confluent Schema Registry nightly end-to-end test failed on Travis with[FAIL] 'Avro Confluent Schema Registry nightly end-to-end test' failed after 2 minutes and 11 seconds! Test exited with exit code 1No taskexecutor daemon (pid: 29044) is running anymore on travis-job-b0823aec-c4ec-4d4b-8b59-e9f968de9501.No standalonesession daemon to stop on host travis-job-b0823aec-c4ec-4d4b-8b59-e9f968de9501.rm: cannot remove '/home/travis/build/apache/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/plugins': No such file or directoryhttps://api.travis-ci.org/v3/job/567273939/log.txt</description>
      <version>1.8.2,1.9.0,1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.confluent.schema.registry.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">tools.travis.splits.split.misc.hadoopfree.sh</file>
      <file type="M">tools.travis.splits.split.misc.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-8-5 01:00:00" id="13579" opendate="2019-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failed launching standalone cluster due to improper configured irrelevant config options for active mode.</summary>
      <description/>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnSessionClusterEntrypoint.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnResourceManagerFactory.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnConfigurationITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ResourceManagerUtil.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-9-6 01:00:00" id="13591" opendate="2019-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>&amp;#39;Completed Job List&amp;#39; in Flink web doesn&amp;#39;t display right when job name is very long</summary>
      <description/>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-list.job-list.component.html</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-8-7 01:00:00" id="13643" opendate="2019-8-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document the workaround for users with a different minor Hive version</summary>
      <description>We officially support two Hive versions. However, we can tell user how to work around the limitation if their Hive version is only minorly differently.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-8 01:00:00" id="13645" opendate="2019-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error in code-gen when using blink planner in scala shell</summary>
      <description/>
      <version>1.9.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGeneratorContext.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-9 01:00:00" id="13663" opendate="2019-8-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client end-to-end test for modern Kafka failed on Travis</summary>
      <description>The SQL Client end-to-end test for modern Kafka failed on Travis because it could not download https://archive.apache.org/dist/kafka/0.11.0.2/kafka_2.11-0.11.0.2.tgz.Maybe we could add a similar retry logic as with the Kinesis end-to-end test FLINK-13599.https://api.travis-ci.org/v3/job/569262834/log.txthttps://api.travis-ci.org/v3/job/569262828/log.txt</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-9 01:00:00" id="13667" opendate="2019-8-9 00:00:00" resolution="Won&amp;#39;t Do">
    <buginformation>
      <summary>Add the utility class for the Table</summary>
      <description>Add the utility class for the Table the operations on column name the operations on column type</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-ml-parent.flink-ml-lib.src.test.java.org.apache.flink.ml.common.utils.TableUtilTest.java</file>
      <file type="M">flink-ml-parent.flink-ml-lib.src.main.java.org.apache.flink.ml.common.utils.TableUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-7 01:00:00" id="1367" opendate="2015-1-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add field aggregations to Streaming Scala api</summary>
      <description>Field aggregations are missing from the streaming scala api for case classes.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.WindowedDataStream.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.package.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.DataStream.scala</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-examples.src.main.scala.org.apache.flink.streaming.scala.examples.windowing.TopSpeedWindowing.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-8-14 01:00:00" id="13711" opendate="2019-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive array values not properly displayed in SQL CLI</summary>
      <description>Array values are displayed like: [Ljava.lang.Integer;@632~ [Ljava.lang.Integer;@6de~</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-14 01:00:00" id="13712" opendate="2019-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add 1.9.0 release notes to documentation</summary>
      <description>Similar to https://ci.apache.org/projects/flink/flink-docs-release-1.9/release-notes/flink-1.8.html, we need a release note page for 1.9.0.This will be linked by the announcement blog post on the Apache Flink website.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.index.zh.md</file>
      <file type="M">docs.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-14 01:00:00" id="13715" opendate="2019-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Program-related english documentation.</summary>
      <description/>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.packaging.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-14 01:00:00" id="13716" opendate="2019-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Program-related chinese documentation</summary>
      <description/>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.packaging.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-14 01:00:00" id="13718" opendate="2019-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable HBase tests</summary>
      <description>The HBase tests are categorically failing on Java 11. Given that HBase itself does not support Java 11 at this point we should just disable these tests for the time being.HBaseConnectorITCase.activateHBaseCluster:81-&gt;HBaseTestingClusterAutostarter.registerHBaseMiniClusterInClasspath:189-&gt;HBaseTestingClusterAutostarter.addDirectoryToClassPath:224 We should get a URLClassLoader HBaseLookupFunctionITCase.activateHBaseCluster:95-&gt;HBaseTestingClusterAutostarter.registerHBaseMiniClusterInClasspath:189-&gt;HBaseTestingClusterAutostarter.addDirectoryToClassPath:224 We should get a URLClassLoader HBaseSinkITCase.activateHBaseCluster:91-&gt;HBaseTestingClusterAutostarter.registerHBaseMiniClusterInClasspath:189-&gt;HBaseTestingClusterAutostarter.addDirectoryToClassPath:224 We should get a URLClassLoader</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-hbase.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-15 01:00:00" id="13737" opendate="2019-8-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-dist should add provided dependency on flink-examples-table</summary>
      <description>In FLINK-13558 we changed the `flink-dist/bin.xml` to also include flink-examples-table in the binary distribution. The flink-dist module though does not depend on the flink-examples-table.If only the flink-dist module is built with its dependencies (this happens in the release scripts). The table examples are not built and thus not included in the distribution</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-16 01:00:00" id="13742" opendate="2019-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix code generation when aggregation contains both distinct aggregate with and without filter</summary>
      <description>The following test will fail when the aggregation contains COUNT(DISTINCT c) and COUNT(DISTINCT c) filter .... @Test def testDistinctWithMultiFilter(): Unit = { val sqlQuery = "SELECT b, " + " SUM(DISTINCT (a * 3)), " + " COUNT(DISTINCT SUBSTRING(c FROM 1 FOR 2))," + " COUNT(DISTINCT c)," + " COUNT(DISTINCT c) filter (where MOD(a, 3) = 0)," + " COUNT(DISTINCT c) filter (where MOD(a, 3) = 1) " + "FROM MyTable " + "GROUP BY b" val t = failingDataSource(StreamTestData.get3TupleData).toTable(tEnv).as('a, 'b, 'c) tEnv.registerTable("MyTable", t) val result = tEnv.sqlQuery(sqlQuery).toRetractStream[Row] val sink = new TestingRetractSink result.addSink(sink) env.execute() val expected = List( "1,3,1,1,0,1", "2,15,1,2,1,0", "3,45,3,3,1,1", "4,102,1,4,1,2", "5,195,1,5,2,1", "6,333,1,6,2,2") assertEquals(expected.sorted, sink.getRetractResults.sorted) }</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.DistinctAggCodeGen.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-16 01:00:00" id="13745" opendate="2019-8-16 00:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Flink cache on Travis does not exist</summary>
      <description>More and more often I observe that Flink builds fail on Travis because of missing Flink caches:Cached flink dir /home/travis/flink_cache/40072/flink does not exist. Exiting build.It seems as if Travis cannot guarantee that a cache survives as long as the different profiles of a build are running. It would be good to solve this problem because now we have regularly failing builds:https://travis-ci.org/apache/flink/builds/572559629https://travis-ci.org/apache/flink/builds/572523730https://travis-ci.org/apache/flink/builds/571576734</description>
      <version>1.9.0,1.10.0,1.11.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.controller.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-16 01:00:00" id="13746" opendate="2019-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elasticsearch (v2.3.5) sink end-to-end test fails on Travis</summary>
      <description>The Elasticsearch (v2.3.5) sink end-to-end test fails on Travis because it logs contain the following line:INFO org.elasticsearch.plugins - [Terror] modules [], plugins [], sites []Due to this, the error check is triggered.https://api.travis-ci.org/v3/job/572255901/log.txt</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.8.3,1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-16 01:00:00" id="13747" opendate="2019-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove some TODOs in Hive connector</summary>
      <description/>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.classloading.ClassLoaderITCase.java</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarSubmissionITCase.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebSubmissionExtension.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarListHandler.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.RemoteExecutor.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgram.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.ClientUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TableUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTableEnvUtil.scala</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.FlinkStandaloneHiveRunner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-18 01:00:00" id="13760" opendate="2019-8-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix hardcode Scala version dependency in hive connector</summary>
      <description>FLINK-13688 introduced a flink-test-utils dependency in flink-connector-hive. However, the Scala version of the artifactId is hardcoded, this result in recent CRON jobs failed. Here is an instance: https://api.travis-ci.org/v3/job/573092374/log.txt11:46:09.078 [INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-versions) @ flink-connector-hive_2.12 ---11:46:09.134 [WARNING] Rule 0: org.apache.maven.plugins.enforcer.BannedDependencies failed with message:Found Banned Dependency: com.typesafe.akka:akka-slf4j_2.11:jar:2.5.21Found Banned Dependency: com.typesafe.akka:akka-actor_2.11:jar:2.5.21Found Banned Dependency: com.typesafe:ssl-config-core_2.11:jar:0.3.7Found Banned Dependency: org.scala-lang.modules:scala-java8-compat_2.11:jar:0.7.0Found Banned Dependency: com.typesafe.akka:akka-protobuf_2.11:jar:2.5.21Found Banned Dependency: org.apache.flink:flink-clients_2.11:jar:1.10-SNAPSHOTFound Banned Dependency: org.apache.flink:flink-streaming-java_2.11:jar:1.10-SNAPSHOTFound Banned Dependency: com.typesafe.akka:akka-stream_2.11:jar:2.5.21Found Banned Dependency: com.github.scopt:scopt_2.11:jar:3.5.0Found Banned Dependency: org.apache.flink:flink-test-utils_2.11:jar:1.10-SNAPSHOTFound Banned Dependency: org.apache.flink:flink-runtime_2.11:jar:1.10-SNAPSHOTFound Banned Dependency: org.apache.flink:flink-runtime_2.11:test-jar:tests:1.10-SNAPSHOTFound Banned Dependency: org.scala-lang.modules:scala-parser-combinators_2.11:jar:1.1.1Found Banned Dependency: com.twitter:chill_2.11:jar:0.7.6Found Banned Dependency: org.clapper:grizzled-slf4j_2.11:jar:1.3.2Found Banned Dependency: org.apache.flink:flink-optimizer_2.11:jar:1.10-SNAPSHOTUse 'mvn dependency:tree' to locate the source of the banned dependencies.</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-8-19 01:00:00" id="13770" opendate="2019-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Netty to 4.1.39.Final</summary>
      <description>I quickly went through all the changelogs for Netty 4.1.32 (which wecurrently use) to the latest Netty 4.1.39.Final. Below, you will find alist of bug fixes and performance improvements that may affect us. Nicechanges we could benefit from, also for the Java &gt; 8 efforts. The mostimportant ones fixing leaks etc are #8921, #9167, #9274, #9394, and thevarious CompositeByteBuf fixes. The rest are mostly performanceimprovements.Since we are still early in the dev cycle for Flink 1.10, it would benice to update now and verify that the new version works correctly.Netty 4.1.33.Final- Fix ClassCastException and native crash when using kqueue transport(#8665)- Provide a way to cache the internal nioBuffer of the PooledByteBufferto reduce GC (#8603)Netty 4.1.34.Final- Do not use GetPrimitiveArrayCritical(...) due multiple not-fixed bugsrelated to GCLocker (#8921)- Correctly monkey-patch id also in whe os / arch is used within libraryname (#8913)- Further reduce ensureAccessible() overhead (#8895)- Support using an Executor to offload blocking / long-running taskswhen processing TLS / SSL via the SslHandler (#8847)- Minimize memory footprint for AbstractChannelHandlerContext forhandlers that execute in the EventExecutor (#8786)- Fix three bugs in CompositeByteBuf (#8773)Netty 4.1.35.Final- Fix possible ByteBuf leak when CompositeByteBuf is resized (#8946)- Correctly produce ssl alert when certificate validation fails on theclient-side when using native SSL implementation (#8949)Netty 4.1.37.Final- Don't filter out TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 (#9274)- Try to mark child channel writable again once the parent channelbecomes writable (#9254)- Properly debounce wakeups (#9191)- Don't read from timerfd and eventfd on each EventLoop tick (#9192)- Correctly detect that KeyManagerFactory is not supported when usingOpenSSL 1.1.0+ (#9170)- Fix possible unsafe sharing of internal NIO buffer in CompositeByteBuf(#9169)- KQueueEventLoop won't unregister active channels reusing a filedescriptor (#9149)- Prefer direct io buffers if direct buffers pooled (#9167)Netty 4.1.38.Final- Prevent ByteToMessageDecoder from overreading when !isAutoRead (#9252)- Correctly take length of ByteBufInputStream into account forreadLine() / readByte() (#9310)- availableSharedCapacity will be slowly exhausted (#9394)</description>
      <version>None</version>
      <fixedVersion>shaded-8.0,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">NOTICE-binary</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-20 01:00:00" id="13805" opendate="2019-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bad Error Message when TaskManager is lost</summary>
      <description>When a TaskManager is lost, the job reports as the failure causeorg.apache.flink.util.FlinkException: The assigned slot 6d0e469d55a2630871f43ad0f89c786c_0 was removed.That is a pretty bad error message, as a user I don't know what that means. Sounds like it could simply refer to internal book keeping, maybe some rebalancing or so.You need to know a lot about Flink to understand that this means actually "TaskManager failure".</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.TestingSlotManager.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImplTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-22 01:00:00" id="13814" opendate="2019-8-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveTableSink should strip quotes from partition values</summary>
      <description/>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.dml.RichSqlInsert.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSinkTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-22 01:00:00" id="13817" opendate="2019-8-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose whether web submissions are enabled</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.DashboardConfigurationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.DashboardConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.RestHandlerConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServer.java</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-22 01:00:00" id="13818" opendate="2019-8-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check whether web submission are enabled</summary>
      <description>The WebUI should preemptively check whether web-submissions are enabled (via FLINK-13817), and adjust the web-submission page accordingly.</description>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.configuration.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.component.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-23 01:00:00" id="13825" opendate="2019-8-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The original plugins dir is not restored after e2e test run</summary>
      <description>Previously, the result of Flink distribution build didn't contain plugins dir.Instead, for some e2e tests, the directory was created (and removed) by a test's setup steps.FLINK-12868 has added a pre-created plugins dir into the Flink distribution build, but without adjusting the e2e tests. As the result, after some e2e tests run, the original directory is removed.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-23 01:00:00" id="13826" opendate="2019-8-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support INSERT OVERWRITE for Hive connector</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.CatalogSinkModifyOperation.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveTestUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSinkTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-23 01:00:00" id="13831" opendate="2019-8-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Free Slots / All Slots display error</summary>
      <description>Free Slots / All Slots display error</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.status.task-manager-status.component.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-24 01:00:00" id="13841" opendate="2019-8-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend Hive version support to all 1.2 and 2.3 versions</summary>
      <description>This is to support all 1.2 (1.2.0, 1.2.1, 1.2.2) and 2.3 (2.3.0-5) versions.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV2.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV1.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-27 01:00:00" id="13872" opendate="2019-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate Operations Playground to Chinese</summary>
      <description>The Operations Playground is a quick and convenient way to learn about Flink's operational features (job submission, failure recovery, job updates, scaling, metrics).We should translate it to Chinese as well.</description>
      <version>1.9.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.try-flink.flink-operations-playground.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-27 01:00:00" id="13873" opendate="2019-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose RocksDB column_family as variable</summary>
      <description>When we use influxdb reporter report rocksdb metric data，influxdb reporter call {#getLogicalScope()} get the measurements,and call {#getAllVariables()} get tags. and we need column family as tags for group by in influxSQL。so It need call addgroup("column_family", columnFamilyName) build metric group</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBNativeMetricOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBNativeMetricMonitor.java</file>
      <file type="M">docs..includes.generated.rocks.db.native.metric.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-27 01:00:00" id="13877" opendate="2019-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive version 2.1.0 and 2.1.1</summary>
      <description>This is to support Hive 2.1 versions (2.1.0 and 2.1.1).</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV230.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-29 01:00:00" id="13896" opendate="2019-8-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala 2.11 maven compile should target Java 1.8</summary>
      <description>When setting TableEnvironment in scala as follwing: // we can repoduce this problem by put following code in // org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImplTest@Testdef testCreateEnvironment(): Unit = { val settings = EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build(); val tEnv = TableEnvironment.create(settings);}Then mvn test would fail with an error message like: error: Static methods in interface require -target:JVM-1.8 We can fix this bug by adding:&lt;configuration&gt; &lt;args&gt; &lt;arg&gt;-target:jvm-1.8&lt;/arg&gt; &lt;/args&gt;&lt;/configuration&gt; to scala-maven-plugin config   </description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-30 01:00:00" id="13903" opendate="2019-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive version 2.3.6</summary>
      <description>Hive 2.3.6 is released a few days ago. We can trivially support this version as well, as we have already provided support for previous 2.3.x releases.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-30 01:00:00" id="13930" opendate="2019-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive version 3.1.x</summary>
      <description>Including 3.1.0, 3.1.1, and 3.1.2.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveSimpleUDFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDTFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.FlinkStandaloneHiveServerContext.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.FlinkStandaloneHiveRunner.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveSimpleUDF.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveGenericUDTF.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveGenericUDF.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveGenericUDAF.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.conversion.HiveInspectors.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV120.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableOutputFormat.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-30 01:00:00" id="13931" opendate="2019-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive version 2.0.x</summary>
      <description>Including 2.0.0 and 2.0.1.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV210.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-2 01:00:00" id="13941" opendate="2019-9-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent data-loss by not cleaning up small part files from S3.</summary>
      <description/>
      <version>1.8.0,1.8.1,1.9.0</version>
      <fixedVersion>1.8.2,1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-2 01:00:00" id="13942" opendate="2019-9-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Overview page for Getting Started section</summary>
      <description>The Getting Started section provide different types of tutorials that target users with different interests and backgrounds.We should add a brief overview page that describes the different tutorials such that users easily find the material that they need to get started with Flink.</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.getting-started.index.zh.md</file>
      <file type="M">docs.getting-started.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-3 01:00:00" id="13943" opendate="2019-9-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide api to convert flink table to java List (blink planner)</summary>
      <description>It would be nice to convert flink table to java List so that I can do other data manipulation in client side after execution flink job. For flink planner, I can convert flink table to DataSet and use DataSet#collect, but for blink planner, there's no such api.EDIT from FLINK-14807:Currently, it is very unconvinient for user to fetch data of flink job unless specify sink expclitly and then fetch data from this sink via its api (e.g. write to hdfs sink, then read data from hdfs). However, most of time user just want to get the data and do whatever processing he want. So it is very necessary for flink to provide api Table#collect for this purpose. Other apis such as Table#head, Table#print is also helpful.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TableUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.StreamingTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.BatchTestBase.scala</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-4 01:00:00" id="13953" opendate="2019-9-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Facilitate enabling new Scheduler in MiniCluster Tests</summary>
      <description>Currently, tests using the MiniCluster use the legacy scheduler by default. Once the new scheduler is implemented, we should run tests with the new scheduler enabled. However, it is not expected that all tests will pass immediately. Therefore, it should be possible to enable the new scheduler for a subset of tests. In the first step the tests should be able to run manually against new scheduler.Acceptance Criteria A junit test category AlsoRunWithSchedulerNG can be used to mark MiniCluster tests. A new maven profile scheduler-ng will be enabled to support running AlsoRunWithSchedulerNG annotated tests with the new scheduler.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-13 01:00:00" id="1396" opendate="2015-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add hadoop input formats directly to the user API.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.9,0.8.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.test.java.org.apache.flink.test.hadoopcompatibility.mapred.HadoopIOFormatsITCase.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.test.java.org.apache.flink.test.hadoopcompatibility.mapred.HadoopInputFormatTest.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.test.java.org.apache.flink.test.hadoopcompatibility.mapreduce.HadoopInputFormatTest.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.wrapper.HadoopInputSplit.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.wrapper.HadoopDummyReporter.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.wrapper.HadoopDummyProgressable.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.utils.HadoopUtils.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.HadoopRecordOutputFormat.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.HadoopRecordInputFormat.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.HadoopReduceFunction.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.HadoopReduceCombineFunction.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.HadoopOutputFormat.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.HadoopMapFunction.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.HadoopInputFormat.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.example.HadoopMapredCompatWordCount.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapreduce.wrapper.HadoopInputSplit.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapreduce.utils.HadoopUtils.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapreduce.HadoopOutputFormat.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapreduce.HadoopInputFormat.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapreduce.example.WordCount.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.DataSet.java</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">docs..includes.sidenav.html</file>
      <file type="M">docs.hadoop.compatibility.md</file>
      <file type="M">docs.css.main.main.css</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-5 01:00:00" id="13973" opendate="2019-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkpoint recovery failed after user set uidHash</summary>
      <description>Checkpoint recovery failed after user set uidHash, the possible reasons are as follows:If altOperatorID is not null, operatorState will be obtained by altOperatorID and will not be given</description>
      <version>1.8.0,1.8.1,1.9.0,1.13.0</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-9-9 01:00:00" id="14005" opendate="2019-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Hive version 2.2.0</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV230.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-9 01:00:00" id="14006" opendate="2019-9-9 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add doc for how to using Java UDFs in Python API</summary>
      <description>Currently, user can not find out the doc for  how to using Java UDFs in Python API. So we should add the detail doc. In https://ci.apache.org/projects/flink/flink-docs-master/dev/table/udfs.html only describe how to using the APIs, but do not mention how to add the JARs to the class path.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-9 01:00:00" id="14007" opendate="2019-9-9 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add doc for how to using Java user-defined source/sink in Python API</summary>
      <description>Currently, user can not find out the doc for how to using Java user-defined source/sink in Python API. So we should add the detail doc. </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sourceSinks.zh.md</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-12-9 01:00:00" id="14033" opendate="2019-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Distributed caches are not registered in Yarn Per Job Cluster Mode</summary>
      <description>CacheFiles in StreamExecutionEnvironment is not used in Job Submission in the Yarn per job cluster mode. Compare to the job submission in session cluster mode that will upload distributed cache files onto http server in application master, we should get the cache files in job graph and register them into blob store in YarnJobClusterEntrypoint.</description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobGraph.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-1-17 01:00:00" id="14100" opendate="2019-9-17 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Introduce OracleDialect</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.xa.JdbcExactlyOnceSinkE2eTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.table.JdbcDynamicTableSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcTestFixture.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcDataTypeTest.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.resources.META-INF.services.org.apache.flink.connector.jdbc.dialect.JdbcDialectFactory</file>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
      <file type="M">docs.content.docs.connectors.table.jdbc.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.jdbc.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-17 01:00:00" id="14101" opendate="2019-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce SqlServerDialect</summary>
      <description/>
      <version>None</version>
      <fixedVersion>jdbc-3.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.resources.META-INF.services.org.apache.flink.connector.jdbc.dialect.JdbcDialectFactory</file>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
      <file type="M">docs.content.docs.connectors.table.jdbc.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.jdbc.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-3-18 01:00:00" id="14121" opendate="2019-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade commons-compress to 1.20</summary>
      <description>See https://commons.apache.org/proper/commons-compress/security-reports.html</description>
      <version>1.9.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-20 01:00:00" id="14150" opendate="2019-9-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unnecessary __pycache__ directories appears in pyflink.zip</summary>
      <description>It seems we are packaging _pycache_ directories into pyflink.zip. These directories contain bytecode cache files that are automatically generated by python3. We should remove them from the python source code folder before packaging.</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-10-24 01:00:00" id="14176" opendate="2019-9-24 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>add taskmanager link in vertex‘s page of taskmanager</summary>
      <description>Add taskmanager's link in vertex's page of taskmanager, so user could go to taskmanegr's page.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.taskmanagers.job-overview-drawer-taskmanagers.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-vertex-task-manager.ts</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-24 01:00:00" id="14179" opendate="2019-9-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the description of &amp;#39;SHOW FUNCTIONS&amp;#39; in SQL Client</summary>
      <description>Currently 'SHOW FUNCTIONS' lists not only user-defined functions, but also system-defined ones, the description 'Shows all registered user-defined functions.' not correctly depicts this functionality. I think we can change the description to 'Shows all system-defined and user-defined functions.'  </description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-24 01:00:00" id="14198" opendate="2019-9-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add type options to all flink python API doc</summary>
      <description>Currently ":type:" and ":rtype:" options in python docstrings have been fully supported in sphinx(https://sphinx-rtd-tutorial.readthedocs.io/en/latest/docstrings.html) and PyCharm(https://www.jetbrains.com/help/pycharm/using-docstrings-to-specify-types.html). Sphinx will generate python API documents with type annotations if these options exist in function docstrings. PyCharm also collects the type information in docstrings to detect potential coding mistakes and provide autocomplete support during python program development. There are already few interfaces in python API that have these options. We should add these options to the rest. </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.table.table.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-10-25 01:00:00" id="14202" opendate="2019-9-25 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Optimize the execution plan for Python Calc when there is a condition</summary>
      <description>As discussed in https://github.com/apache/flink/pull/9748:"For the filter, we calculate these condition UDFs together with other UDFs and do the filter later. I think we can optimize it a bit, i.e., calculate the conditions first and then check whether to call the other UDFs. This can be easily achieved in the SplitRule."</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.PythonScalarFunctionSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.ExpressionReductionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.PythonScalarFunctionSplitRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PythonScalarFunctionSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonScalarFunctionSplitRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.PythonScalarFunctionSplitRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonCalc.scala</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-25 01:00:00" id="14208" opendate="2019-9-25 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Optimize Python UDFs with parameters of constant values</summary>
      <description>We need support Python UDFs with parameters of constant values. It should be noticed that the constant parameters are not needed to be transferred between the Java operator and the Python worker.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCalc.scala</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.AbstractPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.api.common.python.PythonBridgeUtils.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-25 01:00:00" id="14212" opendate="2019-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Python UDFs without arguments</summary>
      <description>We should support Python UDFs without arguments </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.ExpressionReductionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.PythonUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.ExpressionReductionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.ExpressionReductionRulesTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.ScalarSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-26 01:00:00" id="14218" opendate="2019-9-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>support precise function reference</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.util.RexProgramExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.catalog.FunctionCatalogOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RexNodeExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.expressions.CallExpression.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.Catalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.UnresolvedCallExpression.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.rules.QualifyBuiltInFunctionsRule.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.LookupCallResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.expressions.resolver.ExpressionResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.GenericInMemoryCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionLookup.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalogUtil.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.sort.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.join.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.correlate.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.column.operation.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.calc.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.aggregate.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-25 01:00:00" id="14219" opendate="2019-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>support ambiguous function reference</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-27 01:00:00" id="14241" opendate="2019-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ARM installation step for flink e2e container test</summary>
      <description>Flink e2e container test runs under docker, docker-compose and kubernets. But the way Flink using to install the package doesn't work on ARM. docker-compose doesn't have ARM release bin file minikube doesn't support ARM.So we should add the new step for ARM arch.  install docker-compose by `pip install` use kubeadm instead of minikube</description>
      <version>1.9.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.kubernetes.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-container.docker.build.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-29 01:00:00" id="14270" opendate="2019-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>new web ui should display more than 4 metrics</summary>
      <description>The old web UI can display at least 9 metrics at once, and this can be valuable.The new interface is limited to 4 metrics, which is not enough.</description>
      <version>1.9.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-chart.job-chart.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-chart.job-chart.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.customize.job-chart.job-chart.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.common.resize.resize.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.drawer.job-overview-drawer.component.ts</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-29 01:00:00" id="14273" opendate="2019-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve exception message when signature validation of UDAF is failed</summary>
      <description>When UDAF parameters are inconsistent with the definition of accumulate method, all arguments to the accumulate method are listed in the error. But the first argument of accumulate is accumulator, users don't have to care when using SQL.For example:INSERT INTO Orders SELECT name, USERUDAF(id, name) FROM Orders GROUP BY TUMBLE(rowTime, interval '10' second ), id, nameUSERUDAF is a User-Defined Aggregate Functions, and accumulate is defined as follow:public void accumulate(Long acc, String a) {……}At present, error is as follows:Caused by: org.apache.flink.table.api.ValidationException: Given parameters of function do not match any signature. Actual: (java.lang.Integer, java.lang.String) Expected: (java.lang.Integer, java.lang.String)This error will mislead users, and the expected errors are as follows :Caused by: org.apache.flink.table.api.ValidationException: Given parameters of function do not match any signature. Actual: (java.lang.Integer, java.lang.String) Expected: (java.lang.String)</description>
      <version>1.9.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.UserDefinedFunctionValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.utils.AggSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.AggSqlFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-30 01:00:00" id="14301" opendate="2019-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add documentation for functions categories and new function resolution orders</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
      <file type="M">docs.dev.table.functions.builtinFunctions.zh.md</file>
      <file type="M">docs.dev.table.functions.builtinFunctions.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-22 01:00:00" id="1433" opendate="2015-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add HADOOP_CLASSPATH to start scripts</summary>
      <description>With the Hadoop file system wrapper, its important to have access to the hadoop filesystem classes.The HADOOP_CLASSPATH seems to be a standard environment variable used by Hadoop for such libraries.Deployments like Google Compute Cloud set this variable containing the "Google Cloud Storage Hadoop Wrapper". So if users want to use the Cloud Storage in an non-yarn environment, we need to address this issue.</description>
      <version>None</version>
      <fixedVersion>0.9,0.8.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.webclient.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.taskmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.jobmanager.sh</file>
      <file type="M">docs.example.connectors.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-11-9 01:00:00" id="14356" opendate="2019-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce "raw" format to (de)serialize message to a single field</summary>
      <description>I want to use flink sql to write kafka messages directly to hdfs. The serialization and deserialization of messages are not involved in the middle.  The bytes of the message directly convert the first field of Row.  However, the current RowSerializationSchema does not support the conversion of bytes to VARBINARY. Can we add some special RowSerializationSchema and RowDerializationSchema ? ========================================================================Copied from FLINK-9963:Sometimes it might be useful to just read or write a single value into Kafka or other connectors. We should add a single-value SerializationSchemaFactory and single-value DeserializationSchemaFactory, the types below and their array types shall be considered.byte, short, int, long, float, double, stringFor the numeric types, we might want to specify the endian format.A string type single-value format will be added with this issue for future reference.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.formats.SingleValueRowDataSerDeSchemaTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.formats.SingleValueFormatFactoryTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.formats.MockRowDataTypeInfo.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.formats.SingleValueRowDataSerialization.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.formats.SingleValueRowDataDeserialization.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.formats.SingleValueFormatFactory.java</file>
      <file type="M">docs.dev.table.connectors.formats.singleValue.zh.md</file>
      <file type="M">docs.dev.table.connectors.formats.singleValue.md</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">docs.dev.table.connectors.formats.index.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-10-16 01:00:00" id="14408" opendate="2019-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>In OldPlanner, UDF open method can not be invoke when SQL is optimized</summary>
      <description>For now, UDF open method can not be invoked when SQL is optimized. For example, a SQL as follow:SELECT MyUdf(1) as constantValue FROM MyTableMyUdf.open can not be invoked in OldPlanner.So, we can construct a constantFunctionContext or a constantRuntimeContext and invoke it just like BlinkPlanner.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.ExpressionReductionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExpressionReducer.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-16 01:00:00" id="14413" opendate="2019-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shade-plugin ApacheNoticeResourceTransformer uses platform-dependent encoding</summary>
      <description>Some NOTICE files contain quotes that, at least on my system, result in some encoding errors when generating the binary licensing. One example can be found here; the closing quotes would be replaced with a question mark.This is due to the ApacheNoticeResourceTransformer using the platform encoding.</description>
      <version>shaded-8.0,1.8.2,1.9.0,1.10.0</version>
      <fixedVersion>shaded-9.0,1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-16 01:00:00" id="14416" opendate="2019-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Module interface and ModuleManager</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.utils.MockTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.RexProgramExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.rules.logical.PushFilterIntoTableSourceScanRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.scala.internal.BatchTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.java.internal.BatchTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.BatchTableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.RexNodeExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.SelectivityEstimatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.AggCallSelectivityEstimatorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.sqlexec.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.test.scala.org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImplTest.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImpl.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.BatchTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.TableEnvironmentMock.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableEnvironment.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.test.java.org.apache.flink.table.api.java.internal.StreamTableEnvironmentImplTest.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.java.BatchTableEnvironment.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-16 01:00:00" id="14417" opendate="2019-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Develop CoreModule to provide Flink built-in functions</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.FunctionCatalogTest.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.module.ModuleManager.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-11-16 01:00:00" id="14419" opendate="2019-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ModuleFactory, ModuleDescriptor, ModuleValidator for factory discovery service</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.module.ModuleConfig.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.module.ModuleManager.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-10-18 01:00:00" id="14445" opendate="2019-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python module build failed when making sdist</summary>
      <description>From the description of error-log from building python module in travis, it seems invocation failed for sdist-make and then the phase of building python module exited.The instance log: https://api.travis-ci.com/v3/job/246710918/log.txt</description>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.setup.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-19 01:00:00" id="14459" opendate="2019-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python module build hangs</summary>
      <description>The build of python module hangs when installing conda. See travis log: https://api.travis-ci.org/v3/job/599704570/log.txtCan't reproduce it neither on my local mac nor on my repo with travis.</description>
      <version>1.9.0,1.9.1</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-20 01:00:00" id="14468" opendate="2019-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Kubernetes docs</summary>
      <description>Two minor improvements to documented Kubernetes resource definitions: avoid referencing deprecated extensions/v1beta1/Deployment run unprivileged</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.kubernetes.md</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2019-11-12 01:00:00" id="14724" opendate="2019-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Join condition could be simplified in logical phase</summary>
      <description>currently the plan of tpcds q38.sql contains NestedLoopJoin, because it's join condition is CAST(AND(IS NOT DISTINCT FROM($2, $3), IS NOT DISTINCT FROM($1, $4), IS NOT DISTINCT FROM($0, $5))):BOOLEAN, and planner can't find equal join keys from the condition by Join#analyzeCondition.SimplifyJoinConditionRule could solve this.</description>
      <version>1.9.0,1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-13 01:00:00" id="14727" opendate="2019-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update doc of supported Hive versions</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-13 01:00:00" id="14728" opendate="2019-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add reminder for users of potential thread safety issues of hive built-in function</summary>
      <description>remind users of https://issues.apache.org/jira/browse/HIVE-16183</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.hive.functions.zh.md</file>
      <file type="M">docs.dev.table.hive.hive.functions.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-9-21 01:00:00" id="14896" opendate="2019-11-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kinesis connector doesn&amp;#39;t shade jackson dependency</summary>
      <description>flink-kinesis-connector depends on aws java sdk which is shaded to org.apache.flink.kinesis.shaded.com.amazonaws. However, the aws sdk has a transitive dependency to jackson wich is not shaded in the artifact. This creates problem when running flink on YARN: The aws sdk requires jackson-core v2.6 but hadoop pulls in 2.3. See here. If YARN uses the loads wrong jackson version from classpath. Jod fails with2019-11-20 17:23:11,563 ERROR org.apache.flink.runtime.webmonitor.handlers.JarRunHandler - Unhandled exception.org.apache.flink.client.program.ProgramInvocationException: The program caused an error:     at org.apache.flink.client.program.OptimizerPlanEnvironment.getOptimizedPlan(OptimizerPlanEnvironment.java:93)    at org.apache.flink.client.program.PackagedProgramUtils.createJobGraph(PackagedProgramUtils.java:80)    at org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils$JarHandlerContext.toJobGraph(JarHandlerUtils.java:126)    at org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.lambda$getJobGraphAsync$6(JarRunHandler.java:142)    at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)    at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.ObjectMapper.enable([Lcom/fasterxml/jackson/core/JsonParser$Feature;)Lcom/fasterxml/jackson/databind/ObjectMapper;    at com.amazonaws.partitions.PartitionsLoader.&lt;clinit&gt;(PartitionsLoader.java:54)    at com.amazonaws.regions.RegionMetadataFactory.create(RegionMetadataFactory.java:30)    at com.amazonaws.regions.RegionUtils.initialize(RegionUtils.java:65)    at com.amazonaws.regions.RegionUtils.getRegionMetadata(RegionUtils.java:53)    at com.amazonaws.regions.RegionUtils.getRegion(RegionUtils.java:107)    at com.amazonaws.client.builder.AwsClientBuilder.getRegionObject(AwsClientBuilder.java:256)    at com.amazonaws.client.builder.AwsClientBuilder.setRegion(AwsClientBuilder.java:460)    at com.amazonaws.client.builder.AwsClientBuilder.configureMutableProperties(AwsClientBuilder.java:424)    at com.amazonaws.client.builder.AwsAsyncClientBuilder.build(AwsAsyncClientBuilder.java:80)...The flink-kinesis-connector should do as other connectors: shade jackson or use the flink-shaded-jackson core dependency</description>
      <version>1.9.0,1.16.0,1.15.2</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-21 01:00:00" id="14899" opendate="2019-11-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>can not be translated to StreamExecDeduplicate when PROCTIME() is defined in query</summary>
      <description>CREATE TABLE user_log ( user_id VARCHAR, item_id VARCHAR, category_id VARCHAR, behavior VARCHAR, ts TIMESTAMP) WITH ( 'connector.type' = 'kafka', 'connector.version' = 'universal', 'connector.topic' = 'user_behavior', 'connector.startup-mode' = 'earliest-offset', 'connector.properties.0.key' = 'zookeeper.connect', 'connector.properties.0.value' = 'localhost:2181', 'connector.properties.1.key' = 'bootstrap.servers', 'connector.properties.1.value' = 'localhost:9092', 'update-mode' = 'append', 'format.type' = 'json', 'format.derive-schema' = 'true');CREATE TABLE user_dist ( dt VARCHAR, user_id VARCHAR, behavior VARCHAR) WITH ( 'connector.type' = 'jdbc', 'connector.url' = 'jdbc:mysql://localhost:3306/flink-test', 'connector.table' = 'user_behavior_dup', 'connector.username' = 'root', 'connector.password' = ‘******', 'connector.write.flush.max-rows' = '1');INSERT INTO user_distSELECT dt, user_id, behaviorFROM ( SELECT dt, user_id, behavior, ROW_NUMBER() OVER (PARTITION BY dt, user_id, behavior ORDER BY proc asc ) AS rownum FROM (select DATE_FORMAT(ts, 'yyyy-MM-dd HH:00') as dt,user_id,behavior,PROCTIME() as proc from user_log) )WHERE rownum = 1;Exception in thread "main" org.apache.flink.table.api.TableException: UpsertStreamTableSink requires that Table has a full primary keys if it is updated.at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:114)at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:50)at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:54)at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlan(StreamExecSink.scala:50)at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:61)at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:60)at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)at scala.collection.Iterator$class.foreach(Iterator.scala:891)at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)at scala.collection.AbstractIterable.foreach(Iterable.scala:54)at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)at scala.collection.AbstractTraversable.map(Traversable.scala:104)at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:60)at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:149)at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:439)at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:348)</description>
      <version>1.9.0,1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.DeduplicateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.OverAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-11-21 01:00:00" id="14905" opendate="2019-11-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unify packaging process</summary>
      <description>Right now we have a Java 11 profile for customizing the build in various ways, from bumping plugins, setting compiler flags, disabling tests and even some dependencies.The dependencies bit a problematic since it implies that not all jars created by the Java 8 build process can actually run on Java 11.This creates an unnecessary burden for users who want to use Java 11 (reminder: the only supported LTS version) as they'd have to compile Flink themselves for Java 11.Releasing a second set of jars/binaries is out of the question due to the confusion and overhead this would cause, so instead we should ensure that the Java 8 jars are usable on Java 11.My proposal is to use a multi-release jar to package additional dependencies that are only loaded on Java 11.I have verified that this approach works conceptually, whether it works with the dependency we have to deal with (jaxb-api) remains to be seen.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-12-21 01:00:00" id="14912" opendate="2019-11-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>register, drop, and alter catalog functions from DDL via catalog</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.internal.TableEnvImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.sqlexec.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlAlterFunction.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-26 01:00:00" id="14954" opendate="2019-11-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Publish OpenAPI specification of REST Monitoring API</summary>
      <description>Hello,Flink provides a very helpful REST Monitoring API.OpenAPI is convenient standard to generate clients in a variety of language for REST API documented according to their specification. In this case, clients would be helpful to automate management of Flink clusters.Currently, there is no "official" OpenAPI specification of Flink REST Monitoring API. Some have written by users, but their consistency across Flink releases is uncertain.I think it would be beneficial to have an OpenAPI specification provided and maintained by the Flink project. Kind regards, </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.json.SerializedThrowableSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.async.AsynchronousOperationResult.java</file>
      <file type="M">flink-docs.README.md</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">docs.content.docs.ops.rest.api.md</file>
      <file type="M">docs.content.zh.docs.ops.rest.api.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-12-2 01:00:00" id="15001" opendate="2019-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The digest of sub-plan reuse should contain retraction traits for stream physical nodes</summary>
      <description>This bug is found in FLINK-14946:The plan for the given sql in FLINK-14946 is however, the plan after sub-plan reuse is: in the first picture, we could find that the accMode of two joins are different, but the two joins are reused in the second picture. The reason is the digest of sub-plan reuse does not contain retraction traits for stream physical nodes now.</description>
      <version>1.9.0,1.9.1</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.RelDigestUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.digest.testGetDigestWithDynamicFunctionView.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.digest.testGetDigestWithDynamicFunction.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelTreeWriterImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelDigestUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.reuse.SubplanReuser.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-2 01:00:00" id="15010" opendate="2019-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Temp directories flink-netty-shuffle-* are not cleaned up</summary>
      <description>Starting a Flink cluster with 2 TMs and stopping it again will leave 2 temporary directories (and not delete them): flink-netty-shuffle-&lt;uid&gt;</description>
      <version>1.9.0,1.9.1,1.9.2</version>
      <fixedVersion>1.9.3,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.FileChannelManagerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-5 01:00:00" id="15063" opendate="2019-12-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Input group and output group of the task metric are reversed</summary>
      <description>In code, the input group and output group of the task metric are reversed.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NettyShuffleEnvironment.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-2-3 01:00:00" id="15473" opendate="2020-1-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for Linux on ppc64le to MemoryArchitecture</summary>
      <description>Similar to issue FLINK-13449Please add "ppc64le" to MemoryArchitecture.javafinal List&lt;String&gt; names64bit = Arrays.asList("amd64", "x86_64", "aarch64", "ppc64le"); </description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.ProcessorArchitecture.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-8 01:00:00" id="15518" opendate="2020-1-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t hide web frontend side pane automatically</summary>
      <description>As mentioned in FLINK-13386 the side pane hides automatically in some cases but not all cases. When I was debugging or trying the web frontend I found this behaviour a bit disconcerting. Could we disable the hiding by default? The user can still manually hide if they want to.</description>
      <version>1.9.0,1.10.0</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app-routing.module.ts</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-9 01:00:00" id="15543" opendate="2020-1-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Apache Camel not bundled but listed in flink-dist NOTICE</summary>
      <description>Apache Camel dependencies are listed in the flink-dist NOTICE, but we removed the dependency in 1.9.0 (see FLINK-12040).</description>
      <version>1.9.0</version>
      <fixedVersion>1.8.4,1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-9-29 01:00:00" id="15804" opendate="2020-1-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support the new type inference in Scala Table API scalar functions</summary>
      <description>Currently, we cannot distinguish between old and new type inference for Scala Table API because those functions are not registered in a catalog but are used "inline". We should support them as well.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.UserDefinedTableFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.table.JoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.table.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.utils.userDefinedScalarFunctions.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.expressions.UserDefinedScalarFunctionTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.validation.UserDefinedFunctionValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TableAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CorrelateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PythonMapMergeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCorrelateJsonPlanTest.jsonplan.testPythonTableFunction.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCorrelateJsonPlanTest.jsonplan.testJoinWithFilter.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.jsonplan.testPythonFunctionInWhereClause.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.PythonCalcJsonPlanTest.jsonplan.testPythonCalc.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.JoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.table.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.java</file>
      <file type="M">flink-table.flink-table-api-scala.src.main.scala.org.apache.flink.table.api.ImplicitExpressionConversions.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-4 01:00:00" id="15875" opendate="2020-2-4 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Bump Beam to 2.19.0</summary>
      <description>Currently PyFlink depends on Beam's portability framework for Python UDF execution. The current dependent version is 2.15.0. We should bump it to 2.19.0(the latest version) as it includes several critical features/fixes, e.g.1) BEAM-7951: It allows to not serialize the window/timestamp/pane info between the Java operator and the Python worker which could definitely improve the performance a lot2) BEAM-8935: It allows to fail fast if the Python worker start up failed. Currently it takes 2 minutes to detect the failure if the Python worker is started failed. 3) BEAM-7948: It supports periodically flush the data between the Java operator and the Python worker. This feature is especially useful for streaming jobs and could improve the latency.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.functions.python.PythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.AbstractPythonStatelessFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonDriverOptionsParserFactory.java</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.README.md</file>
      <file type="M">flink-python.pyflink.table.table.config.py</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">docs.ops.python.shell.zh.md</file>
      <file type="M">docs.ops.python.shell.md</file>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">flink-python.src.main.java.org.apache.beam.runners.fnexecution.environment.ProcessManager.java</file>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-12 01:00:00" id="16011" opendate="2020-2-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Normalize the within usage in Pattern</summary>
      <description>In CEP, we can use Pattern.within() to set a window in which the pattern should be matched. However, the usage of within is ambiguous and confusing to user.For example: Pattern.begin("a").within(t1).followedBy("b").within(t2) will use the minimal of t1 and t2 as the window time for the whole pattern. Pattern.begin("a").followedBy("b").within(t2) will use t2 as the window time. But Pattern.begin("a").within(t1).followedBy("b") will have no window time While Pattern.begin("a").notFollowedBy("not").within(t1).followedBy("b").within(t2) will use t2 as the window time.So I propose to normalize the usage of within() and make strict checking when compiling the pattern. For example, we can only allow within() at the end of the pattern and point it out if user set it somewhere else when compiling the pattern.</description>
      <version>1.9.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.compiler.NFACompilerTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.compiler.NFACompiler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-20 01:00:00" id="16186" opendate="2020-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Speed up ElasticsearchITCase#testInvalidElasticsearchCluster</summary>
      <description>ElasticsearchITCase#testInvalidElasticsearchCluster runs for a significant time (30-40 seconds), as it tries to connect to a non-existent clusters.We should find a way to reduce the timeout, or implement the test in some other fashion.</description>
      <version>1.9.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-2 01:00:00" id="16389" opendate="2020-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Kafka 0.10 to 0.10.2.2</summary>
      <description>Click to add description</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.3,1.10.1,1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-kafka-0.10.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-10 01:00:00" id="16524" opendate="2020-3-10 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Optimize the execution of Python UDF to use generator to eliminate unnecessary function calls</summary>
      <description>Optimize the result of FlattenRowCoder and ArrowCoder to generator to eliminate unnecessary function calls.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.tests.coders.test.common.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-9 01:00:00" id="17062" opendate="2020-4-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the conversion from Java row type to Python row type</summary>
      <description>It iterate over the result of FieldsDataType.getFieldDataTypes when converting Java row type to Python row type. The result is non-deterministic as the result of FieldsDataType.getFieldDataTypes is of type map.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.3,1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.types.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-9 01:00:00" id="17065" opendate="2020-4-9 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add documentation about the Python versions supported for PyFlink</summary>
      <description>We need to add notes of Python versions supported by PyFlink in doc so that users can choose the correct Python versions</description>
      <version>None</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
      <file type="M">docs.flinkDev.building.zh.md</file>
      <file type="M">docs.flinkDev.building.md</file>
      <file type="M">docs.dev.table.python.python.udfs.zh.md</file>
      <file type="M">docs.dev.table.python.python.udfs.md</file>
      <file type="M">docs.dev.table.python.installation.zh.md</file>
      <file type="M">docs.dev.table.python.installation.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-4-13 01:00:00" id="17107" opendate="2020-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CheckpointCoordinatorConfiguration#isExactlyOnce() is inconsistent with StreamConfig#getCheckpointMode()</summary>
      <description>CheckpointCoordinatorConfiguration#isExactlyOnce() is inconsistent with StreamConfig#getCheckpointMode() when checkpoint is disabled. CheckpointCoordinatorConfiguration#isExactlyOnce() returns true if checkpoint mode is  EXACTLY_ONCE mode and return false if checkpoint mode is AT_LEAST_ONCE while StreamConfig#getCheckpointMode() will always return AT_LEAST_ONCE which means always not exactly once.</description>
      <version>1.6.3,1.7.2,1.8.0,1.9.0,1.10.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-14 01:00:00" id="17132" opendate="2020-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Prometheus version</summary>
      <description>My pushgateway version: 1.2.0Use the flink-metrics-prometheus module to report the metric, pushgateway successfully received the data, but Jobmanager and TaskManager will print "Failed to push metrics to PushGateway" and throw "java.io.IOException: Response code from xxx was 200".The reason is that Flink uses an incompatible low-level pushgateway client. It is recommended to upgrade to the latest version.</description>
      <version>1.9.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-20 01:00:00" id="17254" opendate="2020-4-20 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Improve the PyFlink documentation and examples to use SQL DDL for source/sink definition</summary>
      <description>Currently there are two ways to register a table sink/source in PyFlink table API:1) TableEnvironment.connect2) TableEnvironment.sql_updateI think it's better to provide documentation and examples on how to use 2) in PyFlink.</description>
      <version>None</version>
      <fixedVersion>1.9.4,1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.examples.batch.word.count.py</file>
      <file type="M">docs.getting-started.walkthroughs.python.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-20 01:00:00" id="17256" opendate="2020-4-20 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Suppport keyword arguments in the PyFlink Descriptor API</summary>
      <description>Keyword arguments is a very commonly used feature in Python. We should support it in the PyFlink Descriptor API to make the API more user friendly for Python users.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.descriptor.py</file>
      <file type="M">flink-python.pyflink.table.descriptors.py</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2020-5-8 01:00:00" id="17562" opendate="2020-5-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>POST /jars/:jarid/plan is not working</summary>
      <description>The handlers introduced in FLINK-11853 is not using the correct headers, and registers a second handler under the same URL.</description>
      <version>1.9.0</version>
      <fixedVersion>1.9.4,1.10.2,1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebSubmissionExtension.java</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-3-20 01:00:00" id="1757" opendate="2015-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>java.lang.ClassCastException is thrown while summing Short values on window</summary>
      <description>java.lang.ClassCastException is thrown while summing Short values on windowStack Trace:Caused by: java.lang.RuntimeException: java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Short at org.apache.flink.streaming.api.streamvertex.OutputHandler.invokeUserFunction(OutputHandler.java:232) at org.apache.flink.streaming.api.streamvertex.StreamVertex.invoke(StreamVertex.java:121) at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:205) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Short at org.apache.flink.api.common.typeutils.base.ShortSerializer.copy(ShortSerializer.java:27) at org.apache.flink.api.java.typeutils.runtime.TupleSerializer.copy(TupleSerializer.java:95) at org.apache.flink.api.java.typeutils.runtime.TupleSerializer.copy(TupleSerializer.java:30) at org.apache.flink.streaming.api.invokable.StreamInvokable.copy(StreamInvokable.java:166) at org.apache.flink.streaming.api.invokable.SinkInvokable.collect(SinkInvokable.java:46) at org.apache.flink.streaming.api.collector.DirectedCollectorWrapper.collect(DirectedCollectorWrapper.java:95) at org.apache.flink.streaming.api.invokable.operator.GroupedReduceInvokable.reduce(GroupedReduceInvokable.java:47) at org.apache.flink.streaming.api.invokable.operator.StreamReduceInvokable.invoke(StreamReduceInvokable.java:39) at org.apache.flink.streaming.api.streamvertex.StreamVertex.invokeUserFunction(StreamVertex.java:85) at org.apache.flink.streaming.api.streamvertex.OutputHandler.invokeUserFunction(OutputHandler.java:229)</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.aggregation.SumFunction.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-14 01:00:00" id="17701" opendate="2020-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude jdk:tools dependency from all Hadoop dependencies for Java 9+ compatibility</summary>
      <description>Hadoop transitively pulls the system dependency jdk:tools which is not longer available on Java 9+. This causes errors when importing the code into an IDE with runs Java 11.This dependency is anyways not needed when running the code, because the classes are always present. It can be safely excluded form the transitive dependencies.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-14 01:00:00" id="17702" opendate="2020-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>OperatorCoordinators must be notified of tasks cancelled as part of failover</summary>
      <description>The OperatorCoordinators are currently only notified of tasks that directly fail.However, tasks that are cancelled (as part of the regional failover) must be handled the same was and also send notifications to the OperatorCoordinator.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.SchedulerTestingUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.SchedulerBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.coordination.OperatorCoordinatorSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultScheduler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-25 01:00:00" id="17931" opendate="2020-5-25 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Document fromValues clause</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-7 01:00:00" id="18507" opendate="2020-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move get_config implementation to TableEnvironment to eliminate the duplication</summary>
      <description>Currently, TableEnvironment.get_config is abstract and the implementations in the child classes BatchTableEnvironment/StreamTableEnvironment are duplicate. The implementation could be moved to TableEnvironment to eliminate the duplication.</description>
      <version>1.9.0,1.10.0,1.11.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-9-28 01:00:00" id="19070" opendate="2020-8-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive connector should throw a meaningful exception if user reads/writes ACID tables</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-6-14 01:00:00" id="5340" opendate="2016-12-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a metric exposing jobs uptimes</summary>
      <description>I would like the job manager to expose a metric indicating how long each job has been up. This way I can grab this number and measure the health of my job.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.metrics.md</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.java</file>
    </fixedFiles>
  </bug>
</bugrepository>