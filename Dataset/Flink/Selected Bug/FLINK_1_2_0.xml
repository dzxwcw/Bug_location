<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  
  
  
  
  <bug fixdate="2016-9-5 01:00:00" id="4580" opendate="2016-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check that the RpcEndpoint supports the specified RpcGateway</summary>
      <description>When calling RpcService.connect the user specifies the type of the RpcGateway. At the moment, it is not checked whether the RpcEndpoint actually supports the specified RpcGateway.I think it would be good to add a runtime check that the corresponding RpcEndpoint supports the specified RpcGateway. If not, then we can let the connect method fail fast.</description>
      <version>1.2.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActor.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-5-9 01:00:00" id="4604" opendate="2016-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for standard deviation/variance</summary>
      <description>Calcite's AggregateReduceFunctionsRule can convert SQL AVG, STDDEV_POP, STDDEV_SAMP, VAR_POP, VAR_SAMP to sum/count functions. We should add, test and document this rule. If we also want to add this aggregates to Table API is up for discussion.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.utils.LogicalPlanFormatUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.stringexpr.AggregationsStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.QueryDecorrelationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.SumAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.Sum0AggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.utils.TableProgramsTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.sql.SqlITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateWithNullValuesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.aggregations.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-20 01:00:00" id="4643" opendate="2016-9-20 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Average Clustering Coefficient</summary>
      <description>Gelly has Global Clustering Coefficient and Local Clustering Coefficient. This adds Average Clustering Coefficient. The distinction is discussed in http://jponnela.com/web_documents/twomode.pdf (pdf page 2, document page 32).</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.asm.AsmTestBase.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.examples.ClusteringCoefficient.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-9-21 01:00:00" id="4654" opendate="2016-9-21 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>clean up docs</summary>
      <description>There are some minor but distracting glitches in the docs &amp;#8211; typos, awkward phrases, broken links.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.quickstart.run.example.quickstart.md</file>
      <file type="M">docs.dev.windows.md</file>
      <file type="M">docs.dev.state.backends.md</file>
      <file type="M">docs.dev.state.md</file>
      <file type="M">docs.dev.libs.cep.md</file>
      <file type="M">docs.dev.datastream.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-9-21 01:00:00" id="4656" opendate="2016-9-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port existing code to use Flink&amp;#39;s future abstraction</summary>
      <description>Port existing code to use Flink's future abstraction</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.TestingSerialRpcService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.TestingRpcService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.TestingGatewayBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.RpcCompletenessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.AsyncCallsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.MessageSerializationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.MainThreadValidationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotProtocolTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerHATest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.registration.TestRegistrationGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.registration.RetryingRegistrationTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorToResourceManagerConnection.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.MainThreadExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.registration.RetryingRegistration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.concurrent.impl.FlinkFuture.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-9-22 01:00:00" id="4662" opendate="2016-9-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Calcite version up to 1.9</summary>
      <description>Calcite just released the 1.9 version. We should adopt it also in the Table API especially for FLINK-4294.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.datastream.DataStreamConvention.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetConvention.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.FlinkTypeFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.FlinkPlannerImpl.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.arithmetic.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-11-23 01:00:00" id="4667" opendate="2016-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn Session CLI not listening on correct ZK namespace when HA is enabled to use ZooKeeper backend</summary>
      <description>In Yarn mode, when Flink is configured for HA using ZooKeeper backend, the leader election listener does not provide correct JM/leader info and will timeout since the listener is waiting on default ZK namespace instead of the application specific (Application ID)</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j-yarn-session.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-9-23 01:00:00" id="4668" opendate="2016-9-23 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Fix positive random int generation</summary>
      <description>According to java specMath.abs(Integer.MIN_VALUE) == Integer.MIN_VALUESo, Math.abs(rnd.nextInt()) might return negative valueTo generate positive random int value we can use rnd.nextInt(Integer.MAX_VALUE)Integer.MAX_VALUE will be excluded btw</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgram.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-9-23 01:00:00" id="4671" opendate="2016-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table API can not be built</summary>
      <description>Running mvn clean verify in flink-table results in a build failure.[ERROR] Failed to execute goal on project flink-table_2.10: Could not resolve dependencies for project org.apache.flink:flink-table_2.10:jar:1.2-SNAPSHOT: Failure to find org.apache.directory.jdbm:apacheds-jdbm1:bundle:2.0.0-M2 in https://repo.maven.apache.org/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced -&gt; [Help 1]However, the master can be built successfully.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-27 01:00:00" id="4689" opendate="2016-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement a simple slot provider for the new job manager</summary>
      <description>In flip-6 branch, we need to adjust existing scheduling model. In the first step, we should introduce a simple / naive slot provider which just ignore all the sharing or location constraint, to make whole thing work.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.SlotPool.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-27 01:00:00" id="4693" opendate="2016-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add session group-windows for batch tables</summary>
      <description>Add Session group-windows for batch tables as described in FLIP-11.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.DataSetWindowAggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.TimeWindowPropertyCollector.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetWindowAggregateMapFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetWindowAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-27 01:00:00" id="4700" opendate="2016-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden the TimeProvider test</summary>
      <description>Currently the TimeProvider test fails due to a race condition. This task aims at fixing it.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.DefaultTimeServiceProviderTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AggregatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.TimeProviderTest.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcherTimestampsTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-9-29 01:00:00" id="4708" opendate="2016-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scope Mini Kerberos Cluster dependencies as test dependencies</summary>
      <description/>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-filesystem.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-10-4 01:00:00" id="4732" opendate="2016-10-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Maven junction plugin security threat</summary>
      <description>We use the Maven Junction plugin http://pyx4j.com/pyx4j-maven-plugins/maven-junction-plugin/introduction.html to create a symbolic link to the build directory. On Windows, the plugin downloads an executable from the author's homepage which may be modified by an attacker. The plugin has not been updated since 2007 and the maintainer has not shown interest to fix the issue.I propose to remove the plugin while this security threat persists.</description>
      <version>None</version>
      <fixedVersion>1.1.3,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-10-4 01:00:00" id="4735" opendate="2016-10-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate some job execution related akka messages to rpc calls</summary>
      <description>This includes the following operations about job execution:1. checkpointing2. kvstate3. savepoint4. classloading props</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-4 01:00:00" id="4740" opendate="2016-10-4 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Upgrade testing libraries</summary>
      <description>JUnit 4.12 was released 4 Dec 2014. Flink is currently using JUnit 4.11 from 14 Nov 2012.PowerMock reports "org.powermock.reflect.exceptions.FieldNotFoundException: Field 'fTestClass' was not found in class org.junit.internal.runners.MethodValidator." https://github.com/jayway/powermock/issues/551This is fixed in PowerMock 1.6.1+ (currently using 1.5.5, latest is 1.6.5): https://raw.githubusercontent.com/jayway/powermock/master/changelog.txtThen Mockito causes "java.lang.NoSuchMethodError: org.mockito.mock.MockCreationSettings.getSerializableMode()Lorg/mockito/mock/SerializableMode;".This is fixed by upgrading Mockito from 1.9.5 to the latest 1.10.19.</description>
      <version>1.2.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.chaining.ChainedAllReduceDriverTest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-10-5 01:00:00" id="4749" opendate="2016-10-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove redundant processing time timers and futures in the window operator</summary>
      <description>The window operator maintains redundant sets of processing time timers and futures, which make cleanup of processing time timers hard and inefficient.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TestTimeServiceProvider.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.EvictingWindowOperator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-10-7 01:00:00" id="4762" opendate="2016-10-7 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Use plural in time interval units</summary>
      <description>During the creation of FLIP-11 we decided to rename the time interval units. From minute to minutes and so on in Java and Scala Table API.12.minutes + 2.hours reads better.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-10-11 01:00:00" id="4799" opendate="2016-10-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-add build-target symlink to project root</summary>
      <description>We have previously removed the plugin which created the 'build-target' link to the build target directory. See FLINK-4732. At least one user has requested to re-add the link.</description>
      <version>1.1.3,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-1-12 01:00:00" id="4820" opendate="2016-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Slf4j / log4j version upgrade to support dynamic change of log levels --&gt; Make logging framework exchangeable</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.log4j-travis.properties</file>
      <file type="M">flink-runtime.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-runtime-web.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-libraries.flink-ml.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-core.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.MavenForkNumberPrefixLayout.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-13 01:00:00" id="4821" opendate="2016-10-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement rescalable non-partitioned state for Kinesis Connector</summary>
      <description>FLINK-4379 added the rescalable non-partitioned state feature, along with the implementation for the Kafka connector.The AWS Kinesis connector will benefit from the feature and should implement it too. This ticket tracks progress for this.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.testutils.TestableKinesisDataFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcherTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-13 01:00:00" id="4825" opendate="2016-10-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement a RexExecutor that uses Flink&amp;#39;s code generation</summary>
      <description>The added ReduceExpressionRule leads to inconsistent behavior. Because some parts of an expression are evalutated using Flink's code generation and some parts use Calcite's code generation.A very easy example: boolean expressions casted to string are represented as "TRUE/FALSE" using Calcite and "true/false" using Flink.I propose to implement the RexExecutor interface and forward the calls to Flink's code generation. Additional improvements in order to be more standard compliant could be solved in new Jira issues.I will disable the rule and the corresponding tests till this issue is fixed.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.MapRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.io.ValuesInputFormat.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.FlatMapRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.FlatJoinRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.Compiler.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.FlinkRelBuilder.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-13 01:00:00" id="4826" opendate="2016-10-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add keytab based kerberos support for Mesos environment</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.SecurityContext.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerRunner.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosConfigKeys.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-18 01:00:00" id="4850" opendate="2016-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>FlinkML - SVM predict Operation for Vector and not LaveledVector</summary>
      <description>It seems that evaluate operation is defined for Vector and not LabeledVector.It impacts QuickStart guide for FlinkML when using SVM.We need to update the documentation as follows:val astroTest:DataSet&amp;#91;(Vector,Double)&amp;#93; = MLUtils .readLibSVM(env, "src/main/resources/svmguide1.t") .map(l =&gt; (l.vector, l.label))val predictionPairs = svm.evaluate(astroTest)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.libs.ml.quickstart.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-21 01:00:00" id="4876" opendate="2016-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow web interface to be bound to a specific ip/interface/inetHost</summary>
      <description>Currently the web interface automatically binds to all interfaces on 0.0.0.0. IMHO there are some use cases to only bind to a specific ipadress, (e.g. access through an authenticated proxy, not binding on the management or backup interface)</description>
      <version>1.1.2,1.1.3,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorConfig.java</file>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-11-26 01:00:00" id="4937" opendate="2016-10-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add incremental group window aggregation for streaming Table API</summary>
      <description>Group-window aggregates for streaming tables are currently not done in an incremental fashion. This means that the window collects all records and performs the aggregation when the window is closed instead of eagerly updating a partial aggregate for every added record. Since records are buffered, non-incremental aggregation requires more storage space than incremental aggregation.The DataStream API which is used under the hood of the streaming Table API features incremental aggregation using a ReduceFunction.We should add support for incremental aggregation in group-windows.This is a follow-up task of FLINK-4691.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateReduceCombineFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateMapFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateAllWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.datastream.DataStreamAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-11-27 01:00:00" id="4945" opendate="2016-10-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>KafkaConsumer logs wrong warning about confirmation for unknown checkpoint</summary>
      <description>Checkpoints are currently not registered in all cases. While the code still behaves correctly this leads to misleading warnings.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-27 01:00:00" id="4946" opendate="2016-10-27 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Load jar files from subdirectories of lib</summary>
      <description>Users can more easily track Flink jars with transitive dependencies when copied into subdirectories of lib. This is the arrangement of opt for FLINK-4861.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-dist.src.main.flink-bin.yarn-bin.yarn-session.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-1-28 01:00:00" id="4959" opendate="2016-10-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Write Documentation for ProcessFunction</summary>
      <description/>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.concepts.runtime.md</file>
      <file type="M">docs.concepts.programming-model.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-2 01:00:00" id="4997" opendate="2016-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extending Window Function Metadata</summary>
      <description>https://cwiki.apache.org/confluence/display/FLINK/FLIP-2+Extending+Window+Function+Metadata</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.WindowReduceITCase.scala</file>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.WindowFunctionITCase.scala</file>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.WindowFoldITCase.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.WindowedStream.scala</file>
      <file type="M">docs.dev.windows.md</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.WindowFoldITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.windowing.functions.InternalWindowFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.FoldApplyWindowFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingProcessingTimeWindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingKeyedTimePanes.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.WindowedStream.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2016-1-10 01:00:00" id="5047" opendate="2016-11-10 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Add sliding group-windows for batch tables</summary>
      <description>Add Slide group-windows for batch tables as described in FLIP-11.There are two ways to implement sliding windows for batch:1. replicate the output in order to assign keys for overlapping windows. This is probably the more straight-forward implementation and supports any aggregation function but blows up the data volume.2. if the aggregation functions are combinable / pre-aggregatable, we can also find the largest tumbling window size from which the sliding windows can be assembled. This is basically the technique used to express sliding windows with plain SQL (GROUP BY + OVER clauses). For a sliding window Slide(10 minutes, 2 minutes) this would mean to first compute aggregates of non-overlapping (tumbling) 2 minute windows and assembling consecutively 5 of these into a sliding window (could be done in a MapPartition with sorted input). The implementation could be done as an optimizer rule to split the sliding aggregate into a tumbling aggregate and a SQL WINDOW operator. Maybe it makes sense to implement the WINDOW clause first and reuse this for sliding windows.3. There is also a third, hybrid solution: Doing the pre-aggregation on the largest non-overlapping windows (as in 2) and replicating these results and processing those as in the 1) approach. The benefits of this is that it a) is based on the implementation that supports non-combinable aggregates (which is required in any case) and b) that it does not require the implementation of the SQL WINDOW operator. Internally, this can be implemented again as an optimizer rule that translates the SlidingWindow into a pre-aggregating TublingWindow and a final SlidingWindow (with replication).see FLINK-4692 for more discussion</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.DataSetWindowAggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateAllWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetWindowAggregateMapFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleCountWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSessionWindowAggregateReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetWindowAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.AggregateFunction.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-11-16 01:00:00" id="5075" opendate="2016-11-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kinesis consumer incorrectly determines shards as newly discovered when tested against Kinesalite</summary>
      <description>A user reported that when our Kinesis connector is used against Kinesalite (https://github.com/mhart/kinesalite), we're incorrectly determining already found shards as newly discovered:http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Subtask-keeps-on-discovering-new-Kinesis-shard-when-using-Kinesalite-td10133.htmlI suspect the problem to be the mock Kinesis API implementations of Kinesalite doesn't completely match with the official AWS Kinesis behaviour.</description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-16 01:00:00" id="5076" opendate="2016-11-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shutting down TM when shutting down new mini cluster</summary>
      <description>Currently we don't shut down task manager when shutting down mini cluster. It will cause mini cluster can not exit normally.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-16 01:00:00" id="5084" opendate="2016-11-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Java Table API integration tests by unit tests</summary>
      <description>The Java Table API is a wrapper on top of the Scala Table API. Instead of operating directly with Expressions like the Scala API, the Java API accepts a String parameter which is parsed into Expressions.We could therefore replace the Java Table API ITCases by tests that check that the parsing step produces a valid logical plan.This could be done by creating two Table objects for an identical query once with the Scala Expression API and one with the Java String API and comparing the logical plans of both Table objects. Basically something like the following:val ds1 = CollectionDataSets.getSmall3TupleDataSet(env).toTable(tEnv, 'a, 'b, 'c)val ds2 = CollectionDataSets.get5TupleDataSet(env).toTable(tEnv, 'd, 'e, 'f, 'g, 'h)val joinT1 = ds1.join(ds2).where('b === 'e).select('c, 'g)val joinT2 = ds1.join(ds2).where("b = e").select("c, g")val lPlan1 = joinT1.logicalPlanval lPlan2 = joinT2.logicalPlanAssert.assertEquals("Logical Plans do not match", lPlan1, lPlan2)</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.table.JoinITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.table.CastingITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.table.CalcITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.table.AggregationsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.ExplainTest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-1-21 01:00:00" id="5118" opendate="2016-11-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Inconsistent records sent/received metrics</summary>
      <description>In 1.2-SNAPSHOT running a large scale job you see that the counts for send/received records are inconsistent, e.g. in a simple word count job we see more received records/bytes than we see sent. This is a regression from 1.1 where everything works as expected.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.RecordWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-21 01:00:00" id="5119" opendate="2016-11-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Last taskmanager heartbeat not showing in web frontend</summary>
      <description>The web frontend does not list anything for the last heartbeat in the web frontend.</description>
      <version>1.1.3,1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.index.coffee</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-12-23 01:00:00" id="5145" opendate="2016-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebInterface to aggressive in pulling metrics</summary>
      <description>The WebInterface currently fetches the values for every available metric in a single request, regardless of whether they are selected or not. For larger jobs with a high parallelism this can put put a big burden on connections.I propose to limit the polling to metrics that are actually selected.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobs.metrics.svc.coffee</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-1-23 01:00:00" id="5150" opendate="2016-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebUI metric-related resource leak</summary>
      <description>The WebUI maintains a list of selected metrics for all jobs and vertices. When a metric is selected in the metric screen it is added to this list, and removed if it is unselected.The contents of this list are stored in the browser's localStorage. This allows a user to setup a metric screen, move to another page, and return to the original screen completely intact.However, if the metrics are never unselected by the user they will remain in this list. They will also still be in this list if the WebUI can't even display the corresponding job page anymore, if for example the history size limit was exceeded. They will even survive a browser restart, since they are not stored in a session-based storage.Furthermore, the WebUI still tries to update these metricsd, adding additional overhead to the WebBackend and potentially network.In other words, if you ever checked out metrics tab for some job, chances are that the next time you start the WebInterface it will still try to update the metrics for it.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobs.metrics.svc.coffee</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-25 01:00:00" id="5159" opendate="2016-11-25 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Improve perfomance of inner joins with a single row input</summary>
      <description>All inner joins (including a cross join) can be implemented as a MapFunction if one of their inputs is a single row. This row can be passed to a MapFunction as a BroadcastSet.This approach is going to be more lightweight than the other current strategies.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-28 01:00:00" id="5173" opendate="2016-11-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade RocksDB dependency</summary>
      <description>The current RocksDB version has some bugs which have been observed to cause data corruption in at least one case.Newer RocksDB versions also support Microsoft Windows.</description>
      <version>1.1.3,1.2.0</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-29 01:00:00" id="5187" opendate="2016-11-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create analog of Row in core</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.types.serialization.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-29 01:00:00" id="5188" opendate="2016-11-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adjust all the imports referencing types.Row and move RowCsvInputFormat</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.CountAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.utils.UserDefinedTableFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.datastream.DataStreamCorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.dataset.DataSetCorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.aggregate.AggregateTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.UserDefinedScalarFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.NonDeterministicTests.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.DecimalTypeTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.CompositeAccessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.expressions.ArrayTypeTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.utils.StreamITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.table.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.table.UnionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.TableEnvironmentITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.TableWithSQLITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.ProjectableTableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.stream.sql.SqlITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.table.JoinITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.table.CastingITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.table.CalcITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.table.AggregationsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.TableSourceITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.TableEnvironmentITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.api.java.batch.sql.SqlITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.TypeConverter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.sources.CsvTableSource.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.sinks.CsvTableSink.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.TimeWindowPropertyCollector.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.SumAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.MinAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.MaxAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.IncrementalAggregateWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.IncrementalAggregateTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.IncrementalAggregateReduceFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.IncrementalAggregateAllWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.IncrementalAggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010JsonTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaJsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.JsonRowDeserializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.JsonRowSerializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.JsonRowDeserializationSchemaTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.JsonRowSerializationSchemaTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSinkTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceTestBase.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCInputFormat.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCOutputFormat.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCFullTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCInputFormatTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCTestBase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.BatchTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.java.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.BatchTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.package.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.BatchTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.datastream.DataStreamAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.DataSetAggregateWithNullValuesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.Aggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateAllWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateMapFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateReduceCombineFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AggregateWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.runtime.aggregate.AvgAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-29 01:00:00" id="5192" opendate="2016-11-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide better log config templates</summary>
      <description>Our current log config template is very generic and invites users to always set the root logger to DEBUG if they want to get more details. Since Flink depends on libraries like Akka and it's common to run Flink with other systems like Hadoop or Kafka, this also increases the log levels for those systems.I would propose to split the default logger configuration up and provide a separate debugging template. Furthermore, some noisy loggers might turned off by default, for example HadoopFileSystem.</description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.conf.logback.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-29 01:00:00" id="5196" opendate="2016-11-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t log InputChannelDescriptor</summary>
      <description>Logging the InputChannelDescriptors is very noisy and usually infeasible to parse for larger setups with all-to-all connections.In a log of a larger scale Flink job this lead to a 11 fold reduction in file size (175 to 15 MBs).</description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.InputChannelDeploymentDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-29 01:00:00" id="5199" opendate="2016-11-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve logging of submitted job graph actions in HA case</summary>
      <description>Include the involved paths (ZK and FS) when logging and make sure they happen for each operation (put, get, delete).</description>
      <version>None</version>
      <fixedVersion>1.1.4,1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.ZooKeeperSubmittedJobGraphStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.SubmittedJobGraph.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-30 01:00:00" id="5211" opendate="2016-11-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include an example configuration for all reporters</summary>
      <description>We should extend the reporter documentation to include an example configuration for every reporter.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.metrics.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-2 01:00:00" id="5223" opendate="2016-12-2 00:00:00" resolution="Done">
    <buginformation>
      <summary>Add documentation of UDTF in Table API &amp; SQL</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-12-8 01:00:00" id="5293" opendate="2016-12-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make the Kafka consumer backwards compatible.</summary>
      <description/>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-9 01:00:00" id="5303" opendate="2016-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CUBE/ROLLUP/GROUPING SETS operator in SQL</summary>
      <description>Add support for such operators as CUBE, ROLLUP and GROUPING SETS in SQL.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateReduceCombineFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateMapFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateWithNullValuesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.ExpressionParser.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-9 01:00:00" id="5304" opendate="2016-12-9 00:00:00" resolution="Done">
    <buginformation>
      <summary>Change method name from crossApply to join in Table API</summary>
      <description>Currently, the UDTF in Table API is used with crossApply, but is used with JOIN in SQL. UDTF is something similar to Table, so it make sense to use .join("udtf(c) as (s)") in Table API too, and join is more familiar to users than crossApply.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.datastream.DataStreamCorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.table.runtime.dataset.DataSetCorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.table.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.sql.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.FlinkCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.datastream.DataStreamCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.functions.TableFunction.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-12-9 01:00:00" id="5310" opendate="2016-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden the RocksDB JNI library loading</summary>
      <description>Currently, the RocksDB JNI library is automatically and implicitly loaded by RocksDB upon initialization. If the loading fails, there is little information about why the loading failed.We should explicitly load the JNI library with retries and log better error information.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-9 01:00:00" id="5311" opendate="2016-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Write user documentation for BipartiteGraph</summary>
      <description>We need to add user documentation. The progress on BipartiteGraph can be tracked in the following JIRA:https://issues.apache.org/jira/browse/FLINK-2254</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.libs.gelly.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-12 01:00:00" id="5318" opendate="2016-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make the Rolling sink backwards compatible.</summary>
      <description/>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-filesystem.src.main.java.org.apache.flink.streaming.connectors.fs.RollingSink.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.main.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSink.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-1-16 01:00:00" id="5357" opendate="2016-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WordCountTable fails</summary>
      <description>The execution of org.apache.flink.table.examples.java.WordCountTable fails:Exception in thread "main" org.apache.flink.table.api.TableException: POJO does not define field name: TMP_0 at org.apache.flink.table.typeutils.TypeConverter$$anonfun$2.apply(TypeConverter.scala:85) at org.apache.flink.table.typeutils.TypeConverter$$anonfun$2.apply(TypeConverter.scala:81) at scala.collection.immutable.List.foreach(List.scala:318) at org.apache.flink.table.typeutils.TypeConverter$.determineReturnType(TypeConverter.scala:81) at org.apache.flink.table.plan.nodes.dataset.DataSetCalc.translateToPlan(DataSetCalc.scala:110) at org.apache.flink.table.api.BatchTableEnvironment.translate(BatchTableEnvironment.scala:305) at org.apache.flink.table.api.BatchTableEnvironment.translate(BatchTableEnvironment.scala:289) at org.apache.flink.table.api.java.BatchTableEnvironment.toDataSet(BatchTableEnvironment.scala:146) at org.apache.flink.table.examples.java.WordCountTable.main(WordCountTable.java:56) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.FieldProjectionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-19 01:00:00" id="5365" opendate="2016-12-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mesos AppMaster/TaskManager should obey sigterm</summary>
      <description>The AppMaster and TaskManager are ignoring the sigterm sent by Marathon/Mesos. The reason is simply that the shell scripts used to start them don't pass the signal to java.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-taskmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-appmaster.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-19 01:00:00" id="5366" opendate="2016-12-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add end-to-end tests for Savepoint Backwards Compatibility</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.UserFunctionStateJob.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.SavepointUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-21 01:00:00" id="5377" opendate="2016-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve savepoint docs</summary>
      <description>The savepoint docs are very detailed and focus on the internals. They should better convey what users have to take care of.The following questions should be answered:What happens if I add a new operator that requires state to my flow?What happens if I delete an operator that has state to my flow?What happens if I reorder stateful operators in my flow?What happens if I add or delete or reorder operators that have no state in my flow?Should I apply .uid to all operators in my flow?Should I apply .uid to only the operators that have state?</description>
      <version>1.1.3,1.2.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.savepoints.md</file>
      <file type="M">docs.fig.savepoints-program.ids.png</file>
      <file type="M">docs.fig.savepoints-overview.png</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-21 01:00:00" id="5378" opendate="2016-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Scopt version to 3.5.0</summary>
      <description>Is it possible to increase the Scopt version to 3.5.0? This version does also support comma-separated lists of arguments.I'm using this in my project and indeed I can use Maven to use the latest Scopt version. But, once I want to deploy an uber-Jar to Flink, it obviously fails because of two different versions of Scopt in the classpath - one in my uber-Jar (Scopt 3.5.0) and the one shipped with Flink distribution (Scopt 3.2.0).I know that there is another open issue regarding refactoring the CLI parser (FLINK-1347), but as far as I can see there is no progress yet.</description>
      <version>1.1.3,1.2.0,1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-21 01:00:00" id="5380" opendate="2016-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Number of outgoing records not reported in web interface</summary>
      <description>The web frontend does not report any outgoing records in the web frontend.The amount of data in MB is reported correctly.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-21 01:00:00" id="5381" opendate="2016-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scrolling in some web interface pages doesn&amp;#39;t work (taskmanager details, jobmanager config)</summary>
      <description>It seems that scrolling in the web interface doesn't work anymore on some pages in the 1.2 release branch.Example pages: When you click the "JobManager" tab The TaskManager logs page</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.css.index.css</file>
      <file type="M">flink-runtime-web.web-dashboard.app.styles.index.styl</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-1-6 01:00:00" id="5417" opendate="2017-1-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the wrong config file name</summary>
      <description>As the config file name is conf/flink-conf.yaml, the usage "conf/flink-config.yaml" in document is wrong and easy to confuse user. We should correct them.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.yarn.setup.md</file>
      <file type="M">docs.fig.slots.parallelism.svg</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-6 01:00:00" id="5423" opendate="2017-1-6 00:00:00" resolution="Done">
    <buginformation>
      <summary>Implement Stochastic Outlier Selection</summary>
      <description>I've implemented the Stochastic Outlier Selection (SOS) algorithm by Jeroen Jansen.http://jeroenjanssens.com/2013/11/24/stochastic-outlier-selection.htmlIntegrated as much as possible with the components from the machine learning library.The algorithm itself has been compared to four other algorithms and it it shows that SOS has a higher performance on most of these real-world datasets.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.libs.ml.index.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-1-10 01:00:00" id="5434" opendate="2017-1-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unsupported project() transformation from Scala DataStream docs</summary>
      <description>The Scala DataStream does not have a project() transformation, yet the docs include it as a supported operation.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.datastream.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-11 01:00:00" id="5447" opendate="2017-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sync documentation of built-in functions for Table API with SQL</summary>
      <description>I will split up the documentation for the built-in functions similar to the SQL structure.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-11 01:00:00" id="5452" opendate="2017-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make table unit tests pass under cluster mode</summary>
      <description>Currently if we change the test execution mode to TestExecutionMode.CLUSTER in TableProgramsTestBase, some cases will fail. Need to figure out whether it's the case design problem or there are some bugs.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.SortITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-11 01:00:00" id="5454" opendate="2017-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Documentation about how to tune Checkpointing for large state</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.rest.api.md</file>
      <file type="M">docs.monitoring.large.state.tuning.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-11 01:00:00" id="5455" opendate="2017-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create documentation how to upgrade jobs and Flink framework versions</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.upgrading.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-11 01:00:00" id="5458" opendate="2017-1-11 00:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Add documentation how to migrate from Flink 1.1. to Flink 1.2</summary>
      <description>Docs should go to docs/dev/migration.md</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-11 01:00:00" id="5465" opendate="2017-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>RocksDB fails with segfault while calling AbstractRocksDBState.clear()</summary>
      <description>I'm using Flink 699f4b0.## A fatal error has been detected by the Java Runtime Environment:## SIGSEGV (0xb) at pc=0x00007f91a0d49b78, pid=26662, tid=140263356024576## JRE version: Java(TM) SE Runtime Environment (7.0_67-b01) (build 1.7.0_67-b01)# Java VM: Java HotSpot(TM) 64-Bit Server VM (24.65-b04 mixed mode linux-amd64 compressed oops)# Problematic frame:# C [librocksdbjni-linux64.so+0x1aeb78] rocksdb::GetColumnFamilyID(rocksdb::ColumnFamilyHandle*)+0x8## Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again## An error report file with more information is saved as:# /yarn/nm/usercache/robert/appcache/application_1484132267957_0007/container_1484132267957_0007_01_000010/hs_err_pid26662.logCompiled method (nm) 1869778 903 n org.rocksdb.RocksDB::remove (native) total in heap [0x00007f91b40b9dd0,0x00007f91b40ba150] = 896 relocation [0x00007f91b40b9ef0,0x00007f91b40b9f48] = 88 main code [0x00007f91b40b9f60,0x00007f91b40ba150] = 496## If you would like to submit a bug report, please visit:# http://bugreport.sun.com/bugreport/crash.jsp# The crash happened outside the Java Virtual Machine in native code.# See problematic frame for where to report the bug.#</description>
      <version>1.2.0</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeServiceTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TestProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.ProcessingTimeService.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-12 01:00:00" id="5467" opendate="2017-1-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stateless chained tasks set legacy operator state</summary>
      <description>I discovered this while trying to rescale a job with a Kafka source with a chained stateless operator.Looking into it, it turns out that this fails, because the checkpointed state contains legacy operator state for the chained operator although it is state less./cc aljoscha You mentioned that this might be a possible duplicate?</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RescalingITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-12 01:00:00" id="5474" opendate="2017-1-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend DC/OS documentation</summary>
      <description>We could extend the DC/OS documentation a little bit to include information about how to submit a job (where to find the connection information) and that one has to install the DC/OS cli in order to add the development universe.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.mesos.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-13 01:00:00" id="5481" opendate="2017-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Simplify Row creation</summary>
      <description>When we use ExecutionEnvironment#fromElements(X... data) it takes first element of data to define a type. If first Row in collection has wrong number of fields (there are nulls) TypeExtractor returns not RowTypeInfo, but GenericType&lt;Row&gt;</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.UserDefinedScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.UserDefinedScalarFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ArrayTypeTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.stringexpr.CalcStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.Types.scala</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.Types.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-13 01:00:00" id="5484" opendate="2017-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kryo serialization changed between 1.1 and 1.2</summary>
      <description>I think the way that Kryo serializes data changed between 1.1 and 1.2.I have a generic Object that is serialized as part of a 1.1 savepoint that I cannot resume from with 1.2:org.apache.flink.client.program.ProgramInvocationException: The program execution failed: Job execution failed. at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:427) at org.apache.flink.client.program.StandaloneClusterClient.submitJob(StandaloneClusterClient.java:101) at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:400) at org.apache.flink.streaming.api.environment.StreamContextEnvironment.execute(StreamContextEnvironment.java:68) at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1486) at com.dataartisans.DidKryoChange.main(DidKryoChange.java:74) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:528) at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:419) at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:339) at org.apache.flink.client.CliFrontend.executeProgram(CliFrontend.java:831) at org.apache.flink.client.CliFrontend.run(CliFrontend.java:256) at org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:1073) at org.apache.flink.client.CliFrontend$2.call(CliFrontend.java:1120) at org.apache.flink.client.CliFrontend$2.call(CliFrontend.java:1117) at org.apache.flink.runtime.security.HadoopSecurityContext$1.run(HadoopSecurityContext.java:43) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:40) at org.apache.flink.client.CliFrontend.main(CliFrontend.java:1117)Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply$mcV$sp(JobManager.scala:900) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply(JobManager.scala:843) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply(JobManager.scala:843) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.IllegalStateException: Could not initialize keyed state backend. at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initKeyedState(AbstractStreamOperator.java:286) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:199) at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeOperators(StreamTask.java:649) at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeState(StreamTask.java:636) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:254) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:654) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: com.esotericsoftware.kryo.KryoException: Unable to find class: f at com.twitter.chill.java.UnmodifiableJavaCollectionSerializer.read(UnmodifiableJavaCollectionSerializer.java:62) at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761) at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:232) at org.apache.flink.migration.runtime.state.memory.AbstractMemStateSnapshot.deserialize(AbstractMemStateSnapshot.java:88) at org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.restoreHeapState(HeapKeyedStateBackend.java:448) at org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.restoreOldSavepointKeyedState(HeapKeyedStateBackend.java:406) at org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.restore(HeapKeyedStateBackend.java:240) at org.apache.flink.streaming.runtime.tasks.StreamTask.createKeyedStateBackend(StreamTask.java:784) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initKeyedState(AbstractStreamOperator.java:277) ... 6 moreCaused by: com.esotericsoftware.kryo.KryoException: Unable to find class: f at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:138) at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:115) at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:641) at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:752) at com.twitter.chill.java.UnmodifiableJavaCollectionSerializer.read(UnmodifiableJavaCollectionSerializer.java:59) ... 14 moreCaused by: java.lang.ClassNotFoundException: f at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348) at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:136) ... 18 moreRunning the same program with 1.2 and triggering and resuming a savepoint works.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.runtime.KryoGenericTypeSerializerTest.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-15 01:00:00" id="5494" opendate="2017-1-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Mesos documentation</summary>
      <description>Flink's Mesos documentation could benefit from more details how to set things up and which parameters to use.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.mesos.md</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-15 01:00:00" id="5496" opendate="2017-1-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClassCastException when using Mesos HA mode</summary>
      <description>When using the Mesos' HA mode, one cannot start the Mesos appmaster, because the following class cast exception occurs:java.lang.ClassCastException: org.apache.flink.shaded.org.apache.curator.framework.imps.CuratorFrameworkImpl cannot be cast to org.apache.flink.mesos.shaded.org.apache.curator.framework.CuratorFramework at org.apache.flink.mesos.util.ZooKeeperUtils.startCuratorFramework(ZooKeeperUtils.java:38) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.createWorkerStore(MesosApplicationMasterRunner.java:510) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.runPrivileged(MesosApplicationMasterRunner.java:320) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner$1.call(MesosApplicationMasterRunner.java:178) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner$1.call(MesosApplicationMasterRunner.java:175) at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:29) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.run(MesosApplicationMasterRunner.java:175) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.main(MesosApplicationMasterRunner.java:135)It seems as if the flink-mesos module relocates the curator dependency in another namespace than flink-runtime. Not sure why this is done.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-mesos.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-15 01:00:00" id="5497" opendate="2017-1-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove duplicated tests</summary>
      <description>Now we have test which run the same code 4 times, every run 17+ seconds.Need do small refactoring and remove duplicated code.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReusingReOpenableHashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.NonReusingReOpenableHashTableITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-16 01:00:00" id="5502" opendate="2017-1-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about migrating functions from 1.1 to 1.2</summary>
      <description/>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.migration.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-1-16 01:00:00" id="5508" opendate="2017-1-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Mesos dynamic class loading</summary>
      <description>Mesos uses dynamic class loading in order to load the ZooKeeperStateHandleStore and the CuratorFramework class. This can be replaced by a compile time dependency.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosFlinkResourceManagerTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.store.ZooKeeperMesosWorkerStore.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-16 01:00:00" id="5512" opendate="2017-1-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>RabbitMQ documentation should inform that exactly-once holds for RMQSource only when parallelism is 1</summary>
      <description>See here for the reasoning: FLINK-2624. We should add an informative warning about this limitation in the docs.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.rabbitmq.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-17 01:00:00" id="5517" opendate="2017-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade hbase version to 1.3.0</summary>
      <description>In the thread 'Help using HBase with Flink 1.1.4', Giuliano reported seeing:java.lang.IllegalAccessError: tried to access method com.google.common.base.Stopwatch.&lt;init&gt;()V from class org.apache.hadoop.hbase.zookeeper.MetaTableLocatorThe above has been solved by HBASE-14963hbase 1.3.0 is being released.We should upgrade hbase dependency to 1.3.0</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-hbase.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-17 01:00:00" id="5524" opendate="2017-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support early out for code generated conjunctive conditions</summary>
      <description>Currently, all nested conditions for a conjunctive predicate are evaluated before the conjunction is checked.A condition like (v1 == v2) &amp;&amp; (v3 &lt; 5) would be compiled intoboolean res1;if (v1 == v2) { res1 = true;} else { res1 = false;}boolean res2;if (v3 &lt; 5) { res2 = true;} else { res2 = false;}boolean res3;if (res1 &amp;&amp; res2) { res3 = true;} else { res3 = false;}if (res3) { // emit something}It would be better to leave the generated code as early as possible, e.g., with a return instead of res1 = false. The code generator needs a bit of context information for that.</description>
      <version>1.1.4,1.2.0,1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.UserDefinedScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-17 01:00:00" id="5531" opendate="2017-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SSl code block formatting is broken</summary>
      <description>Most code blocks on the ssl page aren't rendered properly and are simply shown as text.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.security-ssl.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-18 01:00:00" id="5555" opendate="2017-1-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about debugging watermarks</summary>
      <description>This was a frequent question on the mailing list.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.debugging.event.time.md</file>
      <file type="M">docs.dev.event.time.md</file>
      <file type="M">docs.dev.connectors.kafka.md</file>
      <file type="M">docs.dev.connectors.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-18 01:00:00" id="5562" opendate="2017-1-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Driver fixes</summary>
      <description>Improve parametrization and output formatting.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.AnalyticHelper.java</file>
      <file type="M">flink-libraries.flink-gelly.src.main.java.org.apache.flink.graph.AbstractGraphAnalytic.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.TriangleListing.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.JaccardIndex.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.HITS.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.GraphMetrics.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.Graph500.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.ClusteringCoefficient.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-19 01:00:00" id="5568" opendate="2017-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce interface for catalog, and provide an in-memory implementation, and integrate with calcite schema</summary>
      <description>The TableEnvironment now provides a mechanism to register temporary table. It registers the temp table to calcite catalog, so SQL and TableAPI queries can access to those temp tables. Now DatasetTable, DataStreamTable and TableSourceTable can be registered to TableEnvironment as temporary tables.This issue wants to provides a mechanism to connect external catalogs such as HCatalog to the TableEnvironment, so SQL and TableAPI queries could access to tables in the external catalogs without register those tables to TableEnvironment beforehand.First, we should point out that there are two kinds of catalog in Flink actually. The first one is external catalog as we mentioned before, it provides CRUD operations to databases/tables.The second one is calcite catalog, it defines namespace that can be accessed in Calcite queries. It depends on Calcite Schema/Table abstraction. SqlValidator and SqlConverter depends on the calcite catalog to fetch the tables in SQL or TableAPI.So we need to do the following things:1. introduce interface for external catalog, maybe provide an in-memory implementation first for test and develop environment.2. introduce a mechanism to connect external catalog with Calcite catalog so the tables/databases in external catalog can be accessed in Calcite catalog. Including convert databases of externalCatalog to Calcite sub-schemas, convert tables in a database of externalCatalog to Calcite tables (only support TableSourceTable).3. register external catalog to TableEnvironment.Here is the design mode of ExternalCatalogTable. identifier TableIdentifier dbName and tableName of table tableType String type of external catalog table, e.g csv, hbase, kafka schema DataSchema schema of table data, including column names and column types partitionColumnNames List&lt;String&gt; names of partition column properties Map&lt;String, String&gt; properties of external catalog table stats TableStats statistics of external catalog table comment String create time longThere is still a detail problem need to be take into consideration, that is , how to convert ExternalCatalogTable to TableSourceTable. The question is equals to convert ExternalCatalogTable to TableSource because we could easily get TableSourceTable from TableSource.Because different TableSource often contains different fields to initiate an instance. E.g. CsvTableSource needs path, fieldName, fieldTypes, fieldDelim, rowDelim and so on to create a new instance , KafkaTableSource needs configuration and tableName to create a new instance. So it's not a good idea to let Flink framework be responsible for translate ExternalCatalogTable to different kind of TableSourceTable. Here is one solution. Let TableSource specify a converter.1. provide an Annatition named ExternalCatalogCompatible. The TableSource with the annotation means it is compatible with external catalog, that is, it could be converted to or from ExternalCatalogTable. This annotation specifies the tabletype and converter of the tableSource. For example, for CsvTableSource, it specifies the tableType is csv and converter class is CsvTableSourceConverter.@ExternalCatalogCompatible(tableType = "csv", converter = classOf[CsvTableSourceConverter])class CsvTableSource(...) {...}2. Scan all TableSources with the ExternalCatalogCompatible annotation, save the tableType and converter in a Map3. When need to convert ExternalCatalogTable to TableSource , get the converter based on tableType. and let converter do convert</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.CommonTestData.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.exceptions.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-1-19 01:00:00" id="5580" opendate="2017-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kerberos keytabs not working for YARN deployment mode</summary>
      <description>Setup: Kerberos security using keytabs, Flink session on YARN deployment (in standalone, it works fine without problems).Im getting these error messages in the YARN node managers, causing the TaskManager containers to fail to start properly:org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:tzulitai (auth:SIMPLE) cause:org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:&amp;#91;TOKEN, KERBEROS&amp;#93;The security configuration for Hadoop has been set to "kerberos", to the "auto: SIMPLE" seems very strange. It also seems as if credential tokens has not been properly set for the ContainerLaunchContext s, which may be an issue causing this.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnTaskManagerRunner.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnApplicationMasterRunner.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-1-23 01:00:00" id="5617" opendate="2017-1-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check new public APIs in 1.2 release</summary>
      <description>Before releasing Flink 1.2.0, I would like to quickly review which new public methods we are supporting in future releases.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.WindowedStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.AllWindowedStream.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.FileSystem.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-25 01:00:00" id="5639" opendate="2017-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clarify License implications of RabbitMQ Connector</summary>
      <description>The RabbitMQ Connector has a Maven dependency under MPL 1.1.The ASF legal FAQ (https://www.apache.org/legal/resolved) classifies that as "may be included in binary form within an Apache product if the inclusion is appropriately labeled".Because we neither include sources nor binaries (but only define a Maven dependency) it is probably not relevant to Flink. But to be on the safe side for the project and users, we should add a notice as blow to the docs and the project:# License of the Rabbit MQ ConnectorFlink's RabbitMQ connector defines a Maven dependency on the"RabbitMQ AMQP Java Client", licensed under theMozilla Public License v1.1 (MPL 1.1).Flink itself neither reuses source code from the "RabbitMQ AMQP Java Client"nor packages binaries from the "RabbitMQ AMQP Java Client".Users that create and publish derivative work based on Flink'sRabbitMQ connector (thereby re-distributing the "RabbitMQ AMQP Java Client")must be aware that this may be subject to conditions declaredin the Mozilla Public License v1.1 (MPL 1.1).</description>
      <version>1.1.4,1.2.0</version>
      <fixedVersion>1.1.5,1.2.1,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.rabbitmq.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-25 01:00:00" id="5640" opendate="2017-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>configure the explicit Unit Test file suffix</summary>
      <description>There are four types of Unit Test file: *ITCase.java, *Test.java, *ITSuite.scala, *Suite.scalaFile name ending with "IT.java" is integration test. File name ending with "Test.java" is unit test.It's clear for Surefire plugin of default-test execution to declare that "Test." is Java Unit Test.The test file statistics below: Suite total: 10 ITCase total: 378 Test total: 1008 ITSuite total: 14</description>
      <version>None</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-25 01:00:00" id="5644" opendate="2017-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Task#lastCheckpointSize metric broken</summary>
      <description>The lastCheckpointSIze metric was broken when we introduced the key-groups. I couldn't find an easy way to fix the metric, as such i propose to remove it.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-1-27 01:00:00" id="5670" opendate="2017-1-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Local RocksDB directories not cleaned up</summary>
      <description>After cancelling a job with a RocksDB backend all files are properly cleaned up, but the parent directories still exist and are empty:859546fec3dac36bb9fcc8cbdd4e291e+- StreamFlatMap_3_0+- StreamFlatMap_3_3+- StreamFlatMap_3_4+- StreamFlatMap_3_5+- StreamFlatMap_3_6The number of empty folders varies between runs.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-27 01:00:00" id="5672" opendate="2017-1-27 00:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Job fails with java.lang.IllegalArgumentException: port out of range:-1</summary>
      <description>I started the JobManager with start-local.sh and started another TaskManager with taskmanager.sh start. My job is a Table API job with a orderBy (range partitioning with parallelism 2).The job fails with the following exception:java.lang.IllegalArgumentException: port out of range:-1 at java.net.InetSocketAddress.checkPort(InetSocketAddress.java:143) at java.net.InetSocketAddress.&lt;init&gt;(InetSocketAddress.java:188) at org.apache.flink.runtime.io.network.ConnectionID.&lt;init&gt;(ConnectionID.java:47) at org.apache.flink.runtime.deployment.InputChannelDeploymentDescriptor.fromEdges(InputChannelDeploymentDescriptor.java:124) at org.apache.flink.runtime.executiongraph.ExecutionVertex.createDeploymentDescriptor(ExecutionVertex.java:627) at org.apache.flink.runtime.executiongraph.Execution.deployToSlot(Execution.java:358) at org.apache.flink.runtime.executiongraph.Execution$1.apply(Execution.java:284) at org.apache.flink.runtime.executiongraph.Execution$1.apply(Execution.java:279) at org.apache.flink.runtime.concurrent.impl.FlinkFuture$5.onComplete(FlinkFuture.java:259) at akka.dispatch.OnComplete.internal(Future.scala:248) at akka.dispatch.OnComplete.internal(Future.scala:245) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:175) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:172) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) at org.apache.flink.runtime.concurrent.Executors$DirectExecutor.execute(Executors.java:56) at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:122) at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40) at scala.concurrent.impl.Promise$KeptPromise.onComplete(Promise.scala:333) at org.apache.flink.runtime.concurrent.impl.FlinkFuture.handleAsync(FlinkFuture.java:256) at org.apache.flink.runtime.concurrent.impl.FlinkFuture.handle(FlinkFuture.java:270) at org.apache.flink.runtime.executiongraph.Execution.scheduleForExecution(Execution.java:279) at org.apache.flink.runtime.executiongraph.ExecutionVertex.scheduleForExecution(ExecutionVertex.java:479) at org.apache.flink.runtime.executiongraph.Execution$5.call(Execution.java:525) at org.apache.flink.runtime.executiongraph.Execution$5.call(Execution.java:521) at akka.dispatch.Futures$$anonfun$future$1.apply(Future.scala:95) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.stop-cluster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-cluster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-6 01:00:00" id="5721" opendate="2017-2-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add FoldingState to State Documentation</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.state.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-6 01:00:00" id="5722" opendate="2017-2-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement DISTINCT as dedicated operator</summary>
      <description>DISTINCT is currently implemented for batch Table API / SQL as an aggregate which groups on all fields. Grouped aggregates are implemented as GroupReduce with sort-based combiner.This operator can be more efficiently implemented by using ReduceFunction and hinting a HashCombine strategy. The same ReduceFunction can be used for all DISTINCT operations and can be assigned with appropriate forward field annotations.We would need a custom conversion rule which translates distinct aggregations (grouping on all fields and returning all fields) into a custom DataSetRelNode.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.FieldProjectionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.QueryDecorrelationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.DistinctAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateRule.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-6 01:00:00" id="5723" opendate="2017-2-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use "Used" instead of "Initial" to make taskmanager tag more readable</summary>
      <description>Now in JobManager web fronted, the used memory of task managers is presented as "Initial" in table header, which actually means "memory used", from codes.I'd like change it to be more readable, even it is trivial one.</description>
      <version>None</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.taskmanager.taskmanager.metrics.html</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanager.taskmanager.metrics.jade</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-7 01:00:00" id="5731" opendate="2017-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split up CI builds</summary>
      <description>Test builds regularly time out because we are hitting the Travis 50 min limit. Previously, we worked around this by splitting up the tests into groups. I think we have to split them further.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-7-9 01:00:00" id="5750" opendate="2017-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect translation of n-ary Union</summary>
      <description>Calcite's union operator is supports more than two input relations. However, Flink's translation rules only consider the first two relations because we assumed that Calcite's union is binary. This problem exists for batch and streaming queries.It seems that Calcite only generates non-binary Unions in rare cases ((SELECT * FROM t) UNION ALL (SELECT * FROM t) UNION ALL (SELECT * FROM t) results in two binary union operators) but the problem definitely needs to be fixed.The following query can be used to validate the problem. @Test public void testValuesWithCast() throws Exception { ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config()); String sqlQuery = "VALUES (1, cast(1 as BIGINT) )," + "(2, cast(2 as BIGINT))," + "(3, cast(3 as BIGINT))"; String sqlQuery2 = "VALUES (1,1)," + "(2, 2)," + "(3, 3)"; Table result = tableEnv.sql(sqlQuery); DataSet&lt;Row&gt; resultSet = tableEnv.toDataSet(result, Row.class); List&lt;Row&gt; results = resultSet.collect(); Table result2 = tableEnv.sql(sqlQuery2); DataSet&lt;Row&gt; resultSet2 = tableEnv.toDataSet(result2, Row.class); List&lt;Row&gt; results2 = resultSet2.collect(); String expected = "1,1\n2,2\n3,3"; compareResultAsText(results2, expected); compareResultAsText(results, expected); }AR for results variablejava.lang.AssertionError: Different elements in arrays: expected 3 elements and received 2 expected: [1,1, 2,2, 3,3] received: [1,1, 2,2] Expected :3Actual :2</description>
      <version>1.2.0,1.3.4,1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.3,1.6.0,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testUnionStream0.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testUnion1.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testUnion0.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.UnionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.ExternalCatalogTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.GroupingSetsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamUnionRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetUnionRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-9 01:00:00" id="5756" opendate="2017-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>When there are many values under the same key in ListState, RocksDBStateBackend performances poor</summary>
      <description>When using RocksDB as the StateBackend, if there are many values under the same key in ListState, the windowState.get() operator performances very poor. I also the the RocksDB using version 4.11.2, the performance is also very poor. The problem is likely to related to RocksDB itself's get() operator after using merge(). The problem may influences the window operation's performance when the size is very large using ListState. I try to merge 50000 values under the same key in RocksDB, It costs 120 seconds to execute get() operation.///////////////////////////////////////////////////////////////////////////////The flink's code is as follows: class SEventSource extends RichSourceFunction [SEvent] { private var count = 0L private val alphabet = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWZYX0987654321" override def run(sourceContext: SourceContext[SEvent]): Unit = { while (true) { for (i &lt;- 0 until 5000) { sourceContext.collect(SEvent(1, "hello-"+count, alphabet,1)) count += 1L } Thread.sleep(1000) } }}env.addSource(new SEventSource) .assignTimestampsAndWatermarks(new AssignerWithPeriodicWatermarks[SEvent] { override def getCurrentWatermark: Watermark = { new Watermark(System.currentTimeMillis()) } override def extractTimestamp(t: SEvent, l: Long): Long = { System.currentTimeMillis() } }) .keyBy(0) .window(SlidingEventTimeWindows.of(Time.seconds(20), Time.seconds(2))) .apply(new WindowStatistic) .map(x =&gt; (System.currentTimeMillis(), x)) .print()////////////////////////////////////The RocksDB Test code: val stringAppendOperator = new StringAppendOperator val options = new Options() options.setCompactionStyle(CompactionStyle.LEVEL) .setCompressionType(CompressionType.SNAPPY_COMPRESSION) .setLevelCompactionDynamicLevelBytes(true) .setIncreaseParallelism(4) .setUseFsync(true) .setMaxOpenFiles(-1) .setCreateIfMissing(true) .setMergeOperator(stringAppendOperator) val write_options = new WriteOptions write_options.setSync(false) val rocksDB = RocksDB.open(options, "/******/Data/") val key = "key" val value = "abcdefghijklmnopqrstuvwxyz0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ7890654321" val beginmerge = System.currentTimeMillis() for(i &lt;- 0 to 50000) { rocksDB.merge(key.getBytes(), ("s"+ i + value).getBytes()) //rocksDB.put(key.getBytes, value.getBytes) } println("finish") val begin = System.currentTimeMillis() rocksDB.get(key.getBytes) val end = System.currentTimeMillis() println("merge cost:" + (begin - beginmerge)) println("Time consuming:" + (end - begin)) }}</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-10 01:00:00" id="5767" opendate="2017-2-10 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>New aggregate function interface and built-in aggregate functions</summary>
      <description>Add a new aggregate function interface. This includes implementing the aggregate interface, migrating the existing aggregation functions to this interface, and adding the unit tests for these functions.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-10 01:00:00" id="5768" opendate="2017-2-10 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Apply new aggregation functions for datastream and dataset tables</summary>
      <description>Apply new aggregation functions for datastream and dataset tablesThis includes:1. Change the implementation of the DataStream aggregation runtime code to use new aggregation functions and aggregate dataStream API.2. DataStream will be always running in incremental mode, as explained in 06/Feb/2017 in FLINK5564.2. Change the implementation of the Dataset aggregation runtime code to use new aggregation functions.3. Clean up unused class and method.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.DataSetWindowAggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.AggFunctionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateReduceFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateAllWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetWindowAggregateMapFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceCombineFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleCountWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSessionWindowAggregateReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSessionWindowAggregateCombineGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateReduceCombineFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateMapFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateAllWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetWindowAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.AggregateFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.SumAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.CountAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.AvgAggFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-10 01:00:00" id="5772" opendate="2017-2-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Instability with embedded Elasticsearch node in ElasticsearchSink test</summary>
      <description>This was seen in: https://api.travis-ci.org/jobs/199988755/log.txt?deansi=truetestDeprecatedIndexRequestBuilderVariant(org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkITCase) Time elapsed: 60.227 sec &lt;&lt;&lt; ERROR!org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply$mcV$sp(JobManager.scala:915) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply(JobManager.scala:858) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply(JobManager.scala:858) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.RuntimeException: An error occured in ElasticsearchSink. at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkErrorAndRethrow(ElasticsearchSinkBase.java:234) at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.invoke(ElasticsearchSinkBase.java:208) at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:38) at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:185) at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:63) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:263) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:667) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: ProcessClusterEventTimeoutException[failed to process cluster event (acquire index lock) within 1m] at org.apache.flink.streaming.connectors.elasticsearch.Elasticsearch1ApiCallBridge.extractFailureCauseFromBulkItemResponse(Elasticsearch1ApiCallBridge.java:117) at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase$1.afterBulk(ElasticsearchSinkBase.java:169) at org.elasticsearch.action.bulk.BulkProcessor.execute(BulkProcessor.java:316) at org.elasticsearch.action.bulk.BulkProcessor.executeIfNeeded(BulkProcessor.java:299) at org.elasticsearch.action.bulk.BulkProcessor.internalAdd(BulkProcessor.java:281) at org.elasticsearch.action.bulk.BulkProcessor.add(BulkProcessor.java:264) at org.elasticsearch.action.bulk.BulkProcessor.add(BulkProcessor.java:260) at org.apache.flink.streaming.connectors.elasticsearch.BulkProcessorIndexer.add(BulkProcessorIndexer.java:41) at org.apache.flink.streaming.connectors.elasticsearch.IndexRequestBuilderWrapperFunction.process(IndexRequestBuilderWrapperFunction.java:39) at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.invoke(ElasticsearchSinkBase.java:210) at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:38) at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:185) at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:63) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:263) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:667) at java.lang.Thread.run(Thread.java:745)The embedded elasticsearch node returned a ProcessClusterEventTimeoutException and failed the test. We should add retries in the ES tests for these kind of instabilities.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-14 01:00:00" id="5794" opendate="2017-2-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update the documentation about UDF/UDTF" support have parameters constructor.</summary>
      <description>Depends on FLINK-5792 .</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-14 01:00:00" id="5795" opendate="2017-2-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve UDF&amp;UDTF to support constructor with parameter</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.UserDefinedTableFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.DataStreamCorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.DataSetCorrelateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.UserDefinedScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.CompositeFlatteningTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.FieldProjectionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.UserDefinedFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.call.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-15 01:00:00" id="5803" opendate="2017-2-15 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add [partitioned] processing time OVER RANGE BETWEEN UNBOUNDED PRECEDING aggregation to SQL</summary>
      <description>The goal of this issue is to add support for OVER RANGE aggregations on processing time streams to the SQL interface.Queries similar to the following should be supported:SELECT a, SUM(b) OVER (PARTITION BY c ORDER BY procTime() RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS sumB, MIN(b) OVER (PARTITION BY c ORDER BY procTime() RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS minBFROM myStreamThe following restrictions should initially apply: All OVER clauses in the same SELECT clause must be exactly the same. The ORDER BY clause may only have procTime() as parameter. procTime() is a parameterless scalar function that just indicates processing time mode. bounded PRECEDING is not supported (see FLINK-5654) FOLLOWING is not supported.The restrictions will be resolved in follow up issues. If we find that some of the restrictions are trivial to address, we can add the functionality in this issue as well.This issue includes: Design of the DataStream operator to compute OVER ROW aggregates Translation from Calcite's RelNode representation (LogicalProject with RexOver expression).</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-15 01:00:00" id="5804" opendate="2017-2-15 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add [non-partitioned] processing time OVER RANGE BETWEEN UNBOUNDED PRECEDING aggregation to SQL</summary>
      <description>The goal of this issue is to add support for OVER RANGE aggregations on processing time streams to the SQL interface.Queries similar to the following should be supported:SELECT a, SUM(b) OVER (ORDER BY procTime() RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS sumB, MIN(b) OVER (ORDER BY procTime() RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS minBFROM myStreamThe following restrictions should initially apply: All OVER clauses in the same SELECT clause must be exactly the same. Since no PARTITION BY clause is specified, the execution will be single threaded. The ORDER BY clause may only have procTime() as parameter. procTime() is a parameterless scalar function that just indicates processing time mode. bounded PRECEDING is not supported (see FLINK-5654) FOLLOWING is not supported.The restrictions will be resolved in follow up issues. If we find that some of the restrictions are trivial to address, we can add the functionality in this issue as well.This issue includes: Design of the DataStream operator to compute OVER ROW aggregates Translation from Calcite's RelNode representation (LogicalProject with RexOver expression).</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamOverAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-15 01:00:00" id="5805" opendate="2017-2-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve docs for ProcessFunction</summary>
      <description>The documentation for ProcessFunction could be a bit clearer.</description>
      <version>None</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.process.function.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-15 01:00:00" id="5807" opendate="2017-2-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>improved wording for doc home page</summary>
      <description>The Migration Guide section of the home page of the documentation has some awkwardness.</description>
      <version>None</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.index.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-16 01:00:00" id="5814" opendate="2017-2-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-dist creates wrong symlink when not used with cleaned before</summary>
      <description>If &lt;flink-dir&gt;/build-target already exists, 'mvn package' for flink-dist will create a symbolic link inside &lt;flink-dir&gt;/build-target instead of replacing that symlink. This is due to the behaviour of ln &amp;#45;sf for target links that point to directories and may be solved by adding the --no-dereference parameter.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-16 01:00:00" id="5822" opendate="2017-2-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Checkpoint Coordinator aware of State Backend</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.StateBackendITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.BlockingCheckpointsTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.JobManagerITCase.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobSubmitTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerHARecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobgraph.tasks.JobSnapshottingSettingsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ArchivedExecutionGraphTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ExecutionGraphCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CoordinatorShutdownTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointStatsTrackerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateBackendFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.MemoryStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsStateBackendFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AbstractStateBackend.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.tasks.JobSnapshottingSettings.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinator.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointConfigHandlerTest.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.runtime.jobmanager.JMXJobManagerMetricTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-4-17 01:00:00" id="5829" opendate="2017-2-17 00:00:00" resolution="Done">
    <buginformation>
      <summary>Bump Calcite version to 1.12 once available</summary>
      <description>Once Calcite 1.12 is release we should update to remove some copied classes.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.TableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.catalog.ExternalCatalogSchemaTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.TableSourceITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.TableEnvironmentITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.util.RexProgramExtractor.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.literals.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-22 01:00:00" id="5881" opendate="2017-2-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ScalarFunction(UDF) should support variable types and variable arguments</summary>
      <description>As a sub-task of FLINK-5826. We would like to support the ScalarFunction first and make the review a little bit easier.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.UserDefinedScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.UserDefinedScalarFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.utils.UserDefinedScalarFunctions.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.ScalarSqlFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarFunctionCallGen.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-22 01:00:00" id="5882" opendate="2017-2-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TableFunction (UDTF) should support variable types and variable arguments</summary>
      <description>It's the second approach of FLINK-5826.We would like to make table functions (UDTF) of Flink support variable arguments.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.UserDefinedTableFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.DataStreamUserDefinedFunctionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.DataSetUserDefinedFunctionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.TableFunctionCallGen.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-24 01:00:00" id="5899" opendate="2017-2-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the bug in EventTimeTumblingWindow for non-partialMerge aggregate</summary>
      <description>The row length used to initialize DataSetTumbleTimeWindowAggReduceGroupFunction was not set properly. (I think this is introduced by mistake when merging the code).We currently lack the built-in non-partial-merge Aggregates. Therefore this has not been captured by the unit test. Reproduce step:1. set the "supportPartial" to false for SumAggregate2. Then both testAllEventTimeTumblingWindowOverTime and testEventTimeTumblingGroupWindowOverTime will fail.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-5-24 01:00:00" id="5906" opendate="2017-2-24 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add support to register UDAGG in Table and SQL API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.OverWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.validation.AggregationsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.stringexpr.AggregationsStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.ProjectionTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.OverAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.CommonAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.ScalarSqlFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.AggregateFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.CountAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.call.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.aggregations.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.TableFunctionCallGen.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarFunctionCallGen.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.BatchTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.java.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.java.BatchTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-3-3 01:00:00" id="5954" opendate="2017-3-3 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Always assign names to the window in the Stream SQL API</summary>
      <description>CALCITE-1603 and CALCITE-1615 brings in supports for TUMBLE, HOP, SESSION grouped windows, as well as the corresponding auxiliary functions that allow uses to query the start and the end of the windows (e.g., TUMBLE_START() and TUMBLE_END() see http://calcite.apache.org/docs/stream.html for more details).The goal of this jira is to add support for these auxiliary functions in Flink. Flink already has runtime supports for them, as these functions are essential mapped to the WindowStart and WindowEnd classes.To implement this feature in transformation, the transformation needs to recognize these functions and map them to the WindowStart and WindowEnd classes.The problem is that both classes can only refer to the windows using alias. Therefore this jira proposes to assign a unique name for each window to enable the transformation.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.LogicalWindowAggregateRule.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-3 01:00:00" id="5955" opendate="2017-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merging a list of buffered records will have problem when ObjectReuse is turned on</summary>
      <description>Turn on ObjectReuse in MultipleProgramsTestBase:TestEnvironment clusterEnv = new TestEnvironment(cluster, 4, true);Then the tests "testEventTimeSessionGroupWindow", "testEventTimeSessionGroupWindow", and "testEventTimeTumblingGroupWindowOverTime" will fail.The reason is that we have buffered iterated records for group-merge. I think we should change the Agg merge to pair-merge, and later add group-merge when needed (in the future we should add rules to select either pair-merge or group-merge, but for now all built-in aggregates should work fine with pair-merge).</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceCombineFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleCountWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSessionWindowAggregateReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSessionWindowAggregateCombineGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateReduceCombineFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.AggregateFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-3 01:00:00" id="5956" opendate="2017-3-3 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add retract method into the aggregateFunction</summary>
      <description>Retraction method is help for processing updated message. It will also very helpful for window Aggregation. This PR will first add retraction methods into the aggregateFunctions, such that on-going over window Aggregation can get benefit from it.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.SumAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.AggFunctionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.AggregateFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.SumAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.CountAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.AvgAggFunction.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-7-7 01:00:00" id="5987" opendate="2017-3-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade zookeeper dependency to 3.4.10</summary>
      <description>zookeeper 3.4.8 has been released.Among the fixes the following are desirable:ZOOKEEPER-706 large numbers of watches can cause session re-establishment to fail ZOOKEEPER-1797 PurgeTxnLog may delete data logs during rollThis issue upgrades zookeeper dependency to 3.4.8</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-8 01:00:00" id="5990" opendate="2017-3-8 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add [partitioned] event time OVER ROWS BETWEEN x PRECEDING aggregation to SQL</summary>
      <description>The goal of this issue is to add support for OVER ROWS aggregations on event time streams to the SQL interface.Queries similar to the following should be supported:SELECT a, SUM(b) OVER (PARTITION BY c ORDER BY rowTime() ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS sumB, MIN(b) OVER (PARTITION BY c ORDER BY rowTime() ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS minBFROM myStreamThe following restrictions should initially apply: All OVER clauses in the same SELECT clause must be exactly the same. The PARTITION BY clause is required The ORDER BY clause may only have rowTime() as parameter. rowTime() is a parameterless scalar function that just indicates event time mode. UNBOUNDED PRECEDING is not supported (see FLINK-5803) FOLLOWING is not supported.The restrictions will be resolved in follow up issues. If we find that some of the restrictions are trivial to address, we can add the functionality in this issue as well.This issue includes: Design of the DataStream operator to compute OVER ROW aggregates Translation from Calcite's RelNode representation (LogicalProject with RexOver expression).</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.OverAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamOverAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-8 01:00:00" id="5998" opendate="2017-3-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Un-fat Hadoop from Flink fat jar</summary>
      <description>As a first step towards FLINK-2268, I would suggest to put all hadoop dependencies into a jar separate from Flink's fat jar.This would allow users to put a custom Hadoop jar in there, or even deploy Flink without a Hadoop fat jar at all in environments where Hadoop is provided (EMR).</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-4-9 01:00:00" id="6011" opendate="2017-3-9 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support TUMBLE, HOP, SESSION window in streaming SQL</summary>
      <description>CALCITE-1603 and CALCITE-1615 introduces the support of the TUMBLE / HOP / SESSION windows in the parser.This jira tracks the efforts of adding the corresponding supports on the planners / optimizers in Flink.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.LogicalWindowAggregateRule.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-9 01:00:00" id="6012" opendate="2017-3-9 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support WindowStart / WindowEnd functions in streaming SQL</summary>
      <description>This jira proposes to add support for TUMBLE_START() / TUMBLE_END() / HOP_START() / HOP_END() / SESSUIB_START() / SESSION_END() in the planner in Flink.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-6-14 01:00:00" id="6038" opendate="2017-3-14 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add deep links to Apache Bahir Flink streaming connector documentations</summary>
      <description>Recently, the Bahir documentation for Flink streaming connectors in Bahir was added to Bahir's website: BAHIR-90.We should add deep links to the individual Bahir connector dos under /dev/connectors/overview, instead of just shallow links to the source README.md s in the community ecosystem page.</description>
      <version>None</version>
      <fixedVersion>1.3.1,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-15 01:00:00" id="6056" opendate="2017-3-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>apache-rat exclude flink directory in tools</summary>
      <description>The flink* directory in the tools is temporary cloned when build distribution.So when build the Flink project, we should exclude the flink* directory.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-15 01:00:00" id="6059" opendate="2017-3-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reject DataSet&lt;Row&gt; and DataStream&lt;Row&gt; without RowTypeInformation</summary>
      <description>It is not possible to automatically extract proper type information for Row because it is not typed with generics and holds values in an Object[].Consequently is handled as GenericType&lt;Row&gt; unless a RowTypeInfo is explicitly specified.This can lead to unexpected behavior when converting a DataSet&lt;Row&gt; or DataStream&lt;Row&gt; into a Table. If the data set or data stream has a GenericType&lt;Row&gt;, the rows are treated as atomic type and converted into a single field.I think we should reject input types of GenericType&lt;Row&gt; when converting data sets and data streams and request a proper RowTypeInfo.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.2,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.TableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.TableEnvironmentITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.TableEnvironmentITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-16 01:00:00" id="6075" opendate="2017-3-16 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>ORDER BY *time ASC</summary>
      <description>These will be split in 3 separated JIRA issues. However, the design is the same only the processing function differs in terms of the output. Hence, the design is the same for all of them.Time target: Proc Time*SQL targeted query examples:*Sort exampleQ1)` SELECT a FROM stream1 GROUP BY HOP(proctime, INTERVAL '1' HOUR, INTERVAL '3' HOUR) ORDER BY b` Comment: window is defined using GROUP BYComment: ASC or DESC keywords can be placed to mark the ordering typeLimit exampleQ2) `SELECT a FROM stream1 WHERE rowtime BETWEEN current_timestamp - INTERVAL '1' HOUR AND current_timestamp ORDER BY b LIMIT 10`Comment: window is defined using time ranges in the WHERE clauseComment: window is row triggeredTop exampleQ3) `SELECT sum(a) OVER (ORDER BY proctime RANGE INTERVAL '1' HOUR PRECEDING LIMIT 10) FROM stream1` Comment: limit over the contents of the sliding windowGeneral Comments:-All these SQL clauses are supported only over windows (bounded collections of data). -Each of the 3 operators will be supported with each of the types of expressing the windows. *Description*The 3 operations (limit, top and sort) are similar in behavior as they all require a sorted collection of the data on which the logic will be applied (i.e., select a subset of the items or the entire sorted set). These functions would make sense in the streaming context only in the context of a window. Without defining a window the functions could never emit as the sort operation would never trigger. If an SQL query will be provided without limits an error will be thrown (`SELECT a FROM stream1 TOP 10` -&gt; ERROR). Although not targeted by this JIRA, in the case of working based on event time order, the retraction mechanisms of windows and the lateness mechanisms can be used to deal with out of order events and retraction/updates of results.*Functionality example*We exemplify with the query below for all the 3 types of operators (sorting, limit and top). Rowtime indicates when the HOP window will trigger  which can be observed in the fact that outputs are generated only at those moments. The HOP windows will trigger at every hour (fixed hour) and each event will contribute/ be duplicated for 2 consecutive hour intervals. Proctime indicates the processing time when a new event arrives in the system. Events are of the type (a,b) with the ordering being applied on the b field.`SELECT a FROM stream1 HOP(proctime, INTERVAL '1' HOUR, INTERVAL '2' HOUR) ORDER BY b (LIMIT 2/ TOP 2 / &amp;#91;ASC/DESC&amp;#93; `)Rowtime Proctime Stream1 Limit 2 Top 2 Sort &amp;#91;ASC&amp;#93; 10:00:00 (aaa, 11) 10:05:00 (aab, 7) 10-11 11:00:00 aab,aaa aab,aaa aab,aaa 11:03:00 (aac,21) 11-12 12:00:00 aab,aaa aab,aaa aab,aaa,aac 12:10:00 (abb,12) 12:15:00 (abb,12) 12-13 13:00:00 abb,abb abb,abb abb,abb,aac...*Implementation option*Considering that the SQL operators will be associated with window boundaries, the functionality will be implemented within the logic of the window as follows. Window assigner  selected based on the type of window used in SQL (TUMBLING, SLIDING) Evictor/ Trigger  time or count evictor based on the definition of the window boundaries Apply  window function that sorts data and selects the output to trigger (based on LIMIT/TOP parameters). All data will be sorted at once and result outputted when the window is triggeredAn alternative implementation can be to use a fold window function to sort the elements as they arrive, one at a time followed by a flatMap to filter the number of outputs. *General logic of Join*```inputDataStream.window(new &amp;#91;Slide/Tumble&amp;#93;&amp;#91;Time/Count&amp;#93;Window())//.trigger(new &amp;#91;Time/Count&amp;#93;Trigger())  use default//.evictor(new &amp;#91;Time/Count&amp;#93;Evictor())  use default .apply(SortAndFilter());```------------JIRA will contain ORDER BY *time ASC OFFSET FETCHORDER BY *time DESC OFFSET FETCHORDER BY * OFFSET FETCH</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetSort.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-4-22 01:00:00" id="6149" opendate="2017-3-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add additional flink logical relation nodes</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testUnionStream0.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.MockTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.TableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.rules.NormalizationRulesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.ExternalCatalogTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.CalciteConfig.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.cost.FlinkRelMdRowCount.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalWindowAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.BatchTableSourceScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetConvention.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamConvention.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.StreamTableSourceScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.FlinkRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.TableSourceScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.common.PushFilterIntoTableSourceScanRuleBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.common.PushProjectIntoTableSourceScanRuleBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.BatchTableSourceScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateWithNullValuesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetCalcRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetCorrelateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetDistinctRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetIntersectRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetMinusRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetSingleRowJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetSortRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetUnionRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetValuesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.PushFilterIntoBatchTableSourceScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.PushProjectIntoBatchTableSourceScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamCalcRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamCorrelateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamOverAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamUnionRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamValuesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.PushFilterIntoStreamTableSourceScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.PushProjectIntoStreamTableSourceScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.StreamTableSourceScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.EnumerableToLogicalTableScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.TableEnvironmentITCase.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-24 01:00:00" id="6177" opendate="2017-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for "Distributed Cache" in streaming applications</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.distributedCache.DistributedCacheTest.java</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-24 01:00:00" id="6181" opendate="2017-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Zookeeper scripts use invalid regex</summary>
      <description>This issue has been reported by a user: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/unable-to-add-more-servers-in-zookeeper-quorum-peers-in-flink-1-2-td12321.html</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.stop-zookeeper-quorum.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-zookeeper-quorum.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-28 01:00:00" id="6203" opendate="2017-3-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DataSet Transformations</summary>
      <description>the example of GroupReduce on sorted groups can't remove duplicate Strings in a DataSet.need to add "prev=t"such as:val output = input.groupBy(0).sortGroup(1, Order.ASCENDING).reduceGroup { (in, out: Collector[(Int, String)]) =&gt; var prev: (Int, String) = null for (t &lt;- in) { if (prev == null || prev != t) out.collect(t) prev=t // this line is missing in the example } }</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.batch.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-29 01:00:00" id="6211" opendate="2017-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Validation error in Kinesis Consumer when using AT_TIMESTAMP as start position</summary>
      <description>private static void validateOptionalDateProperty(Properties config, String key, String message) { if (config.containsKey(key)) { try { initTimestampDateFormat.parse(config.getProperty(key));  double value = Double.parseDouble(config.getProperty(key));  if (value &lt; 0) { throw new NumberFormatException(); } } catch (ParseException | NumberFormatException e){ throw new IllegalArgumentException(message); } } }}This validation function will always fail regardless of either string format or double type.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-29 01:00:00" id="6212" opendate="2017-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing reference to flink-avro dependency</summary>
      <description>In the Connectors page of the Batch (DataSet API) there is a section called "Avro support in Flink"This section mentions the use of certain classes that are part of the flink-avro dependency but this fact is mentioned nowhere. This explanation should be added as well as an xml snippet with the maven dependency as in other parts of the documentation.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.batch.connectors.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-4-31 01:00:00" id="6236" opendate="2017-3-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Savepoint page needs to include web console possibility</summary>
      <description>Starting Flink 1.2.0 it is also possible to point to the savepoint when starting a job. However, the page only mention the CLI only.</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.savepoints.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-1 01:00:00" id="6237" opendate="2017-4-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>support RAND and RAND_INTEGER on Table API &amp; SQL</summary>
      <description>support RAND and RAND_INTEGER with and without seed on on Table API &amp; SQL.like: RAND(&amp;#91;seed&amp;#93;), RAND_INTEGER(&amp;#91;seed, &amp;#93; bound)</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.random.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.mathExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.generated.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.RandCallGen.scala</file>
      <file type="M">docs.dev.table.sql.md</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.stream.sql.SqlITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.sql.SqlITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-19 01:00:00" id="6326" opendate="2017-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add ProjectMergeRule at logical optimization stage</summary>
      <description>add ProjectMergeRule to merge projections. Some SQLs can not push projection into scan without this rule. e.g.table1: id: int, name: stringtable2: id: string, score: double, first: string, last: stringSELECT a.id, b.score FROM (SELECT id FROM table1 WHERE id &gt; 10) a LEFT OUTER JOIN (SELECT * FROM table2) b ON CAST(a.id AS VARCHAR) = b.id== Optimized Logical Plan without ProjectMergeRule ==DataSetCalc(select=[id, score]) DataSetJoin(where=[=(id0, id1)], join=[id, id0, id1, score, first, last], joinType=[LeftOuterJoin]) DataSetCalc(select=[id, CAST(id) AS id0], where=[&gt;(id, 10)]) BatchTableSourceScan(table=[[table1]], fields=[id]) BatchTableSourceScan(table=[[table2]], fields=[id, score, first, last])== Optimized Logical Plan with ProjectMergeRule ==DataSetCalc(select=[id, score]) DataSetJoin(where=[=(id0, id1)], join=[id, id0, id1, score], joinType=[LeftOuterJoin]) DataSetCalc(select=[id, CAST(id) AS id0], where=[&gt;(id, 10)]) BatchTableSourceScan(table=[[table1]], fields=[id]) BatchTableSourceScan(table=[[table2]], fields=[id, score])</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.QueryDecorrelationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.ExplainTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-19 01:00:00" id="6330" opendate="2017-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Docker documentation</summary>
      <description>The "Docker" page in the docs exists but is blank.Add something useful here, including references to the official images that should exist once 1.2.1 is released, and add a brief "Kubernetes" page as well, referencing the helm chart.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.2,1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.docker.run.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-20 01:00:00" id="6336" opendate="2017-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Placement Constraints for Mesos</summary>
      <description>Fenzo supports placement constraints for tasks, and operators expose agent attributes to frameworks in the form of attributes about the agent offer.It would be extremely helpful in our multi-tenant cluster to be able to make use of this facility.</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParametersTest.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosFlinkResourceManagerTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.LaunchableMesosWorker.java</file>
      <file type="M">docs.setup.mesos.md</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-26 01:00:00" id="6386" opendate="2017-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing bracket in &amp;#39;Compiler Limitation&amp;#39; section</summary>
      <description>"This means that types such as `Tuple2&lt;String,Integer` or `Collector&lt;String&gt;` declared as..."should be "This means that types such as `Tuple2&lt;String,Integer&gt;` or `Collector&lt;String&gt;` declared as..."</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.java8.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-28 01:00:00" id="6415" opendate="2017-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make sure core Flink artifacts have no specific logger dependency</summary>
      <description>Flink's code is written against slf4jTo make sure users can use their custom logging framework we need to have no direct compile-scope dependency in any core a dependency in flink-dist that is not in the fat jar an explicit dependency in examples (to see logs when running in the IDE) an explicit test dependency (for logs of test execution)All except point (1) are already fixed.</description>
      <version>1.2.0,1.2.1</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-15 01:00:00" id="6593" opendate="2017-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Bug in ProctimeAttribute or RowtimeAttribute with CodeGenerator</summary>
      <description>the ProctimeAttribute or RowtimeAttribute should not be take into codegenerator</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamCalc.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-6-26 01:00:00" id="7004" opendate="2017-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Switch to Travis Trusty image</summary>
      <description>As shown in this PR https://github.com/apache/flink/pull/4167 switching to the Trusty image on Travis seems to stabilize the build times.We should switch for 1.2, 1.3 and 1.4.</description>
      <version>1.2.0,1.3.0,1.4.0</version>
      <fixedVersion>1.2.2,1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-5 01:00:00" id="8864" opendate="2018-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CLI query history in SQL Client</summary>
      <description>This issue is a subtask of part two "Full Embedded SQL Client" of the implementation plan mentioned inFLIP-24.It would be great to have the possibility of persisting the CLI's query history. Such that queries can be reused when the CLI Client is started again. Also a search feature as it is offered by terminals would be good.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.SqlClient.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptionsParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptions.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-5 01:00:00" id="8865" opendate="2018-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CLI query code completion in SQL Client</summary>
      <description>This issue is a subtask of part two "Full Embedded SQL Client" of the implementation plan mentioned inFLIP-24.Calcite already offers a code completion functionality. It would be great if we could expose this feature also through the SQL CLI Client.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlMultiLineParser.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCompleter.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
      <file type="M">flink-libraries.flink-sql-client.pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>