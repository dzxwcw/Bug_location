<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  
  <bug fixdate="2015-12-1 01:00:00" id="3092" opendate="2015-12-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Scala API quick start word count example</summary>
      <description>The word count example in the scala api quick start uses both print and execute which results in following exception.Exception in thread "main" java.lang.RuntimeException: No new data sinks have been defined since the last execution. The last execution refers to the latest call to 'execute()', 'count()', 'collect()', or 'print()'.```</description>
      <version>0.10.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.quickstart.scala.api.quickstart.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-5-6 01:00:00" id="30921" opendate="2023-2-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Too many CI failed due to "Could not connect to azure.archive.ubuntu.com"</summary>
      <description> https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45762&amp;view=logs&amp;j=bea52777-eaf8-5663-8482-18fbc3630e81&amp;t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45766&amp;view=logs&amp;j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&amp;t=160c9ae5-96fd-516e-1c91-deb81f59292a</description>
      <version>None</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-4 01:00:00" id="3115" opendate="2015-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Elasticsearch connector to 2.X</summary>
      <description>The Elasticsearch connector is not up to date anymore. In version 2.X the API changed. The code needs to be adapted. Probably it makes sense to have a new class ElasticsearchSink2.</description>
      <version>0.10.0,0.10.1,1.0.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.pom.xml</file>
      <file type="M">docs.apis.streaming.connectors.index.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-12-7 01:00:00" id="3136" opendate="2015-12-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala Closure Cleaner uses wrong ASM import</summary>
      <description>The closure cleaner uses Kryo's ReflectASM's ASM. That is currently in version 4.0, which is incompatible with later Scala versions.Using ASM directly gives version 5.0.Flink also shades that ASM version correctly away in the end.</description>
      <version>0.10.1</version>
      <fixedVersion>0.10.2,1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ClosureCleaner.scala</file>
      <file type="M">flink-libraries.flink-gelly-scala.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-9 01:00:00" id="3157" opendate="2015-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web frontend json files contain author attribution</summary>
      <description>http://mail-archives.apache.org/mod_mbox/jakarta-jmeter-dev/200402.mbox/%3C4039F65E.7020406@atg.com%3Eauthor tags are officially discouraged. these create difficulties inestablishing the proper ownership and the protection of ourcommitters. there are other social issues dealing with collaborativedevelopment, but the Board is concerned about the legal ramificationsaround the use of author tagsFiles: flink-runtime-web/web-dashboard/*.json</description>
      <version>0.10.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.bower.json</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-10 01:00:00" id="3158" opendate="2015-12-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shading does not remove google guava from flink-dist fat jar</summary>
      <description>It seems that guava somehow slipped our checks and made it into the flink-dist fat jar again.</description>
      <version>0.10.1,1.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-3-23 01:00:00" id="31598" opendate="2023-3-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup usage of deprecated TableEnvironment#registerTable</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableToDataStreamITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.LegacyTableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.UnnestITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalTableFunctionJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TemporalSortITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SplitAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SortLimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SortITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.RankITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.OverAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.MatchRecognizeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.LimitITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.Limit0RemoveITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.IntervalJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.DeduplicateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CorrelateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AsyncLookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateRemoveITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.harness.OverAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.table.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.table.validation.LegacyTableSinkValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.MatchRecognizeTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.LegacySinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.LegacySinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.DagOptimizationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableImpl.java</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.src.test.java.org.apache.flink.connector.hbase1.HBaseConnectorITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SemiAntiJoinStreamITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-4-3 01:00:00" id="31690" opendate="2023-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The current key is not set for KeyedCoProcessOperator</summary>
      <description>See https://apache-flink.slack.com/archives/C03G7LJTS2G/p1680294701254239 for more details.</description>
      <version>None</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.datastream.process.operations.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-14 01:00:00" id="3172" opendate="2015-12-14 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Specify jobmanager port in HA mode</summary>
      <description>In HA mode, the job manager port is picked up randomly. In firewalled setups this can be problematic. We should add a way to use HA mode without random ports (like the web frontend currently does).</description>
      <version>0.10.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMasterBase.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.setup.jobmanager.high.availability.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2023-6-23 01:00:00" id="31894" opendate="2023-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ExceptionHistory and REST API failure label integration</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobExceptionsInfoWithHistoryNoRootTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.job.JobExceptionsMessageParameters.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobExceptionsInfoWithHistory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-9-23 01:00:00" id="31895" opendate="2023-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end integration tests for failure labels</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-5-19 01:00:00" id="32131" opendate="2023-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support specifying hadoop config dir for Python HiveCatalog</summary>
      <description>Hadoop config directory could be specified for HiveCatalog in Java, however, this is still not supported in Python HiveCatalog. This JIRA is to align them.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.catalog.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-7-7 01:00:00" id="32280" opendate="2023-6-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HashAgg support operator fusion codegen</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.fusion.spec.HashJoinFusionCodegenSpec.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.fusion.FusionCodegenUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ProjectionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.WindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.SortAggCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.AggWithoutKeysCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecHashAggregate.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.OperatorFusionCodegenITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.codegen.agg.AggTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.AggCodeGenHelper.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.processor.MultipleInputNodeCreationProcessor.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-2-18 01:00:00" id="3254" opendate="2016-1-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CombineFunction interface not respected</summary>
      <description>The DataSet API offers a CombineFunction interface, which differs from the GroupCombineFunction interface by being restricted to return a single value instead of returning arbitrary many values through a Collector.The JavaDocs of the GroupCombineFunction point to the CombineFunction interface, advertising it as more efficient.However, the CombineFunction interface is nor respected by Flink, i.e., a GroupReduceFunction that implements this interface is executed without leveraging the combine method.</description>
      <version>0.10.1,1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.GroupReduceOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-18 01:00:00" id="3255" opendate="2016-1-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Chaining behavior should not depend on parallelism</summary>
      <description>Currently, operators are chained more aggressively when the parallelism is one. That makes debugging tougher as it changes threading behavior.The benefits are also limited: Real installations where that type of efficiency would be needed would not run in parallelism 1, or would not use a partitioning/broadcast step there (if explicitly required to run parallelism 1).In the future, when we want to allow parallelism to be adjusted dynamically, this will be even more tricky.</description>
      <version>0.10.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-7-17 01:00:00" id="32610" opendate="2023-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JSON format supports projection push down</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.formats.json.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.formats.json.md</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonRowDataSerDeSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonFormatFactoryTest.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDataSerializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonRowDataDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonFormatOptions.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonFormatFactory.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonToRowDataConverters.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-2-20 01:00:00" id="3268" opendate="2016-1-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unstable test JobManagerSubmittedJobGraphsRecoveryITCase</summary>
      <description>Logs for the failed test: https://s3.amazonaws.com/archive.travis-ci.org/jobs/103625073/log.txt</description>
      <version>0.10.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperTestEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-21 01:00:00" id="3271" opendate="2016-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Using webhdfs in a flink topology throws classnotfound exception</summary>
      <description>I was just trying to run a storm topology on flink using flink-storm. I got this exception - Caused by: java.lang.NoClassDefFoundError: org/mortbay/util/ajax/JSON at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.jsonParse(WebHdfsFileSystem.java:325) at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathResponseRunner.getResponse(WebHdfsFileSystem.java:727) at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:610) at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:458) at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:487) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:483) at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.listStatus(WebHdfsFileSystem.java:1277)My topology list some files on hdfs using webhdfs API. org.mortbay.util.ajax.JSON was included in the application uber jar. I noticed that flink loads the application jar in a child classloader. This is what most likely happened - 1. WebHdfsFileSystem class was loaded through parent class loader since it is included in flink-dist.jar.2. WebHdfsFileSystem has reference to the org.mortbay.util.ajax.JSON but since it is loaded through parent class loader, WebHdfsFileSystem can't read a class in child class loader. Ideally all the referenced classes should be available in the distribution jar so that these sort of issues may not occur.</description>
      <version>0.10.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2.pom.xml</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop1.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-2-25 01:00:00" id="3286" opendate="2016-1-25 00:00:00" resolution="Done">
    <buginformation>
      <summary>Remove JDEB Debian Package code from flink-dist</summary>
      <description>There is currently code in the flink-dist project to create a debian package for Flink. This has been added by a contributor quite a while back, and never been maintained (probably also never used).I vote to remove that. It is out of date with paths and filenames and there seems no interest in maintaining it so far.</description>
      <version>0.10.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-dist.src.deb.control.postinst</file>
      <file type="M">flink-dist.src.deb.control.control</file>
      <file type="M">flink-dist.src.deb.bin.webclient</file>
      <file type="M">flink-dist.src.deb.bin.taskmanager</file>
      <file type="M">flink-dist.src.deb.bin.jobmanager</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-9-7 01:00:00" id="33061" opendate="2023-9-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate failure-enricher documentation to Chinese</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.deployment.advanced.failure.enrichers.md</file>
      <file type="M">docs.content.docs.deployment.advanced.failure.enrichers.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-1 01:00:00" id="3308" opendate="2016-2-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Remove debug mode</summary>
      <description>The debug mode was broken in a previous commit and so cumbersome that i used it maybe twice since i added it. Hence it should be removed.On a separate note, error reporting should be improved to make narrowing down the error cause easier.</description>
      <version>0.10.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Environment.py</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.data.PythonStreamer.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonPlanBinder.java</file>
      <file type="M">docs.apis.batch.python.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-2-9 01:00:00" id="3373" opendate="2016-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Using a newer library of Apache HttpClient than 4.2.6 will get class loading problems</summary>
      <description>When I trying to use Apache HTTP client 4.5.1 in my flink job it will crash with NoClassDefFound.This has to do that it load some classes from provided httpclient 4.2.5/6 in core flink.17:05:56,193 INFO org.apache.flink.runtime.taskmanager.Task - DuplicateFilter -&gt; InstallKeyLookup (11/16) switched to FAILED with exception.java.lang.NoSuchFieldError: INSTANCE at org.apache.http.conn.ssl.SSLConnectionSocketFactory.&lt;clinit&gt;(SSLConnectionSocketFactory.java:144) at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.getDefaultRegistry(PoolingHttpClientConnectionManager.java:109) at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.&lt;init&gt;(PoolingHttpClientConnectionManager.java:116) ...&lt;internal classes&gt; at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36) at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:89) at org.apache.flink.streaming.runtime.tasks.StreamTask.openAllOperators(StreamTask.java:305) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:227) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566) at java.lang.Thread.run(Thread.java:745)SSLConnectionSocketFactory and finds an earlier version of the AllowAllHostnameVerifier that does have the INSTANCE variable (instance variable was probably added in 4.3).jar tvf lib/flink-dist-1.0-SNAPSHOT.jar |grep AllowAllHostnameVerifier 791 Thu Dec 17 09:55:46 CET 2015 org/apache/http/conn/ssl/AllowAllHostnameVerifier.classSolutions would be: Fix the classloader so that my custom job does not conflict with internal flink-core classes... pretty hard Remove the dependency somehow.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-9 01:00:00" id="3381" opendate="2016-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unstable Test: JobManagerSubmittedJobGraphsRecoveryITCase</summary>
      <description>https://s3.amazonaws.com/archive.travis-ci.org/jobs/108034634/log.txt</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.zookeeper.ZooKeeperTestEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-15 01:00:00" id="3402" opendate="2016-2-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor Common Parts of Stream/Batch Documentation</summary>
      <description>I want to build on the work of uce in refactoring the streaming guide. With the release on the horizon I think it is important to have good structure in the documentation.I propose to move the following sections from the Streaming Doc to a new Section "Basic Concepts" (name up for discussion): Linking With Flink DataStream abstraction (remove, this can be covered by an extended "Lazy Evaluation") Lazy Evaluation Specifying Keys Passing Functions to Flink Data Types Debugging Program Packaging and Distributed Execution Parallel Execution Execution Plans</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.streaming.index.md</file>
      <file type="M">docs.apis.batch.index.md</file>
      <file type="M">docs.apis.batch.fig.plan.visualizer.png</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-15 01:00:00" id="3403" opendate="2016-2-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create Section "Working with Time" in Streaming Guide</summary>
      <description>We should add a proper page for this. Where we explain the notions of time and how to setup programs.Also, we would explain how to work with watermarks and the different timestamp extractors.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.streaming.storm.compatibility.md</file>
      <file type="M">docs.apis.streaming.libs.index.md</file>
      <file type="M">docs.apis.streaming.libs.cep.md</file>
      <file type="M">docs.apis.streaming.index.md</file>
      <file type="M">docs.apis.streaming.fault.tolerance.md</file>
      <file type="M">docs.apis.streaming.connectors.index.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-3-2 01:00:00" id="3564" opendate="2016-3-2 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement distinct() for Table API</summary>
      <description>This is only syntactic sugar for grouping of all fields.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">docs.apis.batch.libs.table.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-11 01:00:00" id="3609" opendate="2016-3-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revisit selection of Calcite rules</summary>
      <description>We should revisit the selection of Calcite rules, e.g., remove rule to reorder joins or join inputs.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.UnionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.GroupedAggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.FlinkJoinUnionTransposeRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
</bugrepository>