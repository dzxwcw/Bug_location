<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  
  <bug fixdate="2016-10-7 01:00:00" id="4770" opendate="2016-10-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate core options</summary>
      <description>The core options contain everything that is specific to job cross TaskManager / JobManager</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnClusterDescriptorTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNHighAvailabilityITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.classloading.ClassLoaderITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.SavepointMigrationTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.RescalingITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.ZooKeeperTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.BootstrapToolsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.HDFSTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.RollingSinkSecuredITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-18 01:00:00" id="4850" opendate="2016-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>FlinkML - SVM predict Operation for Vector and not LaveledVector</summary>
      <description>It seems that evaluate operation is defined for Vector and not LabeledVector.It impacts QuickStart guide for FlinkML when using SVM.We need to update the documentation as follows:val astroTest:DataSet&amp;#91;(Vector,Double)&amp;#93; = MLUtils .readLibSVM(env, "src/main/resources/svmguide1.t") .map(l =&gt; (l.vector, l.label))val predictionPairs = svm.evaluate(astroTest)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.libs.ml.quickstart.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-21 01:00:00" id="5118" opendate="2016-11-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Inconsistent records sent/received metrics</summary>
      <description>In 1.2-SNAPSHOT running a large scale job you see that the counts for send/received records are inconsistent, e.g. in a simple word count job we see more received records/bytes than we see sent. This is a regression from 1.1 where everything works as expected.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.writer.RecordWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-21 01:00:00" id="5119" opendate="2016-11-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Last taskmanager heartbeat not showing in web frontend</summary>
      <description>The web frontend does not list anything for the last heartbeat in the web frontend.</description>
      <version>1.1.3,1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.index.coffee</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-23 01:00:00" id="5150" opendate="2016-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebUI metric-related resource leak</summary>
      <description>The WebUI maintains a list of selected metrics for all jobs and vertices. When a metric is selected in the metric screen it is added to this list, and removed if it is unselected.The contents of this list are stored in the browser's localStorage. This allows a user to setup a metric screen, move to another page, and return to the original screen completely intact.However, if the metrics are never unselected by the user they will remain in this list. They will also still be in this list if the WebUI can't even display the corresponding job page anymore, if for example the history size limit was exceeded. They will even survive a browser restart, since they are not stored in a session-based storage.Furthermore, the WebUI still tries to update these metricsd, adding additional overhead to the WebBackend and potentially network.In other words, if you ever checked out metrics tab for some job, chances are that the next time you start the WebInterface it will still try to update the metrics for it.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobs.metrics.svc.coffee</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-19 01:00:00" id="5365" opendate="2016-12-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mesos AppMaster/TaskManager should obey sigterm</summary>
      <description>The AppMaster and TaskManager are ignoring the sigterm sent by Marathon/Mesos. The reason is simply that the shell scripts used to start them don't pass the signal to java.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-taskmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-appmaster.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-19 01:00:00" id="5366" opendate="2016-12-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add end-to-end tests for Savepoint Backwards Compatibility</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.UserFunctionStateJob.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.utils.SavepointUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-21 01:00:00" id="5378" opendate="2016-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Scopt version to 3.5.0</summary>
      <description>Is it possible to increase the Scopt version to 3.5.0? This version does also support comma-separated lists of arguments.I'm using this in my project and indeed I can use Maven to use the latest Scopt version. But, once I want to deploy an uber-Jar to Flink, it obviously fails because of two different versions of Scopt in the classpath - one in my uber-Jar (Scopt 3.5.0) and the one shipped with Flink distribution (Scopt 3.2.0).I know that there is another open issue regarding refactoring the CLI parser (FLINK-1347), but as far as I can see there is no progress yet.</description>
      <version>1.1.3,1.2.0,1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-21 01:00:00" id="5380" opendate="2016-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Number of outgoing records not reported in web interface</summary>
      <description>The web frontend does not report any outgoing records in the web frontend.The amount of data in MB is reported correctly.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-21 01:00:00" id="5381" opendate="2016-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scrolling in some web interface pages doesn&amp;#39;t work (taskmanager details, jobmanager config)</summary>
      <description>It seems that scrolling in the web interface doesn't work anymore on some pages in the 1.2 release branch.Example pages: When you click the "JobManager" tab The TaskManager logs page</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.css.index.css</file>
      <file type="M">flink-runtime-web.web-dashboard.app.styles.index.styl</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-5 01:00:00" id="5414" opendate="2017-1-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump up Calcite version to 1.11</summary>
      <description>The upcoming Calcite release 1.11 has a lot of stability fixes and new features. We should update it for the Table API.E.g. we can hopefully merge FLINK-4864</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.FieldProjectionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.ProjectionTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.FlinkRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.ScalarSqlFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-6 01:00:00" id="5417" opendate="2017-1-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the wrong config file name</summary>
      <description>As the config file name is conf/flink-conf.yaml, the usage "conf/flink-config.yaml" in document is wrong and easy to confuse user. We should correct them.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.yarn.setup.md</file>
      <file type="M">docs.fig.slots.parallelism.svg</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-10 01:00:00" id="5434" opendate="2017-1-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unsupported project() transformation from Scala DataStream docs</summary>
      <description>The Scala DataStream does not have a project() transformation, yet the docs include it as a supported operation.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.datastream.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-11 01:00:00" id="5447" opendate="2017-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sync documentation of built-in functions for Table API with SQL</summary>
      <description>I will split up the documentation for the built-in functions similar to the SQL structure.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-11 01:00:00" id="5452" opendate="2017-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make table unit tests pass under cluster mode</summary>
      <description>Currently if we change the test execution mode to TestExecutionMode.CLUSTER in TableProgramsTestBase, some cases will fail. Need to figure out whether it's the case design problem or there are some bugs.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.SortITCase.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-11 01:00:00" id="5454" opendate="2017-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Documentation about how to tune Checkpointing for large state</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.rest.api.md</file>
      <file type="M">docs.monitoring.large.state.tuning.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-11 01:00:00" id="5455" opendate="2017-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create documentation how to upgrade jobs and Flink framework versions</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.upgrading.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-11 01:00:00" id="5458" opendate="2017-1-11 00:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Add documentation how to migrate from Flink 1.1. to Flink 1.2</summary>
      <description>Docs should go to docs/dev/migration.md</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-15 01:00:00" id="5496" opendate="2017-1-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClassCastException when using Mesos HA mode</summary>
      <description>When using the Mesos' HA mode, one cannot start the Mesos appmaster, because the following class cast exception occurs:java.lang.ClassCastException: org.apache.flink.shaded.org.apache.curator.framework.imps.CuratorFrameworkImpl cannot be cast to org.apache.flink.mesos.shaded.org.apache.curator.framework.CuratorFramework at org.apache.flink.mesos.util.ZooKeeperUtils.startCuratorFramework(ZooKeeperUtils.java:38) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.createWorkerStore(MesosApplicationMasterRunner.java:510) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.runPrivileged(MesosApplicationMasterRunner.java:320) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner$1.call(MesosApplicationMasterRunner.java:178) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner$1.call(MesosApplicationMasterRunner.java:175) at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:29) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.run(MesosApplicationMasterRunner.java:175) at org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.main(MesosApplicationMasterRunner.java:135)It seems as if the flink-mesos module relocates the curator dependency in another namespace than flink-runtime. Not sure why this is done.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-mesos.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-15 01:00:00" id="5497" opendate="2017-1-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove duplicated tests</summary>
      <description>Now we have test which run the same code 4 times, every run 17+ seconds.Need do small refactoring and remove duplicated code.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReusingReOpenableHashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.NonReusingReOpenableHashTableITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-16 01:00:00" id="5508" opendate="2017-1-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Mesos dynamic class loading</summary>
      <description>Mesos uses dynamic class loading in order to load the ZooKeeperStateHandleStore and the CuratorFramework class. This can be replaced by a compile time dependency.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosFlinkResourceManagerTest.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.store.ZooKeeperMesosWorkerStore.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosApplicationMasterRunner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-16 01:00:00" id="5512" opendate="2017-1-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>RabbitMQ documentation should inform that exactly-once holds for RMQSource only when parallelism is 1</summary>
      <description>See here for the reasoning: FLINK-2624. We should add an informative warning about this limitation in the docs.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.rabbitmq.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-17 01:00:00" id="5517" opendate="2017-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade hbase version to 1.3.0</summary>
      <description>In the thread 'Help using HBase with Flink 1.1.4', Giuliano reported seeing:java.lang.IllegalAccessError: tried to access method com.google.common.base.Stopwatch.&lt;init&gt;()V from class org.apache.hadoop.hbase.zookeeper.MetaTableLocatorThe above has been solved by HBASE-14963hbase 1.3.0 is being released.We should upgrade hbase dependency to 1.3.0</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-hbase.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-17 01:00:00" id="5524" opendate="2017-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support early out for code generated conjunctive conditions</summary>
      <description>Currently, all nested conditions for a conjunctive predicate are evaluated before the conjunction is checked.A condition like (v1 == v2) &amp;&amp; (v3 &lt; 5) would be compiled intoboolean res1;if (v1 == v2) { res1 = true;} else { res1 = false;}boolean res2;if (v3 &lt; 5) { res2 = true;} else { res2 = false;}boolean res3;if (res1 &amp;&amp; res2) { res3 = true;} else { res3 = false;}if (res3) { // emit something}It would be better to leave the generated code as early as possible, e.g., with a return instead of res1 = false. The code generator needs a bit of context information for that.</description>
      <version>1.1.4,1.2.0,1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.UserDefinedScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-17 01:00:00" id="5531" opendate="2017-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SSl code block formatting is broken</summary>
      <description>Most code blocks on the ssl page aren't rendered properly and are simply shown as text.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.security-ssl.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-19 01:00:00" id="5575" opendate="2017-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>in old releases, warn users and guide them to the latest stable docs</summary>
      <description>Old versions of Flink (especially version 0.8) are being frequently studied, downloaded, and used by new users (because google leads them there). I propose to guide folks to the latest stable release by adding a link on every documentation page in the old docs that links to the home page of the latest stable docs. The redirect lives at flink.apache.org/q/stable-docs.html, and will need to be modified with each major release (e.g. when 1.2 is released).This problem affects all releases before 1.1, but the stats show that 0.8, 0.9, 0.10, and 1.0 are the most important to deal with.</description>
      <version>None</version>
      <fixedVersion>1.0.4,1.1.5,1.2.0,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..layouts.base.html</file>
      <file type="M">docs..config.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-24 01:00:00" id="5624" opendate="2017-1-24 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support tumbling window on streaming tables in the SQL API</summary>
      <description>This is a follow up of FLINK-4691.FLINK-4691 adds supports for group-windows for streaming tables. This jira proposes to expose the functionality in the SQL layer via the GROUP BY clauses, as described in http://calcite.apache.org/docs/stream.html#tumbling-windows.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-1-25 01:00:00" id="5644" opendate="2017-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Task#lastCheckpointSize metric broken</summary>
      <description>The lastCheckpointSIze metric was broken when we introduced the key-groups. I couldn't find an easy way to fix the metric, as such i propose to remove it.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-26 01:00:00" id="5653" opendate="2017-1-26 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add processing time OVER ROWS BETWEEN x PRECEDING aggregation to SQL</summary>
      <description>The goal of this issue is to add support for OVER ROWS aggregations on processing time streams to the SQL interface.Queries similar to the following should be supported:SELECT a, SUM(b) OVER (PARTITION BY c ORDER BY procTime() ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS sumB, MIN(b) OVER (PARTITION BY c ORDER BY procTime() ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS minBFROM myStreamThe following restrictions should initially apply: All OVER clauses in the same SELECT clause must be exactly the same. The PARTITION BY clause is optional (no partitioning results in single threaded execution). The ORDER BY clause may only have procTime() as parameter. procTime() is a parameterless scalar function that just indicates processing time mode. UNBOUNDED PRECEDING is not supported (see FLINK-5656) FOLLOWING is not supported.The restrictions will be resolved in follow up issues. If we find that some of the restrictions are trivial to address, we can add the functionality in this issue as well.This issue includes: Design of the DataStream operator to compute OVER ROW aggregates Translation from Calcite's RelNode representation (LogicalProject with RexOver expression).</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamOverAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-26 01:00:00" id="5654" opendate="2017-1-26 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add processing time OVER RANGE BETWEEN x PRECEDING aggregation to SQL</summary>
      <description>The goal of this issue is to add support for OVER RANGE aggregations on processing time streams to the SQL interface.Queries similar to the following should be supported:SELECT a, SUM(b) OVER (PARTITION BY c ORDER BY procTime() RANGE BETWEEN INTERVAL '1' HOUR PRECEDING AND CURRENT ROW) AS sumB, MIN(b) OVER (PARTITION BY c ORDER BY procTime() RANGE BETWEEN INTERVAL '1' HOUR PRECEDING AND CURRENT ROW) AS minBFROM myStreamThe following restrictions should initially apply: All OVER clauses in the same SELECT clause must be exactly the same. The PARTITION BY clause is optional (no partitioning results in single threaded execution). The ORDER BY clause may only have procTime() as parameter. procTime() is a parameterless scalar function that just indicates processing time mode. UNBOUNDED PRECEDING is not supported (see FLINK-5657) FOLLOWING is not supported.The restrictions will be resolved in follow up issues. If we find that some of the restrictions are trivial to address, we can add the functionality in this issue as well.This issue includes: Design of the DataStream operator to compute OVER ROW aggregates Translation from Calcite's RelNode representation (LogicalProject with RexOver expression).</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamOverAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-26 01:00:00" id="5655" opendate="2017-1-26 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add event time OVER RANGE BETWEEN x PRECEDING aggregation to SQL</summary>
      <description>The goal of this issue is to add support for OVER RANGE aggregations on event time streams to the SQL interface.Queries similar to the following should be supported:SELECT a, SUM(b) OVER (PARTITION BY c ORDER BY rowTime() RANGE BETWEEN INTERVAL '1' HOUR PRECEDING AND CURRENT ROW) AS sumB, MIN(b) OVER (PARTITION BY c ORDER BY rowTime() RANGE BETWEEN INTERVAL '1' HOUR PRECEDING AND CURRENT ROW) AS minBFROM myStreamThe following restrictions should initially apply: All OVER clauses in the same SELECT clause must be exactly the same. The PARTITION BY clause is optional (no partitioning results in single threaded execution). The ORDER BY clause may only have rowTime() as parameter. rowTime() is a parameterless scalar function that just indicates processing time mode. UNBOUNDED PRECEDING is not supported (see FLINK-5658) FOLLOWING is not supported.The restrictions will be resolved in follow up issues. If we find that some of the restrictions are trivial to address, we can add the functionality in this issue as well.This issue includes: Design of the DataStream operator to compute OVER ROW aggregates Translation from Calcite's RelNode representation (LogicalProject with RexOver expression).</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamOverAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-26 01:00:00" id="5658" opendate="2017-1-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add event time OVER ROWS BETWEEN UNBOUNDED PRECEDING aggregation to SQL</summary>
      <description>The goal of this issue is to add support for OVER RANGE aggregations on event time streams to the SQL interface.Queries similar to the following should be supported:SELECT a, SUM(b) OVER (PARTITION BY c ORDER BY rowTime() ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS sumB, MIN(b) OVER (PARTITION BY c ORDER BY rowTime() ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS minBFROM myStreamThe following restrictions should initially apply: All OVER clauses in the same SELECT clause must be exactly the same. The PARTITION BY clause is optional (no partitioning results in single threaded execution). The ORDER BY clause may only have rowTime() as parameter. rowTime() is a parameterless scalar function that just indicates processing time mode. bounded PRECEDING is not supported (see FLINK-5655) FOLLOWING is not supported.The restrictions will be resolved in follow up issues. If we find that some of the restrictions are trivial to address, we can add the functionality in this issue as well.An event-time OVER ROWS window will not be able to handle late data, because this would mean in insert a row into a sorted order shift all other computations. This would be too expensive to maintain. Therefore, we will throw an error if a user tries to use an event-time OVER ROWS window with late data handling.This issue includes: Design of the DataStream operator to compute OVER ROW aggregates Translation from Calcite's RelNode representation (LogicalProject with RexOver expression).</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamOverAggregate.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-1-27 01:00:00" id="5670" opendate="2017-1-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Local RocksDB directories not cleaned up</summary>
      <description>After cancelling a job with a RocksDB backend all files are properly cleaned up, but the parent directories still exist and are empty:859546fec3dac36bb9fcc8cbdd4e291e+- StreamFlatMap_3_0+- StreamFlatMap_3_3+- StreamFlatMap_3_4+- StreamFlatMap_3_5+- StreamFlatMap_3_6The number of empty folders varies between runs.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-27 01:00:00" id="5672" opendate="2017-1-27 00:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Job fails with java.lang.IllegalArgumentException: port out of range:-1</summary>
      <description>I started the JobManager with start-local.sh and started another TaskManager with taskmanager.sh start. My job is a Table API job with a orderBy (range partitioning with parallelism 2).The job fails with the following exception:java.lang.IllegalArgumentException: port out of range:-1 at java.net.InetSocketAddress.checkPort(InetSocketAddress.java:143) at java.net.InetSocketAddress.&lt;init&gt;(InetSocketAddress.java:188) at org.apache.flink.runtime.io.network.ConnectionID.&lt;init&gt;(ConnectionID.java:47) at org.apache.flink.runtime.deployment.InputChannelDeploymentDescriptor.fromEdges(InputChannelDeploymentDescriptor.java:124) at org.apache.flink.runtime.executiongraph.ExecutionVertex.createDeploymentDescriptor(ExecutionVertex.java:627) at org.apache.flink.runtime.executiongraph.Execution.deployToSlot(Execution.java:358) at org.apache.flink.runtime.executiongraph.Execution$1.apply(Execution.java:284) at org.apache.flink.runtime.executiongraph.Execution$1.apply(Execution.java:279) at org.apache.flink.runtime.concurrent.impl.FlinkFuture$5.onComplete(FlinkFuture.java:259) at akka.dispatch.OnComplete.internal(Future.scala:248) at akka.dispatch.OnComplete.internal(Future.scala:245) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:175) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:172) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) at org.apache.flink.runtime.concurrent.Executors$DirectExecutor.execute(Executors.java:56) at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:122) at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40) at scala.concurrent.impl.Promise$KeptPromise.onComplete(Promise.scala:333) at org.apache.flink.runtime.concurrent.impl.FlinkFuture.handleAsync(FlinkFuture.java:256) at org.apache.flink.runtime.concurrent.impl.FlinkFuture.handle(FlinkFuture.java:270) at org.apache.flink.runtime.executiongraph.Execution.scheduleForExecution(Execution.java:279) at org.apache.flink.runtime.executiongraph.ExecutionVertex.scheduleForExecution(ExecutionVertex.java:479) at org.apache.flink.runtime.executiongraph.Execution$5.call(Execution.java:525) at org.apache.flink.runtime.executiongraph.Execution$5.call(Execution.java:521) at akka.dispatch.Futures$$anonfun$future$1.apply(Future.scala:95) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.stop-cluster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-cluster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-31 01:00:00" id="5690" opendate="2017-1-31 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>protobuf is not shaded properly</summary>
      <description>Currently distributive contains com/google/protobuf package. Without proper shading client code could fail with:Caused by: java.lang.IllegalAccessError: tried to access method com.google.protobuf.XXXXSteps to reproduce: create job class "com.google.protobuf.TestClass" call com.google.protobuf.TextFormat.escapeText(String) method from this class deploy job to flink cluster (usign web console for example) run job. In logs IllegalAccessError.Issue in package protected method and different classloaders. TestClass loaded by FlinkUserCodeClassLoader, but TextFormat class loaded by sun.misc.Launcher$AppClassLoader</description>
      <version>1.1.4,1.3.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.debugging.classloading.md</file>
      <file type="M">docs.monitoring.best.practices.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-2 01:00:00" id="5698" opendate="2017-2-2 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add NestedFieldsProjectableTableSource interface</summary>
      <description>Add a NestedFieldsProjectableTableSource interface for some TableSource implementation that support nesting projection push-down.The interface could look as followsdef trait NestedFieldsProjectableTableSource { def projectNestedFields(fields: Array[String]): NestedFieldsProjectableTableSource[T]}This interface works together with ProjectableTableSource</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.util.RexProgramExtractorTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.util.RexProgramExtractor.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.common.PushProjectIntoTableSourceScanRuleBase.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-3 01:00:00" id="5702" opendate="2017-2-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka Producer docs should warn if using setLogFailuresOnly, at-least-once is compromised</summary>
      <description>The documentation for FlinkKafkaProducer does not have any information about the setLogFailuresOnly. It should emphasize that if users choose to only log failures instead of failing the sink, at-least-once can not be guaranteed .</description>
      <version>None</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-11-3 01:00:00" id="5706" opendate="2017-2-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement Flink&amp;#39;s own S3 filesystem</summary>
      <description>As part of the effort to make Flink completely independent from Hadoop, Flink needs its own S3 filesystem implementation. Currently Flink relies on Hadoop's S3a and S3n file systems.An own S3 file system can be implemented using the AWS SDK. As the basis of the implementation, the Hadoop File System can be used (Apache Licensed, should be okay to reuse some code as long as we do a proper attribution).</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">flink-filesystems.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-3 01:00:00" id="5709" opendate="2017-2-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Max Parallelism to Parallel Execution Doc</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.parallel.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-3 01:00:00" id="5710" opendate="2017-2-3 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add ProcTime() function to indicate StreamSQL</summary>
      <description>procTime() is a parameterless scalar function that just indicates processing time mode</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.LogicalWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.TimeModeIndicatorFunctions.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-6 01:00:00" id="5722" opendate="2017-2-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement DISTINCT as dedicated operator</summary>
      <description>DISTINCT is currently implemented for batch Table API / SQL as an aggregate which groups on all fields. Grouped aggregates are implemented as GroupReduce with sort-based combiner.This operator can be more efficiently implemented by using ReduceFunction and hinting a HashCombine strategy. The same ReduceFunction can be used for all DISTINCT operations and can be assigned with appropriate forward field annotations.We would need a custom conversion rule which translates distinct aggregations (grouping on all fields and returning all fields) into a custom DataSetRelNode.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.FieldProjectionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.QueryDecorrelationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.DistinctAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetAggregateRule.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-6 01:00:00" id="5723" opendate="2017-2-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use "Used" instead of "Initial" to make taskmanager tag more readable</summary>
      <description>Now in JobManager web fronted, the used memory of task managers is presented as "Initial" in table header, which actually means "memory used", from codes.I'd like change it to be more readable, even it is trivial one.</description>
      <version>None</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.taskmanager.taskmanager.metrics.html</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanager.taskmanager.metrics.jade</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-7 01:00:00" id="5731" opendate="2017-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split up CI builds</summary>
      <description>Test builds regularly time out because we are hitting the Travis 50 min limit. Previously, we worked around this by splitting up the tests into groups. I think we have to split them further.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-2-19 01:00:00" id="5842" opendate="2017-2-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong &amp;#39;since&amp;#39; version for ElasticSearch 5.x connector</summary>
      <description>The documentation claims that ElasticSearch 5.x is supported since Flink 1.2.0 which is not true, as the support was merged after 1.2.0.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.elasticsearch.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-3-24 01:00:00" id="5912" opendate="2017-2-24 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Inputs for CSV and graph generators</summary>
      <description>Create Input classes for reading graphs from CSV as well as for each of the graph generators.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.parameter.ParameterizedBase.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.drivers.parameter.Parameterized.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-25 01:00:00" id="5915" opendate="2017-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for the aggregate on multi fields</summary>
      <description>some UDAGGs have multi-fields as input. For instance,table.window(Tumble over 10.minutes on 'rowtime as 'w ).groupBy('key, 'w).select('key, weightedAvg('value, 'weight))This task will add the support for the aggregate on multi fields.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.UnboundedProcessingOverProcessFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.UnboundedNonPartitionedProcessingOverProcessFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.UnboundedEventTimeOverProcessFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.RowsClauseBoundedOverProcessFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.RangeClauseBoundedOverProcessFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetWindowAggMapFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetPreAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.BoundedProcessingOverRowProcessFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateAggFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-25 01:00:00" id="5916" opendate="2017-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>make env.java.opts.jobmanager and env.java.opts.taskmanager working in YARN mode</summary>
      <description>Now only env.java.opts works in YARN mode, and it applies both to JM and TM. I'd like to make env.java.opts.jobmanager and env.java.opts.taskmanager working in YARN mode in addition, to support fine grained params setting.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnClusterDescriptorTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.BootstrapToolsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-2-9 01:00:00" id="592" opendate="2014-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for secure YARN clusters with Kerberos Auth</summary>
      <description>The current YARN client will throw an exception (as of https://github.com/stratosphere/stratosphere/pull/591) if it detects a secure environment.---------------- Imported from GitHub ----------------Url: https://github.com/stratosphere/stratosphere/issues/592Created by: rmetzgerLabels: enhancement, YARN, Created at: Sun Mar 16 11:05:07 CET 2014State: open</description>
      <version>None</version>
      <fixedVersion>pre-apache</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-dist.src.main.assemblies.yarn.xml</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMaster.scala</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClient.java</file>
      <file type="M">flink-dist.src.main.flink-bin.yarn-bin.yarn-session.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">docs.yarn.setup.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-25 01:00:00" id="5921" opendate="2017-2-25 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Adapt time mode indicator functions return custom data types</summary>
      <description>The functions that indicate event time (rowtime()) and processing time (proctime()) are defined to return TIMESTAMP.These functions should be updated to return custom types in order to ease the identification of the time semantics during optimization.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.LogicalWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.TimeModeIndicatorFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-3-9 01:00:00" id="6005" opendate="2017-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>unit test ArrayList initializations without initial size</summary>
      <description>I found some ArrayList initializations without a sensible initial size although it is possible to select one. The following PR will show some cases that I'd like to fix.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.serialization.LargeRecordsTest.java</file>
      <file type="M">flink-libraries.flink-gelly.src.test.java.org.apache.flink.graph.generator.TestUtils.java</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.test.java.org.apache.flink.graph.library.SummarizationITCase.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.ListSerializerTest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-13 01:00:00" id="6033" opendate="2017-3-13 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support UNNEST query in the stream SQL API</summary>
      <description>It would be nice to support the UNNEST keyword in the stream SQL API. The keyword is widely used in queries that relate to nested fields.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.typeutils.TypeCheckUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.FlinkRelNode.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-4-15 01:00:00" id="6059" opendate="2017-3-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reject DataSet&lt;Row&gt; and DataStream&lt;Row&gt; without RowTypeInformation</summary>
      <description>It is not possible to automatically extract proper type information for Row because it is not typed with generics and holds values in an Object[].Consequently is handled as GenericType&lt;Row&gt; unless a RowTypeInfo is explicitly specified.This can lead to unexpected behavior when converting a DataSet&lt;Row&gt; or DataStream&lt;Row&gt; into a Table. If the data set or data stream has a GenericType&lt;Row&gt;, the rows are treated as atomic type and converted into a single field.I think we should reject input types of GenericType&lt;Row&gt; when converting data sets and data streams and request a proper RowTypeInfo.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.2,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.TableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.TableEnvironmentITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.TableEnvironmentITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-17 01:00:00" id="6089" opendate="2017-3-17 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement decoration phase for rewriting predicated logical plan after volcano optimization phase</summary>
      <description>At present, there is no chance to modify the DataStreamRel tree after the volcano optimization. We consider to add a decoration phase after volcano optimization phase. Decoration phase is dedicated for rewriting predicated logical plan and is independent of cost module. After decoration phase is added, we get the chance to apply retraction rules at this phase.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.CalciteConfig.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-17 01:00:00" id="6090" opendate="2017-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add RetractionRule at the stage of decoration</summary>
      <description>Implement optimizer for retraction: 1.Add RetractionRule at the stage of decoration，which can derive the replace table/append table, NeedRetraction property. 2.Match the NeedRetraction and replace table, mark the accumulating modeWhen this task is finished, we can turn on retraction for different operators according to accumulating mode.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamOverAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupWindowAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamCorrelate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-17 01:00:00" id="6094" opendate="2017-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement stream-stream non-window inner join</summary>
      <description>This includes:1.Implement stream-stream non-window inner join</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.NonWindowHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.JoinHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.RetractionRulesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.validation.JoinValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamWindowJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.typeutils.TypeCheckUtilsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.UpdatingPlanCheckerTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.validation.JoinValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.typeutils.TypeCheckUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.NonWindowInnerJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.CRowKeySelector.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.util.UpdatingPlanChecker.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupWindowAggregate.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-18 01:00:00" id="6111" opendate="2017-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove sleep after python process generation</summary>
      <description>The python api contains some unnecessary (2 second!) sleeps after the python process created. These are now plain unnecessary with the recent refactorings in FLINK-5650.There are furthermore some sleeps after process shutdown, which can be reworked as well.Preliminary tests show that this will have off roughly 40 seconds from the test execution :/</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.plan.PythonPlanStreamer.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.data.PythonStreamer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-18 01:00:00" id="6112" opendate="2017-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Calcite 1.12&amp;#39;s new numerical functions</summary>
      <description>CALCITE-1557 introduces the support of some missing numerical functions.We should add the functions.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.mathExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ConstantCallGen.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-20 01:00:00" id="6124" opendate="2017-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>support max/min aggregations for string type</summary>
      <description>Recently when I port some query to Flink SQL, I found currently min/max aggregations on string type is not supported and should be added.When min/max aggregations are used on string column, return min/max value by lexicographically order.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.MinWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.MaxWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.AggFunctionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunctionWithRetract.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunctionWithRetract.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-20 01:00:00" id="6128" opendate="2017-3-20 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Optimize JVM options for improve test performance</summary>
      <description>Tune JVM options for run tests by maven-surefire-plugin at travis</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-20 01:00:00" id="6134" opendate="2017-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set UUID(0L, 0L) as default leader session id</summary>
      <description>The leader election/retrieval services use the null value as the default leader id in the standalone case but also as no active leader in the ZooKeeper case. This is ambiguous and therefore I propose to change the default leader id to UUID(0L, 0L). Consequently, a null leader id value can then indicate that there is no active leader available.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.UtilsTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.runtime.minicluster.LocalFlinkMiniClusterITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAJobGraphRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingUtils.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.ScalaTestingUtils.scala</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.jobmanager.JobManagerRegistrationTest.scala</file>
      <file type="M">flink-runtime.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerRegistrationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerJobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.JobLeaderIdServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.query.AkkaKvStateLocationLookupServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.TaskManagerMetricsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingLeaderRetrievalService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.StandaloneLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderElectionRetrievalTestingCluster.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerHARecoveryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.client.JobClientActorTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.messages.JobManagerMessages.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.messages.JobClientMessages.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.LeaderSessionMessageFilter.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.StandaloneLeaderRetrievalService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.StandaloneLeaderElectionService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.AkkaActorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.FlinkResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.client.JobClientActor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.akka.FlinkUntypedActor.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.StackTraceSampleCoordinatorITCase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.BackPressureStatsTrackerITCase.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosFlinkResourceManagerTest.java</file>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.ClientTest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-21 01:00:00" id="6139" opendate="2017-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation for building / preparing Flink for MapR</summary>
      <description>MapR users frequently bump into problems with trying to run Flink on YARN in MapR environments. We should have a document for users to reference that answers all these problems once and for all.</description>
      <version>None</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2.pom.xml</file>
      <file type="M">docs.setup.yarn.setup.md</file>
      <file type="M">docs.setup.gce.setup.md</file>
      <file type="M">docs.setup.cluster.setup.md</file>
      <file type="M">docs.setup.aws.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-11-22 01:00:00" id="6163" opendate="2017-3-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document per-window state in ProcessWindowFunction</summary>
      <description>The current windowing documentation mostly describes WindowFunction and treats ProcessWindowFunction as an afterthought. We should reverse that and also document the new per-key state that is only available to ProcessWindowFunction.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.operators.windows.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-23 01:00:00" id="6173" opendate="2017-3-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-table not pack-in com.fasterxml.jackson.* in after #FLINK-5414</summary>
      <description>Currently, flink-table will pack-in com.fasterxml.jackson.* and rename them to org.apache.flink.shaded.calcite.com.fasterxml.jackson.*If a project depends on flink-table, and uses fasterxml as follows(function explain uses fasterxml indirectly):WordCount.scalaobject WordCountWithTable { def main(args: Array[String]): Unit = { // set up execution environment val env = ExecutionEnvironment.getExecutionEnvironment val tEnv = TableEnvironment.getTableEnvironment(env) val input = env.fromElements(WC("hello", 1), WC("hello", 1), WC("ciao", 1)) val expr = input.toTable(tEnv) val result = expr .groupBy('word) .select('word, 'frequency.sum as 'frequency) .filter('frequency === 2) println(tEnv.explain(result)) result.toDataSet[WC].print() } case class WC(word: String, frequency: Long)}It actually uses org.apache.flink.shaded.calcite.com.fasterxml.jackson.*I found after FLINK-5414, flink-table didn't pack-in com.fasterxml.jackson.* and the project would throw class not found exception.Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/flink/shaded/calcite/com/fasterxml/jackson/databind/ObjectMapper at org.apache.flink.table.explain.PlanJsonParser.getSqlExecutionPlan(PlanJsonParser.java:32) at org.apache.flink.table.api.BatchTableEnvironment.explain(BatchTableEnvironment.scala:143) at org.apache.flink.table.api.BatchTableEnvironment.explain(BatchTableEnvironment.scala:164) at org.apache.flink.quickstart.WordCountWithTable$.main(WordCountWithTable.scala:34) at org.apache.flink.quickstart.WordCountWithTable.main(WordCountWithTable.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)Caused by: java.lang.ClassNotFoundException: org.apache.flink.shaded.calcite.com.fasterxml.jackson.databind.ObjectMapper at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 10 more</description>
      <version>None</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-24 01:00:00" id="6181" opendate="2017-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Zookeeper scripts use invalid regex</summary>
      <description>This issue has been reported by a user: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/unable-to-add-more-servers-in-zookeeper-quorum-peers-in-flink-1-2-td12321.html</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.stop-zookeeper-quorum.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-zookeeper-quorum.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-27 01:00:00" id="6198" opendate="2017-3-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the documentation of the CEP library to include all the new features.</summary>
      <description>New features to include: Iterative Functions Quantifiers Time handling Migration from FilterFunction to IterativeCondition</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-28 01:00:00" id="6200" opendate="2017-3-28 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add event time OVER RANGE BETWEEN UNBOUNDED PRECEDING aggregation to SQL</summary>
      <description>The goal of this issue is to add support for OVER RANGE aggregations on event time streams to the SQL interface.Queries similar to the following should be supported:SELECT a, SUM(b) OVER (PARTITION BY c ORDER BY rowTime() RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS sumB, MIN(b) OVER (PARTITION BY c ORDER BY rowTime() RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS minBFROM myStreamThe following restrictions should initially apply:All OVER clauses in the same SELECT clause must be exactly the same.The PARTITION BY clause is optional (no partitioning results in single threaded execution).The ORDER BY clause may only have rowTime() as parameter. rowTime() is a parameterless scalar function that just indicates processing time mode.bounded PRECEDING is not supported (see FLINK-5655)FOLLOWING is not supported.The restrictions will be resolved in follow up issues. If we find that some of the restrictions are trivial to address, we can add the functionality in this issue as well.This issue includes:Design of the DataStream operator to compute OVER ROW aggregatesTranslation from Calcite's RelNode representation (LogicalProject with RexOver expression).</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.UnboundedEventTimeOverProcessFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamOverAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-28 01:00:00" id="6201" opendate="2017-3-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>move python example files from resources to the examples</summary>
      <description>Python example in the resource dir is not suitable. Move them to the examples/python dir.```&lt;fileSet&gt; &lt;directory&gt;../flink-libraries/flink-python/src/main/python/org/apache/flink/python/api&lt;/directory&gt; &lt;outputDirectory&gt;resources/python&lt;/outputDirectory&gt; &lt;fileMode&gt;0755&lt;/fileMode&gt;&lt;/fileSet&gt;```</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-28 01:00:00" id="6203" opendate="2017-3-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DataSet Transformations</summary>
      <description>the example of GroupReduce on sorted groups can't remove duplicate Strings in a DataSet.need to add "prev=t"such as:val output = input.groupBy(0).sortGroup(1, Order.ASCENDING).reduceGroup { (in, out: Collector[(Int, String)]) =&gt; var prev: (Int, String) = null for (t &lt;- in) { if (prev == null || prev != t) out.collect(t) prev=t // this line is missing in the example } }</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.batch.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-29 01:00:00" id="6212" opendate="2017-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing reference to flink-avro dependency</summary>
      <description>In the Connectors page of the Batch (DataSet API) there is a section called "Avro support in Flink"This section mentions the use of certain classes that are part of the flink-avro dependency but this fact is mentioned nowhere. This explanation should be added as well as an xml snippet with the maven dependency as in other parts of the documentation.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.batch.connectors.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-8-3 01:00:00" id="6244" opendate="2017-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Emit timeouted Patterns as Side Output</summary>
      <description>Now that we have SideOuputs I think timeouted patterns should be emitted into them rather than producing a stream of `Either`</description>
      <version>1.3.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPRescalingTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPMigrationTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPMigration11to13Test.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.PatternStream.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.TimeoutKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.KeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.CEPOperatorUtils.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.NFA.java</file>
      <file type="M">flink-libraries.flink-cep-scala.src.test.scala.org.apache.flink.cep.scala.PatternStreamScalaJavaAPIInteroperabilityTest.scala</file>
      <file type="M">flink-libraries.flink-cep-scala.src.main.scala.org.apache.flink.cep.scala.PatternStream.scala</file>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-3 01:00:00" id="6245" opendate="2017-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix late side output documentation in Window documents.</summary>
      <description>There are two things that need to be done:1) in the syntax description in the beginning of the page, we should also include the getSideOutput()2) in the "Getting late data as a side output" section and for the Java example, it should not be a DataStream&lt;T&gt; result ... but a SingleOutputStreamOperator, if we want to get the late event side output.</description>
      <version>1.3.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.operators.windows.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-4 01:00:00" id="6261" opendate="2017-4-4 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add support for TUMBLE, HOP, SESSION to batch SQL</summary>
      <description>Add support for the TUMBLE, HOP, SESSION keywords for batch SQL.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.LogicalWindowAggregateRule.scala</file>
      <file type="M">docs.dev.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-4 01:00:00" id="6265" opendate="2017-4-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix consecutive() for times() pattern.</summary>
      <description>When using next() with times() and times() is not consecutive(), the library ignores that relaxed continuity within the pattern.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFAITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.compiler.NFACompiler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-6 01:00:00" id="6274" opendate="2017-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace usages of org.codehaus.jackson</summary>
      <description>We have a few places left that use org.codehaus.jackson instead of com.fasterxml.jackson.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.optimizer.jsonplan.PreviewPlanDumpTest.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.optimizer.jsonplan.DumpCompiledPlanTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.AbstractDeserializationSchemaTest.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.twitter.TwitterExample.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-7 01:00:00" id="6281" opendate="2017-4-7 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Create TableSink for JDBC</summary>
      <description>It would be nice to integrate the table APIs with the JDBC connectors so that the rows in the tables can be directly pushed into JDBC.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCTestBase.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCOutputFormat.java</file>
      <file type="M">flink-connectors.flink-jdbc.pom.xml</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-4-15 01:00:00" id="6307" opendate="2017-4-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor JDBC tests</summary>
      <description>While glancing over the JDBC related tests I've found a lot of odds things that accumulated over time.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCTestBase.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCOutputFormatTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCInputFormatTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCFullTest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-4-17 01:00:00" id="6313" opendate="2017-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some words was spelled wrong and incorrect LOG.error without print</summary>
      <description>I find some words are spelled wrong and log.error without print information.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnFlinkApplicationMasterRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.query.QueryableStateClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-19 01:00:00" id="6332" opendate="2017-4-19 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Upgrade Scala version to 2.11.11</summary>
      <description>Currently scala-2.11 profile uses Scala 2.11.72.11.11 is the most recent version.This issue is to upgrade to Scala 2.11.11</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-7-22 01:00:00" id="6358" opendate="2017-4-22 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Write job details for Gelly examples</summary>
      <description>Add an option to write job details to a file in JSON format. Job details include: job ID, runtime, parameters with values, and accumulators with values.</description>
      <version>1.3.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.java.org.apache.flink.graph.Runner.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-6-26 01:00:00" id="6389" opendate="2017-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade hbase dependency to 1.3.1</summary>
      <description>hbase 1.3.1 has been released.It fixes compatibility issue in 1.3.0 release, among other bug fixes.We should upgrade to hbase 1.3.1</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-hbase.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-4-26 01:00:00" id="6392" opendate="2017-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change the alias of Window from optional to essential.</summary>
      <description>Currently, The window clause use case looks like as following:tab //Table('a,'b,'c) .window( Slide over 10.milli every 5.milli as 'w) .groupBy('w,'a,'b) .select('a, 'b, 'c.sum, 'w.start, 'w.end)As we see the alias of window is essential. But the current implementation of the TableAPI does not have the constraint for the alias,So we must refactoring the API definition using TYPE SYSTEM lead to constraint for the alias.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.FieldProjectionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.common.WindowStartEndPropertiesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.LogicalWindow.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.groupWindows.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.windows.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.windows.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.java.windows.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-4-28 01:00:00" id="6411" opendate="2017-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>YarnApplicationMasterRunner should not interfere with RunningJobsRegistry</summary>
      <description>The YarnApplicationMasterRunner removes the running job from the RunningJobsRegistry when it is shut down. This should not be its responsibility and rather be delegated to the JobManagerRunner.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnFlinkApplicationMasterRunner.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-5-3 01:00:00" id="6435" opendate="2017-5-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>AsyncWaitOperator does not handle exceptions properly</summary>
      <description>A user reported that the AsyncWaitOperator does not handle exceptions properly. The following code snipped does not make the job fail.public void test() throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream&lt;Integer&gt; withTimestamps = env.fromCollection(Arrays.asList(1,2,3,4,5)); AsyncDataStream.unorderedWait(withTimestamps, (AsyncFunction&lt;Integer, String&gt;) (input, collector) -&gt; { if (input == 3){ collector.collect(new RuntimeException("Test")); return; } collector.collect(Collections.singleton("Ok")); }, 10, TimeUnit.MILLISECONDS) .returns(String.class) .print(); env.execute("unit-test");}</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.async.AsyncWaitOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.async.queue.StreamElementQueueEntry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-3 01:00:00" id="6438" opendate="2017-5-3 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Expand docs home page a little</summary>
      <description>The idea is to improve the documentation home page by adding a few links to valuable items that are too easily overlooked.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-4 01:00:00" id="6443" opendate="2017-5-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more doc links in concepts sections</summary>
      <description>Some sections in the high-level concepts docs don't have any pointers to help you learn more. It can be useful to point people to these concept sections when answering questions on stackoverflow and the mailing list, but that doesn't work well if the writeup there is a dead-end.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.concepts.runtime.md</file>
      <file type="M">docs.concepts.programming-model.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-4 01:00:00" id="6447" opendate="2017-5-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>AWS/EMR docs are out-of-date</summary>
      <description>EMR now has explicit Flink support, so there's no need to install Flink by hand.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.aws.md</file>
      <file type="M">docs.fig.flink-on-emr.png</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-4 01:00:00" id="6448" opendate="2017-5-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web UI TaskManager view: Rename &amp;#39;Free Memory&amp;#39; to &amp;#39;JVM Heap&amp;#39;</summary>
      <description>In the TaskManager view, the laben 'Free Memory' is wrong / misleading and should be 'JVM Heap Size' instead.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanager.taskmanager.metrics.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanager.index.jade</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-4 01:00:00" id="6450" opendate="2017-5-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web UI Subtasks view for TaskManagers has a misleading name</summary>
      <description>The register for the subtasks grouped by TaskManager is simply called TaskManager, which is confusing users. I suggest to rename it to Subtasks by TaskManager.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.jade</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-4 01:00:00" id="6451" opendate="2017-5-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web UI: Rename &amp;#39;Metrics&amp;#39; view to &amp;#39;Task Metrics&amp;#39;</summary>
      <description>In the UI, under the Overview of a specific job, the tab Metrics shows metrics for tasks only, and not all available metrics.We should rename that to Task Metrics. That also differentiates the view clearly from the job-level metrics view proposed in FLINK-6449</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.jade</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-5-8 01:00:00" id="6483" opendate="2017-5-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support time materialization</summary>
      <description>FLINK-5884 added support for time indicators. However, there are still some features missing i.e. materialization of metadata timestamp.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.sources.DefinedTimeAttributes.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.CRowFlatMapRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.CRowCorrelateFlatMapRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.schema.StreamTableSourceTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.schema.RowSchema.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalTableSourceScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.StreamTableSourceScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.CommonCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.CommonCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-5-8 01:00:00" id="6501" opendate="2017-5-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make sure NOTICE files are bundled into shaded JAR files</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-9 01:00:00" id="6504" opendate="2017-5-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lack of synchronization on materializedSstFiles in RocksDBKEyedStateBackend</summary>
      <description>Concurrent checkpoints could access `materializedSstFiles` in the `RocksDBStateBackend` concurrently. This should be avoided.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-9 01:00:00" id="6506" opendate="2017-5-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some tests in flink-tests exceed the memory resources in containerized Travis builds</summary>
      <description>The tests in flink-tests are currently executed on Travis with fork parallelism of 2. It seems that the memory requirements for certain tests have become so high, that certain combinations for two tests in parallel can exceed the available resources on the containerized Travis build infrastructure, triggering the OOM Killer.I think that we can reduce the fork parallelism to 1 for our Travis builds. Tests seem to execute at comparable speed as before without running into resource problems.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">flink-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-9 01:00:00" id="6508" opendate="2017-5-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include license files of packaged dependencies</summary>
      <description>The Maven artifact for flink-table bundles its (non-Flink) dependencies to have a self-contained JAR file that can be moved to the ./lib folder without adding additional dependencies.Currently, we include Apache Calcite, Guava (relocates and required by Calcite), Janino, and Reflections.Janino and Reflections are not under Apache license, so we need to include their license files into the JAR file.</description>
      <version>1.2.1,1.3.0,1.4.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-9 01:00:00" id="6512" opendate="2017-5-9 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>some code examples are poorly formatted</summary>
      <description>Some code examples in the docs are hard to read, mostly because the code highlighting plugin was overlooked.</description>
      <version>1.2.1,1.3.0</version>
      <fixedVersion>1.2.2,1.3.0,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.migration.md</file>
      <file type="M">docs.dev.best.practices.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-9 01:00:00" id="6513" opendate="2017-5-9 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>various typos and grammatical flaws</summary>
      <description>I want to propose small changes to several pages to fix some typos and grammatical flaws.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.internals.stream.checkpointing.md</file>
      <file type="M">docs.dev.stream.side.output.md</file>
      <file type="M">docs.dev.stream.process.function.md</file>
      <file type="M">docs.dev.stream.checkpointing.md</file>
      <file type="M">docs.dev.best.practices.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-10 01:00:00" id="6517" opendate="2017-5-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support multiple consecutive windows</summary>
      <description>FLINK-5884 changed the way how windows can be defined, however, it is not possible to define multiple consecutive windows right now. It should be possible to refine the end property of a window as a new time attribute.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverterTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.typeutils.TimeIndicatorTypeInfo.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupWindowAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.LogicalWindow.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.windowProperties.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.fieldExpression.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-7-11 01:00:00" id="6539" opendate="2017-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add automated end-to-end tests</summary>
      <description>We should add simple tests that exercise all the paths that a user would use when starting a cluster and submitting a program. Preferably with a simple batch program and a streaming program that uses Kafka.This would have catched some of the bugs that we now discovered right before the release.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.kafka.WriteIntoKafka.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.scala.org.apache.flink.streaming.scala.examples.kafka.ReadFromKafka.scala</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.kafka.WriteIntoKafka.java</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.java.org.apache.flink.streaming.examples.kafka.ReadFromKafka.java</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">test-infra.end-to-end-test.common.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-11 01:00:00" id="6543" opendate="2017-5-11 00:00:00" resolution="Done">
    <buginformation>
      <summary>Deprecate toDataStream</summary>
      <description>With retraction support, we should deprecate toDataStream and introduce a new toAppendStream to clearly differentiate between retraction and non-retraction.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.DataStreamUserDefinedFunctionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.DataStreamCalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.DataStreamAggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.UnionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupWindowAggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.stream.sql.SqlITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.TableConversions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.package.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.java.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.flink.table.api.java.package-info.java</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.StreamTableExample.scala</file>
      <file type="M">flink-examples.flink-examples-table.src.main.scala.org.apache.flink.table.examples.scala.StreamSQLExample.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-11 01:00:00" id="6549" opendate="2017-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve error message for type mismatches with side outputs</summary>
      <description>A type mismatch when using side outputs causes a ClassCastException to be thrown. It would be neat to include the name of the OutputTags in the exception message.This can occur when multiple {{OutputTag]}s with different types but identical names are being used.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-11 01:00:00" id="6552" opendate="2017-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Side outputs don&amp;#39;t allow differing output types</summary>
      <description>When calling {SingleOutputStreamOperator#getSideOutput(OutputTag&lt;X&gt;} multiple times with the output tags having different types you get the following exception: "Trying to add a side input for the same id with a different type. This is not allowed." This error message is ambiguous, as it could either mean that you cannot add 2 side outputs with the same name but different types or that 2 side outputs with different types cannot be retrieved from a single operator.Furthermore, the error message contains the concept of node id's (i guess?) which users aren't exposed to. This is confusing and should be reworded to work with operators.Lastly, i find this limitation rather odd. It is possible for an operator to have multiple side outputs. It is also possible to have a side output with a different type than the main output. Yet, it is not possible to have multiple side outputs with different types.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.util.TestListResultSink.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.SideOutputITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-11 01:00:00" id="6560" opendate="2017-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Restore maven parallelism in flink-tests</summary>
      <description>FLINK-6506 added the maven variable flink.forkCountTestPackage which is used by the TravisCI script but no default value is set.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-11 01:00:00" id="6562" opendate="2017-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support implicit table references for nested fields in SQL</summary>
      <description>Currently nested fields can only be accessed through fully qualified identifiers. For example, users need to specify the following query for the table f that has a nested field foo.barSELECT f.foo.bar FROM fOther query engines like Hive / Presto supports implicit table references. For example:SELECT foo.bar FROM fThis jira proposes to support the latter syntax in the SQL API.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.CompositeAccessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.schema.CompositeRelDataType.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-7-13 01:00:00" id="6575" opendate="2017-5-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable all tests on Windows that use HDFS</summary>
      <description>Similar reasoning as FLINK-6558.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.HDFSTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.FsNegativeRunningJobsRegistryTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.FileStateBackendTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.ContinuousFileProcessingTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.ContinuousFileProcessingMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.bucketing.RollingToBucketingMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.bucketing.RollingSinkMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSinkMigrationTest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-15 01:00:00" id="6580" opendate="2017-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink on YARN doesnt start with default parameters</summary>
      <description>Just doing ./bin/yarn-session.sh -n 1 fails with Error while deploying YARN cluster: Couldn't deploy Yarn clusterjava.lang.RuntimeException: Couldn't deploy Yarn cluster at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deploy(AbstractYarnClusterDescriptor.java:436) at org.apache.flink.yarn.cli.FlinkYarnSessionCli.run(FlinkYarnSessionCli.java:626) at org.apache.flink.yarn.cli.FlinkYarnSessionCli$1.call(FlinkYarnSessionCli.java:482) at org.apache.flink.yarn.cli.FlinkYarnSessionCli$1.call(FlinkYarnSessionCli.java:479) at org.apache.flink.runtime.security.HadoopSecurityContext$1.run(HadoopSecurityContext.java:43) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:40) at org.apache.flink.yarn.cli.FlinkYarnSessionCli.main(FlinkYarnSessionCli.java:479)Caused by: java.lang.IllegalArgumentException: The configuration value 'containerized.heap-cutoff-min' is higher (600) than the requested amount of memory 256 at org.apache.flink.yarn.Utils.calculateHeapSize(Utils.java:100) at org.apache.flink.yarn.AbstractYarnClusterDescriptor.setupApplicationMasterContainer(AbstractYarnClusterDescriptor.java:1263) at org.apache.flink.yarn.AbstractYarnClusterDescriptor.startAppMaster(AbstractYarnClusterDescriptor.java:803) at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deployInternal(AbstractYarnClusterDescriptor.java:568) at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deploy(AbstractYarnClusterDescriptor.java:434) ... 9 moreI think this issue has been introduced in FLINK-5904.Flink on YARN is now using the configuration parameters from the configuration file.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-15 01:00:00" id="6582" opendate="2017-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Project from maven archetype is not buildable by default due to ${scala.binary.version}</summary>
      <description>When creating a java project from maven-archetype dependencies to flink are unresolvable due to ${scala.binary.version} placeholder.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-15 01:00:00" id="6583" opendate="2017-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable QueryConfig in count base GroupWindow</summary>
      <description>Enable QueryConfig in count base GroupWindow by Add a custom Trigger `CountTriggerWithCleanupState`. See more in FLINK-6491.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupWindowAggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupWindowAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-15 01:00:00" id="6584" opendate="2017-5-15 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support multiple consecutive windows in SQL</summary>
      <description>Right now, the Table API supports multiple consecutive windows as follows:val table = stream.toTable(tEnv, 'rowtime.rowtime, 'int, 'double, 'float, 'bigdec, 'string)val t = table .window(Tumble over 2.millis on 'rowtime as 'w) .groupBy('w) .select('w.rowtime as 'rowtime, 'int.count as 'int) .window(Tumble over 4.millis on 'rowtime as 'w2) .groupBy('w2) .select('w2.rowtime, 'w2.end, 'int.count)Similar behavior should be supported by the SQL API as well. We need to introduce a new auxiliary group function, but this should happen in sync with Apache Calcite.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.validation.GroupWindowValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.common.WindowStartEndPropertiesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.common.LogicalWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupWindowAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalWindowAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-15 01:00:00" id="6585" opendate="2017-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table examples are not runnable in IDE</summary>
      <description>Running Table API examples in flink-examples-table fails with:Caused by: java.lang.ClassNotFoundException: org.apache.flink.table.api.TableEnvironmentSeems to be a Maven issue.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-15 01:00:00" id="6590" opendate="2017-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate generated tables into documentation</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">flink-libraries.flink-python.pom.xml</file>
      <file type="M">flink-docs.README.md</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.ConfigOptionsDocGeneratorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.ConfigDocsCompletenessChecker.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.QueryableStateOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.MetricOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HistoryServerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">docs.page.css.flink.css</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-16 01:00:00" id="6602" opendate="2017-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table source with defined time attributes allows empty string</summary>
      <description>DefinedRowtimeAttribute and DefinedProctimeAttribute are not checked for empty strings.</description>
      <version>None</version>
      <fixedVersion>1.3.1,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.schema.StreamTableSourceTable.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-17 01:00:00" id="6614" opendate="2017-5-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Applying function on window auxiliary function fails</summary>
      <description>SQL queries that apply a function or expression on a window auxiliary function (TUMBLE_START, TUMBLE_END, HOP_START, etc). cannot be translated and fail with a CodeGenException:Exception in thread "main" org.apache.flink.table.codegen.CodeGenException: Unsupported call: TUMBLE_ENDExample query:SELECT a, toLong(TUMBLE_END(rowtime, INTERVAL '10' MINUTE)) AS t, COUNT(b) AS cntBFROM myTableGROUP BY a, TUMBLE(rowtime, INTERVAL '10' MINUTE)</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.common.WindowStartEndPropertiesRule.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-17 01:00:00" id="6616" opendate="2017-5-17 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Clarify provenance of official Docker images</summary>
      <description>Note that the official Docker images for Flink are community supported and not an official release of the Apache Flink PMC.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.docker.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-18 01:00:00" id="6617" opendate="2017-5-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve JAVA and SCALA logical plans consistent test</summary>
      <description>Currently,we need some `StringExpression` test,for all JAVA and SCALA API.Such as:`GroupAggregations`,`GroupWindowAggregaton`(Session,Tumble),`Calc` etc.</description>
      <version>1.3.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.OverWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.UserDefinedFunctionTestUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.CommonTestData.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sources.validation.TableSourceBalidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sources.TableSourceTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sources.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sources.stream.table.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sources.stream.sql.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sources.batch.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sinks.validation.TableSinksValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.table.UserDefinedFunctionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.table.UnionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.table.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.table.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.table.TableSinksITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.table.RetractionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.table.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.table.GroupWindowAggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.table.GroupAggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.StreamITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.StreamingWithStateTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.TableSinkITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.TableEnvironmentITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.DataSetWindowAggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.DataSetUserDefinedFunctionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.DataSetCalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.CastingITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.sql.TableWithSQLITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.sql.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.sql.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.sql.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggregate.TimeSortProcessFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.util.RexProgramTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.util.RexProgramRewriterTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.util.RexProgramExtractorTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.SumWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.SumAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.Sum0WithRetractAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.Sum0AggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.MinWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.MaxWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.CountAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.AvgFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.functions.aggfunctions.AggFunctionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.validation.ScalarOperatorsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.validation.ScalarFunctionsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.validation.CompositeAccessValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.UserDefinedScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.ScalarOperatorsTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.ScalarFunctionsTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.MapTypeTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.CompositeAccessTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.UserDefinedScalarFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.stream.table.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.stream.sql.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.NonDeterministicTests.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.batch.table.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.batch.sql.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.catalog.utils.ExternalCatalogTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.catalog.stream.table.ExternalCatalogTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.catalog.stream.sql.ExternalCatalogTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.catalog.ExternalCatalogSchemaTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.catalog.batch.table.ExternalCatalogTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.catalog.batch.sql.ExternalCatalogTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.calcite.table.RelTimeIndicatorConverterTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.calcite.sql.RelTimeIndicatorConverterTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.calcite.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.validation.TableSchemaValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.validation.TableEnvironmentValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.utils.StreamTestData.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.validation.UserDefinedTableFunctionValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.validation.UnsupportedOpsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.validation.UnionValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.validation.TimeAttributesValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.validation.TableSourceValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.validation.TableSinksValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.validation.OverWindowValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.validation.GroupWindowAggregationsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.validation.GroupAggregationsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.stringexpr.UserDefinedTableFunctionStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.stringexpr.UnionStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.stringexpr.GroupAggregationsStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.stringexpr.CalcStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupWindowAggregationsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.FieldProjectionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.ExplainTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.TableSchemaTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.TableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.validation.WindowAggregateValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.validation.OverWindowValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.validation.AggregationsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.TimeTestUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SortTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.OverWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.validation.TableEnvironmentValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.utils.TableProgramsTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.utils.TableProgramsCollectionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.utils.SortTestUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.utils.LogicalPlanFormatUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.validation.GroupWindowValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.validation.CompositeFlatteningValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.stringexpr.UserDefinedTableFunctionStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.ExplainTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.CompositeFlatteningTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.AggregationsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.TableSchemaTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.TableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.validation.WindowAggregateValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.validation.SortValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.validation.JoinValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.validation.CalcValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.validation.AggregationsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.GroupingSetsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.DistinctAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.DataSetSingleRowJoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.CompositeFlatteningTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.utils.UserDefinedTableFunctions.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.utils.UserDefinedScalarFunctions.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.utils.UserDefinedAggFunctions.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.utils.Pojos.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.stream.utils.StreamTestData.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.TableSourceITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.TableEnvironmentITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.sql.SqlITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.sql.GroupingSetsITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.TableSchemaTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.TableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sinks.StreamTableSinksITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.DataStreamUserDefinedFunctionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.DataStreamCalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.DataStreamAggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.DataSetWindowAggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.DataSetUserDefinedFunctionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.dataset.DataSetCalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.rules.RetractionRulesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.rules.NormalizationRulesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.ExternalCatalogTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.MapTypeTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.CompositeAccessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ArrayTypeTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.CompositeFlatteningTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverterTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.utils.StreamITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.utils.StreamingWithStateTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.UnsupportedOpsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.UnionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.stringexpr.OverWindowStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.stringexpr.GroupWindowStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.stream.sql.SqlITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.AggregationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.ExplainTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.AggregationsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.QueryDecorrelationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.TableWithSQLITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.TableEnvironmentITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.TableSinkITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.AggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.CastingITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.FieldProjectionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.SortITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.stringexpr.AggregationsStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.stringexpr.CalcStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.stringexpr.CastingStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.stringexpr.JoinStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.UserDefinedTableFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.validation.AggregationsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.validation.CalcValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.validation.JoinValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.validation.SetOperatorsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.validation.SortValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.utils.TableProgramsClusterTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.ExplainStreamTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.RetractionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.AggregationsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.TableSinkITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupAggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupAggregationsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupWindowAggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.OverWindowITCase.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-19 01:00:00" id="6632" opendate="2017-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix parameter case sensitive error for test passing/rejecting filter API</summary>
      <description>TableAPI testAllPassingFilter: val t = util.addTable[(Int, Long, String)]('int, 'long, 'string) val resScala = t.filter(Literal(true)).select('int as 'myInt, 'string) val resJava = t.filter("TrUe").select("int as myInt, string")We got error:org.apache.flink.table.api.ValidationException: Cannot resolve [TrUe] given input [int, long, string].The error is caused by : lazy val boolLiteral: PackratParser[Expression] = ("true" | "false") ^^ { str =&gt; Literal(str.toBoolean) }I want improve the method as follow: lazy val boolLiteral: PackratParser[Expression] = ("(t|T)(r|R)(u|U)(e|E)".r | "(f|F)(a|A)(l|L)(s|S)(e|E)".r) ^^ { str =&gt; Literal(str.toBoolean)}Is there any drawback to this improvement? Welcome anyone feedback ?</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.stringexpr.CalcStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.ExpressionParser.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-22 01:00:00" id="6656" opendate="2017-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate CEP PriorityQueue to MapState</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.resources.cep-non-keyed-1.1-snapshot</file>
      <file type="M">flink-libraries.flink-cep.src.test.resources.cep-keyed-1.1-snapshot</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPMigration11to13Test.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-22 01:00:00" id="6660" opendate="2017-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>expand the streaming connectors overview page</summary>
      <description>The overview page for streaming connectors is too lean &amp;#8211; it should provide more context and also guide the reader toward related topics.Note that FLINK-6038 will add links to the Bahir connectors.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.twitter.md</file>
      <file type="M">docs.dev.connectors.index.md</file>
      <file type="M">docs.dev.connectors.filesystem.sink.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-23 01:00:00" id="6669" opendate="2017-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Build] Scala style check errror on Windows</summary>
      <description>When build the source code on Windows, a scala style check error happend.Here is the error messages.&amp;#91;INFO&amp;#93;&amp;#91;INFO&amp;#93; &amp;#8212; scalastyle-maven-plugin:0.8.0:check (default) @ flink-scala_2.10 &amp;#8212;error file=E:\github\flink\flink-scala\src\main\scala\org\apache\flink\api\scala\utils\package.scala message=Input length = 2Saving to outputFile=E:\github\flink\flink-scala\target\scalastyle-output.xmlProcessed 78 file(s)Found 1 errorsFound 0 warningsFound 0 infosFinished in 1189 ms&amp;#91;INFO&amp;#93; ------------------------------------------------------------------------&amp;#91;INFO&amp;#93; Reactor Summary:&amp;#91;INFO&amp;#93;&amp;#91;INFO&amp;#93; force-shading ...................................... SUCCESS [ 37.206 s]&amp;#91;INFO&amp;#93; flink .............................................. SUCCESS &amp;#91;03:27 min&amp;#93;&amp;#91;INFO&amp;#93; flink-annotations .................................. SUCCESS [ 3.020 s]&amp;#91;INFO&amp;#93; flink-shaded-hadoop ................................ SUCCESS [ 0.928 s]&amp;#91;INFO&amp;#93; flink-shaded-hadoop2 ............................... SUCCESS [ 15.314 s]&amp;#91;INFO&amp;#93; flink-shaded-hadoop2-uber .......................... SUCCESS [ 13.085 s]&amp;#91;INFO&amp;#93; flink-shaded-curator ............................... SUCCESS [ 0.234 s]&amp;#91;INFO&amp;#93; flink-shaded-curator-recipes ....................... SUCCESS [ 3.336 s]&amp;#91;INFO&amp;#93; flink-shaded-curator-test .......................... SUCCESS [ 2.948 s]&amp;#91;INFO&amp;#93; flink-metrics ...................................... SUCCESS [ 0.286 s]&amp;#91;INFO&amp;#93; flink-metrics-core ................................. SUCCESS [ 9.065 s]&amp;#91;INFO&amp;#93; flink-test-utils-parent ............................ SUCCESS [ 0.327 s]&amp;#91;INFO&amp;#93; flink-test-utils-junit ............................. SUCCESS [ 1.452 s]&amp;#91;INFO&amp;#93; flink-core ......................................... SUCCESS [ 54.277 s][INFO] flink-java ......................................... SUCCESS [ 25.244 s]&amp;#91;INFO&amp;#93; flink-runtime ...................................... SUCCESS &amp;#91;03:08 min&amp;#93;&amp;#91;INFO&amp;#93; flink-optimizer .................................... SUCCESS [ 14.540 s]&amp;#91;INFO&amp;#93; flink-clients ...................................... SUCCESS [ 14.457 s]&amp;#91;INFO&amp;#93; flink-streaming-java ............................... SUCCESS [ 58.130 s]&amp;#91;INFO&amp;#93; flink-test-utils ................................... SUCCESS [ 19.906 s]&amp;#91;INFO&amp;#93; flink-scala ........................................ FAILURE [ 56.634 s]&amp;#91;INFO&amp;#93; flink-runtime-web .................................. SKIPPEDI think this is caused by the Windows default encoding. When I set the inputEncoding to UTF-8 in scalastyle-maven-plugin, the error don't happen.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.1,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-23 01:00:00" id="6674" opendate="2017-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update release 1.3 docs</summary>
      <description>Umbrella issue to track required updates to the documentation for the 1.3 release.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.migration.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-5-23 01:00:00" id="6691" opendate="2017-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add checkstyle import block rule for scala imports</summary>
      <description>Similar to java and javax imports we should give scala imports a separate import block.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.strict-checkstyle.xml</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.util.typeutils.FieldAccessor.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.runtime.jobmanager.JMXJobManagerMetricTest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-10-24 01:00:00" id="6697" opendate="2017-5-24 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add batch multi-window support</summary>
      <description>Multiple consecutive windows on batch are not tested yet and I think they are also not supported, because the syntax is not defined for batch yet.The following should be supported:val t = table .window(Tumble over 2.millis on 'rowtime as 'w) .groupBy('w) .select('w.rowtime as 'rowtime, 'int.count as 'int) .window(Tumble over 4.millis on 'rowtime as 'w2) .groupBy('w2) .select('w2.rowtime, 'w2.end, 'int.count)</description>
      <version>1.3.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.GroupWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.GroupWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.sql.AggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.validation.GroupWindowValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.TimeWindowPropertyCollector.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceCombineFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSlideWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSlideWindowAggReduceCombineFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSessionWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.common.WindowPropertiesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.fieldExpression.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.calcite.sql.fun.SqlGroupFunction.java</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-10-24 01:00:00" id="6703" opendate="2017-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document how to take a savepoint on YARN</summary>
      <description>The documentation should have a separate entry for savepoint related CLI commands in combination with YARN. It is currently not documented that you have to supply the application id, nor how you can pass it../bin/flink savepoint &lt;jobID&gt; -m yarn-cluster (-yid|-yarnapplicationId) &lt;appID&gt;</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.savepoints.md</file>
      <file type="M">docs.ops.cli.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-24 01:00:00" id="6704" opendate="2017-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot disable YARN user jar inclusion</summary>
      <description/>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-27 01:00:00" id="6736" opendate="2017-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix UDTF codegen bug when window follow by join( UDTF)</summary>
      <description>When we run the tableAPI as follows:val table = stream.toTable(tEnv, 'long.rowtime, 'int, 'double, 'float, 'bigdec, 'date,'pojo, 'string) val windowedTable = table .join(udtf2('string) as ('a, 'b)) .window(Slide over 5.milli every 2.milli on 'long as 'w) .groupBy('w) .select('int.count, agg1('pojo, 'bigdec, 'date, 'int), 'w.start, 'w.end)We will get the error message:org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply$mcV$sp(JobManager.scala:933) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply(JobManager.scala:876) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply(JobManager.scala:876) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.codegen.Compiler$class.compile(Compiler.scala:36) at org.apache.flink.table.runtime.CRowCorrelateProcessRunner.compile(CRowCorrelateProcessRunner.scala:35) at org.apache.flink.table.runtime.CRowCorrelateProcessRunner.open(CRowCorrelateProcessRunner.scala:59) at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36) at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:111) at org.apache.flink.streaming.api.operators.ProcessOperator.open(ProcessOperator.java:56) at org.apache.flink.streaming.runtime.tasks.StreamTask.openAllOperators(StreamTask.java:377) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:254) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:702) at java.lang.Thread.run(Thread.java:745)Caused by: org.codehaus.commons.compiler.CompileException: Line 77, Column 62: Unknown variable or type "in2" at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:11523) at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6292) at org.codehaus.janino.UnitCompiler.access$12900(UnitCompiler.java:209) at org.codehaus.janino.UnitCompiler$18.visitPackage(UnitCompiler.java:5904) at org.codehaus.janino.UnitCompiler$18.visitPackage(UnitCompiler.java:5901) at org.codehaus.janino.Java$Package.accept(Java.java:4074) at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:5901) at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6287) at org.codehaus.janino.UnitCompiler.access$13500(UnitCompiler.java:209)The reason is val generator = new CodeGenerator(config, false, inputSchema.physicalTypeInfo) `physicalTypeInfo` will remove the TimeIndicator.I think we should fix this. What do you think Fabian Hueske Timo Walther , And hope your suggestions.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.datastream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverterTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.CommonCorrelate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-27 01:00:00" id="6737" opendate="2017-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix over expression parse String error.</summary>
      <description>When we run the TableAPI as follows:val windowedTable = table .window(Over partitionBy 'c orderBy 'proctime preceding UNBOUNDED_ROW as 'w) .select('c, "countFun(b)" over 'w as 'mycount, weightAvgFun('a, 'b) over 'w as 'wAvg)We get the error:org.apache.flink.table.api.TableException: The over method can only using with aggregation expression. at org.apache.flink.table.api.scala.ImplicitExpressionOperations$class.over(expressionDsl.scala:469) at org.apache.flink.table.api.scala.ImplicitExpressionConversions$LiteralStringExpression.over(expressionDsl.scala:756)The reason is, the `over` method of `expressionDsl` not parse the String case.I think we should fix this before 1.3 release.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-28 01:00:00" id="6753" opendate="2017-5-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flaky SqlITCase</summary>
      <description>Tests run: 11, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 6.674 sec &lt;&lt;&lt; FAILURE! - in org.apache.flink.table.api.scala.stream.sql.SqlITCasetestUnnestArrayOfArrayFromTable(org.apache.flink.table.api.scala.stream.sql.SqlITCase) Time elapsed: 0.289 sec &lt;&lt;&lt; ERROR!org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$6.apply$mcV$sp(JobManager.scala:933) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$6.apply(JobManager.scala:876) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$6.apply(JobManager.scala:876) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.codegen.Compiler$class.compile(Compiler.scala:36) at org.apache.flink.table.runtime.CRowCorrelateProcessRunner.compile(CRowCorrelateProcessRunner.scala:35) at org.apache.flink.table.runtime.CRowCorrelateProcessRunner.open(CRowCorrelateProcessRunner.scala:59) at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36) at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:111) at org.apache.flink.streaming.api.operators.ProcessOperator.open(ProcessOperator.java:56) at org.apache.flink.streaming.runtime.tasks.StreamTask.openAllOperators(StreamTask.java:377) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:254) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:702) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.AssertionError: null at org.codehaus.janino.IClass.isAssignableFrom(IClass.java:652) at org.codehaus.janino.UnitCompiler.isWideningReferenceConvertible(UnitCompiler.java:10844) at org.codehaus.janino.UnitCompiler.isMethodInvocationConvertible(UnitCompiler.java:9010) at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:8799) at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:8657) at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:8539) at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:8441) at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4609) at org.codehaus.janino.UnitCompiler.access$8200(UnitCompiler.java:209) at org.codehaus.janino.UnitCompiler$12.visitMethodInvocation(UnitCompiler.java:3969) at org.codehaus.janino.UnitCompiler$12.visitMethodInvocation(UnitCompiler.java:3942) at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:4874) at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3942) at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5125) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3343) at org.codehaus.janino.UnitCompiler.access$5000(UnitCompiler.java:209) at org.codehaus.janino.UnitCompiler$9.visitMethodInvocation(UnitCompiler.java:3322) at org.codehaus.janino.UnitCompiler$9.visitMethodInvocation(UnitCompiler.java:3294) at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:4874) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3294) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2214) at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:209) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1445) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1438) at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2848) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1438) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1518) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2950) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1308) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1281) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:780) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:431) at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:209) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:385) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:380) at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1405) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:380) at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:354) at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:413) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:209) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:200) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:76) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:71) at org.apache.flink.table.codegen.Compiler$class.compile(Compiler.scala:33) at org.apache.flink.table.runtime.CRowCorrelateProcessRunner.compile(CRowCorrelateProcessRunner.scala:35) at org.apache.flink.table.runtime.CRowCorrelateProcessRunner.open(CRowCorrelateProcessRunner.scala:59) at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36) at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:111) at org.apache.flink.streaming.api.operators.ProcessOperator.open(ProcessOperator.java:56) at org.apache.flink.streaming.runtime.tasks.StreamTask.openAllOperators(StreamTask.java:377) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:254) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:702) at java.lang.Thread.run(Thread.java:745)</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-29 01:00:00" id="6756" opendate="2017-5-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide RichAsyncFunction to Scala API suite</summary>
      <description>I can't find any tracking info about the chance to have RichAsyncFunction in the Scala API suite. I think it'd be nice to have this function in order to access open/close methods and the RuntimeContext.I was able to retrieve http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/There-is-no-Open-and-Close-method-in-Async-I-O-API-of-Scala-td11591.html#a11593 only, so my question is if there are some blocking issues avoiding this feature. &amp;#91;~till.rohrmann&amp;#93;If it's possible and nobody already have done it, I can assign the issue to myself in order to implement it.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.AsyncDataStreamITCase.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.async.JavaResultFutureWrapper.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.AsyncDataStream.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-6-31 01:00:00" id="6782" opendate="2017-5-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update savepoint documentation</summary>
      <description>Savepoint documentation is a bit outdated regarding full data being stored in the savepoint path, not just a metadata file</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.savepoints.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-1 01:00:00" id="6805" opendate="2017-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink Cassandra connector dependency on Netty disagrees with Flink</summary>
      <description>The Flink Cassandra connector has a dependency on Netty libraries (via promotion of transitive dependencies by the Maven shade plugin) at version 4.0.33.Final, which disagrees with the version included in Flink of 4.0.27.Final which is included &amp; managed by the parent POM via dependency on netty-all.Due to use of netty-all, the dependency management doesn't take effect on the individual libraries such as netty-handler, netty-codec, etc.I suggest that dependency management of Netty should be added for all Netty libraries individually (netty-handler, etc.) so that all Flink modules use the same version, and similarly I suggest that exclusions be added to the quickstart example POM for the individual Netty libraries so that fat JARs don't include conflicting versions of Netty.It seems like this problem started when FLINK-6084 was implemented: transitive dependencies of the flink-connector-cassandra were previously omitted, and now that they are included we must make sure that they agree with the Flink distribution.</description>
      <version>1.2.1,1.3.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-2 01:00:00" id="6812" opendate="2017-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elasticsearch 5 release artifacts not published to Maven central</summary>
      <description>Release artifacts for the Elasticsearch 5 connector is not published to the Maven Central. Elasticsearch 5 requires Java 8 at minimum, so for the release we need to build with Java 8 for this.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.1,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-7-12 01:00:00" id="6898" opendate="2017-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Limit size of operator component in metric name</summary>
      <description>The operator name for some operators (specifically windows) can be very, very long (250+) characters.I propose to limit the total space that the operator component can take up in a metric name to 60 characters.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.TaskMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.AbstractMetricGroup.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-16 01:00:00" id="6937" opendate="2017-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix link markdown in Production Readiness Checklist doc</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.production.ready.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-18 01:00:00" id="6940" opendate="2017-6-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clarify the effect of configuring per-job state backend</summary>
      <description>The documentation of having different options configuring flink state backend is confusing. We should add explicit doc explaining configuring a per-job flink state backend in code will overwrite any default state backend configured in flink-conf.yaml</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.backends.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-20 01:00:00" id="6951" opendate="2017-6-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incompatible versions of httpcomponents jars for Flink kinesis connector</summary>
      <description>In the following thread, Bowen reported incompatible versions of httpcomponents jars for Flink kinesis connector :http://search-hadoop.com/m/Flink/VkLeQN2m5EySpb1?subj=Re+Incompatible+Apache+Http+lib+in+Flink+kinesis+connectorWe should find a solution such that users don't have to change dependency version(s) themselves when building Flink kinesis connector.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.4,1.4.1,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-20 01:00:00" id="6952" opendate="2017-6-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add link to Javadocs</summary>
      <description>The project webpage and the docs are missing links to the Javadocs.I think we should add them as part of the external links at the bottom of the doc navigation (above "Project Page").In the same manner we could add a link to the Scaladocs, but if I remember correctly there was a problem with the build of the Scaladocs. Correct, Aljoscha Krettek?</description>
      <version>None</version>
      <fixedVersion>1.2.2,1.3.1,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.create.release.files.sh</file>
      <file type="M">docs..layouts.base.html</file>
      <file type="M">docs..includes.sidenav.html</file>
      <file type="M">docs..config.yml</file>
      <file type="M">docs.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-21 01:00:00" id="6965" opendate="2017-6-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avro is missing snappy dependency</summary>
      <description>The shading rework made before 1.3 removed a snappy dependency that was accidentally pulled in through hadoop. This is technically alright, until class-loaders rear their ugly heads.Our kafka connector can read avro records, which may or may not require snappy. Usually this should be solvable by including the snappy dependency in the user-jar if necessary, however since the kafka connector loads classes that it requires using the system class loader this doesn't work.As such we have to add a separate snappy dependency to flink-core.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-22 01:00:00" id="6985" opendate="2017-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove bugfix version from docs title</summary>
      <description>The docs HTML title contains the minor version of the corresponding release. This can be confusing as we build the docs nightly from the respective release branch.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..layouts.base.html</file>
      <file type="M">docs..includes.sidenav.html</file>
      <file type="M">docs..config.yml</file>
      <file type="M">docs.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-23 01:00:00" id="6994" opendate="2017-6-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong base url in master docs</summary>
      <description>The base url of the master docs point to 1.3 instead of 1.4. At the moment the menu items point to the latest stable release docs instead of the nightly master docs.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..config.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-6-26 01:00:00" id="7004" opendate="2017-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Switch to Travis Trusty image</summary>
      <description>As shown in this PR https://github.com/apache/flink/pull/4167 switching to the Trusty image on Travis seems to stabilize the build times.We should switch for 1.2, 1.3 and 1.4.</description>
      <version>1.2.0,1.3.0,1.4.0</version>
      <fixedVersion>1.2.2,1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-26 01:00:00" id="7005" opendate="2017-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimization steps are missing for nested registered tables</summary>
      <description>Tables that are registered (implicitly or explicitly) do not pass the first three optimization steps: decorrelate convert time indicators normalize the logical planE.g. this has the wrong plan right now:val table = stream.toTable(tEnv, 'rowtime.rowtime, 'int, 'double, 'float, 'bigdec, 'string)val table1 = tEnv.sql(s"""SELECT 1 + 1 FROM $table""") // not optimizedval table2 = tEnv.sql(s"""SELECT myrt FROM $table1""")val results = table2.toAppendStream[Row]</description>
      <version>1.3.0,1.3.1</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.rules.NormalizationRulesTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-28 01:00:00" id="7034" opendate="2017-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>GraphiteReporter cannot recover from lost connection</summary>
      <description>Now Flink uses metric version 1.3.0 in which there is a Bug. I think you should use version 1.3.1 or higher</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-29 01:00:00" id="7044" opendate="2017-6-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add methods to the client API that take the stateDescriptor.</summary>
      <description/>
      <version>1.3.0,1.3.1</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.query.AbstractQueryableStateITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.query.QueryableStateClientTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.query.QueryableStateClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-30 01:00:00" id="7058" opendate="2017-6-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-scala-shell unintended dependencies for scala 2.11</summary>
      <description>Activation of profile scala-2.10 in `flink-scala-shell` and `flink-scala` do not work as intended. &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;scala-2.10&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;!scala-2.11&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scalamacros&lt;/groupId&gt; &lt;artifactId&gt;quasiquotes_2.10&lt;/artifactId&gt; &lt;version&gt;${scala.macros.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;jline&lt;/artifactId&gt; &lt;version&gt;2.10.4&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/profile&gt; &lt;/profiles&gt;&lt;activation&gt;&lt;/activation&gt;This activation IMO have nothing to do with `-Pscala-2.11` profile switch used in our build. "properties" are defined by `-Dproperty` switches. As far as I understand that, those additional dependencies would be added only if nobody defined property named `scala-2.11`, which means, they would be added only if switch `-Dscala-2.11` was not used, so it seems like those dependencies were basically added always. This quick test proves that I'm correct:$ mvn dependency:tree -pl flink-scala | grep quasi[INFO] +- org.scalamacros:quasiquotes_2.10:jar:2.1.0:compile$ mvn dependency:tree -pl flink-scala -Pscala-2.11 | grep quasi[INFO] +- org.scalamacros:quasiquotes_2.10:jar:2.1.0:compile$ mvn dependency:tree -pl flink-scala -Pscala-2.10 | grep quasi[INFO] +- org.scalamacros:quasiquotes_2.10:jar:2.1.0:compileregardless of the selected profile those dependencies are always there.</description>
      <version>1.3.0,1.3.1</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-scala-shell.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-11-3 01:00:00" id="7062" opendate="2017-7-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support the basic functionality of MATCH_RECOGNIZE</summary>
      <description>In this JIRA, we will support the basic functionality of MATCH_RECOGNIZE in Flink SQL API which includes the support of syntax MEASURES, PATTERN and DEFINE. This would allow users write basic cep use cases with SQL like the following example:SELECT T.aid, T.bid, T.cidFROM MyTableMATCH_RECOGNIZE ( MEASURES A.id AS aid, B.id AS bid, C.id AS cid PATTERN (A B C) DEFINE A AS A.name = 'a', B AS B.name = 'b', C AS C.name = 'c') AS T</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.CepITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.match.PatternSelectFunctionRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.match.PatternFlatSelectFunctionRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.match.MatchUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.match.IterativeConditionRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.match.ConvertToRow.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.SortUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamMatchRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.FlinkRelNode.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamMatch.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetSort.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.CommonSort.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.MatchCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.Indenter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.generated.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-7 01:00:00" id="7133" opendate="2017-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Elasticsearch version interference</summary>
      <description>At least two users have encountered problems with shading in the Elasticsearch connector: https://lists.apache.org/thread.html/b5bc1f690dc894ea9a8b69e82c89eb89ba6dfc2fec2588d2ccacee2c@%3Cuser.flink.apache.org%3E https://lists.apache.org/thread.html/2356670d168f61c20e34611e3c4aeb9c9b3f959f23a9833f631da1ba@%3Cuser.flink.apache.org%3EThe problem seems to be (quote from the second mail):I've found out the source of the problem when I build flink locally.elastic-search base depends on (by default) ES version 1.7.1 that depends onasm 4.1 and that version is shaded to elasticsearch-base-jar. I tried to setelasticsearch.version property in Maven to 5.1.2 (the same as elasticsearch5connector) but then elasticsearch-base does not compile:[ERROR] Failed to execute goalorg.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile(default-testCompile) on project flink-connector-elasticsearch-base_2.11:Compilation failure[ERROR]/home/adebski/Downloads/flink-release-1.3.1/flink-connectors/flink-connector-elasticsearch-base/src/test/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBaseTest.java:[491,92]no suitable constructor found forBulkItemResponse(int,java.lang.String,org.elasticsearch.action.ActionResponse)[ERROR] constructororg.elasticsearch.action.bulk.BulkItemResponse.BulkItemResponse(int,java.lang.String,org.elasticsearch.action.DocWriteResponse)is not applicable[ERROR] (argument mismatch; org.elasticsearch.action.ActionResponse cannotbe converted to org.elasticsearch.action.DocWriteResponse)[ERROR] constructororg.elasticsearch.action.bulk.BulkItemResponse.BulkItemResponse(int,java.lang.String,org.elasticsearch.action.bulk.BulkItemResponse.Failure)is not applicable[ERROR] (argument mismatch; org.elasticsearch.action.ActionResponse cannotbe converted to org.elasticsearch.action.bulk.BulkItemResponse.Failure)To me, it seems like we have to get rid of the "base" package and have two completely separate packages.</description>
      <version>1.3.0,1.3.1</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-13 01:00:00" id="7176" opendate="2017-7-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failed builds (due to compilation) don&amp;#39;t upload logs</summary>
      <description>If the compile phase fails on travis flink-dist may not be created. This causes the check for the inclusion of snappy in flink-dist to fail.The function doing this check calls exit 1 on error, which exits the entire shell, thus skipping subsequent actions like the upload of logs.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-4 01:00:00" id="7370" opendate="2017-8-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>rework operator documentation</summary>
      <description>The structure of the operator documentation could be improved the following way: Create category Streaming/Operators. Move Streaming/Overview/DataStream Transformations to Streaming/Operators/Overview. Move ProcessFunction, Windows, and Async IO to Streaming/Operators create any necessary redirects for old URLs</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.windows.md</file>
      <file type="M">docs.redirects.windows.2.md</file>
      <file type="M">docs.redirects.windows.md</file>
      <file type="M">docs.ops.state.checkpoints.md</file>
      <file type="M">docs.dev.stream.windows.md</file>
      <file type="M">docs.dev.stream.state.checkpointing.md</file>
      <file type="M">docs.dev.stream.side.output.md</file>
      <file type="M">docs.dev.stream.process.function.md</file>
      <file type="M">docs.dev.stream.operators.md</file>
      <file type="M">docs.dev.stream.asyncio.md</file>
      <file type="M">docs.dev.event.timestamp.extractors.md</file>
      <file type="M">docs.dev.event.time.md</file>
      <file type="M">docs.dev.datastream.api.md</file>
      <file type="M">docs.dev.connectors.index.md</file>
      <file type="M">docs.concepts.programming-model.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-11-2 01:00:00" id="7962" opendate="2017-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add built-in support for min/max aggregation for Timestamp</summary>
      <description>This JIRA adds the built-in support for min/max aggregation for Timestamp.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MinWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MinAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MaxWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MaxAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunctionWithRetract.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunctionWithRetract.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-19 01:00:00" id="8458" opendate="2018-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the switch for keeping both the old mode and the new credit-based mode</summary>
      <description>After the whole feature of credit-based flow control is done, we should add a config parameter to switch on/off the new credit-based mode. To do so, we can roll back to the old network mode for any expected risks.The parameter is defined as taskmanager.network.credit-based-flow-control.enabled and the default value is true. This switch may be removed after next release.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs..includes.generated.netty.configuration.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-11-8 01:00:00" id="8897" opendate="2018-3-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rowtime materialization causes "mismatched type" AssertionError</summary>
      <description>As raised in this thread, the query created by the following code will throw a calcite "mismatch type" (Timestamp(3) and TimeIndicator) exception.String sql1 = "select id, eventTs as t1, count(*) over (partition by id order by eventTs rows between 100 preceding and current row) as cnt1 from myTable1";String sql2 = "select distinct id as r_id, eventTs as t2, count(*) over (partition by id order by eventTs rows between 50 preceding and current row) as cnt2 from myTable2";Table left = tableEnv.sqlQuery(sql1);Table right = tableEnv.sqlQuery(sql2);left.join(right).where("id === r_id &amp;&amp; t1 === t2").select("id, t1").writeToSink(...)The logical plan is as follows.LogicalProject(id=[$0], t1=[$1]) LogicalFilter(condition=[AND(=($0, $3), =($1, $4))]) LogicalJoin(condition=[true], joinType=[inner]) LogicalAggregate(group=[{0, 1, 2}]) LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])]) LogicalProject(id=[$0], eventTs=[$3]) LogicalTableScan(table=[[_DataStreamTable_0]]) LogicalAggregate(group=[{0, 1, 2}]) LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])]) LogicalProject(id=[$0], eventTs=[$3]) LogicalTableScan(table=[[_DataStreamTable_0]])That is because the the rowtime field after an aggregation will be materialized while the RexInputRef type for the filter's operands (t1 === t2) is still TimeIndicator. We should make them unified.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.WindowJoinUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamWindowJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-29 01:00:00" id="9107" opendate="2018-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document timer coalescing for ProcessFunctions</summary>
      <description>In a ProcessFunction, registering timers for each event via ctx.timerService().registerEventTimeTimer() using times like ctx.timestamp() + timeout will get a millisecond accuracy and may thus create one timer per millisecond which may lead to some overhead in the TimerService.This problem can be mitigated by using timer coalescing if the desired accuracy of the timer can be larger than 1ms. A timer firing at full seconds only, for example, can be realised like this:coalescedTime = ((ctx.timestamp() + timeout) / 1000) * 1000;ctx.timerService().registerEventTimeTimer(coalescedTime);As a result, only a single timer may exist for every second since we do not add timers for timestamps that are already there.This should be documented in the ProcessFunction docs.</description>
      <version>1.3.0,1.4.0,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.operators.process.function.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-29 01:00:00" id="9108" opendate="2018-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>invalid ProcessWindowFunction link in Document</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.side.output.md</file>
    </fixedFiles>
  </bug>
</bugrepository>