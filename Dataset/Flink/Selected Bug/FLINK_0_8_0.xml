<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  
  <bug fixdate="2014-10-27 01:00:00" id="1187" opendate="2014-10-27 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>API support for registering persistent states</summary>
      <description/>
      <version>0.8.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.streamvertex.StreamVertex.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.StreamConfig.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.JobGraphBuilder.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.invokable.StreamInvokable.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-11 01:00:00" id="11871" opendate="2019-3-11 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce LongHashTable to improve performance when join key fits in long</summary>
      <description>We can do further optimizations if we know the join key fits in long, a single long field or two integer fields are both ok. For example, we can combine the hash code and actual key into one field, there will be no hash collision, can save a lot of unnecessary logic.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.util.SegmentsUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.BinaryStringSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.BinaryRowSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.BinaryMapSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.BinaryGenericSerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.typeutils.BinaryArraySerializer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.hashtable.BinaryHashPartition.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.dataformat.BinaryFormat.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-11 01:00:00" id="11872" opendate="2019-3-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update lz4 license file</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-runtime-blink.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-3-12 01:00:00" id="11882" opendate="2019-3-12 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce BytesHashMap to batch hash agg</summary>
      <description>Introduce bytes based hash table.It can be used for performing aggregations where the aggregated values are fixed-width.Because the data is stored in continuous memory, AggBuffer of variable length cannot be applied to this HashMap. The KeyValue form in hash map is designed to reduce the cost of key fetching in lookup.Add a test to do a complete hash agg.Â When HashMap has enough memory, pure hash AGG is performed; when memory is insufficient, it degenerates into sort agg.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-11-3 01:00:00" id="1202" opendate="2014-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failed file outputs should remove partially complete files</summary>
      <description>Without this, fault tolerance retries may fail because files already exist.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.DataSinkTaskTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DataSinkTask.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.FileOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-3 01:00:00" id="1203" opendate="2014-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClassCast exceptions in parallel tests (Surefire Bug?)</summary>
      <description>I am frequently seeing weird (non-deterministic) class cast exception in the tests, where apparently casts fail in serial parts of the program. The casting attempts have nothing to do with the program context.java.lang.ClassCastException: org.apache.flink.api.common.operators.base.DeltaIterationBase cannot be cast to org.apache.flink.api.common.operators.base.GroupReduceOperatorBase at org.apache.flink.api.scala.operators.translation.DistinctTranslationTest.testCombinable(DistinctTranslationTest.scala:39)I am wondering whether that might be a strange bug in the forked execution, a possible bug in maven surefire.I propose to deactivate the "reuseFork" option in surefire, to create a clean JVM (and class loaders) for each test.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-4 01:00:00" id="1208" opendate="2014-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip comment lines in CSV input format. Allow user to specify comment character.</summary>
      <description>The current skipFirstLine is limited. Skipping arbitrary lines that start with a certain character would be much more flexible while still easy to implement.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.io.CsvInputFormatTest.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ExecutionEnvironment.scala</file>
      <file type="M">flink-scala.src.main.java.org.apache.flink.api.scala.operators.ScalaCsvInputFormat.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CSVReaderTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CsvInputFormatTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.CsvReader.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.io.CsvInputFormat.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-11-4 01:00:00" id="1209" opendate="2014-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Forgetting to close an iteration leads to a confusing error message</summary>
      <description>The rror message you get is "Unknown operator - SolutionSetPlaceholder / WorksetPlaceholder / PartialSolutionPlaceholder"</description>
      <version>0.7.0-incubating,0.8.0</version>
      <fixedVersion>0.8.0,0.7.1-incubating</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.OperatorTranslation.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.IterativeDataSet.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.PactCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-4 01:00:00" id="1210" opendate="2014-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Error Message in Delta Iteratione when Next Workset does not Depend on Workset.</summary>
      <description>Currently, the job fails with a NullPointerException in the NepheleJobGraphGenerator</description>
      <version>0.7.0-incubating,0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.PactCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-3 01:00:00" id="12100" opendate="2019-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka 0.10/0.11 tests fail on Java 9</summary>
      <description>java.lang.NoClassDefFoundError: javax/xml/bind/DatatypeConverter at kafka.utils.CoreUtils$.urlSafeBase64EncodeNoPadding(CoreUtils.scala:294) at kafka.utils.CoreUtils$.generateUuidAsBase64(CoreUtils.scala:282) at kafka.server.KafkaServer$$anonfun$getOrGenerateClusterId$1.apply(KafkaServer.scala:335) at kafka.server.KafkaServer$$anonfun$getOrGenerateClusterId$1.apply(KafkaServer.scala:335) at scala.Option.getOrElse(Option.scala:121) at kafka.server.KafkaServer.getOrGenerateClusterId(KafkaServer.scala:335) at kafka.server.KafkaServer.startup(KafkaServer.scala:190) at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.getKafkaServer(KafkaTestEnvironmentImpl.java:430) at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.prepare(KafkaTestEnvironmentImpl.java:256) at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.startClusters(KafkaTestBase.java:137) at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.prepare(KafkaTestBase.java:100) at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.prepare(KafkaTestBase.java:92) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)Caused by: java.lang.ClassNotFoundException: javax.xml.bind.DatatypeConverter at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:582) at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:185) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:496) ... 33 more</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-5 01:00:00" id="1214" opendate="2014-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent partitioning pushdown unless partitions fields match exactly</summary>
      <description>Consider an operation grouped on fields (A, B), followed by an operation grouped on field (A).Right now, the optimizer can push down the partitioning on (A), which serves both operations (the first step locally still groups by A and B). This may however by a bad idea for the cases where the field A has a low cardinality, or the value distribution is skewed.Since we cannot determine that robustly yet, I suggest to disable this optimization for now.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.compiler.iterations.ConnectedComponentsTest.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.SingleInputNode.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-11-5 01:00:00" id="1218" opendate="2014-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t use "new Integer". Use "Integer.valueOf" instead.</summary>
      <description>There are lots of "new Integer" or something line it in the code.We should use valueOf to avoid wasted instance creation.</description>
      <version>0.8.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.WordCountITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.WebLogAnalysisITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQueryAsterixITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery9ITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery4ITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TPCHQuery3WithUnionITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.TeraSortITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.GlobalSortingMixedOrderITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recordJobTests.GlobalSortingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.MapITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.DanglingPageRankWithCombinerNepheleITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.nephele.DanglingPageRankNepheleITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.ConnectedComponentsITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.iterative.aggregators.AggregatorsITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.clients.examples.LocalExecutorITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.accumulators.AccumulatorIterativeITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.accumulators.AccumulatorITCase.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.typeutils.NothingSerializer.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.sort.CombiningUnilateralSortMergerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.ReOpenableHashTableITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.HashTableITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.web.JsonFactory.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorILDXC2Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorILDX1Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorILDC3Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorILD3Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.typeutils.runtime.TupleComparatorILD2Test.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.operators.translation.AggregateTranslationTest.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.graph.TransitiveClosureNaive.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.ShortSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.ShortComparatorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.LongSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.LongComparatorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.IntSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.IntComparatorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.FloatSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.FloatComparatorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.DoubleSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.DoubleComparatorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.ByteSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.ByteComparatorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.typeutils.base.BooleanSerializerTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.util.FieldSetTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.operators.util.FieldListTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.util.serialization.TypeSerializationTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.invokable.operator.BatchGroupReduceTest.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
      <file type="M">flink-addons.flink-streaming.flink-streaming-connectors.src.test.java.org.apache.flink.streaming.connectors.db.DBStateTest.java</file>
      <file type="M">flink-addons.flink-avro.src.test.java.org.apache.flink.api.avro.EncoderDecoderTest.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2014-11-12 01:00:00" id="1235" opendate="2014-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compiler rejectes non-nested iterations in constant path of an Iteration.</summary>
      <description/>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.IterationsCompilerTest.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.PactCompiler.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.WorksetIterationNode.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.BulkIterationNode.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-11-19 01:00:00" id="1253" opendate="2014-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tests occasionally die with GarbageCollectionOverhead exceeded</summary>
      <description>I have seen tests occasionally dying from GC Overhead Limit exceeded exception. I assume it happens because we reuse the JVMs across tests in the unit tests, possibly some test artifacts linger.I suggest to add the -XX:-UseGCOverheadLimit option to the tests, so that they do not break on the build server.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-24 01:00:00" id="12611" opendate="2019-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make time indicator nullable in blink</summary>
      <description>SQL:Â select max(rowtime), count(a) from TThere will be a AssertionError: type mismatch:aggCall type:TIMESTAMP(3) NOT NULLinferred type:TIMESTAMP(3)Agg type checking is done before TimeIndicator materializes. So there is a exception.And before introducing nullable of LogicalType, we should modify this to avoid more potential TypeCheck problems.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.schema.TimeIndicatorRelDataTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.join.LookupJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.RelTimeIndicatorConverterTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.schema.TimeIndicatorRelDataType.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.nodes.calcite.WatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.ProctimeSqlFunction.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.functions.sql.ProctimeMaterializeSqlFunction.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-20 01:00:00" id="1263" opendate="2014-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Manual Partition operations are considered to keep data types constant</summary>
      <description>This leads to a big loss in optimization potential and fully voids some cases.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.SingleInputSemanticProperties.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.SingleInputOperator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.PartitionOperatorBase.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.UtilSinkJoinOpDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.OperatorDescriptorDual.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.CrossStreamOuterSecondDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.CrossStreamOuterFirstDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.CrossBlockOuterSecondDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.CrossBlockOuterFirstDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.CoGroupDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.CartesianProductDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.BinaryUnionOpDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.AbstractJoinDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.WorksetIterationNode.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.dag.TwoInputNode.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-20 01:00:00" id="1264" opendate="2014-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Custom partitioners are not properly forwarded to the runtime</summary>
      <description/>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-11-23 01:00:00" id="1276" opendate="2014-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Misspelled class name SlotAvalablbilityListener.java</summary>
      <description>1. Misspelled Class name - 'SlotAvailablblityListener'.2. All methods in MathUtils.java are declared as static final.3. Many other minor fixes</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.scheduler.SlotAvailablilityListener.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.scheduler.Scheduler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.Instance.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-11-27 01:00:00" id="1290" opendate="2014-11-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimizer prunes all candidates when unable to reuse sort properties</summary>
      <description>Programs fail with an exception that no plan could be created.The bug can be reproduced by the following code:val data : DataSet[(Long, Long)] = ...data.distinct(0, 1).groupBy(0).reduceGroup(...)</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.custompartition.JoinCustomPartitioningTest.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.custompartition.CoGroupCustomPartitioningTest.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.CoGroupDescriptor.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.operators.AbstractJoinDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-27 01:00:00" id="1291" opendate="2014-11-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove GC options from JobManager and TaskManager scripts</summary>
      <description>Currently, the start scripts set a series of GC options The Tenured Gen GC is set to ConcurrentMarkSweep The Perm Gen is set to be collected during swipes (CMSClassUnloading) The ratio between Perm Gen and New Gen is fixedI propose to remove all of those options: Users can pass their own GC arguments through the JAVA_OPTS CMS is not considered a competitive GC any more (starting from Java 7, G1 seems much better) PermGen garbage collection happens in all GCs anyways The ratio between TenuredGen and New Gen has proven harmful in the past</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.taskmanager.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.jobmanager.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-6-21 01:00:00" id="12920" opendate="2019-6-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop support for register_table_sink with field_names and field_types parameters</summary>
      <description>The following registerTableSink API in TableEnvironment is deprecated:@Deprecatedvoid registerTableSink(String name, String[] fieldNames, TypeInformation&lt;?&gt;[] fieldTypes, TableSink&lt;?&gt; tableSink);We can drop the supportÂ of it in Python Table API.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.testing.source.sink.utils.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.calc.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.table.sinks.py</file>
      <file type="M">flink-python.pyflink.table.examples.batch.word.count.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.dataset.tests.test.execution.environment.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-6-24 01:00:00" id="12962" opendate="2019-6-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allows pyflink to be pip installed</summary>
      <description>The aim of this JIRA is to support to build a pip installable package.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.update.branch.version.sh</file>
      <file type="M">flink-python.tox.ini</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.pyflink.version.py</file>
      <file type="M">flink-python.pyflink.shell.py</file>
      <file type="M">flink-python.pyflink.find.flink.home.py</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.pyflink-shell.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
      <file type="M">docs.flinkDev.building.zh.md</file>
      <file type="M">docs.flinkDev.building.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-24 01:00:00" id="12963" opendate="2019-6-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add savepoint writer for bootstrapping new savepoints</summary>
      <description>Implement a savepoint writer for bootstrapping new savepoints.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.SavepointTest.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.NewSavepoint.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.BoundedStreamConfig.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.OperatorSubtaskStateReducer.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.BoundedOneInputStreamTaskRunner.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.Savepoint.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.metadata.OnDiskSavepointMetadata.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.ExistingSavepoint.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.OperatorTransformation.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.test.java.org.apache.flink.state.api.BootstrapTransformationTest.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.metadata.SavepointMetadata.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.output.MergeOperatorStates.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.BootstrapTransformation.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.KeyedOperatorTransformation.java</file>
      <file type="M">tools.travis.stage.sh</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.WritableSavepoint.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.metadata.ModifiableSavepointMetadata.java</file>
      <file type="M">docs.dev.libs.state.processor.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-24 01:00:00" id="12964" opendate="2019-6-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add commented-out defaults to sql client yaml file to make it easier for users to adopt</summary>
      <description>Add defaults for catalogs, similar to existing defaults of tables and functions, e.g.tables: [] # empty list# A typical table source definition looks like:# - name: ...# type: source-table# connector: ...# format: ...# schema: ...# A typical view definition looks like:# - name: ...# type: view# query: "SELECT ..."# A typical temporal table definition looks like:# - name: ...# type: temporal-table# history-table: ...# time-attribute: ...# primary-key: ...#==============================================================================# User-defined functions#==============================================================================# Define scalar, aggregate, or table functions here.functions: [] # empty list# A typical function definition looks like:# - name: ...# from: class# class: ...# constructor: ...</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.conf.sql-client-defaults.yaml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-9 01:00:00" id="1310" opendate="2014-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala Closure Cleaner logs very aggressive</summary>
      <description>The Scala Closure Cleaner puts out a lot of messages on INFO level. I vote to reduce this to DEBUG level, as it is not very informative and floods the log.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ClosureCleaner.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-9 01:00:00" id="1311" opendate="2014-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Auxiliary nodes in iterations are not correctly identified as "dynamic" or "static"</summary>
      <description>The static/dynamic path tagger starts on the original roots of the step functions, ignoring possible auxiliary nodes that we need to attach to the root (such as NoOps, when the root is a union)</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.java.IterationCompilerTest.java</file>
      <file type="M">flink-compiler.src.main.java.org.apache.flink.compiler.PactCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-5 01:00:00" id="13110" opendate="2019-7-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add shim of SimpleGenericUDAFParameterInfo for Hive 1.2.1 and 2.3.4</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.functions.hive.HiveGenericUDAFTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveGenericUDAF.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.conversion.HiveInspectors.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV2.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV1.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-5 01:00:00" id="13115" opendate="2019-7-5 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce planner rule to support partition pruning for PartitionableTableSource</summary>
      <description>This issue aims to support partition pruning for PartitionableTableSource</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.CodeGeneratorContext.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.WindowJoinUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RankUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.SelectivityEstimator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdDistinctRowCount.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.metadata.FlinkRelMdColumnNullCount.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.testTableSources.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.util.RexNodeExtractorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.batch.sql.TableSourceTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.RexNodeExtractor.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.physical.batch.BatchExecScanTableSourceRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.ExpressionReducer.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-5 01:00:00" id="13117" opendate="2019-7-5 00:00:00" resolution="Not A Problem">
    <buginformation>
      <summary>Serialize Issue when restoring from savepoint</summary>
      <description>Hi, I have one question.My application should be maintained without loss of data.So I use savepoint to deploy a new application.Â Normally there is no problem in restarting.However, when the schema of some case classes is changed, a serialize error occurs at restart.Â How can I resolve this?Flink version is 1.8.0Here is the error log.Thank you.org.apache.flink.util.StateMigrationException: The new state serializer cannot be incompatible. at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.updateRestoredStateMetaInfo(RocksDBKeyedStateBackend.java:527) at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.tryRegisterKvStateInformation(RocksDBKeyedStateBackend.java:475) at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.createInternalState(RocksDBKeyedStateBackend.java:613) at org.apache.flink.runtime.state.KeyedStateFactory.createInternalState(KeyedStateFactory.java:47) at org.apache.flink.runtime.state.ttl.TtlStateFactory.createStateAndWrapWithTtlIfEnabled(TtlStateFactory.java:72) at org.apache.flink.runtime.state.AbstractKeyedStateBackend.getOrCreateKeyedState(AbstractKeyedStateBackend.java:286) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.getOrCreateKeyedState(AbstractStreamOperator.java:568) at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.open(WindowOperator.java:240) at org.apache.flink.streaming.runtime.tasks.StreamTask.openAllOperators(StreamTask.java:424) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:290) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711) at java.lang.Thread.run(Thread.java:748)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.executor.BatchExecutorTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.executor.BatchExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-5 01:00:00" id="13118" opendate="2019-7-5 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce JDBC table factory</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCInputFormatTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.java.org.apache.flink.api.java.io.jdbc.JDBCFullTest.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.split.NumericBetweenParametersProvider.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCUpsertTableSink.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCTableSource.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCOptions.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCLookupOptions.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JDBCLookupFunction.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.dialect.JDBCDialect.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-12-10 01:00:00" id="1316" opendate="2014-12-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Webclient fails to display plans where nodes are references from multiple iteration closures</summary>
      <description/>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.resources.web-docs.js.graphCreator.js</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-9 01:00:00" id="13160" opendate="2019-7-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveTaleSource should implment PartitionableTableSource</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.batch.connectors.hive.HiveTableSource.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2019-8-22 01:00:00" id="13362" opendate="2019-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Kafka &amp; ES &amp; FileSystem DDL</summary>
      <description>Add documentation for Kafka &amp; ES &amp; FileSystem DDL âConnect to External Systemsâ: Add DDL for Kafka &amp; ES &amp; FileSystem.</description>
      <version>None</version>
      <fixedVersion>1.9.1,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-23 01:00:00" id="13390" opendate="2019-7-23 00:00:00" resolution="Done">
    <buginformation>
      <summary>Clarify the exact meaning of state size when executing incremental checkpoint</summary>
      <description>This issue is inspired from a user mail which confused about the state size meaning.I think changing the description of state size and add some notices on documentation could help this. Moreover, change the log when complete checkpoint should be also taken into account.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.TaskCheckpointStatisticsWithSubtaskDetails.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.TaskCheckpointStatistics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.SubtaskCheckpointStatistics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointStatistics.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointingStatistics.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.subtask.job-checkpoints-subtask.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.detail.job-checkpoints-detail.component.html</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CheckpointingOptions.java</file>
      <file type="M">docs..includes.generated.common.state.backends.section.html</file>
      <file type="M">docs..includes.generated.checkpointing.configuration.html</file>
      <file type="M">docs.ops.state.state.backends.zh.md</file>
      <file type="M">docs.ops.state.state.backends.md</file>
      <file type="M">docs.monitoring.checkpoint.monitoring.zh.md</file>
      <file type="M">docs.monitoring.checkpoint.monitoring.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-24 01:00:00" id="13393" opendate="2019-7-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Blink-planner not support generic TableSource</summary>
      <description>Now there is a exception when user use table source like:class MyTableSource[T] extendÂ StreamTableSource[T]The reason is that blink-planner use TypeExtractor to extract class from TableSource, and use this class to DataFormatConverter.Now, table source has DataType return type, so we don't need extract class from TableSource, we can just use conversionClass of DataType.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.testTableSources.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.ScanUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecTableSourceScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.SinkCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.TableFunctionCallGen.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarFunctionCallGen.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-1-11 01:00:00" id="1386" opendate="2015-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Java Quickstart Fails to import properly in Eclipse</summary>
      <description/>
      <version>0.8.0,0.9</version>
      <fixedVersion>0.8.0,0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-2-12 01:00:00" id="1392" opendate="2015-1-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Serializing Protobuf - issue 1</summary>
      <description>Hi, I started to experiment with Parquet using Protobuf.When I use the standard Protobuf class: com.twitter.data.proto.tutorial.AddressBookProtosThe code which I run, can be found here: https://github.com/FelixNeutatz/incubator-flink/blob/ParquetAtFlink/flink-addons/flink-hadoop-compatibility/src/main/java/org/apache/flink/hadoopcompatibility/mapreduce/example/ParquetProtobufOutput.javaI get the following exception: Exception in thread "main" java.lang.Exception: Deserializing the InputFormat (org.apache.flink.api.java.io.CollectionInputFormat) failed: Could not read the user code wrapper: Error while deserializing element from collection at org.apache.flink.runtime.jobgraph.InputFormatVertex.initializeOnMaster(InputFormatVertex.java:60) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1$$anonfun$applyOrElse$5.apply(JobManager.scala:179) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1$$anonfun$applyOrElse$5.apply(JobManager.scala:172) at scala.collection.Iterator$class.foreach(Iterator.scala:727) at scala.collection.AbstractIterator.foreach(Iterator.scala:1157) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1.applyOrElse(JobManager.scala:172) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:34) at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:27) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118) at org.apache.flink.runtime.ActorLogMessages$$anon$1.applyOrElse(ActorLogMessages.scala:27) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:52) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: org.apache.flink.runtime.operators.util.CorruptConfigurationException: Could not read the user code wrapper: Error while deserializing element from collection at org.apache.flink.runtime.operators.util.TaskConfig.getStubWrapper(TaskConfig.java:285) at org.apache.flink.runtime.jobgraph.InputFormatVertex.initializeOnMaster(InputFormatVertex.java:57) ... 25 moreCaused by: java.io.IOException: Error while deserializing element from collection at org.apache.flink.api.java.io.CollectionInputFormat.readObject(CollectionInputFormat.java:108) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:274) at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:236) at org.apache.flink.runtime.operators.util.TaskConfig.getStubWrapper(TaskConfig.java:281) ... 26 moreCaused by: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationExceptionSerialization trace:phone_ (com.twitter.data.proto.tutorial.AddressBookProtos$Person) at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125) at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528) at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761) at org.apache.flink.api.java.typeutils.runtime.KryoSerializer.deserialize(KryoSerializer.java:130) at org.apache.flink.api.java.typeutils.runtime.TupleSerializer.deserialize(TupleSerializer.java:106) at org.apache.flink.api.java.typeutils.runtime.TupleSerializer.deserialize(TupleSerializer.java:30) at org.apache.flink.api.java.io.CollectionInputFormat.readObject(CollectionInputFormat.java:103) ... 42 moreCaused by: java.lang.UnsupportedOperationException at java.util.Collections$UnmodifiableCollection.add(Collections.java:1075) at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:109) at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:22) at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:679) at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) ... 48 more</description>
      <version>0.8.0,0.9</version>
      <fixedVersion>0.9,0.8.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-shaded.pom.xml</file>
      <file type="M">flink-java.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-6-20 01:00:00" id="1422" opendate="2015-1-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing usage example for "withParameters"</summary>
      <description>I am struggling to find a usage example of the "withParameters" method in the documentation. At the moment I only see this note:Note: As the content of broadcast variables is kept in-memory on each node, it should not become too large. For simpler things like scalar values you can simply make parameters part of the closure of a function, or use the withParameters(...) method to pass in a configuration.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.programming.guide.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-2-5 01:00:00" id="1485" opendate="2015-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo in Documentation - Join with Join-Function</summary>
      <description>Small typo in documentationIn the java example for "Join with Join-Function"</description>
      <version>0.8.0</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-16 01:00:00" id="15620" opendate="2020-1-16 00:00:00" resolution="Done">
    <buginformation>
      <summary>State TTL: Remove deprecated enable default background cleanup</summary>
      <description>Follow-up forÂ FLINK-15606.</description>
      <version>None</version>
      <fixedVersion>1.11.0,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.ttl.RocksDBTtlStateTestBase.java</file>
      <file type="M">flink-end-to-end-tests.flink-stream-state-ttl-test.src.main.java.org.apache.flink.streaming.tests.DataStreamStateTTLTestProgram.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.state.StateTtlConfigTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.state.StateTtlConfig.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-2-19 01:00:00" id="15672" opendate="2020-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Switch to Log4j 2 by default</summary>
      <description>Log4j 1.x is outdated and has multiple problems: No dynamic adjustments of Log Level Problems with newer Java Versions Not actively developed any moreSwitching to Log4J 2 by default would solve this.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-yarn.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-tests.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-streaming-scala.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-streaming-java.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-scala.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-scala-shell.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-runtime.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-runtime-web.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-optimizer.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-metrics.flink-metrics-statsd.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-metrics.flink-metrics-slf4j.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-metrics.flink-metrics-datadog.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-mesos.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-libraries.flink-cep.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-kubernetes.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-java.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-fs-tests.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-formats.flink-sequence-file.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-formats.flink-parquet.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-formats.flink-orc.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-formats.flink-compress.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-formats.flink-avro.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-filesystems.flink-mapr-fs.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-filesystems.flink-hadoop-fs.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-examples.flink-examples-streaming.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-metrics-reporter-prometheus-test.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-metrics-availability-test.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-local-recovery-and-allocation-test.src.main.resources.log4j-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-end-to-end-tests.flink-connector-gcp-pubsub-emulator-tests.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-dist.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-core.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-contrib.flink-connector-wikiedits.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-container.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-jdbc.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-hbase.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-gcp-pubsub.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-clients.src.test.resources.log4j-test.properties</file>
      <file type="M">tools.log4j-travis.properties</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-table-scala.src.main.resources.archetype-resources.src.main.resources.log4j.properties</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-table-java.src.main.resources.archetype-resources.src.main.resources.log4j.properties</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-scala.src.main.resources.archetype-resources.src.main.resources.log4j.properties</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-java.src.main.resources.archetype-resources.src.main.resources.log4j.properties</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.CommonTestUtils.java</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.src.main.resources.log4j.properties</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.src.main.resources.log4j.properties</file>
      <file type="M">flink-libraries.flink-gelly-examples.src.main.resources.log4j.properties</file>
      <file type="M">flink-examples.flink-examples-streaming.src.main.resources.log4j.properties</file>
      <file type="M">flink-examples.flink-examples-batch.src.main.resources.log4j.properties</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.bucketing.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.local.recovery.and.scheduling.sh</file>
      <file type="M">flink-dist.src.main.resources.log4j-bash-utils.properties</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j.properties</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j-yarn-session.properties</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j-console.properties</file>
      <file type="M">flink-dist.src.main.flink-bin.conf.log4j-cli.properties</file>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
      <file type="M">docs.ops.deployment.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.kubernetes.md</file>
      <file type="M">tools.travis.watchdog.sh</file>
      <file type="M">tools.travis.nightly.sh</file>
      <file type="M">tools.releasing.NOTICE-binary.PREAMBLE.txt</file>
      <file type="M">tools.releasing.LICENSE.slf4j</file>
      <file type="M">tools.releasing.collect.license.files.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnClusterDescriptorTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnClusterDescriptor.java</file>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">docs.monitoring.logging.md</file>
      <file type="M">docs.monitoring.logging.zh.md</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.log4j-provider.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.test.java.org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.src.test.java.org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.pom.xml</file>
      <file type="M">flink-connectors.flink-hbase.pom.xml</file>
      <file type="M">flink-connectors.flink-hcatalog.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink-daemon.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.start-cluster.bat</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-appmaster-job.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-appmaster.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.mesos-bin.mesos-taskmanager.sh</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-connector-gcp-pubsub-emulator-tests.pom.xml</file>
      <file type="M">flink-examples.pom.xml</file>
      <file type="M">flink-filesystems.flink-hadoop-fs.pom.xml</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.pom.xml</file>
      <file type="M">flink-fs-tests.pom.xml</file>
      <file type="M">flink-libraries.flink-gelly-examples.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-slf4j.src.test.java.org.apache.flink.metrics.slf4j.Slf4jReporterTest.java</file>
      <file type="M">flink-metrics.flink-metrics-slf4j.src.test.java.org.apache.flink.metrics.slf4j.TestUtils.java</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.BootstrapToolsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunctionTest.java</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-table-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-table-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.UtilsTest.java</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-20 01:00:00" id="15675" opendate="2020-1-20 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add exception and documentation that Python UDF is not supported for Flink Planner under batch mode</summary>
      <description>We should add straightforward exceptions and document to info users that Python UDF is not supported for Flink Planner under batch mode.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-18 01:00:00" id="1578" opendate="2015-2-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Overhaul BLOB manager</summary>
      <description>The BLOB manager need improvements: Decent failure tests Better error handling (letting the client know what happened) Better error logging Retries upon failed fetches A bit of control over the maximum number of concurrent connections and the backlog</description>
      <version>0.8.0,0.9</version>
      <fixedVersion>0.8.0,0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobClientTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobCacheTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.AbstractIDTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobServer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobKey.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobInputStream.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobConnection.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobCache.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.AbstractID.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-18 01:00:00" id="1582" opendate="2015-2-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SocketStream gets stuck when socket closes</summary>
      <description>When the server side of the socket closes the socket stream reader does not terminate. When the socket is reinitiated it does not reconnect just gets stuck.It would be nice to add options for the user have the reader should behave when the socket is down: terminate immediately (good for testing and examples) or wait a specified time - possibly forever.</description>
      <version>0.8.0,0.9</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.SourceTest.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.function.source.SocketTextStreamFunction.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-11 01:00:00" id="15991" opendate="2020-2-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create Chinese documentation for FLIP-49 TM memory model</summary>
      <description>Chinese translation ofÂ FLINK-15143</description>
      <version>None</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.memory.index.zh.md</file>
      <file type="M">docs.ops.memory.mem.tuning.zh.md</file>
      <file type="M">docs.ops.memory.mem.trouble.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.zh.md</file>
      <file type="M">docs.ops.memory.mem.migration.zh.md</file>
      <file type="M">docs.ops.memory.mem.detail.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-11 01:00:00" id="15993" opendate="2020-2-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add timeout to 404 documentation redirect, add explanation</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..layouts.404.base.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-3-18 01:00:00" id="16650" opendate="2020-3-18 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Support LocalZonedTimestampType for Python UDF in blink planner</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCorrelate.scala</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.sdk.worker.main.py</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.coders.common.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.types.py</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.AbstractPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.functions.python.PythonScalarFunctionFlatMap.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.ArrowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.BaseRowArrowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.BaseRowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.BaseRowPythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.AbstractPythonStatelessFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.AbstractGeneralPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.AbstractPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.arrow.AbstractArrowPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.arrow.ArrowPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.arrow.BaseRowArrowPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.BaseRowPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.PythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.table.AbstractPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.table.BaseRowPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.table.PythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.typeutils.PythonTypeUtils.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.ArrowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.BaseRowArrowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.BaseRowPythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.table.BaseRowPythonTableFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.runners.python.scalar.arrow.ArrowPythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.runners.python.scalar.BaseRowPythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.runners.python.scalar.PythonScalarFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.runners.python.table.BaseRowPythonTableFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.runners.python.table.PythonTableFunctionRunnerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughArrowPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonScalarFunctionRunner.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.utils.python.PythonTableUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetPythonCalc.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-9 01:00:00" id="18202" opendate="2020-6-9 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce Protobuf format</summary>
      <description>PB&amp;#91;1&amp;#93; is a very famous and wildly used (de)serialization framework. The ML&amp;#91;2&amp;#93; also has some discussions about this. It's a useful feature.This issue maybe needs some designs, or a FLIP.&amp;#91;1&amp;#93;Â https://developers.google.com/protocol-buffers&amp;#91;2&amp;#93;Â http://apache-flink.147419.n8.nabble.com/Flink-SQL-UDF-td3725.html</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>