<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  
  <bug fixdate="2019-5-29 01:00:00" id="12679" opendate="2019-5-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support &amp;#39;default-database&amp;#39; config for catalog entries in SQL CLI yaml file</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.descriptors.CatalogDescriptorValidator.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.descriptors.CatalogDescriptor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-catalogs.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-6-31 01:00:00" id="12685" opendate="2019-5-31 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Supports UNNEST query in blink planner</summary>
      <description>this issue aim to support queries with UNNEST keyword, which relate to nested fields.for example: table name: MyTableschema: a: int, b int, c array&amp;#91;int&amp;#93;sql:SELECT a, b, s FROM MyTable, UNNEST(MyTable.c) AS A (s)</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.util.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.runtime.batch.sql.agg.AggregateReduceGroupingITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.plan.stream.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.util.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkStreamProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkDecorrelateProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.plan.optimize.program.FlinkBatchProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.codegen.agg.batch.WindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-2 01:00:00" id="16945" opendate="2020-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Execute CheckpointFailureManager.FailJobCallback directly in main thread executor</summary>
      <description>Since we have put all non-IO operations of CheckpointCoordinator into main thread executor, the CheckpointFailureManager.FailJobCallback could be executed directly now. In this way execution graph would fail immediately when CheckpointFailureManager invokes the callback. We could avoid the inconsistent scenario of FLINK-13497.</description>
      <version>1.10.0,1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.ExecutionGraphCheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionGraph.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-2 01:00:00" id="16946" opendate="2020-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update user documentation for job manager memory model</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.state.backends.zh.md</file>
      <file type="M">docs.ops.state.state.backends.md</file>
      <file type="M">docs.ops.memory.mem.tuning.zh.md</file>
      <file type="M">docs.ops.memory.mem.tuning.md</file>
      <file type="M">docs.ops.memory.mem.trouble.zh.md</file>
      <file type="M">docs.ops.memory.mem.trouble.md</file>
      <file type="M">docs.ops.memory.mem.setup.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.md</file>
      <file type="M">docs.ops.memory.mem.migration.zh.md</file>
      <file type="M">docs.ops.memory.mem.migration.md</file>
      <file type="M">docs.ops.memory.mem.detail.zh.md</file>
      <file type="M">docs.ops.memory.mem.detail.md</file>
      <file type="M">docs.ops.config.zh.md</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-2 01:00:00" id="16947" opendate="2020-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ArtifactResolutionException: Could not transfer artifact. Entry [...] has not been leased from this pool</summary>
      <description>https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6982&amp;view=logs&amp;j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&amp;t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5Build of flink-metrics-availability-test failed with:[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (end-to-end-tests) on project flink-metrics-availability-test: Unable to generate classpath: org.apache.maven.artifact.resolver.ArtifactResolutionException: Could not transfer artifact org.apache.maven.surefire:surefire-grouper:jar:2.22.1 from/to google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/): Entry [id:13][route:{s}-&gt;https://maven-central-eu.storage-download.googleapis.com:443][state:null] has not been leased from this pool[ERROR] org.apache.maven.surefire:surefire-grouper:jar:2.22.1[ERROR] [ERROR] from the specified remote repositories:[ERROR] google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/, releases=true, snapshots=false),[ERROR] apache.snapshots (https://repository.apache.org/snapshots, releases=false, snapshots=true)[ERROR] Path to dependency:[ERROR] 1) dummy:dummy:jar:1.0[ERROR] 2) org.apache.maven.surefire:surefire-junit47:jar:2.22.1[ERROR] 3) org.apache.maven.surefire:common-junit48:jar:2.22.1[ERROR] 4) org.apache.maven.surefire:surefire-grouper:jar:2.22.1[ERROR] -&gt; [Help 1][ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException[ERROR] [ERROR] After correcting the problems, you can resume the build with the command[ERROR] mvn &lt;goals&gt; -rf :flink-metrics-availability-test</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-13 01:00:00" id="1695" opendate="2015-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create machine learning library</summary>
      <description>Create the infrastructure for Flink's machine learning library. This includes the creation of the module structure and the implementation of basic types such as vectors and matrices.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-8-12 01:00:00" id="18902" opendate="2020-8-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot serve results of asynchronous REST operations in per-job mode</summary>
      <description>Due to the changes introduced with FLINK-18663 a Flink per-job cluster can no longer properly shut down. The problem is that we no longer serve asynchronous results (e.g. resulting from a cancel-with-savepoint operation) while the cluster waits to shut down.In order to solve this problem, Flink needs to serve REST request also while it waits for shutting itself down.</description>
      <version>1.10.2,1.11.2,1.12.0</version>
      <fixedVersion>1.10.2,1.11.2,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.InFlightRequestTrackerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.InFlightRequestTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.AbstractHandler.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-9-8 01:00:00" id="19163" opendate="2020-9-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add building py38 wheel package of PyFlink in Azure CI</summary>
      <description>Add building py38 wheel package of PyFlink in Azure CI</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.build-wheels.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-9-10 01:00:00" id="19179" opendate="2020-9-10 00:00:00" resolution="Done">
    <buginformation>
      <summary>Implement the managed memory fraction calculation logic</summary>
      <description>This also means migrating the batch operator use cases.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.over.BufferDataOverWindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.String2SortMergeJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.String2HashJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.RandomSortMergeInnerJoinTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.join.Int2HashJoinOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.TableStreamOperator.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.BatchAggTestBase.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.ArrowPythonAggregateFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-11-10 01:00:00" id="19182" opendate="2020-9-10 00:00:00" resolution="Done">
    <buginformation>
      <summary>Update document for intra-slot managed memory sharing</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.memory.mem.setup.tm.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.tm.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-12-16 01:00:00" id="19259" opendate="2020-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use classloader release hooks with Kinesis producer to avoid metaspace leak</summary>
      <description>FLINK-17554 introduced hooks for clearing references before unloading a classloader.The Kinesis Producer library is currently preventing the usercode classloader from being unloaded because it keeps references around.This ticket is to use the hooks with the Kinesis producer.</description>
      <version>None</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.FlinkKinesisProducer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-11-17 01:00:00" id="19282" opendate="2020-9-17 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support watermark push down with WatermarkStrategy</summary>
      <description>Push the WatermarkStrategy into TableSourceScan in the interface SupportsWatermarkPushDown. Sometimes users define watermark on computed column. Therefore, we will push the computed column of rowtime into WatermarkStrategy first. For more info, please take a look at discussion.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.codegen.WatermarkGeneratorCodeGenTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.catalog.JavaCatalogTableTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.NestedProjectionUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.WatermarkGeneratorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.sources.DynamicSourceUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRule.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-22 01:00:00" id="19339" opendate="2020-9-22 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support Avro&amp;#39;s unions with logical types</summary>
      <description>Avro 1.9.x introduced yet another mechanism for registering/looking up conversions for logical types. See https://issues.apache.org/jira/browse/AVRO-1891If a logical type is part of a union a static field MODEL$ of type SpecificData will be added to the generated Avro class with registered conversions for a logical type. We should use that SpecificData in AvroSerializer and Avro(De)SerializationSchema whenever available.</description>
      <version>None</version>
      <fixedVersion>1.10.3,1.11.3,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.src.test.resources.avro.user.avsc</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.utils.AvroTestUtils.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroSerializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.AvroDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.typeutils.AvroSerializerSnapshot.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.typeutils.AvroFactory.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.AvroDeserializationSchema.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-9-25 01:00:00" id="19403" opendate="2020-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Pandas Stream Group Window Aggregation</summary>
      <description>We will add Stream Physical Pandas Group Window RelNode and StreamArrowPythonGroupWindowAggregateFunctionOperator to support Pandas Stream Group Window Aggregation</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecGroupWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonOverWindowAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.ArrowPythonAggregateFunctionOperatorTestBase.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udaf.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-25 01:00:00" id="19404" opendate="2020-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Pandas Stream Over Window Aggregation</summary>
      <description>We will add Stream Physical Pandas Over Window RelNode and StreamArrowPythonOverWindowAggregateFunctionOperator to support Pandas Stream Over Window Aggregation</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecOverAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecOverAggregate.scala</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udaf.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-25 01:00:00" id="19406" opendate="2020-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Casting row time to timestamp loses nullability info</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-10-25 01:00:00" id="19416" opendate="2020-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Python datetime object in from_collection of Python DataStream</summary>
      <description>Support Python datetime object in from_collection of Python DataStream</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.api.common.python.PythonBridgeUtils.java</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-python.pyflink.common.typeinfo.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-26 01:00:00" id="19421" opendate="2020-9-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Python UDAF in streaming mode</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.table.PythonAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.utils.JavaUserDefinedAggFunctions.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecGroupAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonOverAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonGroupWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonGroupAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecPythonAggregateRule.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.python.PythonFunctionInfo.java</file>
      <file type="M">flink-python.tox.ini</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperatorTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamTableStatefulPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.arrow.ArrowUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.aggregate.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operation.utils.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coders.py</file>
      <file type="M">flink-python.pyflink.common.types.py</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-10-29 01:00:00" id="19450" opendate="2020-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize the Python CI Test</summary>
      <description>Currently, the CI test of PyFlink will run 4 versions of Python, which takes a lot of time. We will optimize CI test to run only one version of Python. And then nightly test will run all versions of Python.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-10-3 01:00:00" id="19498" opendate="2020-10-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port LocatableInputSplitAssigner to new File Source API</summary>
      <description>The new File Source API needs a locality aware input split assigner.To preserve the experience, I suggest to port the existing LocatableInputSplitAssigner from the InputFormat API.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.FileSource.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-7 01:00:00" id="19523" opendate="2020-10-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hide sensitive command-line configurations</summary>
      <description>When jobmanager starts up, it logs configurations from jvm options and application arguments. This includes secrets (eg. "s3.secret-key").Ideally, secrets should be masked as it is when loading from configuration file.</description>
      <version>None</version>
      <fixedVersion>1.10.3,1.11.3,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.EnvironmentInformation.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-7 01:00:00" id="19527" opendate="2020-10-7 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Update SQL Pages</summary>
      <description>SQLGoal: Show users the main features early and link to concepts if necessary.How to use SQL? Intended for users with SQL knowledge. Overview Getting started with link to more detailed execution section. Full Reference Available operations in SQL as a table. This location allows to further split the page in the future if we think an operation needs more space without affecting the top-level structure. Data Definition Explain special SQL syntax around DDL. Pattern Matching Make pattern matching more visible. ... more features in the future</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.try-flink.table.api.zh.md</file>
      <file type="M">docs.try-flink.table.api.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-29 01:00:00" id="1956" opendate="2015-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Runtime context not initialized in RichWindowMapFunction</summary>
      <description>Trying to access the runtime context in a rich window map function results in an exception. The following snippet demonstrates the bug: env.generateSequence(0, 1000) .window(Count.of(10)) .mapWindow(new RichWindowMapFunction&lt;Long, Tuple2&lt;Long, Long&gt;&gt;() { @Override public void mapWindow(Iterable&lt;Long&gt; input, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out) throws Exception { long self = getRuntimeContext().getIndexOfThisSubtask(); for (long value : input) { out.collect(new Tuple2&lt;&gt;(self, value)); } } }).flatten().print();</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowReducer.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowMapper.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.operators.windowing.WindowFolder.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-12 01:00:00" id="19569" opendate="2020-10-12 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Upgrade ICU4J to 67.1+</summary>
      <description>https://nvd.nist.gov/vuln/detail/CVE-2020-10531</description>
      <version>None</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
      <file type="M">flink-table.flink-table-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-12 01:00:00" id="19581" opendate="2020-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce OrcColumnarRowInputFormat</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.ColumnarRowIterator.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetFsStreamingSinkITCase.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-parquet.src.test.java.org.apache.flink.formats.parquet.ParquetFileCompactionITCase.java</file>
      <file type="M">flink-formats.flink-parquet.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-formats.flink-parquet.src.main.java.org.apache.flink.formats.parquet.ParquetFileSystemFormatFactory.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcFsStreamingSinkITCase.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcFileSystemITCase.java</file>
      <file type="M">flink-formats.flink-orc.src.test.java.org.apache.flink.orc.OrcFileSystemFilterTest.java</file>
      <file type="M">flink-formats.flink-orc.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcSplitReaderUtil.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcFilters.java</file>
      <file type="M">flink-formats.flink-orc.src.main.java.org.apache.flink.orc.OrcFileSystemFormatFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-12 01:00:00" id="19586" opendate="2020-10-12 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement StreamCommitterOperator for new Sink API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.TestHarnessUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-13 01:00:00" id="19594" opendate="2020-10-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SubTasks start index don&amp;#39;t unified and may confuse users</summary>
      <description>In flink web ui page, subTasks index start from 0 in SubTasks tab while in BackPressure tab start from 1, at the same time the subTasks index start from 1 in Checkpoints page.I think this may confuse users and does there have some design purpose ?</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.watermarks.job-overview-drawer-watermarks.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.subtask.job-checkpoints-subtask.component.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-14 01:00:00" id="19622" opendate="2020-10-14 00:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Flinksql version 1.11 is for the NullPointerException of the Map type value value in the avro format</summary>
      <description>Hello, when I use flinksql version 1.11 to analyze data in avro format in Kafka, I used the Map type. For this type, my definition is MAP&lt;VARCHAR,VARCHAR&gt;, but the analysis is because the value in the map is empty. NullPointerExceptionThe source code is attached below (AvroRowDataDeserializationSchema)private static DeserializationRuntimeConverter createMapConverter(LogicalType type) { final DeserializationRuntimeConverter keyConverter = createConverter( DataTypes.STRING().getLogicalType()); final DeserializationRuntimeConverter valueConverter = createConverter( extractValueTypeToAvroMap(type)); return avroObject -&gt; { final Map&lt;?, ?&gt; map = (Map&lt;?, ?&gt;) avroObject; Map&lt;Object, Object&gt; result = new HashMap&lt;&gt;(); for (Map.Entry&lt;?, ?&gt; entry : map.entrySet()) { Object key = keyConverter.convert(entry.getKey()); Object value = valueConverter.convert(entry.getValue()); result.put(key, value); } return new GenericMapData(result); };}  </description>
      <version>1.11.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosSessionClusterEntrypoint.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.entrypoint.MesosJobClusterEntrypoint.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-14 01:00:00" id="19624" opendate="2020-10-14 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Update deadlock break-up algorithm to cover more cases</summary>
      <description>Current deadlock breakup algorithm fails to cover the following case: We're going to introduce a new deadlock breakup algorithm to cover this.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.DeadlockBreakupTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.DeadlockBreakupTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.reuse.DeadlockBreakupProcessor.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeVisitorImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-14 01:00:00" id="19625" opendate="2020-10-14 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce multi-input exec node</summary>
      <description>For multi-input to work in Blink planner, we should first introduce multi-input exec node in the planner.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeysTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.api.batch.ExplainTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelTreeWriterImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeys.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-14 01:00:00" id="19626" opendate="2020-10-14 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce multi-input operator construction algorithm</summary>
      <description>We should introduce an algorithm to organize exec nodes into multi-input exec nodes.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.reuse.InputPriorityConflictResolverTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.reuse.DeadlockBreakupProcessor.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.reuse.InputPriorityConflictResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.OptimizerConfigOptions.java</file>
      <file type="M">docs..includes.generated.optimizer.config.configuration.html</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-11-14 01:00:00" id="19630" opendate="2020-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sink data in [ORC] format into Hive By using Legacy Table API caused unexpected Exception</summary>
      <description>ENV:Flink version 1.11.2Hive exec version: 2.0.1Hive file storing type ：ORCSQL or Datastream: SQL APIKafka Connector :  custom Kafka connector which is based on Legacy API (TableSource/`org.apache.flink.types.Row`)Hive Connector : totally follows the Flink-Hive-connector (we only made some encapsulation upon it)Using StreamingFileCommitter：YES  Description:   try to execute the following SQL:    """      insert into hive_table (select * from kafka_table)    """   HIVE Table SQL seems like:    """CREATE TABLE `hive_table`( // some fieldsPARTITIONED BY ( `dt` string, `hour` string)STORED AS orcTBLPROPERTIES ( 'orc.compress'='SNAPPY','type'='HIVE','sink.partition-commit.trigger'='process-time','sink.partition-commit.delay' = '1 h','sink.partition-commit.policy.kind' = 'metastore,success-file',)      """When this job starts to process snapshot, here comes the weird exception:As we can see from the message：Owner thread shall be the &amp;#91;Legacy Source Thread&amp;#93;, but actually the streamTaskThread which represents the whole first stage is found. So I checked the Thread dump at once.                                                                     The legacy Source Thread                                                                The StreamTask Thread    According to the thread dump info and the Exception Message, I searched and read certain source code and then DID A TEST    Since the Kafka connector is customed, I tried to make the KafkaSource a serpated stage by changing the TaskChainStrategy to Never. The task topology as follows: Fortunately, it did work! No Exception is throwed and Checkpoint could be snapshot successfully!  So, from my perspective, there shall be something wrong when HiveWritingTask and  LegacySourceTask chained together. the Legacy source task is a seperated thread, which may be the cause of the exception mentioned above.                                                                  </description>
      <version>1.11.2</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-3-15 01:00:00" id="19667" opendate="2020-10-15 00:00:00" resolution="Done">
    <buginformation>
      <summary>Add integration with AWS Glue</summary>
      <description>AWS Glue is releasing new features for the AWS Glue Data Catalog. This request is to add a new format to launch an integration for Apache Flink with AWS Glue Data Catalog</description>
      <version>1.11.0,1.11.1,1.11.2,1.11.3</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-7-20 01:00:00" id="19739" opendate="2020-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CompileException when windowing in batch mode: A method named "replace" is not declared in any enclosing class nor any supertype</summary>
      <description>Example script:from pyflink.table import EnvironmentSettings, BatchTableEnvironmentfrom pyflink.table.window import Tumbleenv_settings = ( EnvironmentSettings.new_instance().in_batch_mode().use_blink_planner().build())table_env = BatchTableEnvironment.create(environment_settings=env_settings)table_env.execute_sql( """ CREATE TABLE table1 ( amount INT, ts TIMESTAMP(3), WATERMARK FOR ts AS ts - INTERVAL '5' SECOND ) WITH ( 'connector.type' = 'filesystem', 'format.type' = 'csv', 'connector.path' = '/home/alex/work/test-flink/data1.csv' )""")table1 = table_env.from_path("table1")table = ( table1 .window(Tumble.over("5.days").on("ts").alias("__window")) .group_by("__window") .select("amount.sum"))print(table.to_pandas())Output:WARNING: An illegal reflective access operation has occurredWARNING: Illegal reflective access by org.apache.flink.api.python.shaded.io.netty.util.internal.ReflectionUtil (file:/home/alex/work/flink/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT/opt/flink-python_2.11-1.12-SNAPSHOT.jar) to constructor java.nio.DirectByteBuffer(long,int)WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.python.shaded.io.netty.util.internal.ReflectionUtilWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operationsWARNING: All illegal access operations will be denied in a future release/* 1 *//* 2 */ public class LocalHashWinAggWithoutKeys$59 extends org.apache.flink.table.runtime.operators.TableStreamOperator/* 3 */ implements org.apache.flink.streaming.api.operators.OneInputStreamOperator, org.apache.flink.streaming.api.operators.BoundedOneInput {/* 4 *//* 5 */ private final Object[] references;/* 6 */ /* 7 */ private static final org.slf4j.Logger LOG$2 =/* 8 */ org.slf4j.LoggerFactory.getLogger("LocalHashWinAgg");/* 9 */ /* 10 */ private transient org.apache.flink.table.types.logical.LogicalType[] aggMapKeyTypes$5;/* 11 */ private transient org.apache.flink.table.types.logical.LogicalType[] aggBufferTypes$6;/* 12 */ private transient org.apache.flink.table.runtime.operators.aggregate.BytesHashMap aggregateMap$7;/* 13 */ org.apache.flink.table.data.binary.BinaryRowData emptyAggBuffer$9 = new org.apache.flink.table.data.binary.BinaryRowData(1);/* 14 */ org.apache.flink.table.data.writer.BinaryRowWriter emptyAggBufferWriterTerm$10 = new org.apache.flink.table.data.writer.BinaryRowWriter(emptyAggBuffer$9);/* 15 */ org.apache.flink.table.data.GenericRowData hashAggOutput = new org.apache.flink.table.data.GenericRowData(2);/* 16 */ private transient org.apache.flink.table.data.binary.BinaryRowData reuseAggMapKey$17 = new org.apache.flink.table.data.binary.BinaryRowData(1);/* 17 */ private transient org.apache.flink.table.data.binary.BinaryRowData reuseAggBuffer$18 = new org.apache.flink.table.data.binary.BinaryRowData(1);/* 18 */ private transient org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.Entry reuseAggMapEntry$19 = new org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.Entry(reuseAggMapKey$17, reuseAggBuffer$18);/* 19 */ org.apache.flink.table.data.binary.BinaryRowData aggMapKey$3 = new org.apache.flink.table.data.binary.BinaryRowData(1);/* 20 */ org.apache.flink.table.data.writer.BinaryRowWriter aggMapKeyWriter$4 = new org.apache.flink.table.data.writer.BinaryRowWriter(aggMapKey$3);/* 21 */ private boolean hasInput = false;/* 22 */ org.apache.flink.streaming.runtime.streamrecord.StreamRecord element = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord((Object)null);/* 23 */ private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);/* 24 *//* 25 */ public LocalHashWinAggWithoutKeys$59(/* 26 */ Object[] references,/* 27 */ org.apache.flink.streaming.runtime.tasks.StreamTask task,/* 28 */ org.apache.flink.streaming.api.graph.StreamConfig config,/* 29 */ org.apache.flink.streaming.api.operators.Output output,/* 30 */ org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception {/* 31 */ this.references = references;/* 32 */ aggMapKeyTypes$5 = (((org.apache.flink.table.types.logical.LogicalType[]) references[0]));/* 33 */ aggBufferTypes$6 = (((org.apache.flink.table.types.logical.LogicalType[]) references[1]));/* 34 */ this.setup(task, config, output);/* 35 */ if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) {/* 36 */ ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this)/* 37 */ .setProcessingTimeService(processingTimeService);/* 38 */ }/* 39 */ }/* 40 *//* 41 */ @Override/* 42 */ public void open() throws Exception {/* 43 */ super.open();/* 44 */ aggregateMap$7 = new org.apache.flink.table.runtime.operators.aggregate.BytesHashMap(this.getContainingTask(),this.getContainingTask().getEnvironment().getMemoryManager(),computeMemorySize(), aggMapKeyTypes$5, aggBufferTypes$6);/* 45 */ /* 46 */ /* 47 */ emptyAggBufferWriterTerm$10.reset();/* 48 */ /* 49 */ /* 50 */ if (true) {/* 51 */ emptyAggBufferWriterTerm$10.setNullAt(0);/* 52 */ } else {/* 53 */ emptyAggBufferWriterTerm$10.writeInt(0, ((int) -1));/* 54 */ }/* 55 */ /* 56 */ emptyAggBufferWriterTerm$10.complete();/* 57 */ /* 58 */ }/* 59 *//* 60 */ @Override/* 61 */ public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {/* 62 */ org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) element.getValue();/* 63 */ /* 64 */ org.apache.flink.table.data.binary.BinaryRowData currentAggBuffer$8;/* 65 */ int field$11;/* 66 */ boolean isNull$11;/* 67 */ int field$12;/* 68 */ boolean isNull$12;/* 69 */ boolean isNull$13;/* 70 */ int result$14;/* 71 */ org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.LookupInfo lookupInfo$20;/* 72 */ org.apache.flink.table.data.TimestampData field$21;/* 73 */ boolean isNull$21;/* 74 */ boolean isNull$22;/* 75 */ long result$23;/* 76 */ boolean isNull$24;/* 77 */ long result$25;/* 78 */ boolean isNull$26;/* 79 */ long result$27;/* 80 */ boolean isNull$28;/* 81 */ long result$29;/* 82 */ boolean isNull$30;/* 83 */ long result$31;/* 84 */ boolean isNull$32;/* 85 */ long result$33;/* 86 */ boolean isNull$34;/* 87 */ boolean result$35;/* 88 */ boolean isNull$36;/* 89 */ long result$37;/* 90 */ boolean isNull$38;/* 91 */ long result$39;/* 92 */ boolean isNull$40;/* 93 */ long result$41;/* 94 */ boolean isNull$42;/* 95 */ long result$43;/* 96 */ boolean isNull$44;/* 97 */ long result$45;/* 98 */ boolean isNull$46;/* 99 */ long result$47;/* 100 */ boolean isNull$48;/* 101 */ long result$49;/* 102 */ boolean isNull$50;/* 103 */ long result$51;/* 104 */ boolean isNull$52;/* 105 */ long result$53;/* 106 */ boolean isNull$55;/* 107 */ long result$56;/* 108 */ boolean isNull$57;/* 109 */ long result$58;/* 110 */ /* 111 */ /* 112 */ if (!in1.isNullAt(1)) {/* 113 */ hasInput = true;/* 114 */ // input field access for group key projection, window/pane assign/* 115 */ // and aggregate map update/* 116 */ isNull$11 = in1.isNullAt(0);/* 117 */ field$11 = -1;/* 118 */ if (!isNull$11) {/* 119 */ field$11 = in1.getInt(0);/* 120 */ }/* 121 */ isNull$21 = in1.isNullAt(1);/* 122 */ field$21 = null;/* 123 */ if (!isNull$21) {/* 124 */ field$21 = in1.getTimestamp(1, 3);/* 125 */ }/* 126 */ // assign timestamp(window or pane)/* 127 */ /* 128 */ /* 129 */ /* 130 */ /* 131 */ /* 132 */ isNull$22 = isNull$21;/* 133 */ result$23 = -1L;/* 134 */ if (!isNull$22) {/* 135 */ /* 136 */ result$23 = field$21.getMillisecond();/* 137 */ /* 138 */ }/* 139 */ /* 140 */ /* 141 */ isNull$24 = isNull$22 || false;/* 142 */ result$25 = -1L;/* 143 */ if (!isNull$24) {/* 144 */ /* 145 */ result$25 = (long) (result$23 * ((long) 1L));/* 146 */ /* 147 */ }/* 148 */ /* 149 */ isNull$26 = isNull$21;/* 150 */ result$27 = -1L;/* 151 */ if (!isNull$26) {/* 152 */ /* 153 */ result$27 = field$21.getMillisecond();/* 154 */ /* 155 */ }/* 156 */ /* 157 */ /* 158 */ isNull$28 = isNull$26 || false;/* 159 */ result$29 = -1L;/* 160 */ if (!isNull$28) {/* 161 */ /* 162 */ result$29 = (long) (result$27 * ((long) 1L));/* 163 */ /* 164 */ }/* 165 */ /* 166 */ /* 167 */ isNull$30 = isNull$28 || false;/* 168 */ result$31 = -1L;/* 169 */ if (!isNull$30) {/* 170 */ /* 171 */ result$31 = (long) (result$29 - ((long) 0L));/* 172 */ /* 173 */ }/* 174 */ /* 175 */ /* 176 */ isNull$32 = isNull$30 || false;/* 177 */ result$33 = -1L;/* 178 */ if (!isNull$32) {/* 179 */ /* 180 */ result$33 = (long) (result$31 % ((long) 432000000L));/* 181 */ /* 182 */ }/* 183 */ /* 184 */ /* 185 */ isNull$34 = isNull$32 || false;/* 186 */ result$35 = false;/* 187 */ if (!isNull$34) {/* 188 */ /* 189 */ result$35 = result$33 &lt; ((int) 0);/* 190 */ /* 191 */ }/* 192 */ /* 193 */ long result$54 = -1L;/* 194 */ boolean isNull$54;/* 195 */ if (result$35) {/* 196 */ /* 197 */ /* 198 */ /* 199 */ /* 200 */ /* 201 */ /* 202 */ isNull$36 = isNull$21;/* 203 */ result$37 = -1L;/* 204 */ if (!isNull$36) {/* 205 */ /* 206 */ result$37 = field$21.getMillisecond();/* 207 */ /* 208 */ }/* 209 */ /* 210 */ /* 211 */ isNull$38 = isNull$36 || false;/* 212 */ result$39 = -1L;/* 213 */ if (!isNull$38) {/* 214 */ /* 215 */ result$39 = (long) (result$37 * ((long) 1L));/* 216 */ /* 217 */ }/* 218 */ /* 219 */ /* 220 */ isNull$40 = isNull$38 || false;/* 221 */ result$41 = -1L;/* 222 */ if (!isNull$40) {/* 223 */ /* 224 */ result$41 = (long) (result$39 - ((long) 0L));/* 225 */ /* 226 */ }/* 227 */ /* 228 */ /* 229 */ isNull$42 = isNull$40 || false;/* 230 */ result$43 = -1L;/* 231 */ if (!isNull$42) {/* 232 */ /* 233 */ result$43 = (long) (result$41 % ((long) 432000000L));/* 234 */ /* 235 */ }/* 236 */ /* 237 */ /* 238 */ isNull$44 = isNull$42 || false;/* 239 */ result$45 = -1L;/* 240 */ if (!isNull$44) {/* 241 */ /* 242 */ result$45 = (long) (result$43 + ((long) 432000000L));/* 243 */ /* 244 */ }/* 245 */ /* 246 */ isNull$54 = isNull$44;/* 247 */ if (!isNull$54) {/* 248 */ result$54 = result$45;/* 249 */ }/* 250 */ }/* 251 */ else {/* 252 */ /* 253 */ /* 254 */ /* 255 */ /* 256 */ /* 257 */ isNull$46 = isNull$21;/* 258 */ result$47 = -1L;/* 259 */ if (!isNull$46) {/* 260 */ /* 261 */ result$47 = field$21.getMillisecond();/* 262 */ /* 263 */ }/* 264 */ /* 265 */ /* 266 */ isNull$48 = isNull$46 || false;/* 267 */ result$49 = -1L;/* 268 */ if (!isNull$48) {/* 269 */ /* 270 */ result$49 = (long) (result$47 * ((long) 1L));/* 271 */ /* 272 */ }/* 273 */ /* 274 */ /* 275 */ isNull$50 = isNull$48 || false;/* 276 */ result$51 = -1L;/* 277 */ if (!isNull$50) {/* 278 */ /* 279 */ result$51 = (long) (result$49 - ((long) 0L));/* 280 */ /* 281 */ }/* 282 */ /* 283 */ /* 284 */ isNull$52 = isNull$50 || false;/* 285 */ result$53 = -1L;/* 286 */ if (!isNull$52) {/* 287 */ /* 288 */ result$53 = (long) (result$51 % ((long) 432000000L));/* 289 */ /* 290 */ }/* 291 */ /* 292 */ isNull$54 = isNull$52;/* 293 */ if (!isNull$54) {/* 294 */ result$54 = result$53;/* 295 */ }/* 296 */ }/* 297 */ isNull$55 = isNull$24 || isNull$54;/* 298 */ result$56 = -1L;/* 299 */ if (!isNull$55) {/* 300 */ /* 301 */ result$56 = (long) (result$25 - result$54);/* 302 */ /* 303 */ }/* 304 */ /* 305 */ /* 306 */ isNull$57 = isNull$55 || false;/* 307 */ result$58 = -1L;/* 308 */ if (!isNull$57) {/* 309 */ /* 310 */ result$58 = (long) (result$56 - ((long) 0L));/* 311 */ /* 312 */ }/* 313 */ /* 314 */ // process each input/* 315 */ /* 316 */ // build aggregate map key/* 317 */ /* 318 */ /* 319 */ aggMapKeyWriter$4.reset();/* 320 */ /* 321 */ /* 322 */ if (false) {/* 323 */ aggMapKeyWriter$4.setNullAt(0);/* 324 */ } else {/* 325 */ aggMapKeyWriter$4.writeLong(0, result$58);/* 326 */ }/* 327 */ /* 328 */ aggMapKeyWriter$4.complete();/* 329 */ /* 330 */ // aggregate by each input with assigned timestamp/* 331 */ // look up output buffer using current key (grouping keys ..., assigned timestamp)/* 332 */ lookupInfo$20 = aggregateMap$7.lookup(aggMapKey$3);/* 333 */ currentAggBuffer$8 = lookupInfo$20.getValue();/* 334 */ if (!lookupInfo$20.isFound()) {/* 335 */ /* 336 */ // append empty agg buffer into aggregate map for current group key/* 337 */ try {/* 338 */ currentAggBuffer$8 =/* 339 */ aggregateMap$7.append(lookupInfo$20, emptyAggBuffer$9);/* 340 */ } catch (java.io.EOFException exp) {/* 341 */ /* 342 */ LOG$2.info("BytesHashMap out of memory with {} entries, output directly.", aggregateMap$7.getNumElements());/* 343 */ // hash map out of memory, output directly/* 344 */ /* 345 */ org.apache.flink.util.MutableObjectIterator&lt;org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.Entry&gt; iterator =/* 346 */ aggregateMap$7.getEntryIterator();/* 347 */ while (iterator.next(reuseAggMapEntry$19) != null) {/* 348 */ /* 349 */ /* 350 */ /* 351 */ hashAggOutput.replace(reuseAggMapKey$17, reuseAggBuffer$18);/* 352 */ /* 353 */ output.collect(outElement.replace(hashAggOutput));/* 354 */ }/* 355 */ /* 356 */ // retry append/* 357 */ /* 358 */ // reset aggregate map retry append/* 359 */ aggregateMap$7.reset();/* 360 */ lookupInfo$20 = aggregateMap$7.lookup(aggMapKey$3);/* 361 */ try {/* 362 */ currentAggBuffer$8 =/* 363 */ aggregateMap$7.append(lookupInfo$20, emptyAggBuffer$9);/* 364 */ } catch (java.io.EOFException e) {/* 365 */ throw new OutOfMemoryError("BytesHashMap Out of Memory.");/* 366 */ }/* 367 */ /* 368 */ /* 369 */ }/* 370 */ }/* 371 */ // aggregate buffer fields access/* 372 */ isNull$12 = currentAggBuffer$8.isNullAt(0);/* 373 */ field$12 = -1;/* 374 */ if (!isNull$12) {/* 375 */ field$12 = currentAggBuffer$8.getInt(0);/* 376 */ }/* 377 */ // do aggregate and update agg buffer/* 378 */ int result$16 = -1;/* 379 */ boolean isNull$16;/* 380 */ if (isNull$11) {/* 381 */ /* 382 */ isNull$16 = isNull$12;/* 383 */ if (!isNull$16) {/* 384 */ result$16 = field$12;/* 385 */ }/* 386 */ }/* 387 */ else {/* 388 */ int result$15 = -1;/* 389 */ boolean isNull$15;/* 390 */ if (isNull$12) {/* 391 */ /* 392 */ isNull$15 = isNull$11;/* 393 */ if (!isNull$15) {/* 394 */ result$15 = field$11;/* 395 */ }/* 396 */ }/* 397 */ else {/* 398 */ /* 399 */ /* 400 */ /* 401 */ isNull$13 = isNull$12 || isNull$11;/* 402 */ result$14 = -1;/* 403 */ if (!isNull$13) {/* 404 */ /* 405 */ result$14 = (int) (field$12 + field$11);/* 406 */ /* 407 */ }/* 408 */ /* 409 */ isNull$15 = isNull$13;/* 410 */ if (!isNull$15) {/* 411 */ result$15 = result$14;/* 412 */ }/* 413 */ }/* 414 */ isNull$16 = isNull$15;/* 415 */ if (!isNull$16) {/* 416 */ result$16 = result$15;/* 417 */ }/* 418 */ }/* 419 */ if (isNull$16) {/* 420 */ currentAggBuffer$8.setNullAt(0);/* 421 */ } else {/* 422 */ currentAggBuffer$8.setInt(0, result$16);/* 423 */ }/* 424 */ /* 425 */ }/* 426 */ }/* 427 *//* 428 */ /* 429 */ @Override/* 430 */ public void endInput() throws Exception {/* 431 */ org.apache.flink.table.data.binary.BinaryRowData currentAggBuffer$8;/* 432 */ int field$11;/* 433 */ boolean isNull$11;/* 434 */ int field$12;/* 435 */ boolean isNull$12;/* 436 */ boolean isNull$13;/* 437 */ int result$14;/* 438 */ org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.LookupInfo lookupInfo$20;/* 439 */ org.apache.flink.table.data.TimestampData field$21;/* 440 */ boolean isNull$21;/* 441 */ boolean isNull$22;/* 442 */ long result$23;/* 443 */ boolean isNull$24;/* 444 */ long result$25;/* 445 */ boolean isNull$26;/* 446 */ long result$27;/* 447 */ boolean isNull$28;/* 448 */ long result$29;/* 449 */ boolean isNull$30;/* 450 */ long result$31;/* 451 */ boolean isNull$32;/* 452 */ long result$33;/* 453 */ boolean isNull$34;/* 454 */ boolean result$35;/* 455 */ boolean isNull$36;/* 456 */ long result$37;/* 457 */ boolean isNull$38;/* 458 */ long result$39;/* 459 */ boolean isNull$40;/* 460 */ long result$41;/* 461 */ boolean isNull$42;/* 462 */ long result$43;/* 463 */ boolean isNull$44;/* 464 */ long result$45;/* 465 */ boolean isNull$46;/* 466 */ long result$47;/* 467 */ boolean isNull$48;/* 468 */ long result$49;/* 469 */ boolean isNull$50;/* 470 */ long result$51;/* 471 */ boolean isNull$52;/* 472 */ long result$53;/* 473 */ boolean isNull$55;/* 474 */ long result$56;/* 475 */ boolean isNull$57;/* 476 */ long result$58;/* 477 */ /* 478 */ org.apache.flink.util.MutableObjectIterator&lt;org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.Entry&gt; iterator =/* 479 */ aggregateMap$7.getEntryIterator();/* 480 */ while (iterator.next(reuseAggMapEntry$19) != null) {/* 481 */ /* 482 */ /* 483 */ /* 484 */ hashAggOutput.replace(reuseAggMapKey$17, reuseAggBuffer$18);/* 485 */ /* 486 */ output.collect(outElement.replace(hashAggOutput));/* 487 */ }/* 488 */ /* 489 */ }/* 490 */ /* 491 *//* 492 */ @Override/* 493 */ public void close() throws Exception {/* 494 */ super.close();/* 495 */ aggregateMap$7.free();/* 496 */ /* 497 */ }/* 498 *//* 499 */ /* 500 */ }/* 501 */ Traceback (most recent call last): File "/home/alex/.config/JetBrains/PyCharm2020.2/scratches/scratch_903.py", line 32, in &lt;module&gt; print(table.to_pandas()) File "/home/alex/work/flink/flink-python/pyflink/table/table.py", line 829, in to_pandas if batches.hasNext(): File "/home/alex/.cache/pypoetry/virtualenvs/relycomply-WA1zLZ2n-py3.8/lib/python3.8/site-packages/py4j/java_gateway.py", line 1285, in __call__ return_value = get_return_value( File "/home/alex/work/flink/flink-python/pyflink/util/exceptions.py", line 147, in deco return f(*a, **kw) File "/home/alex/.cache/pypoetry/virtualenvs/relycomply-WA1zLZ2n-py3.8/lib/python3.8/site-packages/py4j/protocol.py", line 326, in get_return_value raise Py4JJavaError(py4j.protocol.Py4JJavaError: An error occurred while calling o51.hasNext.: java.lang.RuntimeException: Failed to fetch next result at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106) at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:77) at org.apache.flink.table.planner.sinks.SelectTableSinkBase$RowIteratorWrapper.hasNext(SelectTableSinkBase.java:115) at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:355) at org.apache.flink.table.runtime.arrow.ArrowUtils$1.hasNext(ArrowUtils.java:644) at org.apache.flink.table.runtime.arrow.ArrowUtils$2.hasNext(ArrowUtils.java:666) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282) at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79) at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238) at java.base/java.lang.Thread.run(Thread.java:834)Caused by: java.io.IOException: Failed to fetch job execution result at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:175) at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:126) at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:103) ... 16 moreCaused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2022) at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:172) ... 18 moreCaused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147) at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$2(MiniClusterJobClient.java:119) at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:680) at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658) at java.base/java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:2094) at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:117) ... 19 moreCaused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116) at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78) at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:217) at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:210) at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:204) at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:526) at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:413) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.RuntimeException: Could not instantiate generated class 'LocalHashWinAggWithoutKeys$59' at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:67) at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40) at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:70) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperator(OperatorChain.java:613) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:583) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:526) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:574) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:526) at org.apache.flink.streaming.runtime.tasks.OperatorChain.&lt;init&gt;(OperatorChain.java:164) at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:485) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:533) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) at java.base/java.lang.Thread.run(Thread.java:834)Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:68) at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:78) at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:65) ... 13 moreCaused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739) at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:66) ... 15 moreCaused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:81) at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:66) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ... 18 moreCaused by: org.codehaus.commons.compiler.CompileException: Line 351, Column 33: A method named "replace" is not declared in any enclosing class nor any supertype, nor through a static import at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12124) at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:8997) at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5060) at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4421) at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4394) at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5062) at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4394) at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5575) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3781) at org.codehaus.janino.UnitCompiler.access$5900(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3760) at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3732) at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5062) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3732) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360) at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494) at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2871) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) at org.codehaus.janino.Java$Block.accept(Java.java:2776) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1842) at org.codehaus.janino.UnitCompiler.access$2200(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1498) at org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$WhileStatement.accept(Java.java:3052) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) at org.codehaus.janino.Java$Block.accept(Java.java:2776) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileTryCatch(UnitCompiler.java:3136) at org.codehaus.janino.UnitCompiler.compileTryCatchFinally(UnitCompiler.java:2966) at org.codehaus.janino.UnitCompiler.compileTryCatchFinallyWithResources(UnitCompiler.java:2770) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2742) at org.codehaus.janino.UnitCompiler.access$2300(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitTryStatement(UnitCompiler.java:1499) at org.codehaus.janino.UnitCompiler$6.visitTryStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$TryStatement.accept(Java.java:3238) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) at org.codehaus.janino.Java$Block.accept(Java.java:2776) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476) at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$IfStatement.accept(Java.java:2947) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) at org.codehaus.janino.Java$Block.accept(Java.java:2776) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476) at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) at org.codehaus.janino.Java$IfStatement.accept(Java.java:2947) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:78) ... 24 moreHowever it works fine in streaming mode:env_settings = ( EnvironmentSettings.new_instance().in_streaming_mode().use_blink_planner().build())table_env = StreamTableEnvironment.create(environment_settings=env_settings)How the table is created seems irrelevant - this raises the same error:from datetime import datetimefrom pyflink.table import DataTypes, BatchTableEnvironment, EnvironmentSettingsfrom pyflink.table.window import Tumbleenv_settings = ( EnvironmentSettings.new_instance().in_batch_mode().use_blink_planner().build())table_environment = BatchTableEnvironment.create(environment_settings=env_settings)transactions = table_environment.from_elements( [ (1, datetime(2000, 1, 1, 0, 0, 0)), (-2, datetime(2000, 1, 2, 0, 0, 0)), (3, datetime(2000, 1, 3, 0, 0, 0)), (-4, datetime(2000, 1, 4, 0, 0, 0)), ], DataTypes.ROW( [ DataTypes.FIELD("amount", DataTypes.BIGINT()), DataTypes.FIELD("ts", DataTypes.TIMESTAMP(3)), ] ),)table = ( transactions .window(Tumble.over("5.days").on("ts").alias("__window")) .group_by("__window") .select("amount.sum"))print(table.to_pandas())</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.14.0,1.13.3,1.12.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.WindowCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashWindowCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-21 01:00:00" id="19750" opendate="2020-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deserializer is not opened in Kafka consumer when restoring from state</summary>
      <description>When a job using Kafka consumer is recovered from a checkpoint or savepoint, the open method of the record deserializer is not called. This is possibly because this.deserializer.open is put into the else clause by mistake, which will only be called if the job has a clean start. </description>
      <version>1.11.0,1.11.1,1.11.2</version>
      <fixedVersion>1.11.3,1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-22 01:00:00" id="19762" opendate="2020-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Selecting Job-ID and TaskManager-ID in web UI covers more than the ID</summary>
      <description>Not only the ID is selected when trying to copy the Job ID from the web UI by double-clicking it. See the attached screenshot: The same thing happens for the TaskManager ID in the corresponding TaskManager Overview page.</description>
      <version>1.10.2,1.11.2</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.status.task-manager-status.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.status.task-manager-status.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.status.job-status.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.status.job-status.component.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-22 01:00:00" id="19764" opendate="2020-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add More Metrics to TaskManager in Web UI</summary>
      <description>update the UI since https://issues.apache.org/jira/browse/FLINK-14422 and https://issues.apache.org/jira/browse/FLINK-14406 has been fixed</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.task-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.task-manager.ts</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-22 01:00:00" id="19767" opendate="2020-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add AbstractSlotPoolFactory</summary>
      <description>FLINK-19314 introduces another slot pool implementation, with the factory for it being a carbon copy of the factory for the existing slot pool factory.We can introduce an abstract factory class to reduce code duplication.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.slotpool.DefaultSlotPoolFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-1-22 01:00:00" id="19773" opendate="2020-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exponential backoff restart strategy</summary>
      <description>There are situations where the current restart strategies (fixed-delay and failure-rate) seem to be suboptimal.  For example, in HDFS sinks, a delay between restarts shorter than the lease expiration time in HDFS is going to result in many restart attempts which fail, putting somewhat pointless stress on a cluster.  On the other hand, setting a delay of close to the lease expiration time will mean far more downtime than necessary when the cause of failure is something that works itself out quickly. An exponential backoff restart strategy would address this.  For example a backoff strategy where the jobs are contending for a lease on a shared resource that terminates after 1200 seconds of inactivity might have successive delays of 1, 2, 4, 8, 16... 1024 seconds (after which a cumulative delay of more than 1200 seconds has passed).While not intrinsically tied to exponential backoff (it's more of an example of variable delay), in the case of many jobs failing due to an infrastructure failure, a thundering herd scenario can be mitigated by adding jitter to the delays, e.g. 0 -&gt; 1 -&gt; 2 -&gt; 3/4/5 -&gt; 5/6/7/8/9/10/11 seconds.  With this jitter, eventually a set of jobs competing to restart will spread out.(logging the ticket more to start a discussion and perhaps get context around if this had been considered and rejected, etc.)</description>
      <version>1.11.2</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.failover.flip1.RestartBackoffTimeStrategyFactoryLoaderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.failover.flip1.RestartBackoffTimeStrategyFactoryLoader.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestartStrategyOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.restartstrategy.RestartStrategies.java</file>
      <file type="M">docs..includes.generated.restart.strategy.configuration.html</file>
      <file type="M">docs.dev.task.failure.recovery.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-23 01:00:00" id="19781" opendate="2020-10-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade commons_codec to 1.13 or newer</summary>
      <description>A user reported a dependency vulnerability which affects commons_codec &amp;#91;1&amp;#93;. We should try to upgrade this version to 1.13 or newer.&amp;#91;1&amp;#93; https://lists.apache.org/thread.html/r0dd7ff197b2e3bdd80a0326587ca3d0c22e10d1dba17c769d6da7d7a%40%3Cuser.flink.apache.org%3E</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-23 01:00:00" id="19782" opendate="2020-10-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade antlr to 4.7.1 or newer</summary>
      <description>A user reported dependency vulnerabilities which affect antlr &amp;#91;1&amp;#93;. We should upgrade this dependency to 4.7.1 or newer.&amp;#91;1&amp;#93; https://lists.apache.org/thread.html/r0dd7ff197b2e3bdd80a0326587ca3d0c22e10d1dba17c769d6da7d7a%40%3Cuser.flink.apache.org%3E</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-23 01:00:00" id="19783" opendate="2020-10-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade mesos to 1.7 or newer</summary>
      <description>A user reported a dependency vulnerability which affects mesos &amp;#91;1&amp;#93;. We should upgrade mesos to 1.7.0 or newer.&amp;#91;1&amp;#93; https://lists.apache.org/thread.html/r0dd7ff197b2e3bdd80a0326587ca3d0c22e10d1dba17c769d6da7d7a%40%3Cuser.flink.apache.org%3E</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-mesos.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-23 01:00:00" id="19785" opendate="2020-10-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade commons-io to 2.7 or newer</summary>
      <description>A user reported a dependency vulnerability which affects commons-io &amp;#91;1&amp;#93;. We should try to upgrade this dependency to 2.7 or newer.&amp;#91;1&amp;#93; https://lists.apache.org/thread.html/r0dd7ff197b2e3bdd80a0326587ca3d0c22e10d1dba17c769d6da7d7a%40%3Cuser.flink.apache.org%3E</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-10-23 01:00:00" id="19789" opendate="2020-10-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate Hive connector to new table source sink interface</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveLookupJoinITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveDynamicTableFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-25 01:00:00" id="19796" opendate="2020-10-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error results when use constant decimal array</summary>
      <description>The result is wrong when use constant decimal array with different precisions. Here is an example:create table print_sink( data array&lt;decimal(3,2)&gt;) with ( 'connector' = 'print');insert into print_sinkselect array[0.12, 0.5, 0.99];The result will be:+I([0.12, 0.05, 0.99])</description>
      <version>1.11.2</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.CalcTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.CalcTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-11-29 01:00:00" id="19878" opendate="2020-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix WatermarkAssigner shouldn&amp;#39;t be after ChangelogNormalize</summary>
      <description>Cutrrently, for a upsertSource like upsert-kafka, the WatermarkAssigner is followed after ChangelogNormalize Node,  it may returns Long.MaxValue as watermark if some parallelism doesn't have data.   +- Exchange(distribution=[hash[currency]], changelogMode=[I,UA,D]) +- WatermarkAssigner(rowtime=[rowtime], watermark=[rowtime], changelogMode=[I,UA,D]) +- ChangelogNormalize(key=[currency], changelogMode=[I,UA,D]) +- Exchange(distribution=[hash[currency]], changelogMode=[UA,D]) +- TableSourceScan(table=[[default_catalog, default_database, rates_history]], fields=[currency, rate, rowtime], changelogMode=[UA,D]) As an improvement, we can move the WatermarkAssigner to be after the SourceCan Node and thus the watermark will produce like general Source. </description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkStreamProgram.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-29 01:00:00" id="19885" opendate="2020-10-29 00:00:00" resolution="Done">
    <buginformation>
      <summary>Extend the JobManager&amp;#39;s web UI in accordance to FLIP-104</summary>
      <description>See FLIP-104 for more details on the UI.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.job-manager-routing.module.ts</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-11-30 01:00:00" id="19888" opendate="2020-10-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate Hive source to FLIP-27 source interface for streaming</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.PartitionFetcher.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.ContinuousPartitionFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveContinuousPartitionFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.read.HiveContinuousMonitoringFunction.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveSource.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.PendingSplitsCheckpoint.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.ContinuousEnumerationSettings.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.src.AbstractFileSource.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-31 01:00:00" id="19908" opendate="2020-10-31 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>FlinkLogicalTableSourceScan and CommonPhysicalTableSourceScan should respect source reuse config option</summary>
      <description>Currently we have the table.optimizer.reuse-sub-plan-enabled config option to configure the reuse of sources. However FlinkLogicalTableSourceScan and CommonPhysicalTableSourceScan do not respect this option and are always reused.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SubplanReuseTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.reuse.SubplanReuser.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-11-4 01:00:00" id="19959" opendate="2020-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multiple input creation algorithm will deduce an incorrect input order if the inputs are related under PIPELINED shuffle mode</summary>
      <description>Consider the following SQLWITH T1 AS (SELECT x.a AS a, y.d AS b FROM y LEFT JOIN x ON y.d = x.a), T2 AS (SELECT a, b FROM (SELECT a, b FROM T1) UNION ALL (SELECT x.a AS a, x.b AS b FROM x))SELECT * FROM T2 LEFT JOIN t ON T2.a = t.aThe multiple input creation algorithm will currently deduce the following plan under the PIPELINED shuffle mode:MultipleInputNode(readOrder=[0,1,1,0], members=[\nNestedLoopJoin(joinType=[LeftOuterJoin], where=[=(a, a0)], select=[a, b, a0, b0, c], build=[right])\n:- Union(all=[true], union=[a, b])\n: :- Calc(select=[a, CAST(d) AS b])\n: : +- NestedLoopJoin(joinType=[LeftOuterJoin], where=[=(d, a)], select=[d, a], build=[right])\n: : :- [#3] Calc(select=[d])\n: : +- [#4] Exchange(distribution=[broadcast])\n: +- [#2] Calc(select=[a, b])\n+- [#1] Exchange(distribution=[broadcast])\n]):- Exchange(distribution=[broadcast]): +- BoundedStreamScan(table=[[default_catalog, default_database, t]], fields=[a, b, c]):- Calc(select=[a, b]): +- LegacyTableSourceScan(table=[[default_catalog, default_database, x, source: [TestTableSource(a, b, c, nx)]]], fields=[a, b, c, nx], reuse_id=[1]):- Calc(select=[d]): +- LegacyTableSourceScan(table=[[default_catalog, default_database, y, source: [TestTableSource(d, e, f, ny)]]], fields=[d, e, f, ny])+- Exchange(distribution=[broadcast]) +- Calc(select=[a]) +- Reused(reference_id=[1])It's obvious that the 2nd and the 4th input for the multiple input node should have the same input priority, otherwise a deadlock will occur.This is because the current algorithm fails to consider the case when the inputs are related out of the multiple input node.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.MultipleInputCreationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.MultipleInputCreationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.processors.utils.TopologyGraphTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.processors.utils.InputOrderCalculatorTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.processors.utils.TopologyGraph.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.processors.utils.InputOrderCalculator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-4 01:00:00" id="19971" opendate="2020-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-connector-hbase-base depends on hbase-server, instead of hbase-client</summary>
      <description>flink-connector-hbase-base depends on hbase-server, instead of hbase-client and hbase-common.I believe hbase-server is only needed for starting a Minicluster and therefore should be in the test scope.This was introduced by FLINK-1928.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hbase-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hbase-1.4.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-11-5 01:00:00" id="19998" opendate="2020-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalid Links in docs</summary>
      <description>Multiple broken links matching pattern: site.baseurl }}{% linkAll these need to be replaced. Eg:In the docs/concepts/stateful-stream-processing.md file, under the first section (What is State), the following two links are broken: Checkpoints: &amp;#91;checkpoints&amp;#93;({{ site.baseurl}}{% link dev/stream/state/checkpointing.md %}) Savepoints: &amp;#91;savepoints&amp;#93;({{ site.baseurl }}{%link ops/state/savepoints.md %})This results in the target link as follows: For Checkpoints: https://ci.apache.org/projects/flink/flink-docs-master//ci.apache.org/projects/flink/flink-docs-master/dev/stream/state/checkpointing.html https://ci.apache.org/projects/flink/flink-docs-master//ci.apache.org/projects/flink/flink-docs-master/ops/state/savepoints.html</description>
      <version>1.11.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.parallel.zh.md</file>
      <file type="M">docs.dev.java.lambdas.zh.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-11 01:00:00" id="2000" opendate="2015-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add SQL-style aggregations for Table API</summary>
      <description>Right now, the syntax for aggregations is "a.count, a.min" and so on. We could in addition offer "COUNT(a), MIN(a)" and so on.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.GroupedAggreagationsITCase.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.parser.ExpressionParser.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-6-12 01:00:00" id="2002" opendate="2015-5-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Iterative test fails when ran with other tests in the same environment</summary>
      <description>I run tests in the same StreamExecutionEnvironment with MultipleProgramsTestBase. One of the tests uses an iterative data stream. It fails as well as all tests after that. (When I put the iterative test in a separate environment, all tests passes.) For me it seems that it is a state-related issue but there is also some problem with the broker slots.The iterative test throws:java.lang.Exception: TaskManager sent illegal state update: CANCELING at org.apache.flink.runtime.executiongraph.ExecutionGraph.updateState(ExecutionGraph.java:618) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1$$anonfun$applyOrElse$2.apply$mcV$sp(JobManager.scala:222) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1$$anonfun$applyOrElse$2.apply(JobManager.scala:221) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1$$anonfun$applyOrElse$2.apply(JobManager.scala:221) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:401) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1.applyOrElse(JobManager.scala:314) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:36) at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:29) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118) at org.apache.flink.runtime.ActorLogMessages$$anon$1.applyOrElse(ActorLogMessages.scala:29) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:95) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.pollAndExecAll(ForkJoinPool.java:1253) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1346) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Not enough free slots available to run the job. You can decrease the operator parallelism or increase the number of slots per TaskManager in the configuration. Task to schedule: &lt; Attempt #0 (GroupedActiveDiscretizer (2/4)) @ (unassigned) - &amp;#91;SCHEDULED&amp;#93; &gt; with groupID &lt; e8f7c9c85e64403962648bc7e2aead8b &gt; in sharing group &lt; SlotSharingGroup &amp;#91;5e62f1cc5cae2c088430ef935470a8d5, 5bc227941969d1daa1ebb1ba070b55ce, d999ee6c10730775a8fef1c6f1af1dbd, 45b73caa75424d84adbb7bb92671591d, 5c94c54d9316b827c6eba6c721329549, 794d6c56bee347dcdd62ffdf189de267, 4c3b72e17a4acecde4241fe6e63355b8, f6a6028c224a7b81e4802eeaf9c8487e, 989c68790fc7c5e2f8b8c150a33fef89, db93daa1f9e5194f0079df2629b08efb, bf7dbb1fd756ce322249eb973844b375, 9ddf3bd146c21c574077c58a1f64aeaa, e888ff4653070b9c4adcbb22a8121292, 9c620fd6d784bc4f5d7e100ad1dcb442, e8f7c9c85e64403962648bc7e2aead8b, 4fa798b9eab295876fdd21aeb6c7cfec, 32851c5f48ac128f71df0ec76f5b5ccd, c3f65a51704444b676cd392fbda91872&amp;#93; &gt;. Resources available to scheduler: Number of instances=1, total number of slots=1, available slots=0 at org.apache.flink.runtime.jobmanager.scheduler.Scheduler.scheduleTask(Scheduler.java:212) at org.apache.flink.runtime.jobmanager.scheduler.Scheduler.scheduleImmediately(Scheduler.java:110) at org.apache.flink.runtime.executiongraph.Execution.scheduleForExecution(Execution.java:263) at org.apache.flink.runtime.executiongraph.ExecutionVertex.scheduleForExecution(ExecutionVertex.java:437) at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.scheduleAll(ExecutionJobVertex.java:306) at org.apache.flink.runtime.executiongraph.ExecutionGraph.scheduleForExecution(ExecutionGraph.java:447) at org.apache.flink.runtime.jobmanager.JobManager.org$apache$flink$runtime$jobmanager$JobManager$$submitJob(JobManager.scala:580) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1.applyOrElse(JobManager.scala:194) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:36) at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:29) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118) at org.apache.flink.runtime.ActorLogMessages$$anon$1.applyOrElse(ActorLogMessages.scala:29) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:95) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ... 2 morewhile the following tests throw:java.lang.Exception: Error setting up runtime environment: java.lang.RuntimeException: Could not register the given element, broker slot is already occupied. at org.apache.flink.runtime.execution.RuntimeEnvironment.&lt;init&gt;(RuntimeEnvironment.java:192) at org.apache.flink.runtime.taskmanager.TaskManager.org$apache$flink$runtime$taskmanager$TaskManager$$initializeTask(TaskManager.scala:855) at org.apache.flink.runtime.taskmanager.TaskManager$$anonfun$submitTask$1.apply$mcV$sp(TaskManager.scala:799) at org.apache.flink.runtime.taskmanager.TaskManager$$anonfun$submitTask$1.apply(TaskManager.scala:799) at org.apache.flink.runtime.taskmanager.TaskManager$$anonfun$submitTask$1.apply(TaskManager.scala:799) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:401) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.RuntimeException: java.lang.RuntimeException: Could not register the given element, broker slot is already occupied. at org.apache.flink.streaming.runtime.tasks.StreamIterationHead.setInputsOutputs(StreamIterationHead.java:65) at org.apache.flink.streaming.runtime.tasks.StreamTask.registerInputOutput(StreamTask.java:86) at org.apache.flink.runtime.execution.RuntimeEnvironment.&lt;init&gt;(RuntimeEnvironment.java:189) ... 12 moreCaused by: java.lang.RuntimeException: Could not register the given element, broker slot is already occupied. at org.apache.flink.runtime.iterative.concurrent.Broker.handIn(Broker.java:39) at org.apache.flink.streaming.runtime.tasks.StreamIterationHead.setInputsOutputs(StreamIterationHead.java:62) ... 14 more</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.test.java.org.apache.flink.streaming.api.complex.ComplexIntegrationTest.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-11-12 01:00:00" id="20102" opendate="2020-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update HBase connector documentation for HBase 2.x supporting</summary>
      <description>Currently, the HBase connector page says it only supports HBase 1.4.x.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.index.md</file>
      <file type="M">docs..data.sql-connectors.yml</file>
      <file type="M">docs.dev.table.connectors.hbase.zh.md</file>
      <file type="M">docs.dev.table.connectors.hbase.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-12 01:00:00" id="20130" opendate="2020-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ZStandard format to inputs</summary>
      <description>Allow Flink to read files compressed in ZStandard (.zst)</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.compression.ZStandardInputStreamFactory.java</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.api.common.io.GenericCsvInputFormatTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.FileInputFormat.java</file>
      <file type="M">docs.content.zh.docs.dev.dataset.overview.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-11-13 01:00:00" id="20144" opendate="2020-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change link format to "{% link ... %}" in docs/ops dir</summary>
      <description>Some documents' link format in docs/ops is "{{site.baseurl}/... }". But it is preferred to use "{% link ... %}" format.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.savepoints.md</file>
      <file type="M">docs.ops.upgrading.zh.md</file>
      <file type="M">docs.ops.upgrading.md</file>
      <file type="M">docs.ops.state.state.backends.zh.md</file>
      <file type="M">docs.ops.state.state.backends.md</file>
      <file type="M">docs.ops.state.savepoints.zh.md</file>
      <file type="M">docs.ops.cli.md</file>
      <file type="M">docs.ops.cli.zh.md</file>
      <file type="M">docs.ops.config.md</file>
      <file type="M">docs.ops.config.zh.md</file>
      <file type="M">docs.ops.deployment.cluster.setup.md</file>
      <file type="M">docs.ops.deployment.docker.md</file>
      <file type="M">docs.ops.deployment.docker.zh.md</file>
      <file type="M">docs.ops.deployment.kubernetes.md</file>
      <file type="M">docs.ops.deployment.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.local.md</file>
      <file type="M">docs.ops.deployment.local.zh.md</file>
      <file type="M">docs.ops.deployment.mesos.md</file>
      <file type="M">docs.ops.deployment.mesos.zh.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.md</file>
      <file type="M">docs.ops.deployment.native.kubernetes.zh.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.zh.md</file>
      <file type="M">docs.ops.filesystems.azure.md</file>
      <file type="M">docs.ops.filesystems.azure.zh.md</file>
      <file type="M">docs.ops.filesystems.index.md</file>
      <file type="M">docs.ops.filesystems.index.zh.md</file>
      <file type="M">docs.ops.filesystems.oss.md</file>
      <file type="M">docs.ops.filesystems.oss.zh.md</file>
      <file type="M">docs.ops.filesystems.s3.md</file>
      <file type="M">docs.ops.filesystems.s3.zh.md</file>
      <file type="M">docs.ops.jobmanager.high.availability.md</file>
      <file type="M">docs.ops.jobmanager.high.availability.zh.md</file>
      <file type="M">docs.ops.memory.mem.migration.md</file>
      <file type="M">docs.ops.memory.mem.migration.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.md</file>
      <file type="M">docs.ops.memory.mem.setup.jobmanager.md</file>
      <file type="M">docs.ops.memory.mem.setup.jobmanager.zh.md</file>
      <file type="M">docs.ops.memory.mem.setup.tm.md</file>
      <file type="M">docs.ops.memory.mem.setup.tm.zh.md</file>
      <file type="M">docs.ops.memory.mem.trouble.md</file>
      <file type="M">docs.ops.memory.mem.trouble.zh.md</file>
      <file type="M">docs.ops.memory.mem.tuning.md</file>
      <file type="M">docs.ops.memory.mem.tuning.zh.md</file>
      <file type="M">docs.ops.production.ready.md</file>
      <file type="M">docs.ops.production.ready.zh.md</file>
      <file type="M">docs.ops.python.shell.md</file>
      <file type="M">docs.ops.python.shell.zh.md</file>
      <file type="M">docs.ops.security-kerberos.md</file>
      <file type="M">docs.ops.security-kerberos.zh.md</file>
      <file type="M">docs.ops.security-ssl.md</file>
      <file type="M">docs.ops.security-ssl.zh.md</file>
      <file type="M">docs.ops.state.checkpoints.md</file>
      <file type="M">docs.ops.state.checkpoints.zh.md</file>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
      <file type="M">docs.ops.state.large.state.tuning.zh.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-11-17 01:00:00" id="20184" opendate="2020-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update hive streaming read and temporal table documents</summary>
      <description>The hive streaming read and temporal table document has been out of style, we need to update it.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.hive.hive.read.write.zh.md</file>
      <file type="M">docs.dev.table.connectors.hive.hive.read.write.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-17 01:00:00" id="20195" opendate="2020-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jobs endpoint returns duplicated jobs</summary>
      <description>The GET /jobs endpoint can, for a split second, return a duplicated job after it has been cancelled. This occurred in Ververica Platform after canceling a job (using PATCH /jobs/{jobId}) and calling GET /jobs.I've reproduced this and queried the endpoint in a relatively tight loop (~ every 0.5s) to log the responses of GET /jobs and got this:  …{"jobs":[{"id":"e110531c08dd4e3dbbfcf7afc1629c3d","status":"RUNNING"},{"id":"53fd11db25394308862c997dce9ef990","status":"CANCELLING"}]}{"jobs":[{"id":"e110531c08dd4e3dbbfcf7afc1629c3d","status":"RUNNING"},{"id":"53fd11db25394308862c997dce9ef990","status":"CANCELLING"}]}{"jobs":[{"id":"e110531c08dd4e3dbbfcf7afc1629c3d","status":"FAILED"},{"id":"53fd11db25394308862c997dce9ef990","status":"CANCELED"},{"id":"53fd11db25394308862c997dce9ef990","status":"CANCELED"}]}{"jobs":[{"id":"53fd11db25394308862c997dce9ef990","status":"CANCELED"},{"id":"e110531c08dd4e3dbbfcf7afc1629c3d","status":"FAILED"}]}{"jobs":[{"id":"53fd11db25394308862c997dce9ef990","status":"CANCELED"},{"id":"e110531c08dd4e3dbbfcf7afc1629c3d","status":"FAILED"}]}… You can see in in between that for just a moment, the endpoint returned the same Job ID twice. </description>
      <version>1.11.2</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.TestingJobManagerRunner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-11-18 01:00:00" id="20202" opendate="2020-11-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add check of unsupported result type in Pandas UDAF</summary>
      <description>Currently the return type of Pandas UDAF should be a primitive data type, and the returned scalar can be either a python primitive type, e.g., int or float or a numpy data type, e.g., numpy.int64 or numpy.float64. Any should ideally be a specific scalar type accordingly. We will add related DataType check and throw a more readable exception for unsupported DataTypes. What's more, we will add related notes in docs.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udaf.py</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.udfs.vectorized.python.udfs.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-11-19 01:00:00" id="20245" opendate="2020-11-19 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Document how to create a Hive catalog from DDL</summary>
      <description>I'd appreciate if the documentation contained a description how to create the hive catalog from DDL. What I am missing especially are the options that HiveCatalog expects (type, conf-dir). We should have a table somewhere with a description possible values etc. the same way as we have such tables for other connectors and formats. See e.g. https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/kafka.html#connector-options</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connectors.hive.index.zh.md</file>
      <file type="M">docs.dev.table.connectors.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-20 01:00:00" id="20258" opendate="2020-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configured memory sizes on the JM metrics page should be displayed with proper units.</summary>
      <description>Configured memory sizes for JM are displayed in bytes. It would be better to use proper units here, same as what we do for TM. </description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.share.pipes.pipe.module.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.metrics.job-manager-metrics.component.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-20 01:00:00" id="20259" opendate="2020-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add explanation that "configured values" for JM/TM memory sizes include automatically derived values</summary>
      <description>The column title `Configured Values` might be a bit confusing. Not all of the values are explicitly configured by users. There could be values that are automatically derived from users' configuration. I would suggest to add a bit explanation (e.g., a  with some hidden texts) for both JM and TM.</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.task-manager.metrics.task-manager-metrics.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.metrics.job-manager-metrics.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job-manager.metrics.job-manager-metrics.component.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-11-22 01:00:00" id="20275" opendate="2020-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The path delimiter for add_jars and add_classpaths in Python StreamExecutionEnvironment should be ;</summary>
      <description>Currently, the path delimiter for add_jars and add_classpaths in Python StreamExecutionEnvironment is ",", this would cause the rest client fail to upload the specified jars and stuck forever without errors. It should be ";" instead.</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-11-26 01:00:00" id="20365" opendate="2020-11-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The native k8s cluster could not be unregistered when executing Python DataStream application attachedly.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonEnvUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonDriver.java</file>
      <file type="M">flink-python.pyflink.util.exceptions.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-27 01:00:00" id="20387" opendate="2020-11-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support defining event time attribute on TIMESTAMP_LTZ column</summary>
      <description>Currently, only TIMESTAMP type can be used as rowtime attribute. Would be better to support TIMESTAMP WITH LOCAL ZONE TIME as well.As a workaround, users can cast the TIMESTAMP WITH LOCAL ZONE TIME into TIMESTAMP, CAST(ts AS TIMESTAMP).</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.window.WindowOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.test.java.org.apache.flink.table.runtime.operators.aggregate.window.SlicingWindowAggOperatorTest.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.util.TimeWindowUtil.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.typeutils.TypeCheckUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.types.PlannerTypeUtils.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.WindowOperatorBuilder.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.WindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.triggers.Trigger.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.triggers.ProcessingTimeTriggers.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.triggers.EventTimeTriggers.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.slicing.SlicingWindowOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.slicing.SliceAssigners.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.internal.MergingWindowProcessFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.internal.InternalWindowProcessFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.window.combines.WindowCombineFunction.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.window.WindowRankOperatorBuilder.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.window.processors.WindowRankProcessor.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.rank.window.combines.TopNRecordsCombiner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.SlicingWindowAggOperatorBuilder.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.processors.SliceSharedWindowAggProcessor.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.processors.AbstractWindowAggProcessor.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.LocalSlicingWindowAggOperator.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.combines.GlobalAggAccCombiner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.combines.AggRecordsCombiner.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.buffers.WindowBuffer.java</file>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.operators.aggregate.window.buffers.RecordsWindowBuffer.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.DateTimeTestUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.utils.TestData.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.WindowRankITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.WindowDistinctAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.WindowAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.TimeAttributeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SourceWatermarkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.MatchRecognizeITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.GroupWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.WindowAggregateUseDaylightTimeHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.harness.WindowAggregateHarnessTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.validation.TemporalTableJoinValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.UnionTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.IntervalJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.schema.TimeIndicatorRelDataTypeTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.TemporalTableFunctionJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowTableFunctionTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowRankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.UnionTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SourceWatermarkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RelTimeIndicatorConverterTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.MiniBatchIntervalInferTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.TemporalFunctionJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.IntervalJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ExpandWindowTableFunctionTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupWindowAggregateOperator.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PassThroughPythonStreamGroupWindowAggregateOperator.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.DefaultSchemaResolver.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.catalog.SchemaResolutionTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.Schema.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.TableSchema.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.types.logical.utils.LogicalTypeChecks.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.api.TableSchemaTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.catalog.CatalogSchemaTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.expressions.PlannerRowtimeAttribute.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RelDataTypeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RelDataTypeJsonSerializer.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLegacySink.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLocalWindowAggregate.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushWatermarkIntoTableSourceScanAcrossCalcRule.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.ExprCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.GenerateUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.MatchCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.WatermarkGeneratorCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.calcite.WatermarkAssigner.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.LogicalCorrelateToJoinFromTemporalTableRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalGroupWindowAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalIntervalJoinRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.LegacyCatalogSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.schema.TimeIndicatorRelDataType.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.WindowUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RelDataTypeSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.GroupWindowAggregateJsonITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.TemporalSortJsonITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.WindowAggregateJsonITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.DeduplicationJsonPlanTest.jsonplan.testDeduplication.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeSessionWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinJsonPlanTest.jsonplan.testProcessingTimeInnerJoinWithOnClause.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTable.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.LookupJoinJsonPlanTest.jsonplan.testJoinTemporalTableWithProjectionPushDown.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.MatchRecognizeJsonPlanTest.jsonplan.testMatch.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProctimeBoundedDistinctWithNonDistinctPartitionedRowOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedNonPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeBoundedPartitionedRowsOverWithBuiltinProctime.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.OverAggregateJsonPlanTest.jsonplan.testProcTimeUnboundedPartitionedRangeOver.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.TemporalSortJsonPlanTest.jsonplan.testSortProcessingTime.out</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-12-7 01:00:00" id="20508" opendate="2020-12-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce PythonStreamGroupTableAggregateOperator</summary>
      <description>Adds PythonStreamGroupTableAggregateOperator to support running Python TableAggregateFunction</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperatorTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupAggregateOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-12-7 01:00:00" id="20521" opendate="2020-12-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Null result values are being swallowed by RPC system</summary>
      <description>If an RPC method returns a null value, then it seems that the request future won't get completed as reported in FLINK-17921.We should either not allow to return null values as responses or make sure that a null value is properly transmitted to the caller.</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActor.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-3-10 01:00:00" id="20557" opendate="2020-12-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support STATEMENT SET in SQL CLI</summary>
      <description>Support to submit multiple insert into in a single job on SQL CLI, this can be done by support statement set syntax in SQL CLI. The syntax had been discussed and reached an consensus on the mailing list: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-SQL-Syntax-for-Table-API-StatementSet-td42515.html</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.misc.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.TestingExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.Executor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-20 01:00:00" id="2056" opendate="2015-5-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add guide to create a chainable predictor in docs</summary>
      <description>The upcoming API for pipelines should have good documentation to guide and encourage the implementation of more algorithms.For this task we will create a guide that shows how the pipeline mechanism works through Scala implicits, and a full guide to implementing a chainable predictor, using Generalized Linear Models as an example.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.libs.ml.index.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-12-18 01:00:00" id="20666" opendate="2020-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the deserialized Row losing the field_name information in PyFlink</summary>
      <description>Now, the deserialized Row loses the field_name information.@udf(result_type=DataTypes.STRING())def get_string_element(my_list): my_string = 'xxx' for element in my_list: if element.integer_element == 2: # element lost the field_name information my_string = element.string_element return my_stringt = t_env.from_elements( [("1", [Row(3, "flink")]), ("3", [Row(2, "pyflink")]), ("2", [Row(2, "python")])], DataTypes.ROW( [DataTypes.FIELD("Key", DataTypes.STRING()), DataTypes.FIELD("List_element", DataTypes.ARRAY(DataTypes.ROW( [DataTypes.FIELD("integer_element", DataTypes.INT()), DataTypes.FIELD("string_element", DataTypes.STRING())])))]))print(t.select(get_string_element(t.List_element)).to_pandas())element lost the field_name information</description>
      <version>1.11.2,1.12.0</version>
      <fixedVersion>1.11.4,1.12.1,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.fast.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.tests.test.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.datastream.tests.test.data.stream.py</file>
      <file type="M">flink-python.pyflink.common.types.py</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-1-26 01:00:00" id="21140" opendate="2021-1-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extract zip file dependencies before adding to PYTHONPATH</summary>
      <description>Not all zip files are importable in Python and so we should expand zip file dependencies and add the root directory to PYTHONPATH.</description>
      <version>None</version>
      <fixedVersion>1.12.2,1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.python.env.beam.ProcessPythonEnvironmentManagerTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.client.python.PythonEnvUtilsTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.PythonOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.env.beam.ProcessPythonEnvironmentManager.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.client.python.PythonEnvUtils.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">docs..includes.generated.python.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-5 01:00:00" id="21289" opendate="2021-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Application mode ignores the pipeline.classpaths configuration</summary>
      <description>我尝试将flink作业以application mode方式提交到kubernetes上运行。但程序的依赖包并不完全存在于local:///opt/flink/usrlib/xxxx.jar中。导致找不到类。在yarn上可以工作，是因为我们用 -C http://xxxx 的方式，让依赖可以被URLClassloader加载。但我发现，当实验提交到kubernetes时，-C只会在 configmap/flink-conf.yaml 中生成一个pipeline.classpaths 配置条目。我们的main函数可以执行，但是在加载外部依赖类的时候提示找不到类。通过阅读源码，我发现运行用户代码的类加载器实际并没有把 pipeline.classpaths 中的条目加入候选URL，这导致了无法加载类的情况。从源码中，我也发现，通过将依赖包放在usrlib目录下（默认的userClassPath）可以解决问题。但我们的依赖可能是动态的，不合适一次性打到镜像里面。我提议可以改进这个过程，将pipeline.classpaths也加入到对应的类加载器。这个改动很小，我自己经过测试，可以完美解决问题。  English translation:I'm trying to submit flink job to kubernetes cluster with application mode, but throw ClassNotFoundException when some dependency class is not shipped in kind of local:///opt/flink/usrlib/xxxx.jar.This works on yarn, since we use -C http://xxxx command line style that let dependency class  can be load by URLClassloader.But i figure out that not works on kubernetes. When submit to kubernetes cluster, -C is only shipped as item "pipeline.classpaths" in configmap/flink-conf.yaml。After read the source code, i find out that the Classloader launching the "main" entry of user code miss consider adding pipeline.classpaths into candidates URLs. from source code, i also learn that we can ship the dependency jar in the usrlib dir to solve the problem. But that failed for us, we are not preferred to ship dependencies in image at compile time, since dependencies are known dynamically in runtimeI proposed to improve the process, let the Classloader consider usrlib as well as pipeline.classpaths, this is a quite little change. I test the solution and it works quite well  </description>
      <version>1.11.2,1.12.1</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.DefaultPackagedProgramRetrieverTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.DefaultPackagedProgramRetriever.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-5 01:00:00" id="21290" opendate="2021-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Projection push down for Window TVF</summary>
      <description>@Test def testTumble_ProjectionPushDown(): Unit = { // TODO: [b, c, e, proctime] are never used, should be pruned val sql = """ |SELECT | a, | window_start, | window_end, | count(*), | sum(d) |FROM TABLE(TUMBLE(TABLE MyTable, DESCRIPTOR(rowtime), INTERVAL '15' MINUTE)) |GROUP BY a, window_start, window_end """.stripMargin util.verifyRelPlan(sql) }For the above test, currently we get the following plan:Calc(select=[a, window_start, window_end, EXPR$3, EXPR$4])+- WindowAggregate(groupBy=[a], window=[TUMBLE(time_col=[rowtime], size=[15 min])], select=[a, COUNT(*) AS EXPR$3, SUM(d) AS EXPR$4, start('w$) AS window_start, end('w$) AS window_end]) +- Exchange(distribution=[hash[a]]) +- Calc(select=[a, d, rowtime]) +- WatermarkAssigner(rowtime=[rowtime], watermark=[-(rowtime, 1000:INTERVAL SECOND)]) +- Calc(select=[a, b, c, d, e, rowtime, PROCTIME() AS proctime]) +- TableSourceScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c, d, e, rowtime])It should be able to prune fields and get the following plan:Calc(select=[a, window_start, window_end, EXPR$3, EXPR$4])+- WindowAggregate(groupBy=[a], window=[TUMBLE(time_col=[rowtime], size=[15 min])], select=[a, COUNT(*) AS EXPR$3, SUM(d) AS EXPR$4, start('w$) AS window_start, end('w$) AS window_end]) +- Exchange(distribution=[hash[a]]) +- WatermarkAssigner(rowtime=[rowtime], watermark=[-(rowtime, 1000:INTERVAL SECOND)]) +- TableSourceScan(table=[[default_catalog, default_database, MyTable]], fields=[a, d, rowtime])The reason is we didn't transpose Project and WindowTableFunction in logical phase. LogicalAggregate(group=[{0, 1, 2}], EXPR$3=[COUNT()], EXPR$4=[SUM($3)])+- LogicalProject(a=[$0], window_start=[$7], window_end=[$8], d=[$3]) +- LogicalTableFunctionScan(invocation=[TUMBLE($6, DESCRIPTOR($5), 900000:INTERVAL MINUTE)], rowType=[RecordType(INTEGER a, BIGINT b, VARCHAR(2147483647) c, DECIMAL(10, 3) d, BIGINT e, TIME ATTRIBUTE(ROWTIME) rowtime, TIME ATTRIBUTE(PROCTIME) proctime, TIMESTAMP(3) window_start, TIMESTAMP(3) window_end, TIME ATTRIBUTE(ROWTIME) window_time)]) +- LogicalProject(a=[$0], b=[$1], c=[$2], d=[$3], e=[$4], rowtime=[$5], proctime=[$6]) +- LogicalWatermarkAssigner(rowtime=[rowtime], watermark=[-($5, 1000:INTERVAL SECOND)]) +- LogicalProject(a=[$0], b=[$1], c=[$2], d=[$3], e=[$4], rowtime=[$5], proctime=[PROCTIME()]) +- LogicalTableScan(table=[[default_catalog, default_database, MyTable]])</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.WindowRankTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.WindowJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.stream.ExpandWindowTableFunctionTransposeRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionJsonPlanTest.jsonplan.testFollowedByWindowJoin.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowJoinJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testProcTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeTumbleWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeHopWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindowWithOffset.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testEventTimeCumulateWindow.out</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.nodes.exec.stream.WindowAggregateJsonPlanTest.jsonplan.testDistinctSplitEnabled.out</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-11-8 01:00:00" id="24825" opendate="2021-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update nifi-site-to-site-client to v1.14.0</summary>
      <description>We should update org.apache.nifi:nifi-site-to-site-client from 1.6.0 to 1.14.0</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-nifi.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-1-4 01:00:00" id="25516" opendate="2022-1-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add catalog object compile/restore options</summary>
      <description>A prerequisite for serialization/deserialization of various entities.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.TableConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.table.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-1-4 01:00:00" id="25518" opendate="2022-1-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden JSON Serialization utilities</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.groupwindow.WindowReference.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.TemporalTableSourceSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexWindowBoundSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RelDataTypeSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LookupKeySerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalWindowSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalTypeSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.JoinSpecJsonSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.IntervalJoinSpecJsonSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.InputPropertySerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DynamicTableSourceSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DynamicTableSinkSpecSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DurationJsonSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.serde.ChangelogModeJsonSerdeTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.SerdeContext.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexWindowBoundJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RexLiteralJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.RelDataTypeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalWindowJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.LogicalTypeJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.JsonSerdeUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.FlinkDeserializationContext.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.ExecNodeGraphJsonPlanGenerator.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.DurationJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.CatalogTableJsonDeserializer.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.serde.AggregateCallJsonDeserializer.java</file>
    </fixedFiles>
  </bug>
</bugrepository>