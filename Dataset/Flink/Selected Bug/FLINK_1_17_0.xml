<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2020-3-13 01:00:00" id="16583" opendate="2020-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalid classloader during pipeline creation</summary>
      <description>The end-to-end test SQLClientKafkaITCase.testKafka failed with18:13:02.425 [ERROR] testKafka[0: kafka-version:0.10 kafka-sql-version:.*kafka-0.10.jar](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase) Time elapsed: 32.246 s &lt;&lt;&lt; ERROR!java.io.IOException: Process execution failed due error. Error output:Mar 12, 2020 6:11:46 PM org.jline.utils.Log logrWARNING: Unable to create a system terminal, creating a dumb terminal (enable debug logging for more information)Exception in thread "main" org.apache.flink.table.client.SqlClientException: Could not submit given SQL update statement to cluster. at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:131) at org.apache.flink.table.client.SqlClient.start(SqlClient.java:104) at org.apache.flink.table.client.SqlClient.main(SqlClient.java:178) at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.insertIntoAvroTable(SQLClientKafkaITCase.java:178) at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:151)18:13:02.425 [ERROR] testKafka[1: kafka-version:0.11 kafka-sql-version:.*kafka-0.11.jar](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase) Time elapsed: 34.539 s &lt;&lt;&lt; ERROR!java.io.IOException: Process execution failed due error. Error output:Mar 12, 2020 6:12:21 PM org.jline.utils.Log logrWARNING: Unable to create a system terminal, creating a dumb terminal (enable debug logging for more information)Exception in thread "main" org.apache.flink.table.client.SqlClientException: Could not submit given SQL update statement to cluster. at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:131) at org.apache.flink.table.client.SqlClient.start(SqlClient.java:104) at org.apache.flink.table.client.SqlClient.main(SqlClient.java:178) at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.insertIntoAvroTable(SQLClientKafkaITCase.java:178) at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:151)https://api.travis-ci.org/v3/job/661535183/log.txt</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-3-9 01:00:00" id="26028" opendate="2022-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Write documentation for new PulsarSink</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0,1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">docs.content.docs.connectors.datastream.pulsar.md</file>
      <file type="M">docs.content.zh.docs.connectors.datastream.pulsar.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-5-24 01:00:00" id="27757" opendate="2022-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elasticsearch connector should not use flink-table-planner but flink-table-planner-loader</summary>
      <description>Connectors should not rely on flink-table-planner but on flink-table-planner-loader by default. We can should change this for the Elasticsearch connector as this is being externalized at the moment</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-1 01:00:00" id="28759" opendate="2022-8-1 00:00:00" resolution="Done">
    <buginformation>
      <summary>Enable speculative execution for in AdaptiveBatchScheduler TPC-DS e2e tests</summary>
      <description>To verify the correctness of speculative execution, we can enabled it in AdaptiveBatchScheduler TPC-DS e2e tests, which runs a lot of different batch jobs and verifies the result.Note that we need to disable the blocklist (by setting block duration to 0) in such single machine e2e tests.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-1-16 01:00:00" id="29000" opendate="2022-8-16 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support python UDF in the SQL Gateway</summary>
      <description>Currently Flink SQL Client supports python UDF, the Gateway should also support this feature if the SQL Client is able to submit SQL to the Gateway.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.session.SessionManagerTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.DefaultContext.java</file>
      <file type="M">flink-table.flink-sql-gateway.pom.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink-daemon.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink-console.sh</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-16 01:00:00" id="29002" opendate="2022-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecate Datadog reporter &amp;#39;tags&amp;#39; option</summary>
      <description>The option us subsumed by the generic scope.variables.additional option.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-datadog.src.main.java.org.apache.flink.metrics.datadog.DatadogHttpReporterFactory.java</file>
      <file type="M">docs.content.docs.deployment.metric.reporters.md</file>
      <file type="M">docs.content.zh.docs.deployment.metric.reporters.md</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2022-9-21 01:00:00" id="29377" opendate="2022-9-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make RPC timeout extraction reusable</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2022-10-28 01:00:00" id="29435" opendate="2022-9-28 00:00:00" resolution="Done">
    <buginformation>
      <summary>securityConfiguration supports dynamic configuration</summary>
      <description>when different tenants submit jobs using the same flink-conf.yaml, the same user is displayed on the Yarn page. SecurityConfiguration does not support dynamic configuration. Therefore, the user displayed on the Yarn page is the security.kerberos.login.principal in the flink-conf.yaml.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.cli.CliFrontendDynamicPropertiesTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-9-28 01:00:00" id="29436" opendate="2022-9-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Spotless Maven Plugin to 2.27.1</summary>
      <description>This blocker is fixed by: https://github.com/diffplug/spotless/pull/1224 and https://github.com/diffplug/spotless/pull/1228.</description>
      <version>None</version>
      <fixedVersion>1.17.0,connector-parent-1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-10-28 01:00:00" id="29455" opendate="2022-9-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add OperatorIdentifier</summary>
      <description>Add a class for identifying operators, that supports both uids and uidhashes, and integrate into the low-level APIs.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.WindowSavepointReader.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.SavepointWriter.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.SavepointReader.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.metadata.SavepointMetadataV2.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.EvictingWindowSavepointReader.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-4-30 01:00:00" id="2946" opendate="2015-10-30 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add orderBy() to Table API</summary>
      <description>In order to implement a FLINK-2099 prototype that uses the Table APIs code generation facilities, the Table API needs a sorting feature.I would implement it the next days. Ideas how to implement such a sorting feature are very welcome. Is there any more efficient way instead of .sortPartition(...).setParallism(1)? Is it better to sort locally on the nodes first and finally sort on one node afterwards?</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.BatchScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-10-5 01:00:00" id="29511" opendate="2022-10-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sort properties/schemas in OpenAPI spec</summary>
      <description>The properties/schema order is currently based on whatever order they were looked up, which varies as the spec is being extended.Sort them by name to prevent this.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.OpenApiSpecGenerator.java</file>
      <file type="M">docs.static.generated.rest.v1.sql.gateway.yml</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-10-5 01:00:00" id="29514" opendate="2022-10-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Minikdc to v3.2.4</summary>
      <description>Bump Minikdc to v3.2.4 to remove false positive scans on CVEs like CVE-2021-29425 and CVE-2020-15250</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-hbase.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-5-8 01:00:00" id="29542" opendate="2022-10-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unload.md wrongly writes UNLOAD operation as LOAD operation</summary>
      <description>UNLOAD statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns ‘OK’ for a successful LOAD operation; otherwise it will throw an exception. which should be UNLOAD </description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0,1.17.1,1.16.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.unload.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-11-8 01:00:00" id="29543" opendate="2022-10-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jar Run Rest Handler Support Flink Configuration</summary>
      <description>Flink JM Rest Api Support Flink Configuration field</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtilsTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBodyTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBody.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRequestBody.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanRequestBody.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler.java</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-10-10 01:00:00" id="29562" opendate="2022-10-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JM/SQl gateway OpenAPI specs should have different titles</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0,1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.rest.OpenApiSpecGeneratorTest.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.SqlGatewayOpenApiSpecGenerator.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RuntimeOpenApiSpecGenerator.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.OpenApiSpecGenerator.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2022-11-13 01:00:00" id="29624" opendate="2022-10-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade org.apache.commons:commons-lang3 from 3.3.2 to 3.12.0</summary>
      <description>Upgrade org.apache.commons:commons-lang3 from 3.3.2 to 3.12.0 to avoid being falsely flagged for CVEs CVE-2021-29425 and CVE-2020-15250</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.java</file>
      <file type="M">flink-table.flink-sql-parser-hive.src.main.java.org.apache.flink.sql.parser.hive.ddl.HiveDDLUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.jsonplan.JsonPlanGenerator.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plandump.PlanJSONDumpGenerator.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFileFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvCommons.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.3.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-11-13 01:00:00" id="29628" opendate="2022-10-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump aws-java-sdk-s3 to 1.12.319</summary>
      <description>As reported by Dependabot in https://github.com/apache/flink/pull/20285</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-4 01:00:00" id="2963" opendate="2015-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dependence on SerializationUtils#deserialize() should be avoided</summary>
      <description>There is a problem with `SerializationUtils` from Apache CommonsLang. Here is an open issue where the class will throw a`ClassNotFoundException` even if the class is in the classpath in amultiple-classloader environment:https://issues.apache.org/jira/browse/LANG-1049 state = (HashMap&lt;String, Serializable&gt;) SerializationUtils.deserialize(bais);./flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/windowing/NonKeyedWindowOperator.java state = (HashMap&lt;String, Serializable&gt;) SerializationUtils.deserialize(bais);./flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperator.java return SerializationUtils.deserialize(message);./flink-streaming-java/src/main/java/org/apache/flink/streaming/util/serialization/JavaDefaultStringSchema.java T copied = SerializationUtils.deserialize(SerializationUtils./flink-streaming-java/src/test/java/org/apache/flink/streaming/util/MockOutput.javaWe should move away from SerializationUtils.deserialize()</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.checkstyle.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-10-14 01:00:00" id="29638" opendate="2022-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update jackson bom because of CVE-2022-42003</summary>
      <description>There is a CVE-2022-42003 fixed in 2.13.4.1 and 2.14.0-rc1https://nvd.nist.gov/vuln/detail/CVE-2022-42003P.S. It seems there will not be 2.14.0 release until end of October according to https://github.com/FasterXML/jackson-databind/issues/3590#issuecomment-1270363915</description>
      <version>1.16.0,1.17.0,1.15.2</version>
      <fixedVersion>1.16.0,1.17.0,1.15.3</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-11-14 01:00:00" id="29644" opendate="2022-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reference Kubernetes operator from Flink Kubernetes deploy docs</summary>
      <description>Currently the Flink deployment/resource provider docs provide some information for the Standalone and Native Kubernetes integration without any reference to the operator. We should provide a bit more visibility and value to the users by directly proposing to use the operator when considering Flink on Kubernetes. We should make the point that for most users the easiest way to use Flink on Kubernetes is probably through the operator (where they can now benefit from both standalone and native integration under the hood). This should help us avoid cases where a new user completely misses the existence of the operator when starting out based on the Flink docs.</description>
      <version>1.16.0,1.17.0,1.15.3</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.kubernetes.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2022-11-20 01:00:00" id="29709" opendate="2022-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Pulsar to 2.10.2</summary>
      <description>Pulsar released a new version 2.10.2 which contains a lot of bugfix.</description>
      <version>1.17.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.util.DockerImageVersions.java</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-pulsar.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.schema.PulsarSchemaUtils.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.src.main.java.org.apache.flink.connector.pulsar.common.schema.PulsarSchema.java</file>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-4 01:00:00" id="2971" opendate="2015-11-4 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add outer joins to the Table API</summary>
      <description>Since Flink now supports outer joins, the Table API can also support left, right and full outer joins.Given that null values are properly supported by RowSerializer etc.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.TypeConverter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.dataSet.DataSetJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.logical.operators.scala</file>
      <file type="M">docs.apis.table.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-12-26 01:00:00" id="29767" opendate="2022-10-26 00:00:00" resolution="Done">
    <buginformation>
      <summary>VertexWiseSchedulingStrategy supports hybrid shuffle</summary>
      <description>`AdaptiveBatchScheduler` using `VertexWiseSchedulingStrategy` to schedule tasks, but it only supports blocking edge,  so we should make it also support hybrid shuffle edge.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.VertexwiseSchedulingStrategyTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.VertexwiseSchedulingStrategy.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulingTopology.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.strategy.TestingSchedulingResultPartition.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adapter.DefaultResultPartitionTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.strategy.ConsumerVertexGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.EdgeManagerBuildUtil.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-10-28 01:00:00" id="29787" opendate="2022-10-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix ci METHOD_NEW_DEFAULT issue</summary>
      <description>`org.apache.flink.api.connector.source.SourceReader` declared a new default function `pauseOrResumeSplits()`, japicmp plugin failed during ci running, we should configure the plugin to make it compatible.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.update.japicmp.configuration.sh</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-11-31 01:00:00" id="29812" opendate="2022-10-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove deprecated Netty API usages</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyConnectionManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpointConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.KeepAliveWrite.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerRedirectUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.router.RouterHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.FileUploadHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyServer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyClient.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.testutils.HttpTestClient.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.utils.WebFrontendBootstrap.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.PipelineErrorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.HttpRequestHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandler.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KvStateServerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.Client.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.AbstractServerBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-2-3 01:00:00" id="29859" opendate="2022-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TPC-DS end-to-end test with adaptive batch scheduler failed due to oo non-empty .out files.</summary>
      <description>Nov 03 02:02:12 &amp;#91;FAIL&amp;#93; 'TPC-DS end-to-end test with adaptive batch scheduler' failed after 21 minutes and 44 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out files https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42766&amp;view=logs&amp;s=ae4f8708-9994-57d3-c2d7-b892156e7812&amp;j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a</description>
      <version>1.16.0,1.17.0</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-11-3 01:00:00" id="29868" opendate="2022-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dependency convergence error for org.osgi:org.osgi.core:jar</summary>
      <description>While working on FLINK-29867, the following new error is popping up while running./mvnw clean install -pl flink-dist -am -DskipTests -Dflink.convergence.phase=install -Pcheck-convergence(this is also done by CI which therefore fails)[WARNING] Dependency convergence error for org.osgi:org.osgi.core:jar:4.3.0:runtime paths to dependency are:+-org.apache.flink:flink-table-planner-loader-bundle:jar:1.17-SNAPSHOT +-org.apache.flink:flink-table-planner_2.12:jar:1.17-SNAPSHOT:runtime +-org.apache.flink:flink-table-api-java-bridge:jar:1.17-SNAPSHOT:runtime +-org.apache.flink:flink-streaming-java:jar:1.17-SNAPSHOT:runtime +-org.apache.flink:flink-runtime:jar:1.17-SNAPSHOT:runtime +-org.xerial.snappy:snappy-java:jar:1.1.8.3:runtime +-org.osgi:org.osgi.core:jar:4.3.0:runtimeand+-org.apache.flink:flink-table-planner-loader-bundle:jar:1.17-SNAPSHOT +-org.apache.flink:flink-table-planner_2.12:jar:1.17-SNAPSHOT:runtime +-org.apache.flink:flink-scala_2.12:jar:1.17-SNAPSHOT:runtime +-org.apache.flink:flink-core:jar:1.17-SNAPSHOT:runtime +-org.apache.commons:commons-compress:jar:1.21:runtime +-org.osgi:org.osgi.core:jar:6.0.0:runtime</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-3-11 01:00:00" id="2997" opendate="2015-11-11 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support range partition with user customized data distribution.</summary>
      <description>This is a followup work of FLINK-7, sometime user have better knowledge of the source data, and they can build customized data distribution to do range partition more efficiently.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.utils.package.scala</file>
      <file type="M">flink-optimizer.src.test.java.org.apache.flink.optimizer.dataproperties.MockDistribution.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.traversals.RangePartitionRewriter.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plan.Channel.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dataproperties.GlobalProperties.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dag.PartitionNode.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.utils.DataSetUtils.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.PartitionOperator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.Keys.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.operators.base.PartitionOperatorBase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.distributions.DataDistribution.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2022-11-14 01:00:00" id="30010" opendate="2022-11-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-quickstart-test failed due to could not resolve dependencies</summary>
      <description>Nov 13 02:10:37 [ERROR] Failed to execute goal on project flink-quickstart-test: Could not resolve dependencies for project org.apache.flink:flink-quickstart-test:jar:1.17-SNAPSHOT: Could not find artifact org.apache.flink:flink-quickstart-scala:jar:1.17-SNAPSHOT in apache.snapshots (https://repository.apache.org/snapshots) -&gt; [Help 1]Nov 13 02:10:37 [ERROR] Nov 13 02:10:37 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.Nov 13 02:10:37 [ERROR] Re-run Maven using the -X switch to enable full debug logging.Nov 13 02:10:37 [ERROR] Nov 13 02:10:37 [ERROR] For more information about the errors and possible solutions, please read the following articles:Nov 13 02:10:37 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionExceptionNov 13 02:10:37 [ERROR] Nov 13 02:10:37 [ERROR] After correcting the problems, you can resume the build with the commandNov 13 02:10:37 [ERROR] mvn &lt;goals&gt; -rf :flink-quickstart-testNov 13 02:10:38 Process exited with EXIT CODE: 1.Nov 13 02:10:38 Trying to KILL watchdog (293)./__w/1/s/tools/ci/watchdog.sh: line 100: 293 Terminated watchdogNov 13 02:10:38 ==============================================================================Nov 13 02:10:38 Compilation failure detected, skipping test execution.Nov 13 02:10:38 ==============================================================================https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43102&amp;view=logs&amp;j=298e20ef-7951-5965-0e79-ea664ddc435e&amp;t=d4c90338-c843-57b0-3232-10ae74f00347&amp;l=18363</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-quickstart-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-11-17 01:00:00" id="30056" opendate="2022-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make polling for metadata no more than specified timeout by using new Consumer#poll(Duration)</summary>
      <description>New Consumer#poll poll for metadata responses (counts against timeout) if no response within timeout, return an empty collection immediatelyAlso more details are at https://cwiki.apache.org/confluence/display/KAFKA/KIP-266%3A+Fix+consumer+indefinite+blocking+behavior#KIP266:Fixconsumerindefiniteblockingbehavior-Consumer#pollThe behavior was changed at https://issues.apache.org/jira/browse/KAFKA-5697</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaInternalProducerITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaConsumerThread.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-1-18 01:00:00" id="30083" opendate="2022-11-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump maven-shade-plugin to 3.4.1</summary>
      <description>FLINK-24273 proposes to relocate the io.fabric8 dependencies of flink-kubernetes.This is not possible because of a problem with the maven shade plugin ("mvn install" doesn't work, it needs to be "mvn clean install").MSHADE-425 solves this issue, and has been released with maven-shade-plugin 3.4.0.Upgrading the shade plugin will solve the problem, unblocking the K8s relocation.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-11-18 01:00:00" id="30085" opendate="2022-11-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JVM hanging after executing YARNSessionCapacitySchedulerITCase.testNonexistingQueueWARNmessage</summary>
      <description>After some analysis I've concluded the following: the test submitted an app into a wrong queue so fails. At submit time YARN added a shutdown hook named DeploymentFailureHook. The mentioned hook is executed after the YARN cluster is down and tries to kill the application. The kill call is hanging for 15 minutes blocking the JVM to shut down.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-11-18 01:00:00" id="30089" opendate="2022-11-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove dependency promotion from kinesis connector</summary>
      <description>The shade-plugin in the kinesis connector is configured to promote transitive dependencies.This adds a special case in our shading setup, breaks the dependency tree structure (since all transitive dependencies are moved to the top) and it makes the sql-kinesis packaging overly complicated.Get rid of the dependency promotion and explicitly depend on anything that we pull in transitively but don't shade in the kinesis connector itself.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2022-11-24 01:00:00" id="30197" opendate="2022-11-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update slf4j from 1.7.32 to 1.7.36</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">docs.content.docs.dev.configuration.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.configuration.overview.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-11-25 01:00:00" id="30214" opendate="2022-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The resource is not enough</summary>
      <description>When turn up the parallelism, the resources isn't enough for MiniCluster. I'm not sure whether the flink on yarn or k8s is right? I can test later. I guess the parallelism should be changed in a right place. And we should add more unit test or ITCase to check these cases. And it's my honor to make these improvements and bug fixes. </description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.PerJobMiniClusterFactoryTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PerJobMiniClusterFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-17 01:00:00" id="3023" opendate="2015-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show Flink version + commit id for -SNAPSHOT versions in web frontend</summary>
      <description>The old frontend was showing the Flink version and the commit id for SNAPSHOT builds.This is a helpful feature to quickly see which Flink version is running.It would be nice to add this again to the web interface.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.EnvironmentInformation.java</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.overview.html</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.overview.jade</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.ClusterOverviewHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-11-29 01:00:00" id="30237" opendate="2022-11-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Only bundle a single Zookeeper version</summary>
      <description>Way back when we added support for ZK 3.5 we started bundling 2 zookeeper clients, because of incompatibilities between 3.4 and 3.5. This is no longer required, and we could simplify things again.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.ha.per.job.cluster.datastream.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.ha.datastream.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">docs.content.docs.deployment.ha.zookeeper.ha.md</file>
      <file type="M">docs.content.zh.docs.deployment.ha.zookeeper.ha.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-12-30 01:00:00" id="30250" opendate="2022-11-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The flame graph type is wrong</summary>
      <description>When the flame graph type is switched from On-CPU to Mixed. It still show the graph of On-CPU.Root cause:When click the other types, the web frontend will call the requestFlameGraph and update the graphType. However, the graphType is the old type during requestFlameGraph. So the graph type show the new type, but the flame graph is the result of old type. code link</description>
      <version>1.15.0,1.16.0,1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.flamegraph.job-overview-drawer-flamegraph.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.flamegraph.job-overview-drawer-flamegraph.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-flamegraph.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.components.flame-graph.flame-graph.component.ts</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-1-4 01:00:00" id="30289" opendate="2022-12-4 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>RateLimitedSourceReader uses wrong signal for checkpoint rate-limiting</summary>
      <description>The checkpoint rate limiter is notified when the checkpoint is complete, but since this signal comes at some point in the future (or not at all) it can result in no records being emitted for a checkpoint, or more records than expected being emitted.</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-12-9 01:00:00" id="30355" opendate="2022-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>crictl causes long wait in e2e tests</summary>
      <description>We observed strange behavior in the e2e test where the e2e test run times out: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43824&amp;view=logs&amp;j=bea52777-eaf8-5663-8482-18fbc3630e81&amp;s=ae4f8708-9994-57d3-c2d7-b892156e7812&amp;t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&amp;l=7446The issue seems to be related to crictl again because we see the following error message in multiple tests. No logs are produced afterwards for ~30mins resulting in the overall test run taking too long:Dec 09 08:55:39 crictlfatal: destination path 'cri-dockerd' already exists and is not an empty directory.fatal: a branch named 'v0.2.3' already existsmkdir: cannot create directory ‘bin’: File existsDec 09 09:26:41 fs.protected_regular = 0</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.kubernetes.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-1-12 01:00:00" id="30365" opendate="2022-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>New dynamic partition pruning strategy to support more dpp patterns</summary>
      <description>New dynamic partition pruning strategy to support more dpp patterns. Now, dpp rules is coupled with the join reorder rules, which will affect the result of join reorder. At the same time, the dpp rule don't support these patterns like union node in fact side.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.physical.batch.DynamicPartitionPruningRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.physical.batch.DynamicPartitionPruningRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkBatchProgram.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.physical.batch.DynamicPartitionPruningRule.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-12-12 01:00:00" id="30368" opendate="2022-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix calcite method RelMdUtil$numDistinctVals() wrongly return zero if the method input domainSize much larger than numSelected</summary>
      <description>Fix calcite method RelMdUtil$numDistinctVals() wrongly return zero if the method input domainSize much larger than numSelected. This wrong zero value will affect the selection of join type。 </description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRelMdUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRelMdUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdDistinctRowCount.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-1-12 01:00:00" id="30376" opendate="2022-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce a new flink bushy join reorder rule which based on greedy algorithm</summary>
      <description>Introducing a new Flink bushy join reorder strategy which based on the greedy algorithm. The old join reorder rule will also be the default join reorder rule and the new bushy join reorder strategy will be optional.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.join.JoinReorderTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.RewriteMultiJoinConditionRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.common.JoinReorderTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.JoinReorderTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.JoinReorderTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.RewriteMultiJoinConditionRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.optimize.program.DynamicPartitionPruningProgramTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.JoinReorderTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.optimize.program.DynamicPartitionPruningProgramTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.utils.JavaScalaConversionUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.logical.RewriteMultiJoinConditionRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.OptimizerConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.optimizer.config.configuration.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-1-14 01:00:00" id="30416" opendate="2022-12-14 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add configureSession REST API in the SQL Gateway</summary>
      <description/>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.UtilCaseITTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.TestingSqlGatewayRestEndpoint.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.SqlGatewayRestEndpointStatementITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.SqlGatewayRestEndpointITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.SessionCaseITTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.RestAPIITTestBase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.rest.OperationCaseITTest.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.util.SqlGatewayRestAPIVersion.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.SqlGatewayRestEndpoint.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.rest.header.SqlGatewayMessageHeaders.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-12-21 01:00:00" id="30468" opendate="2022-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The SortOrder of BusyRatio should be descend by default</summary>
      <description>Currently, the sort order is ascend by default, it should be descend.The most busy subtask should be displayed on top.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.overview.backpressure.job-overview-drawer-backpressure.component.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-1-21 01:00:00" id="30472" opendate="2022-12-21 00:00:00" resolution="Done">
    <buginformation>
      <summary>Modify the default value of the max network memory config option</summary>
      <description>This issue mainly focuses on the second issue in FLIP-266, modifying the default value of taskmanager.memory.network.max</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.task.manager.memory.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.common.memory.section.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-1-3 01:00:00" id="30542" opendate="2023-1-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support adaptive local hash aggregate in runtime</summary>
      <description>Introduce a new strategy to adaptively determine whether local hash aggregate is required according to the aggregation degree of local hash aggregate.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.HashAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.metadata.FlinkRelMdHandlerTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGeneratorTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.EnforceLocalAggRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalSortAggRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalJoinRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalHashAggRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchPhysicalAggRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalLocalHashAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchPhysicalHashAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ProjectionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecHashAggregate.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-11-21 01:00:00" id="3056" opendate="2015-11-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show bytes sent/received as MBs/GB and so on in web interface</summary>
      <description>It would be great if the web interface would round show the bytes in an appropriate (=human readable) unit.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.taskmanager.taskmanager.metrics.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.taskmanager.index.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.plan.node.subtasks.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.plan.node-list.overview.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.common.filters.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanager.taskmanager.metrics.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanager.index.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.node.subtasks.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.node-list.overview.jade</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-1-6 01:00:00" id="30578" opendate="2023-1-6 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Publish SBOM artifacts</summary>
      <description/>
      <version>1.17.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-6-6 01:00:00" id="30585" opendate="2023-1-6 00:00:00" resolution="Done">
    <buginformation>
      <summary>Improve flame graph performance at subtask level</summary>
      <description>After FLINK-30185 , we can view the flame graph of subtask level. However, it always collects flame graphs for all subtasks.We should collect the flame graph of single subtask instead of all subtasks.</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.stats.JobVertexStatsTracker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTrackerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTrackerBuilder.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTracker.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-1-10 01:00:00" id="30618" opendate="2023-1-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-connector-pulsar not retrievable from Apache&amp;#39;s Snapshot Maven repository</summary>
      <description>The build failure was caused by flink-connector-pulsar not being retrievable from the Apache Snapshot Maven repository:https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44633&amp;view=logs&amp;j=de826397-1924-5900-0034-51895f69d4b7&amp;t=f311e913-93a2-5a37-acab-4a63e1328f94&amp;l=10132Jan 10 02:03:24 [WARNING] The requested profile "skip-webui-build" could not be activated because it does not exist.Jan 10 02:03:24 [ERROR] Failed to execute goal on project flink-python: Could not resolve dependencies for project org.apache.flink:flink-python:jar:1.17-SNAPSHOT: Could not find artifact org.apache.flink:flink-sql-connector-pulsar:jar:4.0-SNAPSHOT in apache.snapshots (https://repository.apache.org/snapshots) -&gt; [Help 1]Jan 10 02:03:24 [ERROR] Jan 10 02:03:24 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.Jan 10 02:03:24 [ERROR] Re-run Maven using the -X switch to enable full debug logging.Jan 10 02:03:24 [ERROR] Jan 10 02:03:24 [ERROR] For more information about the errors and possible solutions, please read the following articles:Jan 10 02:03:24 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionExceptionJan 10 02:03:24 [ERROR] Jan 10 02:03:24 [ERROR] After correcting the problems, you can resume the build with the commandJan 10 02:03:24 [ERROR] mvn &lt;goals&gt; -rf :flink-python</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-3-12 01:00:00" id="30640" opendate="2023-1-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unstable test in CliClientITCase</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44743&amp;view=logs&amp;j=0c940707-2659-5648-cbe6-a1ad63045f0a&amp;t=075c2716-8010-5565-fe08-3c4bb45824a4 The failed test can work normally in my local environment.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.set.q</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-1-13 01:00:00" id="30661" opendate="2023-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce interfaces for Delete/Update</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.abilities.SupportsRowLevelModificationScan.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.RowLevelModificationScanContext.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-1-13 01:00:00" id="30662" opendate="2023-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for Delete</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.sql.DeleteTableITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.batch.sql.DeleteTableITCase.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestUpdateDeleteTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.sink.SinkAbilitySpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSourceUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSinkUtils.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.ContextResolvedTable.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRexUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoSourceScanRuleBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterInCalcIntoTableSourceScanRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.SinkModifyOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-1-13 01:00:00" id="30665" opendate="2023-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add implementation for Update</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestUpdateDeleteTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.sink.SinkAbilitySpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.sink.RowLevelDeleteSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.connectors.DynamicSinkUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-2-13 01:00:00" id="30666" opendate="2023-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for Delete/Update API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.connector.source.abilities.SupportsRowLevelModificationScan.java</file>
      <file type="M">docs.content.docs.dev.table.sourcesSinks.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sourcesSinks.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-1-13 01:00:00" id="30672" opendate="2023-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support &amp;#39;EXPLAIN PLAN_ADVICE&amp;#39; statement</summary>
      <description/>
      <version>1.17.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.TwoStageOptimizedAggregateRule.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.StreamPlanner.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.BatchPlanner.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.ExplainDetail.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.dql.SqlRichExplain.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.RelTreeWriterImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.analyze.FlinkStreamPlanAnalyzers.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.table.q</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-2-13 01:00:00" id="30673" opendate="2023-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for "EXPLAIN PLAN_ADVICE" statement</summary>
      <description/>
      <version>1.17.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.explain.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.explain.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-2-13 01:00:00" id="30674" opendate="2023-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enabling consume partial finished partition by default for speculative execution in hybrid shuffle mode</summary>
      <description>At present, if hybrid shuffle enabled speculative execution, it will only consume all finished partition by default. It is better to change this default behavior to consume partial finished upstream partition.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchSchedulerFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-1-15 01:00:00" id="30691" opendate="2023-1-15 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>SQL Client supports to specify endpoint address</summary>
      <description/>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.test.util.SQLJobSubmission.java</file>
      <file type="M">flink-end-to-end-tests.flink-sql-gateway-test.src.test.java.org.apache.flink.table.gateway.SqlGatewayE2ECase.java</file>
      <file type="M">flink-end-to-end-tests.flink-sql-gateway-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common.src.main.java.org.apache.flink.tests.util.flink.FlinkDistribution.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.SqlClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.SqlClient.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalContextUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptionsParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptions.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2023-3-17 01:00:00" id="30713" opendate="2023-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Hadoop related k8s decorators exclude possibility</summary>
      <description>Hadoop related k8s decorators are making a lot of assumptions. There are some users who mount Hadoop specific things (like Hadoop config, Kerberos config/keytab) in a custom way and not depending on Flink. As a result of these assumptions the actual decorators making the workload fail and there is no workaround.</description>
      <version>1.17.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesTaskManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactoryTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesTaskManagerFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.factory.KubernetesJobManagerFactory.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.configuration.KubernetesConfigOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.kubernetes.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-1-18 01:00:00" id="30749" opendate="2023-1-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Delegation token provider enabled flag documentation is wrong</summary>
      <description/>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.security.security-kerberos.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-1-19 01:00:00" id="30752" opendate="2023-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support &amp;#39;EXPLAIN PLAN_ADVICE&amp;#39; statement in PyFlink</summary>
      <description>For API completeness</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.util.java.utils.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.sql.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.explain.py</file>
      <file type="M">flink-python.pyflink.table.explain.detail.py</file>
      <file type="M">flink-python.pyflink.examples.table.basic.operations.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-1-19 01:00:00" id="30753" opendate="2023-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Py4J cannot acquire Table.explain() method</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45044&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=c67e71ed-6451-5d26-8920-5a8cf9651901</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.table.py</file>
      <file type="M">flink-python.pyflink.table.statement.set.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-2-25 01:00:00" id="30785" opendate="2023-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>RocksDB Memory Management end-to-end test failed due to unexpected exception</summary>
      <description>We see a test instability with RocksDB Memory Management end-to-end test. The test failed because an exception was detected in the logs:2023-01-25T02:47:38.7172354Z Jan 25 02:47:38 Checking for errors...2023-01-25T02:47:39.1661969Z Jan 25 02:47:39 No errors in log files.2023-01-25T02:47:39.1662430Z Jan 25 02:47:39 Checking for exceptions...2023-01-25T02:47:39.2893767Z Jan 25 02:47:39 Found exception in log files; printing first 500 lines; see full logs for details:[...]2023-01-25T02:47:39.5674568Z Jan 25 02:47:39 Checking for non-empty .out files...2023-01-25T02:47:39.5675055Z Jan 25 02:47:39 No non-empty .out files.2023-01-25T02:47:39.5675352Z Jan 25 02:47:39 2023-01-25T02:47:39.5676104Z Jan 25 02:47:39 [FAIL] 'RocksDB Memory Management end-to-end test' failed after 1 minutes and 50 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out filesThe only exception being reported in the Flink logs is due to a warning:2023-01-25 02:47:38,242 WARN org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger or complete checkpoint 1 for job 421e4c00ef175b3b133d63cbfe9bca8b. (0 consecutive failed attempts so far)org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint Coordinator is suspending. at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.stopCheckpointScheduler(CheckpointCoordinator.java:1970) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.checkpoint.CheckpointCoordinatorDeActivator.jobStatusChanges(CheckpointCoordinatorDeActivator.java:46) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifyJobStatusChange(DefaultExecutionGraph.java:1578) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.transitionState(DefaultExecutionGraph.java:1173) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.transitionState(DefaultExecutionGraph.java:1145) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.cancel(DefaultExecutionGraph.java:973) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.scheduler.SchedulerBase.cancel(SchedulerBase.java:671) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.jobmaster.JobMaster.cancel(JobMaster.java:461) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_352] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_352] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_352] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_352] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at akka.actor.ActorCell.invoke(ActorCell.scala:547) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT] at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_352] at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_352] at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_352] at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_352]https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45185&amp;view=logs&amp;j=bea52777-eaf8-5663-8482-18fbc3630e81&amp;t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&amp;l=5117</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-1-26 01:00:00" id="30797" opendate="2023-1-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump json5 from 1.0.1 to 1.0.2</summary>
      <description>Dependabot has created https://github.com/apache/flink/pull/21617This is the corresponding Jira ticket</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2023-1-28 01:00:00" id="30808" opendate="2023-1-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MultipleInputITCase failed with AdaptiveBatch Scheduler</summary>
      <description>MultipleInputITCase#testRelatedInputs failed with AdaptiveBatch Scheduler.java.lang.UnsupportedOperationException: Forward partitioning does not allow change of parallelism. Upstream operation: Calc[10]-14 parallelism: 1, downstream operation: HashJoin[15]-20 parallelism: 3 You must use another partitioning strategy, such as broadcast, rebalance, shuffle or global.</description>
      <version>1.16.0,1.17.0</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2023-2-30 01:00:00" id="30824" opendate="2023-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for new introduced hive agg option</summary>
      <description/>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.hive.hive.functions.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.functions.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-2-30 01:00:00" id="30827" opendate="2023-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the SQL Client code structure</summary>
      <description/>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectBatchResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.ChangelogCollectResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.result.BaseMaterializedResultTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.ExecutorImplITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliTableauResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliResultViewTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.SqlClient.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.ResultDescriptor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.SingleSessionManager.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectResultBase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.MaterializedCollectBatchResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.DynamicResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.CollectResultBase.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.ChangelogResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.result.ChangelogCollectResult.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.DefaultContextUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliTableResultView.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliTableauResultView.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliChangelogResultView.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-1-30 01:00:00" id="30828" opendate="2023-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SortAggITCase.testLeadLag failed</summary>
      <description>https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45389&amp;view=logs&amp;j=0c940707-2659-5648-cbe6-a1ad63045f0a&amp;t=075c2716-8010-5565-fe08-3c4bb45824a4&amp;l=12560Jan 30 11:03:32 [ERROR] Tests run: 72, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 37.42 s &lt;&lt;&lt; FAILURE! - in org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCaseJan 30 11:03:32 [ERROR] org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase.testLeadLag Time elapsed: 0.547 s &lt;&lt;&lt; FAILURE!Jan 30 11:03:32 java.lang.AssertionError: Jan 30 11:03:32 Jan 30 11:03:32 Results do not match for query:Jan 30 11:03:32 Jan 30 11:03:32 SELECTJan 30 11:03:32 a,Jan 30 11:03:32 b, LEAD(b, 1) over (order by a) AS bLead, LAG(b, 1) over (order by a) AS bLag,Jan 30 11:03:32 c, LEAD(c, 1) over (order by a) AS cLead, LAG(c, 1) over (order by a) AS cLag,Jan 30 11:03:32 d, LEAD(d, 1) over (order by a) AS dLead, LAG(d, 1) over (order by a) AS dLag,Jan 30 11:03:32 e, LEAD(e, 1) over (order by a) AS eLead, LAG(e, 1) over (order by a) AS eLag,Jan 30 11:03:32 f, LEAD(f, 1) over (order by a) AS fLead, LAG(f, 1) over (order by a) AS fLag,Jan 30 11:03:32 g, LEAD(g, 1) over (order by a) AS gLead, LAG(g, 1) over (order by a) AS gLag,Jan 30 11:03:32 h, LEAD(h, 1) over (order by a) AS hLead, LAG(h, 1) over (order by a) AS hLag,Jan 30 11:03:32 i, LEAD(i, 1) over (order by a) AS iLead, LAG(i, 1) over (order by a) AS iLag,Jan 30 11:03:32 j, LEAD(j, 1) over (order by a) AS jLead, LAG(j, 1) over (order by a) AS jLag,Jan 30 11:03:32 k, LEAD(k, 1) over (order by a) AS kLead, LAG(k, 1) over (order by a) AS kLag,Jan 30 11:03:32 l, LEAD(l, 1) over (order by a) AS lLead, LAG(l, 1) over (order by a) AS lLag,Jan 30 11:03:32 m, LEAD(m, 1) over (order by a) AS mLead, LAG(m, 1) over (order by a) AS mLag,Jan 30 11:03:32 n, LEAD(n, 1) over (order by a) AS nLead, LAG(n, 1) over (order by a) AS nLagJan 30 11:03:32 Jan 30 11:03:32 FROM UnnamedTable$230Jan 30 11:03:32 order by aJan 30 11:03:32 Jan 30 11:03:32 Jan 30 11:03:32 ResultsJan 30 11:03:32 == Correct Result - 3 == == Actual Result - 3 ==Jan 30 11:03:32 +I[Alice, 1, 1, null, 1, 1, null, 2, 2, null, 9223, 9223, null, -2.3, -2.3, null, 9.9, 9.9, null, true, true, null, varchar, varchar, null, char , char , null, 2021-08-03, 2021-08-03, null, 20:08:17, 20:08:17, null, 2021-08-03T20:08:29, 2021-08-03T20:08:29, null, 9.99, 9.99, null] +I[Alice, 1, 1, null, 1, 1, null, 2, 2, null, 9223, 9223, null, -2.3, -2.3, null, 9.9, 9.9, null, true, true, null, varchar, varchar, null, char , char , null, 2021-08-03, 2021-08-03, null, 20:08:17, 20:08:17, null, 2021-08-03T20:08:29, 2021-08-03T20:08:29, null, 9.99, 9.99, null]Jan 30 11:03:32 +I[Alice, 1, null, 1, 1, null, 1, 2, null, 2, 9223, null, 9223, -2.3, null, -2.3, 9.9, null, 9.9, true, null, true, varchar, null, varchar, char , null, char , 2021-08-03, null, 2021-08-03, 20:08:17, null, 20:08:17, 2021-08-03T20:08:29, null, 2021-08-03T20:08:29, 9.99, null, 9.99] +I[Alice, 1, null, 1, 1, null, 1, 2, null, 2, 9223, null, 9223, -2.3, null, -2.3, 9.9, null, 9.9, true, null, true, varchar, null, varchar, char , null, char , 2021-08-03, null, 2021-08-03, 20:08:17, null, 20:08:17, 2021-08-03T20:08:29, null, 2021-08-03T20:08:29, 9.99, null, 9.99]Jan 30 11:03:32 !+I[Alice, null, null, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99] +I[Alice, null, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99, null]Jan 30 11:03:32 Jan 30 11:03:32 Plan:Jan 30 11:03:32 == Abstract Syntax Tree ==Jan 30 11:03:32 LogicalSort(sort0=[$0], dir0=[ASC-nulls-first])Jan 30 11:03:32 +- LogicalProject(inputs=[0..1], exprs=[[LEAD($1, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($1, 1) OVER (ORDER BY $0 NULLS FIRST), $2, LEAD($2, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($2, 1) OVER (ORDER BY $0 NULLS FIRST), $3, LEAD($3, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($3, 1) OVER (ORDER BY $0 NULLS FIRST), $4, LEAD($4, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($4, 1) OVER (ORDER BY $0 NULLS FIRST), $5, LEAD($5, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($5, 1) OVER (ORDER BY $0 NULLS FIRST), $6, LEAD($6, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($6, 1) OVER (ORDER BY $0 NULLS FIRST), $7, LEAD($7, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($7, 1) OVER (ORDER BY $0 NULLS FIRST), $8, LEAD($8, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($8, 1) OVER (ORDER BY $0 NULLS FIRST), $9, LEAD($9, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($9, 1) OVER (ORDER BY $0 NULLS FIRST), $10, LEAD($10, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($10, 1) OVER (ORDER BY $0 NULLS FIRST), $11, LEAD($11, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($11, 1) OVER (ORDER BY $0 NULLS FIRST), $12, LEAD($12, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($12, 1) OVER (ORDER BY $0 NULLS FIRST), $13, LEAD($13, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($13, 1) OVER (ORDER BY $0 NULLS FIRST)]])Jan 30 11:03:32 +- LogicalUnion(all=[true])[...]</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-2-31 01:00:00" id="30846" opendate="2023-1-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SpeculativeSchedulerITCase.testSpeculativeExecutionOfInputFormatSource fails</summary>
      <description>SpeculativeSchedulerITCase.testSpeculativeExecutionOfInputFormatSource is timing outhttps://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45440&amp;view=logs&amp;j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&amp;t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&amp;l=8599Jan 31 02:02:28 "ForkJoinPool-1-worker-25" #27 daemon prio=5 os_prio=0 tid=0x00007fcf74f2b800 nid=0x5476 waiting on condition [0x00007fce2b078000]Jan 31 02:02:28 java.lang.Thread.State: WAITING (parking)Jan 31 02:02:28 at sun.misc.Unsafe.park(Native Method)Jan 31 02:02:28 - parking to wait for &lt;0x00000000a22933e0&gt; (a java.util.concurrent.CompletableFuture$Signaller)Jan 31 02:02:28 at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)Jan 31 02:02:28 at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)Jan 31 02:02:28 at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)Jan 31 02:02:28 at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)Jan 31 02:02:28 at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)Jan 31 02:02:28 at org.apache.flink.test.scheduling.SpeculativeSchedulerITCase.executeJob(SpeculativeSchedulerITCase.java:216)Jan 31 02:02:28 at org.apache.flink.test.scheduling.SpeculativeSchedulerITCase.testSpeculativeExecutionOfInputFormatSource(SpeculativeSchedulerITCase.java:162)</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactoryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-2-2 01:00:00" id="30875" opendate="2023-2-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix usages of legacy AdaptiveBatchScheduler configuration</summary>
      <description>In FLINK-30686, we deprecated the JobManagerOptions's AdaptiveBatchScheduler configuration. However, these configuration items still have some calls. And we should change these calls to new configuration.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.SpeculativeSchedulerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismAndInputInfosDeciderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismAndInputInfosDecider.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.tpcds.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-2-2 01:00:00" id="30876" opendate="2023-2-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ResetTransformationProcessor don&amp;#39;t reset the transformation of ExecNode in BatchExecMultiInput.rootNode</summary>
      <description>Now, ResetTransformationProcessor don't reset the transformation of ExecNode in BatchExecMultiInput.rootNode. This may cause error while creating StreamGraph for BatchExecMultiInput due to different id of rootNode and inputNode.</description>
      <version>1.17.0,1.16.1</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.nodes.exec.processor.MultipleInputNodeCreationProcessorTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecMultipleInput.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-2-2 01:00:00" id="30878" opendate="2023-2-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>KubernetesHighAvailabilityRecoverFromSavepointITCase fails due to a deadlock</summary>
      <description>We're seeing a test failure in KubernetesHighAvailabilityRecoverFromSavepointITCase due to a deadlock:2023-02-01T18:53:35.5540322Z "ForkJoinPool-1-worker-1" #14 daemon prio=5 os_prio=0 tid=0x00007f68ecb18000 nid=0x43dd1 waiting on condition [0x00007f68c1711000]2023-02-01T18:53:35.5540900Z java.lang.Thread.State: TIMED_WAITING (parking)2023-02-01T18:53:35.5541272Z at sun.misc.Unsafe.park(Native Method)2023-02-01T18:53:35.5541932Z - parking to wait for &lt;0x00000000d14d7b60&gt; (a java.util.concurrent.CompletableFuture$Signaller)2023-02-01T18:53:35.5542496Z at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)2023-02-01T18:53:35.5543088Z at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1709)2023-02-01T18:53:35.5543672Z at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)2023-02-01T18:53:35.5544240Z at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1788)2023-02-01T18:53:35.5544801Z at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)2023-02-01T18:53:35.5545632Z at org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityRecoverFromSavepointITCase.testRecoverFromSavepoint(KubernetesHighAvailabilityRecoverFromSavepointITCase.java:113)2023-02-01T18:53:35.5546409Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45565&amp;view=logs&amp;j=bea52777-eaf8-5663-8482-18fbc3630e81&amp;t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&amp;l=61916The build failure happens on 1.16. I'm adding 1.17 and 1.15 as fixVersions as well because it might be due to some recent changes which were introduced with FLINK-30462 and/or FLINK-30474</description>
      <version>1.17.0,1.15.4,1.16.2</version>
      <fixedVersion>1.17.0,1.15.4,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.DefaultMultipleComponentLeaderElectionServiceTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.DefaultMultipleComponentLeaderElectionService.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2023-2-6 01:00:00" id="30905" opendate="2023-2-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>doc generation fails with "concurrent map read and map write"</summary>
      <description>We experience a build failure in master (but since it looks like a Hugo issue, I added already released version to the affected versions as well) with a concurrent map read and map write within hugo:https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45707&amp;view=logs&amp;j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&amp;t=ddd6d61a-af16-5d03-2b9a-76a279badf98Start building sites … fatal error: concurrent map read and map writegoroutine 233 [running]:runtime.throw(0x23054e4, 0x21) /usr/local/go/src/runtime/panic.go:1116 +0x72 fp=0xc0016ea860 sp=0xc0016ea830 pc=0x4f5ff2runtime.mapaccess1_faststr(0x1f71280, 0xc000764a20, 0xc000aa60e1, 0x18, 0xcd) /usr/local/go/src/runtime/map_faststr.go:21 +0x465 fp=0xc0016ea8d0 sp=0xc0016ea860 pc=0x4d29c5[...]</description>
      <version>1.17.0,1.15.3,1.16.1</version>
      <fixedVersion>1.17.0,1.15.4,1.16.2</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.docs.sh</file>
      <file type="M">.github.workflows.docs.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2023-2-9 01:00:00" id="30971" opendate="2023-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify the default value of parameter &amp;#39;table.exec.local-hash-agg.adaptive.sampling-threshold&amp;#39;</summary>
      <description>In our test environment, we set the default parallelism to  1 and got the most appropriate default value of parameter 'table.exec.local-hash-agg.adaptive.sampling-threshold'  is 5000000. However, for these batch jobs with high parallelism in produce environment,  the amount of data in single parallelism is almost less than 5000000. Therefore, after testing, we found that set to 500000 can get better results.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-2-9 01:00:00" id="30972" opendate="2023-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>E2e tests always fail in phase "Prepare E2E run"</summary>
      <description>Installing required softwareReading package lists...Building dependency tree...Reading state information...bc is already the newest version (1.07.1-2build1).bc set to manually installed.libapr1 is already the newest version (1.6.5-1ubuntu1).libapr1 set to manually installed.0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.--2023-02-09 04:38:47-- http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.10_amd64.debResolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 185.125.190.36, 185.125.190.39, ...Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.HTTP request sent, awaiting response... 404 Not Found2023-02-09 04:38:47 ERROR 404: Not Found.WARNING: apt does not have a stable CLI interface. Use with caution in scripts.Reading package lists...E: Unsupported file ./libssl1.0.0_1.0.2n-1ubuntu5.10_amd64.deb given on commandline##[error]Bash exited with code '100'.Finishing: Prepare E2E run</description>
      <version>1.17.0,1.15.4,1.16.2,1.18.0</version>
      <fixedVersion>1.17.0,1.15.4,1.16.2,1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.e2e-template.yml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2023-3-15 01:00:00" id="31082" opendate="2023-2-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setting maven property &amp;#39;flink.resueForks&amp;#39; to false in table planner module</summary>
      <description>This issue is created to alleviate the OOM problem mentioned in issue: https://issues.apache.org/jira/browse/FLINK-18356Setting maven property 'flink.resueForks' to false in table planner module can only reduce the frequency of oom, but can't solve this problem. To completely solve this problem, we need to identify the specific reasons, but this is a time-consuming work.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2023-2-20 01:00:00" id="31136" opendate="2023-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client Gateway mode should not read read execution config</summary>
      <description/>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.utils.SqlGatewayServiceExtension.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.SqlGateway.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.DefaultContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.SqlClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.DefaultContextUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-3-20 01:00:00" id="31157" opendate="2023-2-20 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Propose a pull request for website updates</summary>
      <description>The final step of building the candidate is to propose a website pull request containing the following changes: update apache/flink-web:_config.yml update FLINK_VERSION_STABLE and FLINK_VERSION_STABLE_SHORT as required update version references in quickstarts (q/ directory) as required (major only) add a new entry to flink_releases for the release binaries and sources (minor only) update the entry for the previous release in the series in flink_releases Please pay notice to the ids assigned to the download entries. They should be unique and reflect their corresponding version number. add a new entry to release_archive.flink add a blog post announcing the release in _posts add a organized release notes page under docs/content/release-notes and docs/content.zh/release-notes (like https://nightlies.apache.org/flink/flink-docs-release-1.15/release-notes/flink-1.15/). The page is based on the non-empty release notes collected from the issues, and only the issues that affect existing users should be included (e.g., instead of new functionality). It should be in a separate PR since it would be merged to the flink project. Don’t merge the PRs before finalizing the release. Expectations Website pull request proposed to list the release (major only) Check docs/config.toml to ensure that the version constants refer to the new version the baseurl does not point to flink-docs-master  but flink-docs-release-X.Y instead</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content..index.md</file>
      <file type="M">docs.content.zh..index.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-6-27 01:00:00" id="31233" opendate="2023-2-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>no error should be logged when retrieving the task manager&amp;#39;s stdout if it does not exist</summary>
      <description>When running Flink on Kubernetes, the stdout logs is not redirected to files so it will not shown in WEB UI. This is as expected.But It returns “500 Internal error” in REST API and produces an error log in jobmanager.log. This is confusing and misleading. I think this API should return “404 Not Found” without any error logs, similar to how jobmanager/stdout works.  </description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.TestingChannelHandlerContext.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.AbstractTaskManagerFileHandler.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2023-3-1 01:00:00" id="31273" opendate="2023-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Left join with IS_NULL filter be wrongly pushed down and get wrong join results</summary>
      <description>Left join with IS_NULL filter be wrongly pushed down and get wrong join results. The sql is:SELECT * FROM MyTable1 LEFT JOIN MyTable2 ON a1 = a2 WHERE a2 IS NULL AND a1 &lt; 10The wrongly plan is:LogicalProject(a1=[$0], b1=[$1], c1=[$2], b2=[$3], c2=[$4], a2=[$5])+- LogicalFilter(condition=[IS NULL($5)])   +- LogicalJoin(condition=[=($0, $5)], joinType=[left])      :- LogicalValues(tuples=[[]])      +- LogicalTableScan(table=[[default_catalog, default_database, MyTable2]])</description>
      <version>1.17.0,1.16.1</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopJoinTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.join.JoinTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.FlinkFilterJoinRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.SortMergeJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.ShuffledHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.NestedLoopJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.join.BroadcastHashJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.logical.FlinkFilterJoinRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.FlinkFilterJoinRule.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-3-1 01:00:00" id="31284" opendate="2023-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase KerberosLoginProvider test coverage</summary>
      <description/>
      <version>1.17.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.security.token.hadoop.KerberosLoginProviderITCase.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2023-3-6 01:00:00" id="31337" opendate="2023-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>EmbeddedDataStreamBatchTests.test_keyed_co_broadcast_side_output</summary>
      <description>Same build, multiple times: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&amp;view=logs&amp;j=9cada3cb-c1d3-5621-16da-0f718fb86602&amp;t=c67e71ed-6451-5d26-8920-5a8cf9651901&amp;l=24566 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&amp;view=logs&amp;j=821b528f-1eed-5598-a3b4-7f748b13f261&amp;t=6bb545dd-772d-5d8c-f258-f5085fba3295&amp;l=24235 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&amp;view=logs&amp;j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&amp;t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&amp;l=24545 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&amp;view=logs&amp;j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&amp;t=b4612f28-e3b5-5853-8a8b-610ae894217a&amp;l=24481 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&amp;view=logs&amp;j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&amp;t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&amp;l=24757Mar 04 01:21:35 pyflink/datastream/tests/test_data_stream.py:743: Mar 04 01:21:35 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ Mar 04 01:21:35 pyflink/datastream/tests/test_data_stream.py:63: in assert_equals_sortedMar 04 01:21:35 self.assertEqual(expected, actual)Mar 04 01:21:35 E AssertionError: Lists differ: ['0', '1', '2', '4', '5', '5', '6', '6'] != ['0', '1', '2', '3', '5', '5', '6', '6']Mar 04 01:21:35 E Mar 04 01:21:35 E First differing element 3:Mar 04 01:21:35 E '4'Mar 04 01:21:35 E '3'Mar 04 01:21:35 E Mar 04 01:21:35 E - ['0', '1', '2', '4', '5', '5', '6', '6']Mar 04 01:21:35 E ? ^Mar 04 01:21:35 E Mar 04 01:21:35 E + ['0', '1', '2', '3', '5', '5', '6', '6']Mar 04 01:21:35 E ?</description>
      <version>1.17.0,1.16.1</version>
      <fixedVersion>1.17.0,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.translators.BatchExecutionUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.runtime.translators.python.PythonKeyedBroadcastStateTransformationTranslator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.process.ExternalPythonBatchKeyedCoBroadcastProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.EmbeddedPythonBatchKeyedCoBroadcastProcessOperator.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2023-7-15 01:00:00" id="31476" opendate="2023-3-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>AdaptiveScheduler should take lower bound parallelism settings into account</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.DefaultVertexParallelismStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.allocator.TestVertexInformation.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.allocator.SlotSharingSlotAllocatorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.VertexParallelismInformation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultVertexParallelismStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultVertexParallelismInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.JobGraphJobInformation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.allocator.SlotSharingSlotAllocator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptive.allocator.JobInformation.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-1-19 01:00:00" id="31512" opendate="2023-3-19 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Move SqlAlterView conversion logic to SqlAlterViewConverter</summary>
      <description>Introduce SqlAlterViewConverter and move the conversion logic of SqlAlterView -&gt; AlterViewOperation to it.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConvertUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConverters.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-3-20 01:00:00" id="31521" opendate="2023-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Initialize flink jdbc driver module in flink-table</summary>
      <description>Initialize jdbc driver module</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-3-20 01:00:00" id="31522" opendate="2023-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce FlinkResultSet and related classes for jdbc driver</summary>
      <description>Introduce FlinkResultSet and related classes for jdbc driver to support data iterator</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-jdbc-driver.pom.xml</file>
      <file type="M">flink-table.flink-sql-jdbc-driver-bundle.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-3-21 01:00:00" id="31541" opendate="2023-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Get metrics in Flink WEB UI error</summary>
      <description>When i get a metrics from a operator which name contains '&amp;#91;&amp;#39; or &amp;#39;&amp;#93;', it will be return 400 from rest response.The reason is we can not submit an GET request which params contains '&amp;#91;&amp;#39; or &amp;#39;&amp;#93;', it is invaild in REST. </description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.task-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.metrics.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job-manager.service.ts</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-3-22 01:00:00" id="31568" opendate="2023-3-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update japicmp configuration</summary>
      <description>Update the japicmp reference version and wipe exclusions / enable API compatibility checks for @PublicEvolving APIs on the corresponding SNAPSHOT branch with the update_japicmp_configuration.sh script (see below).For a new major release (x.y.0), run the same command also on the master branch for updating the japicmp reference version and removing out-dated exclusions in the japicmp configuration.Make sure that all Maven artifacts are already pushed to Maven Central. Otherwise, there's a risk that CI fails due to missing reference artifacts.tools $ NEW_VERSION=$RELEASE_VERSION releasing/update_japicmp_configuration.shtools $ cd ..$ git add *$ git commit -m "Update japicmp configuration for $RELEASE_VERSION"</description>
      <version>None</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2023-3-25 01:00:00" id="31612" opendate="2023-3-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClassNotFoundException when using GCS path as HA directory</summary>
      <description>Hi,When I am trying to run Flink job in HA mode with GCS path as a HA directory (eg: &amp;#91;gs://flame-poc/ha&amp;#93;) or while starting a job from checkpoints in GCS I am getting following exception:Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback not found at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688) ~[?:?] at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2712) ~[?:?] at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.Groups.&lt;init&gt;(Groups.java:107) ~[?:?] at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.Groups.&lt;init&gt;(Groups.java:102) ~[?:?] at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451) ~[?:?] at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:338) ~[?:?] at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300) ~[?:?] at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575) ~[?:?] at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getUgiUserName(GoogleHadoopFileSystemBase.java:1226) ~[?:?] at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.listStatus(GoogleHadoopFileSystemBase.java:858) ~[?:?] at org.apache.flink.fs.gs.org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.listStatus(HadoopFileSystem.java:170) ~[?:?] Observations:While using File system as a HA path and GCS as checkpointing directory the job is able to write checkpoints to GCS checkpoint path. After debugging what I found was all the org.apache.hadoop paths are shaded to org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop. Ideally the code should look for  org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback instead of  org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.I think it is not getting shaded over here due to reflection being used here:https://github.com/apache/hadoop/blob/branch-3.3.4/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/Groups.java#L108As a workaround I rebuilt flink-gs-fs-hadoop plugin removing this relocation and it worked for me.&lt;relocation&gt;&lt;pattern&gt;org.apache.hadoop&lt;/pattern&gt;&lt;shadedPattern&gt;org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop&lt;/shadedPattern&gt;&lt;/relocation&gt;</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-gs-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-4-3 01:00:00" id="31705" opendate="2023-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Conjars</summary>
      <description>With Conjars no longer being available (only https://conjars.wensel.net/ is there), we should remove all the notices to Conjars in Flink. We've already removed the need for Conjars because we've excluded Pentaho as part of FLINK-27640, which eliminates having any dependency that relies on Conjars.</description>
      <version>None</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.google-mirror-settings.xml</file>
      <file type="M">tools.ci.alibaba-mirror-settings.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-4-3 01:00:00" id="31707" opendate="2023-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Constant string cannot be used as input arguments of Pandas UDAF</summary>
      <description>It will throw exceptions as following when using constant strings in Pandas UDAF:E raise ValueError("field_type %s is not supported." % field_type)E ValueError: field_type type_name: CHARE char_info {E length: 3E }E is not supported.</description>
      <version>None</version>
      <fixedVersion>1.16.2,1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udaf.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-4-3 01:00:00" id="31711" opendate="2023-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>OpenAPI spec omits complete-statement request body</summary>
      <description>The OpenAPI generator omits request bodies for get requests because it is usually a bad idea.Still, the generator shouldn't omit this on it's own.</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.OpenApiSpecGenerator.java</file>
      <file type="M">docs.static.generated.rest.v2.sql.gateway.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-4-4 01:00:00" id="31728" opendate="2023-4-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Scala API dependencies from batch/streaming examples</summary>
      <description>The example modules have leftover Scala API dependencies and build infrastructure. Remove them, along with the scala suffix on these modules.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-walkthroughs.flink-walkthrough-datastream-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-fs-tests.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-examples.flink-examples-build-helper.flink-examples-streaming-state-machine.pom.xml</file>
      <file type="M">flink-examples.flink-examples-batch.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-high-parallelism-iterations-test.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-15 01:00:00" id="3173" opendate="2015-12-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump org.apache.httpcomponents.httpclient version to 4.2.6</summary>
      <description>Currently, the TwitterStream example is not working with the TwitterSource, because we're using httpclient version 4.2. There seems to be bug in the httpclient which causes the connection to the Twitter stream to fail. With version 4.2.6 this issue is fixed. I propose, therefore, to bump the version to 4.2.6.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2023-4-10 01:00:00" id="31758" opendate="2023-4-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some external connectors sql client jar has a wrong download url in document</summary>
      <description>After FLINK-30378, we can load sql connector data from external connector's own data file. However, we did not replace $full_version, resulting in an incorrect URL in the download link. for example: https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-mongodb/$full_version/flink-sql-connector-mongodb-$full_version.jar.</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.layouts.shortcodes.sql.connector.download.table.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-4-11 01:00:00" id="31767" opendate="2023-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the implementation for "analyze table" execution on partitioned table</summary>
      <description>Currently, for partitioned table, the "analyze table" command will generate a separate SQL statement for each partition. When there are too many partitions, the compilation/submission/execution time will be very long. This issue aims to improve it: we can combine the sql statements for each partition into one with "union all", and just need to execution one sql.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.AnalyzeTableUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-4-13 01:00:00" id="31792" opendate="2023-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Errors are not reported in the Web UI</summary>
      <description>After FLINK-29747, NzNotificationService can no longer be resolved by injector, and because we're using the injector directly, this is silently ignored.</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.main.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.interceptor.ts</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-6-13 01:00:00" id="31797" opendate="2023-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move LeaderElectionService out of LeaderContender</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.util.DocumentingDispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.utils.MockResourceManagerRuntimeServices.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.TestingResourceManagerService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerPartitionLifecycleTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerJobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerHATest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingLeaderElectionService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingLeaderElection.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.TestingContender.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.StandaloneLeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.LeaderElectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.DefaultLeaderElectionServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterExecutionDeploymentReconciliationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.TestingHighAvailabilityServicesBuilder.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.TestingHighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.nonha.standalone.StandaloneHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedHaServicesTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.entrypoint.ClusterEntrypointTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunnerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunnerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherCleanupITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.SessionRestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.JobRestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderelection.StandaloneLeaderElectionService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.nonha.standalone.StandaloneHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.HighAvailabilityServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.AbstractHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.DispatcherRunnerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunnerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.JobMasterServiceLeadershipRunnerFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2023-4-19 01:00:00" id="31859" opendate="2023-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update maven cyclonedx plugin to 2.7.7</summary>
      <description>there are at least 2 related improvements1. current version depends on jackson-databind 2.14.0 and has a memory issue described at https://github.com/FasterXML/jackson-databind/issues/3665 which is fixed in later versions2. current version leads to lots of traces in logs (e.g. mvn clean verify for flink-core) which is fixed in later versions[ERROR] An error occurred attempting to read POMorg.codehaus.plexus.util.xml.pull.XmlPullParserException: UTF-8 BOM plus xml decl of ISO-8859-1 is incompatible (position: START_DOCUMENT seen &lt;?xml version="1.0" encoding="ISO-8859-1"... @1:42) at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDeclWithVersion (MXParser.java:3423) at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDecl (MXParser.java:3345) at org.codehaus.plexus.util.xml.pull.MXParser.parsePI (MXParser.java:3197) at org.codehaus.plexus.util.xml.pull.MXParser.parseProlog (MXParser.java:1828) at org.codehaus.plexus.util.xml.pull.MXParser.nextImpl (MXParser.java:1757) at org.codehaus.plexus.util.xml.pull.MXParser.next (MXParser.java:1375) at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:3940) at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:612) at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:627) at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:759) at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:746) at org.cyclonedx.maven.BaseCycloneDxMojo.retrieveParentProject (BaseCycloneDxMojo.java:694) at org.cyclonedx.maven.BaseCycloneDxMojo.getClosestMetadata (BaseCycloneDxMojo.java:524) at org.cyclonedx.maven.BaseCycloneDxMojo.convert (BaseCycloneDxMojo.java:481) at org.cyclonedx.maven.CycloneDxMojo.execute (CycloneDxMojo.java:70) at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137) at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:370) at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:351) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:215) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:171) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:163) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81) at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56) at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128) at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:294) at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192) at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105) at org.apache.maven.cli.MavenCli.execute (MavenCli.java:960) at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:293) at org.apache.maven.cli.MavenCli.main (MavenCli.java:196) at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke (Method.java:498) at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282) at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225) at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406) at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)</description>
      <version>1.17.0,1.18.0</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2023-7-4 01:00:00" id="31996" opendate="2023-5-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Chaining operators with different max parallelism prevents rescaling</summary>
      <description>We might chain operators with different max parallelism together if they are set to have the same parallelism initially.When we decide to rescale the JobGraph vertices (using AdaptiveScheduler), we're gapped by the lowest maxParallelism of the operator chain. This is especially visible with things like CollectSink, TwoPhaseCommitSink, CDC, and a GlobalCommiter with maxParallelism set to 1. An obvious solution would be to prevent the chaining of operators with different maxParallelism, but we need to double-check this doesn't introduce a breaking change.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.StreamingScalaAPICompletenessTest.scala</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.PipelineOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.pipeline.configuration.html</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2023-5-16 01:00:00" id="32110" opendate="2023-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TM native memory leak when using time window in Pyflink ThreadMode</summary>
      <description>If job use time window in Pyflink thread mode, TM native memory will grow slowly during the job running until TM can't allocate memory from operate system.The leak rate is likely proportional to the number of key.</description>
      <version>1.17.0</version>
      <fixedVersion>1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.embedded.EmbeddedPythonWindowOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-5-16 01:00:00" id="32112" opendate="2023-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the deprecated state backend sample config in Chinese document</summary>
      <description>Current Version Avaliable State Backends : HashMapStateBackend EmbeddedRocksDBStateBackend But in the Operations/State &amp; Fault Tolerance page of flink v1.17.0, a sample section in the configuration set state.backend: filesystem  in zh-doc.The correct configuration should be:  state.backend: hashmap I think it may cause misunderstandings for users.</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0,1.17.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.ops.state.state.backends.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-11 01:00:00" id="3216" opendate="2016-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Define pattern specification</summary>
      <description>In order to detect event patterns we first have to define the pattern. This issue tracks the progress of implementing a user facing API to define event patterns. Patterns should support the following operations next(): The given event has to follow directly after the preceding eventfollowedBy(): The given event has to follow the preceding event. There might occur other events in-between every(): In a follow-by relationship a starting event can be matched with multiple successive events. Consider the pattern a → b where → denotes the follow-by relationship. The event sequence a, b, b can be matched as a, b or a, (b), b where the first b is left out. The essential question is whether a is allowed to match multiple times or only the first time. The method every specifies exactly that. Every events in a pattern can match with multiple successive events. This makes only sense in a follow-by relationship, though. followedByEvery(): Similar to followedBy just that the specified element can be matched with multiple successive events or(): Alternative event which can be matched instead of the original event: every(“e1”).where().or(“e2”).where() within(): Defines a time interval in which the pattern has to be completed, otherwise an incomplete pattern can be emitted (timeout case)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.StreamOperator.java</file>
      <file type="M">flink-libraries.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-5-24 01:00:00" id="32174" opendate="2023-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Cloudera product and link in doc page</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.overview.md</file>
      <file type="M">docs.content.zh.docs.deployment.overview.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-6-31 01:00:00" id="32220" opendate="2023-5-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improving the adaptive local hash agg code to avoid get value from RowData repeatedly</summary>
      <description/>
      <version>1.17.0</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.ProjectionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.agg.batch.HashAggCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-8-12 01:00:00" id="32581" opendate="2023-7-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for atomic CTAS</summary>
      <description>add docs for atomic CTAS</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.docs.dev.table.sourcesSinks.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sourcesSinks.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-9-4 01:00:00" id="32755" opendate="2023-8-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add quick start guide for Flink OLAP</summary>
      <description>I propose to add a new QUICKSTART.md guide that provides instructions for beginner to build a production ready Flink OLAP Service by using flink-jdbc-driver, flink-sql-gateway and flink session cluster.</description>
      <version>None</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.overview.md</file>
      <file type="M">docs.content.zh.docs.dev.table.overview.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-8-17 01:00:00" id="32888" opendate="2023-8-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>File upload runs into EndOfDataDecoderException</summary>
      <description>With the right request the FIleUploadHandler runs into a EndOfDataDecoderException although everything is fine.</description>
      <version>1.17.0</version>
      <fixedVersion>1.18.0,1.16.3,1.17.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.FileUploadHandlerITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.FileUploadHandler.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.MultipartUploadExtension.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-9-7 01:00:00" id="33053" opendate="2023-9-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Watcher leak in Zookeeper HA mode</summary>
      <description>We observe a watcher leak in our OLAP stress test when enabling Zookeeper HA mode. TM's watches on the leader of JobMaster has not been stopped after job finished.Here is how we re-produce this issue: Start a session cluster and enable Zookeeper HA mode. Continuously and concurrently submit short queries, e.g. WordCount to the cluster. echo -n wchp | nc {zk host} {zk port} to get current watches.We can see a lot of watches on /flink/{cluster_name}/leader/{job_id}/connection_info.</description>
      <version>1.17.0,1.18.0,1.17.1</version>
      <fixedVersion>1.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ZooKeeperUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.java</file>
    </fixedFiles>
  </bug>
</bugrepository>