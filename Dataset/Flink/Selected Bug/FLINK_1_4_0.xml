<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  
  <bug fixdate="2018-7-22 01:00:00" id="10195" opendate="2018-8-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>RabbitMQ Source With Checkpointing Doesn&amp;#39;t Backpressure Correctly</summary>
      <description>The connection between the RabbitMQ server and the client does not appropriately back pressure when auto acking is disabled. This becomes very problematic when a downstream process throttles the data processing to slower then RabbitMQ sends the data to the client.The difference in records ends up being stored in the flink's heap space, which grows indefinitely (or technically to "Integer Max" Deliveries). Looking at RabbitMQ's metrics the number of unacked messages looks like steadily rising saw tooth shape.Upon further invesitgation it looks like this is due to how the QueueingConsumer works, messages are added to the BlockingQueue faster then they are being removed and processed, resulting in the previously described behavior.This may be intended behavior, however this isn't explicitly obvious in the documentation or any of the examples I have seen.</description>
      <version>1.4.0,1.5.0,1.5.1,1.6.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.test.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.test.java.org.apache.flink.streaming.connectors.rabbitmq.common.RMQConnectionConfigTest.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSource.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.common.RMQConnectionConfig.java</file>
      <file type="M">docs.dev.connectors.rabbitmq.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-11 01:00:00" id="10527" opendate="2018-10-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup constant isNewMode in YarnTestBase</summary>
      <description>This seems to be a residual problem with FLINK-10396. It is set to true in that PR. Currently it has three usage scenarios:1. assert, caused an errorassumeTrue("The new mode does not start TMs upfront.", !isNewMode);2. if (!isNewMode) the logic in the block would not have invoked, the if block can be removed3. if (isNewMode) always been invoked, the if statement can be removed.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-4 01:00:00" id="5406" opendate="2017-1-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add normalization phase for predicate logical plan rewriting between decorrelate query phase and volcano optimization phase</summary>
      <description>Normalization phase is for predicate logical plan rewriting and is independent of cost module. The rules in normalization phase do not need to repeatedly applied to different logical plan which is different to volcano optimization phase. And the benefit of normalization phase is to reduce the running time of volcano planner.ReduceExpressionsRule can apply various simplifying transformations on RexNode trees. Currently, there are two transformations:1) Constant reduction, which evaluates constant subtrees, replacing them with a corresponding RexLiteral2) Removal of redundant casts, which occurs when the argument into the cast is the same as the type of the resulting cast expressionthe above transformations do not depend on the cost module, so we can move the rules in ReduceExpressionsRule from DATASET_OPT_RULES/DATASTREAM_OPT_RULES to DataSet/DataStream Normalization Rules.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.TableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.ExpressionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.CalciteConfigBuilderTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.FieldProjectionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.AggregationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.api.java.batch.TableEnvironmentITCase.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.CalciteConfig.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-11 01:00:00" id="5454" opendate="2017-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Documentation about how to tune Checkpointing for large state</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.rest.api.md</file>
      <file type="M">docs.monitoring.large.state.tuning.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-11 01:00:00" id="5455" opendate="2017-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create documentation how to upgrade jobs and Flink framework versions</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.upgrading.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-11 01:00:00" id="5458" opendate="2017-1-11 00:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Add documentation how to migrate from Flink 1.1. to Flink 1.2</summary>
      <description>Docs should go to docs/dev/migration.md</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.index.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-24 01:00:00" id="6367" opendate="2017-4-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>support custom header settings of allow origin</summary>
      <description>`jobmanager.web.access-control-allow-origin`: Enable custom access control parameter for allow origin header, default is `*`.</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorConfig.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.RuntimeMonitorHandler.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-9 01:00:00" id="6508" opendate="2017-5-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include license files of packaged dependencies</summary>
      <description>The Maven artifact for flink-table bundles its (non-Flink) dependencies to have a self-contained JAR file that can be moved to the ./lib folder without adding additional dependencies.Currently, we include Apache Calcite, Guava (relocates and required by Calcite), Janino, and Reflections.Janino and Reflections are not under Apache license, so we need to include their license files into the JAR file.</description>
      <version>1.2.1,1.3.0,1.4.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-11 01:00:00" id="6549" opendate="2017-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve error message for type mismatches with side outputs</summary>
      <description>A type mismatch when using side outputs causes a ClassCastException to be thrown. It would be neat to include the name of the OutputTags in the exception message.This can occur when multiple {{OutputTag]}s with different types but identical names are being used.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-11 01:00:00" id="6552" opendate="2017-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Side outputs don&amp;#39;t allow differing output types</summary>
      <description>When calling {SingleOutputStreamOperator#getSideOutput(OutputTag&lt;X&gt;} multiple times with the output tags having different types you get the following exception: "Trying to add a side input for the same id with a different type. This is not allowed." This error message is ambiguous, as it could either mean that you cannot add 2 side outputs with the same name but different types or that 2 side outputs with different types cannot be retrieved from a single operator.Furthermore, the error message contains the concept of node id's (i guess?) which users aren't exposed to. This is confusing and should be reworded to work with operators.Lastly, i find this limitation rather odd. It is possible for an operator to have multiple side outputs. It is also possible to have a side output with a different type than the main output. Yet, it is not possible to have multiple side outputs with different types.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.util.TestListResultSink.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.streaming.runtime.SideOutputITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-11 01:00:00" id="6560" opendate="2017-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Restore maven parallelism in flink-tests</summary>
      <description>FLINK-6506 added the maven variable flink.forkCountTestPackage which is used by the TravisCI script but no default value is set.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-11 01:00:00" id="6562" opendate="2017-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support implicit table references for nested fields in SQL</summary>
      <description>Currently nested fields can only be accessed through fully qualified identifiers. For example, users need to specify the following query for the table f that has a nested field foo.barSELECT f.foo.bar FROM fOther query engines like Hive / Presto supports implicit table references. For example:SELECT foo.bar FROM fThis jira proposes to support the latter syntax in the SQL API.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.CompositeAccessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.schema.CompositeRelDataType.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-13 01:00:00" id="6575" opendate="2017-5-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable all tests on Windows that use HDFS</summary>
      <description>Similar reasoning as FLINK-6558.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.HDFSTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.FsNegativeRunningJobsRegistryTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.FileStateBackendTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.ContinuousFileProcessingTest.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.ContinuousFileProcessingMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.bucketing.RollingToBucketingMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.bucketing.RollingSinkMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-filesystem.src.test.java.org.apache.flink.streaming.connectors.fs.bucketing.BucketingSinkMigrationTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-15 01:00:00" id="6583" opendate="2017-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable QueryConfig in count base GroupWindow</summary>
      <description>Enable QueryConfig in count base GroupWindow by Add a custom Trigger `CountTriggerWithCleanupState`. See more in FLINK-6491.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupWindowAggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupWindowAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-15 01:00:00" id="6584" opendate="2017-5-15 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support multiple consecutive windows in SQL</summary>
      <description>Right now, the Table API supports multiple consecutive windows as follows:val table = stream.toTable(tEnv, 'rowtime.rowtime, 'int, 'double, 'float, 'bigdec, 'string)val t = table .window(Tumble over 2.millis on 'rowtime as 'w) .groupBy('w) .select('w.rowtime as 'rowtime, 'int.count as 'int) .window(Tumble over 4.millis on 'rowtime as 'w2) .groupBy('w2) .select('w2.rowtime, 'w2.end, 'int.count)Similar behavior should be supported by the SQL API as well. We need to introduce a new auxiliary group function, but this should happen in sync with Apache Calcite.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.validation.GroupWindowValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetLogicalWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.common.WindowStartEndPropertiesRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.common.LogicalWindowAggregateRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupWindowAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalWindowAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-15 01:00:00" id="6585" opendate="2017-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table examples are not runnable in IDE</summary>
      <description>Running Table API examples in flink-examples-table fails with:Caused by: java.lang.ClassNotFoundException: org.apache.flink.table.api.TableEnvironmentSeems to be a Maven issue.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-15 01:00:00" id="6590" opendate="2017-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate generated tables into documentation</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">flink-libraries.flink-python.pom.xml</file>
      <file type="M">flink-docs.README.md</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.ConfigOptionsDocGeneratorTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.configuration.ConfigDocsCompletenessChecker.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.QueryableStateOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.MetricOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HistoryServerOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">docs.page.css.flink.css</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-16 01:00:00" id="6604" opendate="2017-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Java Serialization from the CEP library.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPMigration11to13Test.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPFrom12MigrationTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.SharedBufferTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFATest.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.State.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.SharedBuffer.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.NFA.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.DeweyNumber.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.compiler.NFACompiler.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-5-17 01:00:00" id="6614" opendate="2017-5-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Applying function on window auxiliary function fails</summary>
      <description>SQL queries that apply a function or expression on a window auxiliary function (TUMBLE_START, TUMBLE_END, HOP_START, etc). cannot be translated and fail with a CodeGenException:Exception in thread "main" org.apache.flink.table.codegen.CodeGenException: Unsupported call: TUMBLE_ENDExample query:SELECT a, toLong(TUMBLE_END(rowtime, INTERVAL '10' MINUTE)) AS t, COUNT(b) AS cntBFROM myTableGROUP BY a, TUMBLE(rowtime, INTERVAL '10' MINUTE)</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.WindowAggregateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.common.WindowStartEndPropertiesRule.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-19 01:00:00" id="6632" opendate="2017-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix parameter case sensitive error for test passing/rejecting filter API</summary>
      <description>TableAPI testAllPassingFilter: val t = util.addTable[(Int, Long, String)]('int, 'long, 'string) val resScala = t.filter(Literal(true)).select('int as 'myInt, 'string) val resJava = t.filter("TrUe").select("int as myInt, string")We got error:org.apache.flink.table.api.ValidationException: Cannot resolve [TrUe] given input [int, long, string].The error is caused by : lazy val boolLiteral: PackratParser[Expression] = ("true" | "false") ^^ { str =&gt; Literal(str.toBoolean) }I want improve the method as follow: lazy val boolLiteral: PackratParser[Expression] = ("(t|T)(r|R)(u|U)(e|E)".r | "(f|F)(a|A)(l|L)(s|S)(e|E)".r) ^^ { str =&gt; Literal(str.toBoolean)}Is there any drawback to this improvement? Welcome anyone feedback ?</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.batch.table.stringexpr.CalcStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.ExpressionParser.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-22 01:00:00" id="6660" opendate="2017-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>expand the streaming connectors overview page</summary>
      <description>The overview page for streaming connectors is too lean &amp;#8211; it should provide more context and also guide the reader toward related topics.Note that FLINK-6038 will add links to the Bahir connectors.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.twitter.md</file>
      <file type="M">docs.dev.connectors.index.md</file>
      <file type="M">docs.dev.connectors.filesystem.sink.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-23 01:00:00" id="6669" opendate="2017-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Build] Scala style check errror on Windows</summary>
      <description>When build the source code on Windows, a scala style check error happend.Here is the error messages.&amp;#91;INFO&amp;#93;&amp;#91;INFO&amp;#93; &amp;#8212; scalastyle-maven-plugin:0.8.0:check (default) @ flink-scala_2.10 &amp;#8212;error file=E:\github\flink\flink-scala\src\main\scala\org\apache\flink\api\scala\utils\package.scala message=Input length = 2Saving to outputFile=E:\github\flink\flink-scala\target\scalastyle-output.xmlProcessed 78 file(s)Found 1 errorsFound 0 warningsFound 0 infosFinished in 1189 ms&amp;#91;INFO&amp;#93; ------------------------------------------------------------------------&amp;#91;INFO&amp;#93; Reactor Summary:&amp;#91;INFO&amp;#93;&amp;#91;INFO&amp;#93; force-shading ...................................... SUCCESS [ 37.206 s]&amp;#91;INFO&amp;#93; flink .............................................. SUCCESS &amp;#91;03:27 min&amp;#93;&amp;#91;INFO&amp;#93; flink-annotations .................................. SUCCESS [ 3.020 s]&amp;#91;INFO&amp;#93; flink-shaded-hadoop ................................ SUCCESS [ 0.928 s]&amp;#91;INFO&amp;#93; flink-shaded-hadoop2 ............................... SUCCESS [ 15.314 s]&amp;#91;INFO&amp;#93; flink-shaded-hadoop2-uber .......................... SUCCESS [ 13.085 s]&amp;#91;INFO&amp;#93; flink-shaded-curator ............................... SUCCESS [ 0.234 s]&amp;#91;INFO&amp;#93; flink-shaded-curator-recipes ....................... SUCCESS [ 3.336 s]&amp;#91;INFO&amp;#93; flink-shaded-curator-test .......................... SUCCESS [ 2.948 s]&amp;#91;INFO&amp;#93; flink-metrics ...................................... SUCCESS [ 0.286 s]&amp;#91;INFO&amp;#93; flink-metrics-core ................................. SUCCESS [ 9.065 s]&amp;#91;INFO&amp;#93; flink-test-utils-parent ............................ SUCCESS [ 0.327 s]&amp;#91;INFO&amp;#93; flink-test-utils-junit ............................. SUCCESS [ 1.452 s]&amp;#91;INFO&amp;#93; flink-core ......................................... SUCCESS [ 54.277 s][INFO] flink-java ......................................... SUCCESS [ 25.244 s]&amp;#91;INFO&amp;#93; flink-runtime ...................................... SUCCESS &amp;#91;03:08 min&amp;#93;&amp;#91;INFO&amp;#93; flink-optimizer .................................... SUCCESS [ 14.540 s]&amp;#91;INFO&amp;#93; flink-clients ...................................... SUCCESS [ 14.457 s]&amp;#91;INFO&amp;#93; flink-streaming-java ............................... SUCCESS [ 58.130 s]&amp;#91;INFO&amp;#93; flink-test-utils ................................... SUCCESS [ 19.906 s]&amp;#91;INFO&amp;#93; flink-scala ........................................ FAILURE [ 56.634 s]&amp;#91;INFO&amp;#93; flink-runtime-web .................................. SKIPPEDI think this is caused by the Windows default encoding. When I set the inputEncoding to UTF-8 in scalastyle-maven-plugin, the error don't happen.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.1,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-23 01:00:00" id="6691" opendate="2017-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add checkstyle import block rule for scala imports</summary>
      <description>Similar to java and javax imports we should give scala imports a separate import block.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.strict-checkstyle.xml</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.util.typeutils.FieldAccessor.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.runtime.jobmanager.JMXJobManagerMetricTest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-10-24 01:00:00" id="6703" opendate="2017-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document how to take a savepoint on YARN</summary>
      <description>The documentation should have a separate entry for savepoint related CLI commands in combination with YARN. It is currently not documented that you have to supply the application id, nor how you can pass it../bin/flink savepoint &lt;jobID&gt; -m yarn-cluster (-yid|-yarnapplicationId) &lt;appID&gt;</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.savepoints.md</file>
      <file type="M">docs.ops.cli.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-24 01:00:00" id="6704" opendate="2017-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot disable YARN user jar inclusion</summary>
      <description/>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  
  <bug fixdate="2017-7-26 01:00:00" id="6732" opendate="2017-5-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Activate strict checkstyle for flink-java</summary>
      <description>Long term issue for incrementally introducing the strict checkstyle to flink-java.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions-java.xml</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">docs.internals.ide.setup.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-30 01:00:00" id="6769" opendate="2017-5-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace usage of deprecated FileSystem#create(Path, boolean)</summary>
      <description/>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStateOutputStreamTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.savepoint.SavepointStoreTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.savepoint.MigrationV0ToV1Test.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.filesystem.FileSystemStateStorageHelper.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.filecache.FileCache.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.FileSystemBlobStore.java</file>
      <file type="M">flink-fs-tests.src.test.java.org.apache.flink.hdfstests.HDFSTest.java</file>
      <file type="M">flink-examples.flink-examples-batch.src.main.java.org.apache.flink.examples.java.distcp.DistCp.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.fs.SafetyNetCloseableRegistryTest.java</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.fs.InitOutputPathTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-30 01:00:00" id="6776" opendate="2017-5-30 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Use skip instead of seek for small forward repositioning in DFS streams</summary>
      <description>Reading checkpoint meta data and finding key-groups in restores sometimes require to seek in input streams. Currently, we always use a seek, even for small position changes. As small true seeks are far more expensive than small reads/skips, we should just skip over small gaps instead of performing the seek.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.fs.hdfs.HadoopDataInputStream.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-30 01:00:00" id="6777" opendate="2017-5-30 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Activate strict checkstyle for flink-scala-shell</summary>
      <description/>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.test.java.org.apache.flink.api.java.FlinkILoopTest.java</file>
      <file type="M">flink-scala-shell.src.main.java.org.apache.flink.api.java.ScalaShellRemoteStreamEnvironment.java</file>
      <file type="M">flink-scala-shell.src.main.java.org.apache.flink.api.java.ScalaShellRemoteEnvironment.java</file>
      <file type="M">flink-scala-shell.src.main.java.org.apache.flink.api.java.JarHelper.java</file>
      <file type="M">flink-scala-shell.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-5-31 01:00:00" id="6780" opendate="2017-5-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ExternalTableSource should add time attributes in the row type</summary>
      <description>We observed that all streaming queries that refer to external tables fail when the Volcano planner converting LogicalTableScan to FlinkLogicalTableSourceScan:Type mismatch:rowtype of new rel:RecordType(&lt;table schema&gt;, TIMESTAMP(3) NOT NULL proctime) NOT NULLrowtype of set:RecordType(&lt;table schema&gt;, ...) NOT NULLTables that are registered through StreamTableEnvironment#registerTableSource() do not suffer from this problem as StreamTableSourceTable adds the processing time / event time attribute automatically.</description>
      <version>None</version>
      <fixedVersion>1.3.1,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.catalog.ExternalTableSourceUtil.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-31 01:00:00" id="6787" opendate="2017-5-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Job-/StoppableException should extend FlinkException</summary>
      <description/>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.StoppingException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobManagerException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.JobException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.client.JobExecutionException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.client.JobClientActorSubmissionTimeoutException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.client.JobClientActorRegistrationTimeoutException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.client.JobClientActorConnectionTimeoutException.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ProgramMissingJobException.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.ClusterClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-1 01:00:00" id="6792" opendate="2017-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-yarn-tests always fail on travis</summary>
      <description>flink-yarn-tests is currently failing all builds:Results :Failed tests: YARNSessionFIFOITCase.testJavaAPI:244 Error while deploying YARN cluster: Couldn't deploy Yarn clusterTests in error: YARNHighAvailabilityITCase.testMultipleAMKill:140 Â» Runtime Couldn't deploy Ya... YARNSessionCapacitySchedulerITCase.perJobYarnCluster:120-&gt;YarnTestBase.runWithArgs:612 Â» Runtime YARNSessionCapacitySchedulerITCase.perJobYarnClusterWithParallelism:344-&gt;YarnTestBase.runWithArgs:612 Â» Runtime YARNSessionCapacitySchedulerITCase.testClientStartup:99-&gt;YarnTestBase.runWithArgs:528-&gt;YarnTestBase.runWithArgs:612 Â» Runtime YARNSessionCapacitySchedulerITCase.testDetachedPerJobYarnCluster:373-&gt;testDetachedPerJobYarnClusterInternal:419-&gt;YarnTestBase.startWithArgs:515 Â» Runtime YARNSessionCapacitySchedulerITCase.testDetachedPerJobYarnClusterWithStreamingJob:390-&gt;testDetachedPerJobYarnClusterInternal:419-&gt;YarnTestBase.startWithArgs:515 Â» Runtime YARNSessionCapacitySchedulerITCase.testTaskManagerFailure:140-&gt;YarnTestBase.startWithArgs:515 Â» Runtime YARNSessionFIFOITCase.testDetachedMode:84-&gt;YarnTestBase.startWithArgs:515 Â» Runtime YARNSessionFIFOSecuredITCase&gt;YARNSessionFIFOITCase.testDetachedMode:84-&gt;YarnTestBase.startWithArgs:515 Â» Runtime}</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-7-2 01:00:00" id="6811" opendate="2017-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add TIMESTAMPADD supported in SQL</summary>
      <description>TIMESTAMPADD(unit,interval,datetime_expr) Adds the integer expression interval to the date or datetime expression datetime_expr. The unit for interval is given by the unit argument, which should be one of the following values: MICROSECOND (microseconds), SECOND, MINUTE, HOUR, DAY, WEEK, MONTH, QUARTER, or YEAR. Syntax:TIMESTAMPADD(unit,interval,datetime_expr) Arguments**unit: -**interval: -**datetime_expr: - Return Types DATAETIME/DATE/TIME Example: SELECT TIMESTAMPADD(month, 1, '2017-05-31') --&gt; '2017-06-30 00:00:00.000' SELECT TIMESTAMPADD(WEEK,1,'2003-01-02') -&gt; '2003-01-09' See more: MySQL Note: Due to the difference of [&amp;#91;org.apache.calcite.rex.Rex Literal&amp;#93;] between calcite 1.12 and calcite master we should temp close support the construce of TIMESTAMPADD(SqlTypeFamily.ANY, SqlTypeFamily.INTEGER, SqlTypeFamily.DATE), until upgrade to calcite 1.13.See more: https://issues.apache.org/jira/browse/CALCITE-1639https://issues.apache.org/jira/browse/FLINK-6851See more: https://issues.apache.org/jira/browse/CALCITE-1827</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.validation.ScalarFunctionsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.typeutils.TypeCheckUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-2 01:00:00" id="6815" opendate="2017-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Javadocs don&amp;#39;t work anymore in Flink 1.4-SNAPSHOT</summary>
      <description>https://ci.apache.org/projects/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/scala/KeyedStream.html results in a 404 error.The problem (https://ci.apache.org/builders/flink-docs-master/builds/731/steps/Java%20&amp;%20Scala%20docs/logs/stdio) is the following:[ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:3.2.2:compile (doc) on project flink-annotations: wrap: org.apache.maven.artifact.resolver.ArtifactNotFoundException: Could not find artifact com.typesafe.genjavadoc:genjavadoc-plugin_2.10.6:jar:0.8 in central (https://repo.maven.apache.org/maven2)I think the problem is that we upgraded the scala version to 2.10.6, but the plugin doesn't have version 0.8 for that scala version.</description>
      <version>1.4.0</version>
      <fixedVersion>1.3.1,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-2 01:00:00" id="6816" opendate="2017-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix wrong usage of Scala string interpolation in Table API</summary>
      <description>This issue is to fix some wrong usage of Scala string interpolation, such as missing the "s" prefix .</description>
      <version>None</version>
      <fixedVersion>1.3.1,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.RowTimeUnboundedOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.RowTimeBoundedRowsOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.RowTimeBoundedRangeOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeUnboundedPartitionedOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeUnboundedNonPartitionedOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeBoundedRowsOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeBoundedRangeOver.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  
  
  <bug fixdate="2017-6-2 01:00:00" id="6837" opendate="2017-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix a small error message bug, And improve some message info.</summary>
      <description>Fix a variable reference error, and improve some error message info.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.call.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.table.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-2 01:00:00" id="6840" opendate="2017-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MultipleLinearRegression documentation contains outdated information</summary>
      <description>The documentation for MultipleLinearRegression contains outdated information. We should correct the documentation.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.libs.ml.multiple.linear.regression.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-4 01:00:00" id="6846" opendate="2017-6-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add TIMESTAMPADD supported in TableAPI</summary>
      <description>See FLINK-6811 for detail.</description>
      <version>1.4.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.time.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.symbols.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.expressions.time.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-6 01:00:00" id="6852" opendate="2017-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix misuse of GCD</summary>
      <description>There's no need to compare and swap values for gcd input</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-7-7 01:00:00" id="6865" opendate="2017-6-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expand checkstyle docs to include import in intellij</summary>
      <description/>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.internals.ide.setup.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-8 01:00:00" id="6867" opendate="2017-6-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elasticsearch 1.x ITCase still instable due to embedded node instability</summary>
      <description>The integration tests for Elasticsearch 1.x seems to still be instable, being that the test is failing more frequently recently. One example is &amp;#91;1&amp;#93;.The last attempt to fix this was FLINK-5772, in which the test attempts was increased to 3. This doesn't seem to fix the issue. In the worst scenario, since the root cause is an instability with Elasticsearch 1.x's embedded node, and ES 1.x is a very old version that is usually recommended to be upgraded from, we could also consider removing the IT test for ES 1.x.&amp;#91;1&amp;#93; https://travis-ci.org/apache/flink/jobs/240444523</description>
      <version>None</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-8 01:00:00" id="6868" opendate="2017-6-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Using `scala.binary.version` for `flink-streaming-scala` in `Cassandra Connector`</summary>
      <description>Shoud using `scala.binary.version` for `flink-streaming-scala` in `Cassandra Connector`.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2017-7-12 01:00:00" id="6891" opendate="2017-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add LOG(X) supported in SQL</summary>
      <description>LOG(X), LOG(B,X)If called with one parameter, this function returns the natural logarithm of X. If X is less than or equal to 0.0E0, the function returns NULL and (as of MySQL 5.7.4) a warning “Invalid argument for logarithm” is reported.The inverse of this function (when called with a single argument) is the EXP() function.If called with two parameters, this function returns the logarithm of X to the base B. If X is less than or equal to 0, or if B is less than or equal to 1, then NULL is returned. Example: LOG(2) -&gt; 0.69314718055995 LOG(-2) -&gt; NULL LOG(2,65536) -&gt; 16 LOG(10,100) -&gt; 2 LOG(1,100) -&gt; NULL See more: MySQL -NOTE- In this JIRA. NULL case will throw IllegalArgumentException.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.validation.ScalarFunctionsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.functions.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.sql.ScalarSqlFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-12 01:00:00" id="6892" opendate="2017-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add L/RPAD supported in SQL</summary>
      <description>L/RPAD(str,len,padstr) Returns the string str, left/right-padded with the string padstr to a length of len characters. If str is longer than len, the return value is shortened to len characters. Syntax:LPAD(str,len,padstr) Arguments**str: -**len: -**padstr: - Return Types String Example: LPAD('hi',4,'??') -&gt; '??hi' LPAD('hi',1,'??') -&gt; 'h' RPAD('hi',4,'') -&gt; 'hi' RPAD('hi',1,'??') -&gt; 'h' See more: MySQL</description>
      <version>1.4.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.sql.ScalarSqlFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.functions.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-12 01:00:00" id="6893" opendate="2017-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add BIN supported in SQL &amp; Table API</summary>
      <description>BIN(N) Returns a string representation of the binary value of N, where N is a longlong (BIGINT) number. This is equivalent to CONV(N,10,2). Returns NULL if N is NULL. Syntax:BIN(num) Arguments**num: a long/bigint value Return Types String Example: BIN(12) -&gt; '1100' See more: MySQL</description>
      <version>1.4.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.validation.ScalarFunctionsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.typeutils.TypeCheckUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.sql.ScalarSqlFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.mathExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-12 01:00:00" id="6898" opendate="2017-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Limit size of operator component in metric name</summary>
      <description>The operator name for some operators (specifically windows) can be very, very long (250+) characters.I propose to limit the total space that the operator component can take up in a metric name to 60 characters.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.TaskMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.TaskMetricGroup.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.groups.AbstractMetricGroup.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-4-15 01:00:00" id="6924" opendate="2017-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ADD LOG(X) supported in TableAPI</summary>
      <description>See FLINK-6891 for detail.</description>
      <version>1.4.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.mathExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-15 01:00:00" id="6925" opendate="2017-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CONCAT/CONCAT_WS supported in SQL</summary>
      <description>CONCAT(str1,str2,...)Returns the string that results from concatenating the arguments. May have one or more arguments. If all arguments are nonbinary strings, the result is a nonbinary string. If the arguments include any binary strings, the result is a binary string. A numeric argument is converted to its equivalent nonbinary string form.CONCAT() returns NULL if any argument is NULL. Syntax:CONCAT(str1,str2,...) Arguments str1,str2,... - Return Types string Example: CONCAT('F', 'lin', 'k') -&gt; 'Flink' CONCAT('M', NULL, 'L') -&gt; NULL CONCAT(14.3) -&gt; '14.3' See more: MySQL CONCAT_WS() stands for Concatenate With Separator and is a special form of CONCAT(). The first argument is the separator for the rest of the arguments. The separator is added between the strings to be concatenated. The separator can be a string, as can the rest of the arguments. If the separator is NULL, the result is NULL. Syntax:CONCAT_WS(separator,str1,str2,...) Arguments separator - str1,str2,... - Return Types string Example: CONCAT_WS(',','First name','Second name','Last Name') -&gt; 'First name,Second name,Last Name' See more: MySQL</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.MathFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.sql.ScalarSqlFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.CallGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-15 01:00:00" id="6926" opendate="2017-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for MD5, SHA1 and SHA2</summary>
      <description>MD5(str)Calculates an MD5 128-bit checksum for the string. The value is returned as a string of 32 hexadecimal digits, or NULL if the argument was NULL. The return value can, for example, be used as a hash key. See the notes at the beginning of this section about storing hash values efficiently.The return value is a nonbinary string in the connection character set. Example: MD5('testing') - 'ae2b1fca515949e5d54fb22b8ed95575' See more: MySQL SHA1(str), SHA(str)Calculates an SHA-1 160-bit checksum for the string, as described in RFC 3174 (Secure Hash Algorithm). The value is returned as a string of 40 hexadecimal digits, or NULL if the argument was NULL. One of the possible uses for this function is as a hash key. See the notes at the beginning of this section about storing hash values efficiently. You can also use SHA1() as a cryptographic function for storing passwords. SHA() is synonymous with SHA1().The return value is a nonbinary string in the connection character set. Example: SHA1('abc') -&gt; 'a9993e364706816aba3e25717850c26c9cd0d89d'SHA2(str, hash_length)Calculates the SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384, and SHA-512). The first argument is the cleartext string to be hashed. The second argument indicates the desired bit length of the result, which must have a value of 224, 256, 384, 512, or 0 (which is equivalent to 256). If either argument is NULL or the hash length is not one of the permitted values, the return value is NULL. Otherwise, the function result is a hash value containing the desired number of bits. See the notes at the beginning of this section about storing hash values efficiently.The return value is a nonbinary string in the connection character set. Example:SHA2('abc', 224) -&gt; '23097d223405d8228642a477bda255b32aadbce4bda0b3f7e36c9da7' See more: MySQL</description>
      <version>1.4.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.ScalarTypesTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.ProjectionTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.LogicalNode.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.HashCalcCallGen.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.sql.DateTimeSqlFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.time.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.CallGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.sql.ScalarSqlFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.hashExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-6-16 01:00:00" id="6937" opendate="2017-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix link markdown in Production Readiness Checklist doc</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.production.ready.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-18 01:00:00" id="6940" opendate="2017-6-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clarify the effect of configuring per-job state backend</summary>
      <description>The documentation of having different options configuring flink state backend is confusing. We should add explicit doc explaining configuring a per-job flink state backend in code will overwrite any default state backend configured in flink-conf.yaml</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.backends.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-19 01:00:00" id="6942" opendate="2017-6-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add E() supported in TableAPI</summary>
      <description>See FLINK-6960 for detail.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.mathExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-21 01:00:00" id="6960" opendate="2017-6-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add E() supported in SQL</summary>
      <description>E=Math.E</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-21 01:00:00" id="6962" opendate="2017-6-21 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add a create table SQL DDL</summary>
      <description>This Jira adds support to allow user define the DDL for source and sink tables, including the waterMark(on source table) and emit SLA (on result table). The detailed design doc will be attached soon.This issue covered adding batch DDL support. Streaming-specific DDL support will be added later.</description>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-7-22 01:00:00" id="6975" opendate="2017-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CONCAT/CONCAT_WS supported in TableAPI</summary>
      <description>See FLINK-6925 for detail.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.InputTypeSpec.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-22 01:00:00" id="6982" opendate="2017-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace guava dependencies</summary>
      <description/>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.FileSystemBlobStore.java</file>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">tools.maven.checkstyle.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UdfStreamOperatorCheckpointingITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.TestHarnessUtil.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.ProcessingTimeTriggerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.ProcessingTimeSessionWindowsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.MergingWindowSetTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.EventTimeTriggerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.EventTimeSessionWindowsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.CountTriggerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.ContinuousEventTimeTriggerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.AggregationFunctionTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.EvictingWindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.windowing.evictors.DeltaEvictor.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.UnionTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.TwoInputTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SplitTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SinkTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SideOutputTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.SelectTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.PartitionTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.OneInputTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.FeedbackTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.transformations.CoFeedbackTransformation.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraphHasherV2.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.SplitStream.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.migration.streaming.api.graph.StreamGraphHasherV1.java</file>
      <file type="M">flink-streaming-java.pom.xml</file>
      <file type="M">flink-shaded-curator.flink-shaded-curator-test.pom.xml</file>
      <file type="M">flink-shaded-curator.flink-shaded-curator-recipes.pom.xml</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.testingUtils.TestingUtils.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.StateBackendTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.RightOuterJoinTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.LeftOuterJoinTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.hash.InPlaceMutableHashTableTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.operators.AbstractOuterJoinTaskTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.scheduler.ScheduleOrUpdateConsumersTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerStartupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.iterative.concurrent.BrokerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.iterative.concurrent.BlockingBackChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.util.TestPooledBufferProvider.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SpilledSubpartitionViewTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannelTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.LocalBufferPoolTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.filecache.FileCacheDeleteValidationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.security.SecurityUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.query.netty.KvStateServer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.query.netty.KvStateClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.TaskEventDispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.UnionInputGate.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyServer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.TaskEventHandler.java</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.sampling.RandomSamplerTest.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.tuple.TupleGenerator.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.utils.ParameterToolTest.java</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.accumulators.AccumulatorITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.example.java.PageRankITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.example.scala.PageRankITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.io.CsvReaderITCase.java</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.io.ScalaCsvReaderWithPOJOITCase.scala</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.java</file>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnApplicationMasterRunnerTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-twitter.pom.xml</file>
      <file type="M">flink-connectors.flink-hbase.pom.xml</file>
      <file type="M">flink-connectors.flink-hcatalog.pom.xml</file>
      <file type="M">flink-contrib.flink-storm-examples.pom.xml</file>
      <file type="M">flink-contrib.flink-storm-examples.src.test.java.org.apache.flink.storm.join.SingleJoinITCase.java</file>
      <file type="M">flink-libraries.flink-cep.pom.xml</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.compiler.NFACompiler.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.NFA.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.compiler.NFACompilerTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.GroupITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.IterativeConditionsITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFAITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFATestUtilities.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NotPatternITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.SameElementITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.TimesRangeITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.UntilConditionITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPOperatorTest.java</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
      <file type="M">flink-optimizer.pom.xml</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dag.SingleInputNode.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dag.TwoInputNode.java</file>
      <file type="M">flink-runtime-web.pom.xml</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.BackPressureStatsTracker.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.checkpoints.CheckpointStatsCache.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.StackTraceSampleCoordinator.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarAccessDeniedHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobCancellationHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobDetailsHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JobStoppingHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.TaskManagersHandlerTest.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-22 01:00:00" id="6985" opendate="2017-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove bugfix version from docs title</summary>
      <description>The docs HTML title contains the minor version of the corresponding release. This can be confusing as we build the docs nightly from the respective release branch.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..layouts.base.html</file>
      <file type="M">docs..includes.sidenav.html</file>
      <file type="M">docs..config.yml</file>
      <file type="M">docs.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-26 01:00:00" id="7004" opendate="2017-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Switch to Travis Trusty image</summary>
      <description>As shown in this PR https://github.com/apache/flink/pull/4167 switching to the Trusty image on Travis seems to stabilize the build times.We should switch for 1.2, 1.3 and 1.4.</description>
      <version>1.2.0,1.3.0,1.4.0</version>
      <fixedVersion>1.2.2,1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-27 01:00:00" id="7011" opendate="2017-6-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Instable Kafka testStartFromKafkaCommitOffsets failures on Travis</summary>
      <description>Example:https://s3.amazonaws.com/archive.travis-ci.org/jobs/246703474/log.txt?X-Amz-Expires=30&amp;X-Amz-Date=20170627T065647Z&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAJRYRXRSVGNKPKO5A/20170627/us-east-1/s3/aws4_request&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=dbfc90cfc386fef0990325b54ff74ee4d441944687e7fdaa73ce7b0c2b2ec0eaIn general, the test testStartFromKafkaCommitOffsets implementation is a bit of an overkill. Before continuing with the test, it writes some records just for the sake of committing offsets to Kafka and waits for some offsets to be committed (which leads to the instability), whereas we can do that simply using the test base's OffsetHandler.</description>
      <version>1.3.1,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09ITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08ITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010ITCase.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-6-27 01:00:00" id="7014" opendate="2017-6-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose isDeterministic interface to ScalarFunction and TableFunction</summary>
      <description>Currently, the `isDeterministic` method of implementations of `SqlFuntion` are always returning true, which cause inappropriate optimization in Calcite, such as taking user's stateful UDF as a pure functional procedure.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.ExpressionReductionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.TableSqlFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.ScalarSqlFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.AggSqlFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.UserDefinedFunction.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-6-28 01:00:00" id="7025" opendate="2017-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Using NullByteKeySelector for Unbounded ProcTime NonPartitioned Over</summary>
      <description>Currently we added `Cleanup State` feature. But It not work well if we enabled the stateCleaning on Unbounded ProcTime NonPartitioned Over window, Because in `ProcessFunctionWithCleanupState` we has using the keyed state.So, In this JIRA. I'll change the `Unbounded ProcTime NonPartitioned Over` to `partitioned Over` by using NullByteKeySelector. OR created a `NonKeyedProcessFunctionWithCleanupState`. But I think the first way is simpler. What do you think? Fabian Hueske</description>
      <version>1.3.1,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.OverWindowHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeUnboundedPartitionedOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeUnboundedNonPartitionedOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamOverAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-28 01:00:00" id="7026" opendate="2017-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add shaded asm dependency</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">tools.maven.checkstyle.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-shaded-curator.flink-shaded-curator-recipes.pom.xml</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.ClosureCleaner.scala</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.JarFileCreator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.DependencyVisitor.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-libraries.flink-gelly-scala.pom.xml</file>
      <file type="M">flink-libraries.flink-cep-scala.pom.xml</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.UdfAnalyzerUtils.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.UdfAnalyzer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.TaggedValue.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.NestedMethodAnalyzer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.ModifiedASMFrame.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.ModifiedASMAnalyzer.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ClosureCleaner.java</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractionUtils.java</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-7-28 01:00:00" id="7030" opendate="2017-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build with scala-2.11 by default</summary>
      <description>As proposed recently on the dev mailing list.I propose to switch to Scala 2.11 as a default and to have a Scala 2.10 build profile. Now it is the other way around. The reason for that is poor support for build profiles in Intellij, I was unable to make it work after I added Kafka 0.11 dependency (Kafka 0.11 dropped support for Scala 2.10).</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">docs.setup.building.md</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-29 01:00:00" id="7039" opendate="2017-6-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase forkCountTestPackage for sudo-enabled TravisCI</summary>
      <description>The switch from the container-based to sudo-enabled environment in TravisCI has increased available memory from 4 GB to 7.5 GB so use a forkCount of 2 in all packages including flink-test. See https://docs.travis-ci.com/user/ci-environment/The sudo-enabled machines look to be Google Compute Engine n1-standard-2 with 2 "virtual CPUs". See https://cloud.google.com/compute/pricing.</description>
      <version>1.4.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-29 01:00:00" id="7040" opendate="2017-6-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flip-6 client-cluster communication</summary>
      <description>With the new Flip-6 architecture, the client will communicate with the cluster in a RESTful manner.The cluster shall support the following REST calls: List jobs (GET): Get list of all running jobs on the cluster Submit job (POST): Submit a job to the cluster (only supported in session mode) Lookup job leader (GET): Gets the JM leader for the given job Get job status (GET): Get the status of an executed job (and maybe the JobExecutionResult) Cancel job (PUT): Cancel the given job Stop job (PUT): Stops the given job Take savepoint (POST): Take savepoint for given job (How to return the savepoint under which the savepoint was stored? Maybe always having to specify a path) Get KV state (GET): Gets the KV state for the given job and key (Queryable state) Poll/subscribe to notifications for job (GET, WebSocket): Polls new notifications from the execution of the given job/Opens WebSocket to receive notificationsThe first four REST calls will be served by the REST endpoint running in the application master/cluster entrypoint. The other calls will be served by a REST endpoint running along side to the JobManager.Detailed information about different implementations and their pros and cons can be found in this document:https://docs.google.com/document/d/1eIX6FS9stwraRdSUgRSuLXC1sL7NAmxtuqIXe_jSi-k/edit?usp=sharingThe implementation will most likely be Netty based.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.MessageParametersTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.util.RestClientException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpointConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClientEndpointConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClientEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.MessageParameters.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.MessageParameter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.RestHandlerException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.PipelineErrorHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.HandlerRequest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.AbstractRestHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-29 01:00:00" id="7046" opendate="2017-6-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hide logging about downloaded artifacts on travis</summary>
      <description>We can reduce the verbosity of the travis logs by hiding messages about downloaded artifacts, such as this:[INFO] Downloading: https://repo.maven.apache.org/maven2/org/eclipse/tycho/tycho-compiler-jdt/0.21.0/tycho-compiler-jdt-0.21.0.pom[INFO] Downloaded: https://repo.maven.apache.org/maven2/org/eclipse/tycho/tycho-compiler-jdt/0.21.0/tycho-compiler-jdt-0.21.0.pom (2 KB at 62.0 KB/sec)</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-29 01:00:00" id="7047" opendate="2017-6-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reorganize build profiles</summary>
      <description>With the current build times once again hitting the timeout it is time to revisit our approach.The current approach of splitting all tests by name, while easy to maintain or extend, has the big disadvantage that it's fairly binary in regards to the timeout: either we're below the timeout and all builds pass, or we're above and the entire merging process stalls. Furthermore, it requires all modules to be compiled.I propose a different approach by which we bundle several modules, only execute the tests of these modules and skip the compilation of some modules that are not required for these tests.5 groups are my current suggestion, which will result in 10 build profiles total.The groups are: core - core flink modules like core,runtime,streaming-java,metrics,rocksdb libraries - flink-libraries and flink-storm connectors - flink-connectors, flink-connector-wikiedits, flink-tweet-inputformat tests - flink-tests misc - flink-yarn, fink-yarn-tests, flink-mesos, flink-examples, flink-distTo not increase the total number of profiles to ridiculous numbers i also propose to only test against 2 combinations of jdk+hadoop+scala: oraclejdk8 + hadoop 2.8.0 + scala 2.11 openjdk7 + hadoop 2.4.1 + scala 2.10My current estimate is that this will cause profiles to take at most 40 minutes.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-7-30 01:00:00" id="7052" opendate="2017-6-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove NAME_ADDRESSABLE mode</summary>
      <description>Remove the BLOB store's NAME_ADDRESSABLE mode as it is currently not used and partly broken.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobServerPutTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobServerDeleteTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobRecoveryITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobClientTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.blob.BlobClientSslTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.VoidBlobStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.FileSystemBlobStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobView.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobServerProtocol.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobServerConnection.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobServer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobClient.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2017-7-4 01:00:00" id="7103" opendate="2017-7-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement skeletal structure of dispatcher component</summary>
      <description>Implement the skeletal structure of the Dispatcher component. The initial functionality will support job submissions and listing of jobs.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnFlinkApplicationMasterRunner.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobManagerRunnerMockTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniClusterJobDispatcher.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobManagerServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.SubmittedJobGraph.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-5 01:00:00" id="7105" opendate="2017-7-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make ActorSystem creation per default non-daemonic</summary>
      <description>At the moment, we create all ActorSystems with the setting daemonic=on. This has the consequence that we have to wait in the main thread on the ActorSystem's termination. By making the ActorSystems non-daemonic, we could get rid of this artifact. Especially since we have the ProcessReapers which terminate the process once a registered actor terminates.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-6 01:00:00" id="7111" opendate="2017-7-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-scala-shell fails on mvn verify</summary>
      <description>Running mvn verify after mvn clean install -DskipTests causes the build to fail in flink-scala-shell with[ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:2.4:single (create-library-loading-jar) on project flink-scala-shell_2.11: Failed to create assembly: Error creating assembly archive test-jar: You must set at least one file. -&gt; [Help 1]</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-scala-shell.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-7 01:00:00" id="7126" opendate="2017-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Distinct for Stream SQL and Table API</summary>
      <description/>
      <version>1.4.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.UnsupportedOpsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.table.GroupAggregationsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.scala.stream.sql.AggregationsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-7 01:00:00" id="7131" opendate="2017-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming wordcount jar does not contain WordCountData</summary>
      <description>The WordCountData isn't included in the jar.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-7 01:00:00" id="7136" opendate="2017-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Docs search can be customized to be more useful</summary>
      <description>The google custom search engine we're using for search can be customized to make it more useful.I propose to turn off ads (which is allowed, since this site belongs to a non-profit org) add additional sources of information (mailing lists, JIRA, flips, stack overflow, flink forward) use refinements (tabs) to make it easy to navigate between these sources</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.sidenav.html</file>
      <file type="M">docs.search-results.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-8 01:00:00" id="7138" opendate="2017-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Storm example jars do not contain WordCountData</summary>
      <description>Same as FLINK-7131.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm-examples.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-12 01:00:00" id="7166" opendate="2017-7-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>generated avro sources not cleaned up or re-created after changes</summary>
      <description>Since the AVRO upgrade to 1.8.2, I could compile the flink-avro module any more with a failure like this in mvn clean install -DskipTests -pl flink-connectors/flink-avro:Compilation failure[ERROR] flink-connectors/flink-avro/src/test/java/org/apache/flink/api/io/avro/generated/Fixed16.java:[10,8] org.apache.flink.api.io.avro.generated.Fixed16 is not abstract and does not override abstract method readExternal(java.io.ObjectInput) in org.apache.avro.specific.SpecificFixedThis was caused by maven both not cleaning up the generated sources and also not overwriting them with new ones itself. Only a manual rm -rf flink-connectors/flink-avro/src/test/java/org/apache/flink/api/io/avro/generated solved the issue.The cause for this, though, is that the avro files are generated under the src directory, not target/generated-test-sources as they should be. Either the generated sources should be cleaned up as well, or the generated files should be moved to this directory which is a more invasive change due to some hacks with respect to these files.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-avro.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-13 01:00:00" id="7174" opendate="2017-7-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump dependency of Kafka 0.10.x to the latest one</summary>
      <description>We are using pretty old Kafka version for 0.10. Besides any bug fixes and improvements that were made between 0.10.0.1 and 0.10.2.1, it 0.10.2.1 version is more similar to 0.11.0.</description>
      <version>1.2.1,1.3.1,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractPartitionDiscoverer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThreadTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-13 01:00:00" id="7176" opendate="2017-7-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failed builds (due to compilation) don&amp;#39;t upload logs</summary>
      <description>If the compile phase fails on travis flink-dist may not be created. This causes the check for the inclusion of snappy in flink-dist to fail.The function doing this check calls exit 1 on error, which exits the entire shell, thus skipping subsequent actions like the upload of logs.</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>1.3.0,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-17 01:00:00" id="7206" opendate="2017-7-17 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implementation of DataView to support state access for UDAGG</summary>
      <description>Implementation of MapView and ListView to support state access for UDAGG.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.OverWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.GroupWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.AggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.runtime.utils.JavaUserDefinedAggFunctions.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.RowTimeUnboundedOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.RowTimeBoundedRowsOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.RowTimeBoundedRangeOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeUnboundedOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeBoundedRowsOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeBoundedRangeOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.GroupAggProcessFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.GeneratedAggregations.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-17 01:00:00" id="7208" opendate="2017-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor build-in agg(MaxWithRetractAccumulator and MinWithRetractAccumulator) using the DataView</summary>
      <description>Refactor build-in agg(MaxWithRetractAccumulator and MinWithRetractAccumulator) using the DataView.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.AggFunctionHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.AggFunctionTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunctionWithRetract.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunctionWithRetract.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-17 01:00:00" id="7211" opendate="2017-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude Gelly javadoc jar from release</summary>
      <description/>
      <version>1.3.2,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-18 01:00:00" id="7220" opendate="2017-7-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update RocksDB dependency to 5.5.5</summary>
      <description>The latest release of RocksDB (5.5.5) fixes the issues from previous versions (slow merge performance, segfaults) in connection with Flink and seems stable for us to use. We can move away from our custom FRocksDB build, back to the latest release.</description>
      <version>1.3.2,1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.benchmark.RocksDBPerformanceTest.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.PredefinedOptions.java</file>
      <file type="M">flink-contrib.flink-statebackend-rocksdb.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-19 01:00:00" id="7228" opendate="2017-7-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden HistoryServerStaticFileHandlerTest</summary>
      <description>We can harden the test to use a free port instead of the hard-coded 8081.</description>
      <version>1.3.1,1.4.0</version>
      <fixedVersion>1.3.2,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandlerTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-19 01:00:00" id="7230" opendate="2017-7-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Travis sometimes fails due to downloaded snapshot artifacts</summary>
      <description>The travis builds currently fail sometimes because for some reason snapshot artifacts are downloaded when executing tests, after compilation. This causes issues for the java 7 profiles, as the snapshot artifacts are released with java 8.The cause for this is currently unknown; I'm currently trying builds that forbid downloading snapshot artifacts or downloading artifacts during testing in general.Since this issue doesn't always occur I will run these tests multiple times until tomorrow, and if the issue hasn't appeared again will open a PR.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-7-24 01:00:00" id="7249" opendate="2017-7-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Java version in build plugin</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">docs.setup.building.md</file>
      <file type="M">docs.quickstart.setup.quickstart.md</file>
      <file type="M">docs.quickstart.scala.api.quickstart.md</file>
      <file type="M">docs.quickstart.java.api.quickstart.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-24 01:00:00" id="7250" opendate="2017-7-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop the jdk8 build profile</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-24 01:00:00" id="7253" opendate="2017-7-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove all &amp;#39;assume Java 8&amp;#39; code in tests</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.core.testutils.CommonTestUtils.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.JvmExitOnFatalErrorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.BlockingShutdownTest.java</file>
      <file type="M">flink-connectors.flink-connector-cassandra.src.test.java.org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-24 01:00:00" id="7256" opendate="2017-7-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end tests should only be run after successful compilation</summary>
      <description>If the compilation fails (for example due to checkstyle) the end-to-end tests are currently still run, even though flink-dist most likely wasn't even built.Similar to FLINK-7176.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-7-25 01:00:00" id="7263" opendate="2017-7-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Pull Request Template</summary>
      <description>As discussed in the mailing list, the suggestion is to update the pull request template as follows:Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review you contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community. Contribution Checklist Make sure that the pull request corresponds to a &amp;#91;JIRA issue&amp;#93;(https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue. Name the pull request in the form "FLINK-1234 &amp;#91;component&amp;#93; Title of the pull request", where FLINK-1234 should be replaced by the actual issue number. Skip component if you are unsure about which is the best component. Typo fixes that have no associated JIRA issue should be named following this pattern: `&amp;#91;hotfix&amp;#93; &amp;#91;docs&amp;#93; Fix typo in event time introduction` or `&amp;#91;hotfix&amp;#93; &amp;#91;javadocs&amp;#93; Expand JavaDoc for PuncuatedWatermarkGenerator`. Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review. Make sure that the change passes the automated tests, i.e., `mvn clean verify` Each pull request should address only one issue, not mix up code from multiple issues. Each commit in the pull request has a meaningful commit message (including the JIRA id) Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.*(The sections below can be removed for hotfixes of typos)* What is the purpose of the change (For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).) Brief change log (for example The TaskInfo is stored in the blob store on job creation time as a persistent artifact Deployments RPC transmits only the blob storage reference TaskManagers retrieve the TaskInfo from the blob cache Verifying this change (Please pick either of the following options)This change is a trivial rework / code cleanup without any test coverage.(or)This change is already covered by existing tests, such as (please describe tests).(or)This change added tests and can be verified as follows:(example Added integration tests for end-to-end deployment with large payloads (100MB) Extended integration test for recovery after master (JobManager) failure Added test that validates that TaskInfo is transferred only once across recoveries Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and to TaskManagers during the execution, verifying that recovery happens correctly. Does this pull request potentially affect one of the following parts: Dependencies (does it add or upgrade a dependency): *(yes / no)* The public API, i.e., is any changed class annotated with `@Public(Evolving)`: *(yes / no)* The serializers: *(yes / no / don't know)* The runtime per-record code paths (performance sensitive): *(yes / no / don't know)* Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: *(yes / no / don't know)*: Documentation Does this pull request introduce a new feature? *(yes / no)* If yes, how is the feature documented? *(not applicable / docs / JavaDocs / not documented)*</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.github.PULL.REQUEST.TEMPLATE.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-31 01:00:00" id="7300" opendate="2017-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end tests are instable on Travis</summary>
      <description>It seems like the end-to-end tests are instable, causing the misc build profile to sporadically fail.Incorrect matched output:https://s3.amazonaws.com/archive.travis-ci.org/jobs/258569408/log.txt?X-Amz-Expires=30&amp;X-Amz-Date=20170731T060526Z&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAJRYRXRSVGNKPKO5A/20170731/us-east-1/s3/aws4_request&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=4ef9ff5e60fe06db53a84be8d73775a46cb595a8caeb806b05dbbf824d3b69e8Another failure example of a different cause then the above, also on the end-to-end tests:https://s3.amazonaws.com/archive.travis-ci.org/jobs/258841693/log.txt?X-Amz-Expires=30&amp;X-Amz-Date=20170731T060007Z&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAJRYRXRSVGNKPKO5A/20170731/us-east-1/s3/aws4_request&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=4a106b3990228b7628c250cc15407bc2c131c8332e1a94ad68d649fe8d32d726</description>
      <version>1.4.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
      <file type="M">test-infra.end-to-end-test.test.streaming.kafka010.sh</file>
      <file type="M">test-infra.end-to-end-test.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-31 01:00:00" id="7303" opendate="2017-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build elasticsearch5 by default</summary>
      <description>The elasticsearch-5 connector is optionally included in flink-connectors, based on whether jdk8 is used or not. Now that we drop java 7 support we can include it by default.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">flink-connectors.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-31 01:00:00" id="7305" opendate="2017-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add new import block for shaded dependencies</summary>
      <description>Since we will start working against shaded namespaces I propose a new import block for these, to differentiate them from "original" flink imports.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.checkstyle.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-8-31 01:00:00" id="7313" opendate="2017-7-31 00:00:00" resolution="Done">
    <buginformation>
      <summary>Add conversion utilities (Scala and old Flink futures)</summary>
      <description>In order to replace Flink's own Futures with Java 8's CompletableFutures we need some conversion utility to transform Scala's Futures and Flink's old Futures into Java 8's CompletableFutures until we have completed the replacement.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.concurrent.FutureUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.concurrent.Executors.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-8-31 01:00:00" id="7321" opendate="2017-7-31 00:00:00" resolution="Done">
    <buginformation>
      <summary>Remove Flink&amp;#39;s futures from HeartbeatManager</summary>
      <description/>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.heartbeat.HeartbeatManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.heartbeat.HeartbeatManagerSenderImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.heartbeat.HeartbeatManagerImpl.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.heartbeat.HeartbeatListener.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-1 01:00:00" id="7334" opendate="2017-8-1 00:00:00" resolution="Done">
    <buginformation>
      <summary>Replace Flink&amp;#39;s futures by CompletableFuture in RpcGateway</summary>
      <description/>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTerminationTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.async.queue.UnorderedStreamElementQueueTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.async.queue.StreamElementQueueTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.async.queue.OrderedStreamElementQueueTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskexecutor.TaskExecutorITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.TestingSerialRpcService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.TestingRpcService.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.TestingGatewayBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.RpcConnectionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.RpcCompletenessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.AsyncCallsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.MessageSerializationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotProtocolTest.java</file>
      <file type="M">flink-mesos.src.test.java.org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.TaskManagerLogHandler.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.TaskManagerLogHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.SlotPool.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.instance.SlotPoolGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.slots.ActorTaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.slots.TaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMasterGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.RpcTaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.registration.RetryingRegistration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.JobLeaderIdService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcActor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.MainThreadExecutable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.RpcService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.SelfGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.JobLeaderService.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.rpc.RpcInputSplitProvider.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.rpc.RpcPartitionStateChecker.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.rpc.RpcResultPartitionConsumableNotifier.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutor.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskExecutorToResourceManagerConnection.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerRunner.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.akka.QuarantineMonitorTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.checkpoint.CheckpointCoordinatorMasterHooksTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.ResourceManagerTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.dispatcher.DispatcherTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphMetricsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphSchedulingTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.ExecutionGraphStopTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.utils.NotCancelAckingTaskGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.executiongraph.utils.SimpleAckingTaskManagerGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.instance.SlotPoolRpcTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.instance.SlotPoolTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.registration.RetryingRegistrationTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.registration.TestRegistrationGateway.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.JobLeaderIdServiceTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerJobMasterTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-8-1 01:00:00" id="7337" opendate="2017-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor handling of time indicator attributes</summary>
      <description>After a discussion on the dev mailing list I propose the following changes to the current handling of time indicator attributes: Remove the separation of logical and physical row type. Hold the event-time timestamp as regular Long field in Row Represent the processing-time indicator type as a null-valued field in Row (1 bit overhead) Remove materialization of event-time timestamps because timestamp is already accessible in Row. Add ProcessFunction to set timestamp into the timestamp field of a StreamRecord.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSlideWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.NonWindowHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.JoinHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.SortTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.WindowJoinUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.CRowOutputMapRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.SortUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeSortProcessFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeBoundedRangeOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.IncrementalAggregateAllTimeWindowFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetTumbleTimeWindowAggReduceGroupFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.ProctimeSqlFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupWindowAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.RowTimeBoundedRangeOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.RowTimeBoundedRowsOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.RowTimeSortProcessFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.RowTimeUnboundedOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.TimeWindowPropertyCollector.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.CRowInputMapRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.CRowInputTupleOutputMapRunner.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.TimestampSetterProcessFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.WrappingTimestampSetterProcessFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.typeutils.TimeIndicatorTypeInfo.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.OverWindowHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.SortProcessFunctionHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSinkITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.TimeMaterializationSqlFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.CommonCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.CommonCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.BatchTableSourceScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamGroupAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamOverAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamSort.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamValues.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamWindowJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.StreamScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.StreamTableSourceScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalTableSourceScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.PhysicalTableSourceScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamWindowJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.schema.DataStreamTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.schema.FlinkTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.schema.RowSchema.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.schema.StreamTableSourceTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.DataSetSessionWindowAggReduceGroupFunction.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-8-2 01:00:00" id="7349" opendate="2017-8-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Only execute checkstyle in one build profile</summary>
      <description>We can save some time in 4/5 build profiles by skipping checkstyle. One of the build profiles builds flink completely and would suffice as a check.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-2 01:00:00" id="7350" opendate="2017-8-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>only execute japicmp in one build profile</summary>
      <description>Similarly to FLINK-7349 we improve build times (and stability!) by only executing the japicmp plugin in the build profile that builds the all of flink.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-3 01:00:00" id="7363" opendate="2017-8-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add hashes and signatures to the download page</summary>
      <description>As part of the releases, we also generate MD5 hashes and cryptographic signatures but neither link to those nor do we explain which keys are valid release-signing keys. This should be added.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualProducerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualConsumerProducerTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.examples.ProduceIntoKinesis.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.ProducerConfigConstants.java</file>
      <file type="M">docs.dev.connectors.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-4 01:00:00" id="7370" opendate="2017-8-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>rework operator documentation</summary>
      <description>The structure of the operator documentation could be improved the following way: Create category Streaming/Operators. Move Streaming/Overview/DataStream Transformations to Streaming/Operators/Overview. Move ProcessFunction, Windows, and Async IO to Streaming/Operators create any necessary redirects for old URLs</description>
      <version>1.3.0,1.4.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.windows.md</file>
      <file type="M">docs.redirects.windows.2.md</file>
      <file type="M">docs.redirects.windows.md</file>
      <file type="M">docs.ops.state.checkpoints.md</file>
      <file type="M">docs.dev.stream.windows.md</file>
      <file type="M">docs.dev.stream.state.checkpointing.md</file>
      <file type="M">docs.dev.stream.side.output.md</file>
      <file type="M">docs.dev.stream.process.function.md</file>
      <file type="M">docs.dev.stream.operators.md</file>
      <file type="M">docs.dev.stream.asyncio.md</file>
      <file type="M">docs.dev.event.timestamp.extractors.md</file>
      <file type="M">docs.dev.event.time.md</file>
      <file type="M">docs.dev.datastream.api.md</file>
      <file type="M">docs.dev.connectors.index.md</file>
      <file type="M">docs.concepts.programming-model.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-7 01:00:00" id="7379" opendate="2017-8-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove `HighAvailabilityServices` from QS client constructor.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.query.QueryableStateITCaseRocksDBBackend.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.query.QueryableStateITCaseMemoryBackend.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.query.QueryableStateITCaseFsBackend.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.query.AbstractQueryableStateITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.query.QueryableStateClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-7 01:00:00" id="7382" opendate="2017-8-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Broken links in `Apache Flink Documentation` page</summary>
      <description>Some links in the * External Resources * section are Broken.</description>
      <version>None</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.index.md</file>
      <file type="M">docs.examples.index.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-10-10 01:00:00" id="7414" opendate="2017-8-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hardcode scala.version to 2.11 in flink-quickstart-scala</summary>
      <description>Currently, the scala.binary.version of the Scala Quickstart is derived from the scala.binary.version of Flink at the time when the Quickstart is built. This means that whatever Scala version is active when we push the Quickstart takes precedence. Currently, when deploying SNAPSHOT versions we deploy 2.10 first, then 2.11, i.e. the 1.4-SNAPSHOT Scala Quickstart has Scala version 2.11. The release script deploys first 2.11 and then 2.10, meaning the final 1.4.0 Scala Quickstart would have 2.10.Simply fixing it to the latest supported (by Flink) will circumvent that issue and users can easily change the Scala version in the Quickstart.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-11-10 01:00:00" id="7419" opendate="2017-8-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shade jackson dependency in flink-avro</summary>
      <description>Avro uses org.codehouse.jackson which also exists in multiple incompatible versions. We should shade it to org.apache.flink.shaded.avro.org.codehouse.jackson.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">flink-formats.flink-avro.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-connectors.flink-avro.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-9-14 01:00:00" id="7439" opendate="2017-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support variable arguments for UDTF in SQL</summary>
      <description>Currently, both UDF and UDAF support variable parameters, but UDTF not. FLINK-5882 supports variable UDTF for Table API only, but missed SQL.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.TableFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.ScalarFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.validation.CorrelateValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.CorrelateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.CorrelateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.runtime.utils.JavaUserDefinedTableFunctions.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.schema.FlinkTableFunctionImpl.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.logical.LogicalUnnestRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.TableSqlFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.UserDefinedFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.TableFunctionCallGen.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarFunctionCallGen.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">docs.dev.table.udfs.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-8-14 01:00:00" id="7445" opendate="2017-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove FLINK-1234 reference from PR template</summary>
      <description>The PR template on github contains a reference to FLINK-1234 as an example for the PR title. The problem is that every PR that doesn't fill out the template, or rather does not delete that part of the template, will now be referenced in FLINK-1234, leading to spam on the mailing list.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.github.PULL.REQUEST.TEMPLATE.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-15 01:00:00" id="7446" opendate="2017-8-15 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support to define an existing field as the rowtime field for TableSource</summary>
      <description>Currently DefinedRowtimeAttribute can define an appended rowtime field for a TableSource. But it would be helpful if we can support to define an existing field as the rowtime field. Just like registering a DataStream, the rowtime field can be appended but also can replace an existing field.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.validation.TableSourceValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.validation.TableSourceValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.sources.definedTimeAttributes.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.schema.StreamTableSourceTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.logical.PushProjectIntoTableSourceScanRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalTableSourceScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.StreamTableSourceScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">docs.dev.table.streaming.md</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-8-16 01:00:00" id="7462" opendate="2017-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add very obvious warning about outdated docs</summary>
      <description>The current warning for outdated docs is not very obvious in the footer of the page. I would like to increase the visibility of this by adjusting this footer and adding a warning to actual content.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..layouts.plain.html</file>
      <file type="M">docs..layouts.base.html</file>
      <file type="M">docs..config.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-18 01:00:00" id="7480" opendate="2017-8-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set HADOOP_CONF_DIR to sane default if not set</summary>
      <description>Currently, both AWS and GCE don't have a HADOOP_CONF_DIR set by default. This makes the out-of-box experience on these cloud environments bad because not setting it results in errors that are not obviously clear.In case HADOOP_CONF_DIR is not set we should check if /etc/hadoop/conf exits and set HADOOP_CONF_DIR to that.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.config.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-22 01:00:00" id="7491" opendate="2017-8-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support COLLECT Aggregate function in Flink SQL</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.sql.AggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.FlinkRelNode.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.Types.scala</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.MapTypeInfo.java</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-24 01:00:00" id="7505" opendate="2017-8-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use lambdas in suppressed exception idiom</summary>
      <description>We can use Java 8 lamdas for the suppressed exception idiom on loops to unify the code for all possible methods without parameters that throw exceptions, such as close(), dispose(), etc.</description>
      <version>1.4.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.StateUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-25 01:00:00" id="7511" opendate="2017-8-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove dead code after dropping backward compatibility with &lt;=1.2</summary>
      <description/>
      <version>1.4.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.migration.MigrationVersion.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.resources.cep-starting-1.2-snapshot</file>
      <file type="M">flink-libraries.flink-cep.src.test.resources.cep-single-pattern-1.2-snapshot</file>
      <file type="M">flink-libraries.flink-cep.src.test.resources.cep-non-keyed-1.1-snapshot</file>
      <file type="M">flink-libraries.flink-cep.src.test.resources.cep-migration-starting-new-pattern-flink1.2-snapshot</file>
      <file type="M">flink-libraries.flink-cep.src.test.resources.cep-migration-single-pattern-afterwards-flink1.2-snapshot</file>
      <file type="M">flink-libraries.flink-cep.src.test.resources.cep-migration-after-branching-flink1.2-snapshot</file>
      <file type="M">flink-libraries.flink-cep.src.test.resources.cep-keyed-1.1-snapshot</file>
      <file type="M">flink-libraries.flink-cep.src.test.resources.cep-branching-1.2-snapshot</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.operator.CEPMigrationTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.SharedBufferTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.SubtypeFilterFunction.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.OrFilterFunction.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.AndFilterFunction.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.NonDuplicatingTypeSerializer.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.StateTransition.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.State.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.SharedBuffer.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.NFA.java</file>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-11-29 01:00:00" id="7551" opendate="2017-8-29 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add VERSION to the REST urls.</summary>
      <description>This is to guarantee that we can update the REST API without breaking existing third-party clients.</description>
      <version>1.4.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestClientTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.RestHandlerSpecification.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandler.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">docs..includes.generated.rest.dispatcher.html</file>
      <file type="M">docs.monitoring.rest.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-9-1 01:00:00" id="7571" opendate="2017-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Execution of TableSources with Time Indicators fails</summary>
      <description>The translation of queries that include a TableSource with time indicators fails during the code generation because field names and field indicies are not adjusted for the time indicators.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.schema.StreamTableSourceTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.StreamTableSourceScan.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-5 01:00:00" id="7579" opendate="2017-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation of yarn.application-attempts is not accurate</summary>
      <description>In current documentation yarn.application-attempts: 10 is documented as resulting in at most 10 application restarts. That is a bit misleading: YARN can perform up to 9 restarts for "failed" attempts, resulting in 10 attempts. It can perform additional restarts for YARN attempts that failed because of YARN operations: Node failures, preemption, NodeManager resyncs, see http://johnjianfang.blogspot.de/2015/04/the-number-of-maximum-attempts-of-yarn.html</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.jobmanager.high.availability.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-12-7 01:00:00" id="7599" opendate="2017-9-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support aggregation functions in the define and measures clause of MatchRecognize</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.match.MatchRecognizeValidationTest.scala</file>
      <file type="M">docs.dev.table.streaming.match.recognize.md</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.MatchRecognizeITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.match.MatchOperatorValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamMatchRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.MatchCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-14 01:00:00" id="7623" opendate="2017-9-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Detecting whether an operator is restored doesn&amp;#39;t work with chained state</summary>
      <description>Originally reported on the ML: https://lists.apache.org/thread.html/22a2cf83de3107aa81a03a921325a191c29df8aa8676798fcd497199@%3Cuser.flink.apache.org%3EIf we have a chain of operators where multiple of them have operator state, detection of the context.isRestored() flag (of CheckpointedFunction) does not work correctly. It's best exemplified using this minimal example where both the source and the flatMap have state:final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env .addSource(new MaSource()).uid("source-1") .flatMap(new MaFlatMap()).uid("flatMap-1");env.execute("testing");If I do a savepoint with these UIDs, then change "source-1" to "source-2" and restore from the savepoint context.isRestored() still reports true for the source.</description>
      <version>1.3.2,1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-9-20 01:00:00" id="7654" opendate="2017-9-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update RabbitMQ Java client to 4.x</summary>
      <description>RabbitMQ Java ClientStarting with 4.0, this client releases are independent from RabbitMQ server releases.These versions can still be used with RabbitMQ server 3.x.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.test.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.test.java.org.apache.flink.streaming.connectors.rabbitmq.common.RMQSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSource.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSink.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.README.md</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.pom.xml</file>
      <file type="M">docs.dev.connectors.rabbitmq.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-20 01:00:00" id="7658" opendate="2017-9-20 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support COLLECT Aggregate function in Flink TABLE API</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.AggregateITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.stringexpr.AggregateStringExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.aggregations.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-10-21 01:00:00" id="7662" opendate="2017-9-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary packaged licenses</summary>
      <description>With the new shading approach, we no longer shade ASM into Flink artifacts, so we do not need to package the ASM license into those artifacts any more.Instead, a shaded ASM artifact already containing a packaged license is used in the distribution build.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala.packaged.licenses.LICENSE.asm.txt</file>
      <file type="M">flink-runtime.packaged.licenses.LICENSE.asm.txt</file>
      <file type="M">flink-java.packaged.licenses.LICENSE.asm.txt</file>
      <file type="M">flink-core.packaged.licenses.LICENSE.asm.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-11-27 01:00:00" id="7698" opendate="2017-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Join with null literals leads to NPE</summary>
      <description>The following query fails: @Test def testProcessTimeInnerJoin(): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment val tEnv = TableEnvironment.getTableEnvironment(env) env.setStateBackend(getStateBackend) StreamITCase.clear env.setParallelism(1) val sqlQuery = "SELECT t2.a, t2.c, t1.c from T1 as t1 join T2 as t2 on t1.a = t2.a and t1.nullField = t2.nullField and " + "t1.proctime between t2.proctime - interval '5' second and t2.proctime + interval '5' second" val data1 = new mutable.MutableList[(Int, Long, String)] data1.+=((1, 1L, "Hi1")) data1.+=((1, 2L, "Hi2")) data1.+=((1, 5L, "Hi3")) data1.+=((2, 7L, "Hi5")) data1.+=((1, 9L, "Hi6")) data1.+=((1, 8L, "Hi8")) data1.+=((1, 8L, "Hi8")) val data2 = new mutable.MutableList[(Int, Long, String)] data2.+=((1, 1L, "HiHi")) data2.+=((2, 2L, "HeHe")) val t1 = env.fromCollection(data1).toTable(tEnv, 'a, 'b, 'c, 'proctime.proctime) .select('a, 'b, 'c, 'proctime, Null(Types.LONG) as 'nullField) val t2 = env.fromCollection(data2).toTable(tEnv, 'a, 'b, 'c, 'proctime.proctime) .select('a, 'b, 'c, 'proctime, 12L as 'nullField) tEnv.registerTable("T1", t1) tEnv.registerTable("T2", t2) val result = tEnv.sqlQuery(sqlQuery).toAppendStream[Row] result.addSink(new StreamITCase.StringSink[Row]) env.execute() }It leads to:java.lang.NullPointerException at org.apache.calcite.rex.RexUtil.gatherConstraint(RexUtil.java:437) at org.apache.calcite.rex.RexUtil.gatherConstraints(RexUtil.java:399) at org.apache.calcite.rex.RexUtil.predicateConstants(RexUtil.java:336) at org.apache.calcite.plan.RelOptPredicateList.of(RelOptPredicateList.java:144) at org.apache.calcite.rel.metadata.RelMdPredicates$JoinConditionBasedPredicateInference.inferPredicates(RelMdPredicates.java:654) at org.apache.calcite.rel.metadata.RelMdPredicates.getPredicates(RelMdPredicates.java:326) at GeneratedMetadataHandler_Predicates.getPredicates_$(Unknown Source) at GeneratedMetadataHandler_Predicates.getPredicates(Unknown Source) at GeneratedMetadataHandler_Predicates.getPredicates_$(Unknown Source) at GeneratedMetadataHandler_Predicates.getPredicates(Unknown Source) at org.apache.calcite.rel.metadata.RelMetadataQuery.getPulledUpPredicates(RelMetadataQuery.java:803) at org.apache.calcite.rel.rules.ReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(ReduceExpressionsRule.java:264) at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:317) at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:506) at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:385) at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:251) at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:125) at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:210) at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:197) at org.apache.flink.table.api.TableEnvironment.runHepPlanner(TableEnvironment.scala:257) at org.apache.flink.table.api.StreamTableEnvironment.optimize(StreamTableEnvironment.scala:663) at org.apache.flink.table.api.StreamTableEnvironment.translate(StreamTableEnvironment.scala:728) at org.apache.flink.table.api.scala.StreamTableEnvironment.toAppendStream(StreamTableEnvironment.scala:219) at org.apache.flink.table.api.scala.StreamTableEnvironment.toAppendStream(StreamTableEnvironment.scala:195) at org.apache.flink.table.api.scala.TableConversions.toAppendStream(TableConversions.scala:121) at org.apache.flink.table.runtime.stream.sql.JoinITCase.testProcessTimeInnerJoin(JoinITCase.scala:67) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:51) at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242) at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)Seems to be a Calcite bug, but we have to investigate this first. Replacing Null(Types.LONG) with a value works.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.JoinTest.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-27 01:00:00" id="7701" opendate="2017-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>IllegalArgumentException in Netty bootstrap with small memory state segment size</summary>
      <description>FLINK-7258 broke setting high and low watermarks for small segment sizes. We should tackle both use cases.</description>
      <version>1.3.2,1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyServerLowAndHighWatermarkTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyServer.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.AbstractServerBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-27 01:00:00" id="7702" opendate="2017-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Javadocs are not being built</summary>
      <description>The "Javadocs" link in the left side menu of this page doesn't work:https://ci.apache.org/projects/flink/flink-docs-master/Note that it works in 1.3:https://ci.apache.org/projects/flink/flink-docs-release-1.3/</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-filesystem.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-9-28 01:00:00" id="7725" opendate="2017-9-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add test base for marshalling requests</summary>
      <description>We should have a test base for marshalling rest request bodies as we have for responses.Additionally, it should be possible to throw an exception when creating the instance.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.messages.RestResponseMarshallingTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-28 01:00:00" id="7727" opendate="2017-9-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend logging in file server handlers</summary>
      <description>The file server handlers check several failure conditions but don't log anything (like the path), making debugging difficult.</description>
      <version>1.4.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandler.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-10-2 01:00:00" id="7755" opendate="2017-10-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Null values are not correctly handled by batch inner and outer joins</summary>
      <description>Join predicates of batch joins are not correctly evaluated according to three-value logic.This affects inner as well as outer joins.The problem is that some equality predicates are only evaluated by the internal join algorithms of Flink which are based on TypeComparator. The field TypeComparator for Row are implemented such that null == null results in TRUE to ensure correct ordering and grouping. However, three-value logic requires that null == null results to UNKNOWN (or null). The code generator implements this logic correctly, but for equality predicates, no code is generated.For outer joins, the problem is a bit tricker because these do not support code-generated predicates yet (see FLINK-5520). FLINK-5498 proposes a solution for this issue.We also need to extend several of the existing tests and add null values to ensure that the join logic is correctly implemented.</description>
      <version>1.3.2,1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.validation.JoinValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.validation.JoinValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.FunctionCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-4 01:00:00" id="7761" opendate="2017-10-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Twitter example is not self-contained</summary>
      <description>The Twitter example jar is not self-contained as it excludes the shaded guava dependency from the twitter connector.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-5 01:00:00" id="7765" opendate="2017-10-5 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Enable dependency convergence</summary>
      <description>For motivation check https://issues.apache.org/jira/browse/FLINK-7739SubTasks of this task depends on one another - to enable convergence in `flink-runtime` it has to be enabled for `flink-shaded-hadoop` first.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-libraries.pom.xml</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">flink-fs-tests.pom.xml</file>
      <file type="M">flink-filesystems.pom.xml</file>
      <file type="M">flink-examples.pom.xml</file>
      <file type="M">flink-connectors.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-10-6 01:00:00" id="7770" opendate="2017-10-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hide Queryable State behind a proxy.</summary>
      <description/>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerComponentsStartupShutdownTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.NetworkEnvironmentTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServicesConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.TaskManagerServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskexecutor.QueryableStateConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.query.QueryableStateUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.query.netty.KvStateRequestStats.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.query.KvStateServer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.query.KvStateLocation.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.NetworkEnvironment.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.concurrent.FutureUtils.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.test.java.org.apache.flink.queryablestate.network.QueryableStateClientTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.test.java.org.apache.flink.queryablestate.network.KvStateServerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.test.java.org.apache.flink.queryablestate.network.KvStateServerHandlerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.test.java.org.apache.flink.queryablestate.network.KvStateRequestSerializerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.test.java.org.apache.flink.queryablestate.network.KvStateClientTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.test.java.org.apache.flink.queryablestate.network.KvStateClientHandlerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.test.java.org.apache.flink.queryablestate.network.AkkaKvStateLocationLookupServiceTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.test.java.org.apache.flink.queryablestate.itcases.NonHAAbstractQueryableStateITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.test.java.org.apache.flink.queryablestate.itcases.HAAbstractQueryableStateITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.test.java.org.apache.flink.queryablestate.itcases.AbstractQueryableStateITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.main.java.org.apache.flink.queryablestate.UnknownKvStateKeyGroupLocation.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.main.java.org.apache.flink.queryablestate.UnknownKvStateID.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.main.java.org.apache.flink.queryablestate.UnknownKeyOrNamespace.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.main.java.org.apache.flink.queryablestate.UnknownJobManager.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.main.java.org.apache.flink.queryablestate.server.KvStateServerImpl.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.main.java.org.apache.flink.queryablestate.server.KvStateServerHandler.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.main.java.org.apache.flink.queryablestate.server.ChunkedByteBuf.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.main.java.org.apache.flink.queryablestate.network.messages.MessageType.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.main.java.org.apache.flink.queryablestate.network.messages.MessageSerializer.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.main.java.org.apache.flink.queryablestate.messages.KvStateRequestResult.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.main.java.org.apache.flink.queryablestate.messages.KvStateRequestFailure.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.main.java.org.apache.flink.queryablestate.messages.KvStateRequest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.main.java.org.apache.flink.queryablestate.client.QueryableStateClient.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.main.java.org.apache.flink.queryablestate.client.KvStateLocationLookupService.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.main.java.org.apache.flink.queryablestate.client.KvStateClientHandlerCallback.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.main.java.org.apache.flink.queryablestate.client.KvStateClientHandler.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.main.java.org.apache.flink.queryablestate.client.KvStateClient.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.src.main.java.org.apache.flink.queryablestate.client.AkkaKvStateLocationLookupService.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.AbstractID.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.QueryableStateOptions.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-9 01:00:00" id="7778" opendate="2017-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Relocate ZooKeeper</summary>
      <description>If possible, then we should also try to relocate ZooKeeper in order to avoid dependency clashes between Flink's ZooKeeper and Hadoop's ZooKeeper dependency.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
      <file type="M">flink-shaded-curator-recipes.pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStore.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.zookeeper.ZookeeperAccess.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.store.ZooKeeperMesosWorkerStore.java</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.recovery.JobManagerHAJobGraphRecoveryITCase.java</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-shaded-curator.pom.xml</file>
      <file type="M">flink-shaded-curator.flink-shaded-curator-test.pom.xml</file>
      <file type="M">flink-shaded-curator.flink-shaded-curator-recipes.pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-10-10 01:00:00" id="7790" opendate="2017-10-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unresolved query parameters are not omitted from request</summary>
      <description/>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.MessageParametersTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.MessageParameters.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-10 01:00:00" id="7797" opendate="2017-10-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for windowed outer joins for streaming tables</summary>
      <description>Currently, only windowed inner joins for streaming tables are supported.This issue is about adding support for windowed LEFT, RIGHT, and FULL OUTER joins.</description>
      <version>1.4.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.JoinHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.TimeBoundedStreamInnerJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.RowTimeBoundedStreamInnerJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.ProcTimeBoundedStreamInnerJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamWindowJoin.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-10 01:00:00" id="7798" opendate="2017-10-10 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add support for windowed joins to Table API</summary>
      <description>Currently, windowed joins on streaming tables are only supported through SQL.The Table API should support these joins as well. For that, we have to adjust the Table API validation and translate the API into the respective logical plan. Since most of the code should already be there for the batch Table API joins, this should be fairly straightforward.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.logical.operators.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-11 01:00:00" id="7802" opendate="2017-10-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exception occur when empty field collection was pushed into CSVTableSource</summary>
      <description>Consider such SQL: select count(1) from csv_table. When above SQL was executed, an exception will occur:java.lang.IllegalArgumentException: At least one field must be specifiedat org.apache.flink.api.java.io.RowCsvInputFormat.&lt;init&gt;(RowCsvInputFormat.java:50)So if no fields will be used, we should also keep some columns for CSVTableSource to get row count.</description>
      <version>None</version>
      <fixedVersion>1.3.4,1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.sources.CsvTableSource.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-11 01:00:00" id="7810" opendate="2017-10-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Switch from custom Flakka to Akka 2.4.x</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.misc.AutoParallelismITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.SavepointITCase.java</file>
      <file type="M">flink-tests.pom.xml</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerComponentsStartupShutdownTest.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-runtime-web.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-queryable-state.flink-queryable-state-java.pom.xml</file>
      <file type="M">flink-mesos.pom.xml</file>
      <file type="M">flink-clients.pom.xml</file>
      <file type="M">test-infra.end-to-end-test.common.sh</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-11 01:00:00" id="7814" opendate="2017-10-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add BETWEEN and NOT BETWEEN expression to Table API</summary>
      <description>The Table API does not have a BETWEEN expression. BETWEEN is quite handy when defining join predicates for window joins.</description>
      <version>1.4.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.validation.ScalarOperatorsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.comparison.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-11-13 01:00:00" id="7838" opendate="2017-10-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka011ProducerExactlyOnceITCase do not finish</summary>
      <description>See attached log</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaProducer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-14 01:00:00" id="7840" opendate="2017-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shade Akka&amp;#39;s Netty Dependency</summary>
      <description>In order to avoid clashes between different Netty versions we should shade Akka's Netty away.These dependency version clashed manifest themselves in very subtle ways, like occasional deadlocks.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">flink-yarn.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.main.java.org.apache.flink.networking.NetworkFailureHandler.java</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-14 01:00:00" id="7841" opendate="2017-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add docs for Flink&amp;#39;s S3 support</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.aws.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-14 01:00:00" id="7842" opendate="2017-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shade jackson (org.codehouse.jackson) in flink-shaded-hadoop2</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-11-16 01:00:00" id="7847" opendate="2017-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix typo in flink-avro shading pattern</summary>
      <description>&lt;relocation&gt; &lt;pattern&gt;org.codehaus.jackson&lt;/pattern&gt; &lt;shadedPattern&gt;org.apache.flink.avro.shaded.org.codehouse.jackson&lt;/shadedPattern&gt;&lt;/relocation&gt;The shaded pattern should be "org.apache.flink.avro.shaded.org.codehaus.jackson".</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-16 01:00:00" id="7849" opendate="2017-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove guava shading from hcatalog connector</summary>
      <description>Same issue as FLINK-7846.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-hcatalog.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-16 01:00:00" id="7850" opendate="2017-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Given each maven profile an activation property</summary>
      <description>We should give every maven profile an activation property so that they can be activated with -Dabcde. This makes them a lot easier to work with in scripts that want to control profile activation, since you can just append -D switches. This doesn't work with the -P switch as it can only be specified once.&lt;activation&gt; &lt;property&gt; &lt;name&gt;profile_name_or_something&lt;/name&gt; &lt;/property&gt;&lt;/activation&gt;</description>
      <version>1.4.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.force-shading.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2.pom.xml</file>
      <file type="M">flink-libraries.flink-python.pom.xml</file>
      <file type="M">flink-libraries.flink-ml.pom.xml</file>
      <file type="M">flink-formats.flink-json.pom.xml</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-hbase.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-17 01:00:00" id="7861" opendate="2017-10-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Suppress ActorKilledExceptions</summary>
      <description>When stopping a RpcEndpoint, the AkkaRpcService sends a Kill message which causes an ActorKilledException to be thrown. This exception is logged by the StoppingSupervisorStrategy. This is not necessary because we voluntarily stopped the RpcEndpoint. In order to clean up logs, I think we should not log this kind of exception.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-17 01:00:00" id="7862" opendate="2017-10-17 00:00:00" resolution="Done">
    <buginformation>
      <summary>Add TaskManagerDetailsHandler</summary>
      <description>In order to server detailed TaskManager information we need a TaskManagerDetailsHandler.</description>
      <version>1.4.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerInfoTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.taskmanager.TaskManagerInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerGateway.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricRegistry.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.dump.QueryScopeInfo.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.dump.MetricDump.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.SessionClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.TaskManagersHandler.java</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.taskmanager.taskmanager.svc.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.taskmanager.taskmanager.ctrl.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.taskmanager.taskmanager.metrics.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.hs.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanager.taskmanager.metrics.jade</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-18 01:00:00" id="7866" opendate="2017-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Weigh list of preferred locations for scheduling</summary>
      <description>Sihua Zhou proposed to not only use the list of preferred locations to decide where to schedule a task, but to also weigh the list according to how often a location appeared and then select the location based on the weight. That way, we would obtain better locality in some cases.Example:Preferred locations list: &amp;#91;location1, location2, location2&amp;#93;Weighted preferred locations list &amp;#91;(location2 , 2), (location1, 1)&amp;#93;</description>
      <version>1.3.2,1.4.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.SlotProfileTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.SlotProfile.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-18 01:00:00" id="7868" opendate="2017-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Only run checkstyleonce for compilation</summary>
      <description>We currently run checkstyle twice in the Misc profile, once during compilation and again during tests which wastes build time.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-10-20 01:00:00" id="7879" opendate="2017-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>only execute apache-rat in one build profile</summary>
      <description>Similarly to FLINK-7350 we improve build times (and stability!) by only executing the Apache Rat plugin in the build profile that builds the all of flink.Bump apache-rat-plugin to 0.12, [RAT-173Cannot skip plugin run completely, but check only | https://issues.apache.org/jira/browse/RAT-173]</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-20 01:00:00" id="7880" opendate="2017-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-queryable-state-java fails with core-dump</summary>
      <description>The flink-queryable-state-java module fails on Travis with a core dump.https://travis-ci.org/tillrohrmann/flink/jobs/289949829</description>
      <version>1.4.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.NonHAAbstractQueryableStateTestBase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.HAAbstractQueryableStateTestBase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.AbstractQueryableStateTestBase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.NonHAQueryableStateRocksDBBackendITCase.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.KVStateRequestSerializerRocksDBTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.HAQueryableStateRocksDBBackendITCase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-20 01:00:00" id="7882" opendate="2017-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Writing to S3 from EMR fails with exception</summary>
      <description>Writing to S3 from EMR fails with exception:The program finished with the following exception:org.apache.flink.client.program.ProgramInvocationException: The program execution failed: Job execution failed. at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:485) at org.apache.flink.yarn.YarnClusterClient.submitJob(YarnClusterClient.java:215) at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:449) at org.apache.flink.streaming.api.environment.StreamContextEnvironment.execute(StreamContextEnvironment.java:66) at org.apache.flink.streaming.examples.wordcount.WordCount.main(WordCount.java:89) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:525) at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:417) at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:389) at org.apache.flink.client.CliFrontend.executeProgram(CliFrontend.java:819) at org.apache.flink.client.CliFrontend.run(CliFrontend.java:282) at org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:1071) at org.apache.flink.client.CliFrontend$2.call(CliFrontend.java:1118) at org.apache.flink.client.CliFrontend$2.call(CliFrontend.java:1115) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.client.CliFrontend.main(CliFrontend.java:1115)Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$6.apply$mcV$sp(JobManager.scala:923) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$6.apply(JobManager.scala:866) at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$6.apply(JobManager.scala:866) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.NoClassDefFoundError: com/sun/xml/bind/v2/model/impl/ModelBuilderI at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) at java.net.URLClassLoader.defineClass(URLClassLoader.java:467) at java.net.URLClassLoader.access$100(URLClassLoader.java:73) at java.net.URLClassLoader$1.run(URLClassLoader.java:368) at java.net.URLClassLoader$1.run(URLClassLoader.java:362) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:361) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) at java.net.URLClassLoader.defineClass(URLClassLoader.java:467) at java.net.URLClassLoader.access$100(URLClassLoader.java:73) at java.net.URLClassLoader$1.run(URLClassLoader.java:368) at java.net.URLClassLoader$1.run(URLClassLoader.java:362) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:361) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.getTypeInfoSet(JAXBContextImpl.java:444) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.&lt;init&gt;(JAXBContextImpl.java:292) at com.sun.xml.bind.v2.runtime.JAXBContextImpl.&lt;init&gt;(JAXBContextImpl.java:139) at com.sun.xml.bind.v2.runtime.JAXBContextImpl$JAXBContextBuilder.build(JAXBContextImpl.java:1138) at com.sun.xml.bind.v2.ContextFactory.createContext(ContextFactory.java:162) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:247) at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:234) at javax.xml.bind.ContextFinder.find(ContextFinder.java:441) at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:641) at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:584) at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.util.Base64.&lt;clinit&gt;(Base64.java:44) at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.util.BinaryUtils.fromBase64(BinaryUtils.java:71) at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1715) at com.amazon.ws.emr.hadoop.fs.s3.lite.call.PutObjectCall.performCall(PutObjectCall.java:34) at com.amazon.ws.emr.hadoop.fs.s3.lite.call.PutObjectCall.performCall(PutObjectCall.java:9) at com.amazon.ws.emr.hadoop.fs.s3.lite.call.AbstractUploadingS3Call.perform(AbstractUploadingS3Call.java:62) at com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:80) at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:176) at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.putObject(AmazonS3LiteClient.java:104) at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.storeFile(Jets3tNativeFileSystemStore.java:165) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102) at com.sun.proxy.$Proxy26.storeFile(Unknown Source) at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem$NativeS3FsOutputStream.close(S3NativeFileSystem.java:364) at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72) at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106) at org.apache.flink.runtime.fs.hdfs.HadoopDataOutputStream.close(HadoopDataOutputStream.java:52) at org.apache.flink.core.fs.ClosingFSDataOutputStream.close(ClosingFSDataOutputStream.java:64) at org.apache.flink.api.common.io.FileOutputFormat.close(FileOutputFormat.java:267) at org.apache.flink.streaming.api.functions.sink.OutputFormatSinkFunction.close(OutputFormatSinkFunction.java:93) at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:43) at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:109) at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:394) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:281) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:712) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.ClassNotFoundException: com.sun.xml.bind.v2.model.impl.ModelBuilderI at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 68 moreGit bisect between commits 84a07a34ac22af14f2dd0319447ca5f45de6d0bb (good) and c81a6db44817ce818c949c2fd55ebfc2af0cc913 (bad)identified 5a5006ceb8d19bc0f3cc490451a18b8fc21197cb to be the first bad commit.Command to start the job:HADOOP_CONF_DIR=/etc/hadoop/conf bin/flink run -m yarn-cluster -yn 1 examples/streaming/WordCount.jar --output s3://mybucket/out --input s3://mybucket/inputEMR release label: emr-5.9.0Hadoop distribution: Amazon 2.7.3 Commands used to compile Flink:mvn clean install -Pdocs-and-source -DskipTests -Dhadoop.version=2.7.3cd flink-distmvn clean install -Pdocs-and-source -DskipTests -Dhadoop.version=2.7.3Java and Maven version used to compile Flink:java -versionopenjdk version "1.8.0_144"OpenJDK Runtime Environment (Zulu 8.23.0.3-macosx) (build 1.8.0_144-b01)OpenJDK 64-Bit Server VM (Zulu 8.23.0.3-macosx) (build 25.144-b01, mixed mode)mvn -versionApache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-10T17:41:47+01:00)</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-22 01:00:00" id="7898" opendate="2017-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TaskExecutorTest.testTriggerRegistrationOnLeaderChange fails on Travis</summary>
      <description>The TaskExecutorTest.testTriggerRegistrationOnLeaderChange fails spuriously on Travis.https://travis-ci.org/aljoscha/flink/jobs/291074999</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.registration.RetryingRegistration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.registration.RegisteredRpcConnection.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-1-23 01:00:00" id="7904" opendate="2017-10-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable Flip6 build profile on Travis</summary>
      <description>In order to continuously test Flip-6 components, we should add a new Travis build matrix entry which runs the Flip-6 test cases.</description>
      <version>1.4.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-25 01:00:00" id="7923" opendate="2017-10-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support accessing subfields of a Composite element in an Object Array type column</summary>
      <description>Access type such as:SELECT a[1].f0 FROM MyTablewill cause problem. See following test sample for more details:https://github.com/walterddr/flink/commit/03c93bcb0fb30bd2d327e35b5e244322d449b06a</description>
      <version>1.4.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.CompositeTypeTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.CompositeAccessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-1-26 01:00:00" id="7934" opendate="2017-10-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Calcite dependency to 1.15</summary>
      <description>Umbrella issue for all related issues for Apache Calcite 1.15 release.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.symbols.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.ExpressionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ExtractCallGen.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.CalcTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.GroupWindowTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.time.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.aggregations.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.java.org.apache.calcite.sql.fun.SqlGroupFunction.java</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-5 01:00:00" id="7986" opendate="2017-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce FilterSetOpTransposeRule to Flink</summary>
      <description>A.unionAll(B).where.groupBy.select =&gt;A.where.unionAll(B.where).groupBy.selectthis rule will reduce networkIO</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-11-6 01:00:00" id="7992" opendate="2017-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>extend PR template with S3 question</summary>
      <description>S3 file system tests are only run if AWS credentials are specified, i.e. ARTIFACTS_AWS_BUCKET, ARTIFACTS_AWS_ACCESS_KEY, and ARTIFACTS_AWS_SECRET_KEY. Since these must remain secret, they are only set in Apache Flink's Travis CI configuration and not available in the Travis runs on pull requests (PR) to not leak them in any way. This however means that if a contributor changes something S3-related, the PR's test results will not reflect the actual changes and if something breaks there, we will only see it once merged.Therefore, I propose to add one more question to the PR template so that the committer is aware of this fact and the need to run the tests in his own Travis CI configuration first with proper AWS credentials set up.</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.github.PULL.REQUEST.TEMPLATE.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-6 01:00:00" id="7993" opendate="2017-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka 08 curator shading pattern out-of-sync</summary>
      <description>The kafka 08 shading pattern for curator is out-of-sync with flink-runtime.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-11-6 01:00:00" id="7996" opendate="2017-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for (left.time = right.time) predicates to window join.</summary>
      <description>A common operation is to join the result of two window aggregations on the same timestamp. However, window joins do not support equality predicates on time attributes such as left.time = right.time but require two range predicates such as left.time &gt;= right.time AND left.time &lt;= right.time.This can be fixed in the translation code (the operator does not have to be touched).</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.JoinITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.validation.JoinValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.WindowJoinUtil.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-6 01:00:00" id="7997" opendate="2017-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avro should be always in the user code</summary>
      <description>Having Avro in the user code space makes it possible for users to use different Avro versions that the ones pulled in by an overloaded classpath (for example when having Hadoop in the classpath)This is possible through the new child-first classloading in Flink 1.4.Also, this should fix the problem of "X cannot be cast to X", because Avro classes will be scoped to the user code class loader, and the Avro schema cache will not be JVM-wide-</description>
      <version>None</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-filesystem.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-6 01:00:00" id="8002" opendate="2017-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect join window boundaries for LESS_THAN and GREATER_THAN predicates</summary>
      <description>The boundaries of LESS_THAN and GREATER_THAN predicates are not correctly computed if the time attribute of the right table is referenced on the left side of the join predicate.Instead of adding (subtracting) 1 millisecond, 1 millisecond is subtracted (added). Hence, the boundary is off-by-2.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.WindowJoinUtil.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-7 01:00:00" id="8009" opendate="2017-11-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-dist pulls in flink-runtime&amp;#39;s transitive avro/jackson dependency</summary>
      <description>The promotion of transitive dependencies in flink-runtime causes flink-dist to contain some transitive dependencies from flink-shaded-hadoop. (most notably, avro and codehaus.jackson)We will either have to add an exclusion for each dependency to flink-dist, set flink-shaded-hadoop to provided in flink-runtime (hacky, but less intrusive), or remove the promotion and explicitly depend on various akka dependencies.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-7 01:00:00" id="8010" opendate="2017-11-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump remaining flink-shaded dependencies</summary>
      <description/>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-7 01:00:00" id="8014" opendate="2017-11-7 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add Kafka010JsonTableSink</summary>
      <description>Offer a TableSource for JSON-encoded Kafka 0.10 topics but no TableSink.Since, the required base classes are already there, a Kafka010JsonTableSink can be easily added.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSinkTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.JsonRowSerializationSchemaTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.JsonRowSerializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaJsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-7 01:00:00" id="8016" opendate="2017-11-7 00:00:00" resolution="Done">
    <buginformation>
      <summary>Add documentation for KafkaJsonTableSink</summary>
      <description>The documentation of available TableSources should be extended to include the KafkaJsonTableSinks.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sourceSinks.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-8 01:00:00" id="8033" opendate="2017-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDK 9 support</summary>
      <description>This is a JIRA to track all issues that found to make Flink compatible with Java 9.</description>
      <version>1.4.0</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-8 01:00:00" id="8038" opendate="2017-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support MAP value constructor</summary>
      <description>Similar to https://issues.apache.org/jira/browse/FLINK-4554We want to support Map value constructor which is supported by Calcite:https://calcite.apache.org/docs/reference.html#value-constructorsSELECT MAP['key1', f0, 'key2', f1] AS stringKeyedMap, MAP['key', 'value'] AS literalMap, MAP[f0, f1] AS fieldMapFROM tableThis should enable users to construct MapTypeInfo, one of the CompositeType.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.validation.MapTypeValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.MapTypeTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ArrayTypeTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.typeutils.TypeCheckUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.ProjectionTranslator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkTypeFactory.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.md</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.MapTypeTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.map.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.item.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.ExpressionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.cardinality.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.array.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-11-13 01:00:00" id="8059" opendate="2017-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Querying the state of a non-existing jobs should throw JobNotFoundException</summary>
      <description>When querying the state for a non-existing job you currently get a IllegalStateException. Given that this isn't an illegal state we should return a JobNotFoundException instead.</description>
      <version>1.4.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmanager.JobManagerTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.itcases.AbstractQueryableStateTestBase.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-13 01:00:00" id="8064" opendate="2017-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend dependency section to list flink-core</summary>
      <description>The dependency section of the Queryable State documentation should also list flink-core as it is inherently required when working with the client. (Which has flink-core set to provided)</description>
      <version>1.4.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.state.queryable.state.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-14 01:00:00" id="8069" opendate="2017-11-14 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Support empty watermark strategy for TableSources</summary>
      <description>In case the underlying data stream source emits watermarks, it should be possible to define an empty watermark strategy for rowtime attributes in the RowtimeAttributeDescriptor.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.testTableSources.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.TableSourceITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.sources.wmstrategies.watermarkStrategies.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.StreamTableSourceScan.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-14 01:00:00" id="8071" opendate="2017-11-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Akka shading sometimes produces invalid code</summary>
      <description>On 2 separate occasions on separate machines I hit the exception below when starting a cluster. Once it happened in the yarn tests, another time after starting a standalone cluster with flink-dist.The issue appears to be related to some asm bug that affects both sbt-assembly and the maven-shade-plugin.References: https://github.com/akka/akka/issues/21596 https://github.com/sbt/sbt-assembly/issues/205From what I have found this should be fixable my bumping the asm version of the maven-shade-plugin to 5.1 (our version uses 5.0.2), or just increment the plugin version to 3.0.0 (which already uses 5.1).Important: This only occurs if a relocation is performed in flink-dist.java.lang.Exception: Could not create actor system at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:171) at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:115) at org.apache.flink.yarn.YarnApplicationMasterRunner.runApplicationMaster(YarnApplicationMasterRunner.java:313) at org.apache.flink.yarn.YarnApplicationMasterRunner$1.call(YarnApplicationMasterRunner.java:199) at org.apache.flink.yarn.YarnApplicationMasterRunner$1.call(YarnApplicationMasterRunner.java:196) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.yarn.YarnApplicationMasterRunner.run(YarnApplicationMasterRunner.java:196) at org.apache.flink.yarn.YarnApplicationMasterRunner.main(YarnApplicationMasterRunner.java:123)Caused by: java.lang.VerifyError: Inconsistent stackmap frames at branch target 152Exception Details: Location: akka/dispatch/Mailbox.processAllSystemMessages()V @152: getstatic Reason: Type top (current frame, locals[9]) is not assignable to 'akka/dispatch/sysmsg/SystemMessage' (stack map, locals[9]) Current Frame: bci: @131 flags: { } locals: { 'akka/dispatch/Mailbox', 'java/lang/InterruptedException', 'akka/dispatch/sysmsg/SystemMessage', top, 'akka/dispatch/Mailbox', 'java/lang/Throwable', 'java/lang/Throwable' } stack: { integer } Stackmap Frame: bci: @152 flags: { } locals: { 'akka/dispatch/Mailbox', 'java/lang/InterruptedException', 'akka/dispatch/sysmsg/SystemMessage', top, 'akka/dispatch/Mailbox', 'java/lang/Throwable', 'java/lang/Throwable', top, top, 'akka/dispatch/sysmsg/SystemMessage' } stack: { } Bytecode: 0x0000000: 014c 2ab2 0132 b601 35b6 0139 4db2 013e 0x0000010: 2cb6 0142 9900 522a b600 c69a 004b 2c4e 0x0000020: b201 3e2c b601 454d 2db9 0148 0100 2ab6 0x0000030: 0052 2db6 014b b801 0999 000e bb00 e759 0x0000040: 1301 4db7 010f 4cb2 013e 2cb6 0150 99ff 0x0000050: bf2a b600 c69a ffb8 2ab2 0132 b601 35b6 0x0000060: 0139 4da7 ffaa 2ab6 0052 b600 56b6 0154 0x0000070: b601 5a3a 04a7 0091 3a05 1905 3a06 1906 0x0000080: c100 e799 0015 1906 c000 e73a 0719 074c 0x0000090: b200 f63a 08a7 0071 b201 5f19 06b6 0163 0x00000a0: 3a0a 190a b601 6899 0006 1905 bf19 0ab6 0x00000b0: 016c c000 df3a 0b2a b600 52b6 0170 b601 0x00000c0: 76bb 000f 5919 0b2a b600 52b6 017a b601 0x00000d0: 80b6 0186 2ab6 018a bb01 8c59 b701 8e13 0x00000e0: 0190 b601 9419 09b6 0194 1301 96b6 0194 0x00000f0: 190b b601 99b6 0194 b601 9ab7 019d b601 0x0000100: a3b2 00f6 3a08 b201 3e2c b601 4299 0026 0x0000110: 2c3a 09b2 013e 2cb6 0145 4d19 09b9 0148 0x0000120: 0100 1904 2ab6 0052 b601 7a19 09b6 01a7 0x0000130: a7ff d62b c600 09b8 0109 572b bfb1 Exception Handler Table: bci [290, 307] =&gt; handler: 120 Stackmap Table: append_frame(@13,Object[#231],Object[#177]) append_frame(@71,Object[#177]) chop_frame(@102,1) full_frame(@120,{Object[#2],Object[#231],Object[#177],Top,Object[#2],Object[#177]},{Object[#223]}) full_frame(@152,{Object[#2],Object[#231],Object[#177],Top,Object[#2],Object[#223],Object[#223],Top,Top,Object[#177]},{}) append_frame(@173,Object[#357]) full_frame(@262,{Object[#2],Object[#231],Object[#177],Top,Object[#2]},{}) same_frame(@307) same_frame(@317) at akka.dispatch.Mailboxes.&lt;init&gt;(Mailboxes.scala:33) at akka.actor.ActorSystemImpl.&lt;init&gt;(ActorSystem.scala:800) at akka.actor.ActorSystem$.apply(ActorSystem.scala:245) at akka.actor.ActorSystem$.apply(ActorSystem.scala:288) at akka.actor.ActorSystem$.apply(ActorSystem.scala:263) at akka.actor.ActorSystem$.create(ActorSystem.scala:191) at org.apache.flink.runtime.akka.AkkaUtils$.createActorSystem(AkkaUtils.scala:106) at org.apache.flink.runtime.akka.AkkaUtils.createActorSystem(AkkaUtils.scala) at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:158)</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-15 01:00:00" id="8076" opendate="2017-11-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade KinesisProducer to 0.10.6 to set properties approperiately</summary>
      <description/>
      <version>1.4.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtilTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-15 01:00:00" id="8079" opendate="2017-11-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip remaining E2E tests if one failed</summary>
      <description>I propose that if one end-to-end tests fails the remaining tests are skipped.aljoscha What do you think?</description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-11-15 01:00:00" id="8084" opendate="2017-11-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove japicmp deactivations in several modules</summary>
      <description>The japicmp module is explicitly deactivated in the following modules: java8 quickstart yarn-testsSince the module has to be explicitly enabled these entries can be removed.</description>
      <version>1.4.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-quickstart.pom.xml</file>
      <file type="M">flink-java8.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-17 01:00:00" id="8095" opendate="2017-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce ProjectSetOpTransposeRule to Flink</summary>
      <description>ProjectSetOpTransposeRule is similar to FilterSetOpTransposeRule, adding ProjectSetOpTransposeRule is necessary.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.FlinkRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-17 01:00:00" id="8097" opendate="2017-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add built-in support for min/max aggregation for Date/Time</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MinWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MinAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MaxWithRetractAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.aggfunctions.MaxAggFunctionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.Ordering.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunctionWithRetract.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MinAggFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunctionWithRetract.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.aggfunctions.MaxAggFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-19 01:00:00" id="8105" opendate="2017-11-19 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Removed unnecessary null check</summary>
      <description>eg.if (value != null &amp;&amp; value instanceof String)null instanceof String returns false hence replaced the check withif (value instanceof String)</description>
      <version>1.4.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.GenericWriteAheadSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.TimestampedFileInputSplit.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerRegistrationTest.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.traversals.PlanFinalizer.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plantranslate.JobGraphGenerator.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dataproperties.RequestedGlobalProperties.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dataproperties.InterestingProperties.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.dataproperties.GlobalProperties.java</file>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.io.CollectionInputFormatTest.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.UdfAnalyzerUtils.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.sca.UdfAnalyzer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.io.LocatableInputSplit.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.io.GenericInputSplit.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.FileInputSplit.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.runtime.RuntimeSerializerFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.FileInputFormat.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.DelimitedInputFormat.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.io.BinaryInputFormat.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkLocalCluster.java</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.src.main.java.org.apache.flink.api.java.hadoop.mapred.HadoopInputFormatBase.java</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.src.main.java.org.apache.flink.api.java.hadoop.mapreduce.HadoopInputFormatBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-20 01:00:00" id="8110" opendate="2017-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>fasterxml jackson services are not relocated</summary>
      <description>The jackson services aren't properly relocated. Flink-shaded uses maven-shade-plugin:2.4.1 which doesn't properly relocate services as described in the flink-s3-fs-presto shade-plugin configuration.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-20 01:00:00" id="8113" opendate="2017-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump maven-shade-plugin to 3.0.0</summary>
      <description>We should investigate whether we can bump the shade plugin to 3.0.0. Earlier versions do not properly relocate services, forcing some modules to set a different plugin version in their own configuration (flink-s3-fs-presto for example, or flink-dist after FLINK-8111).</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-20 01:00:00" id="8115" opendate="2017-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka E2E tests fail on travis</summary>
      <description>The kafka E2E tests fail on travis since the download of kafka fails.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">test-infra.end-to-end-test.test.streaming.kafka010.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-11-23 01:00:00" id="8142" opendate="2017-11-23 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Cleanup reference to deprecated constants in ConfigConstants</summary>
      <description>ConfigConstants contains several deprecated String constants that are used by other Flink modules. Those should be cleaned up.</description>
      <version>1.4.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.cli.FlinkYarnSessionCli.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskManagerStartupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.restart.RestartStrategyFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualExactlyOnceWithStreamReshardingTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualExactlyOnceTest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-1-27 01:00:00" id="8156" opendate="2017-11-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump commons-beanutils version to 1.9.3</summary>
      <description>Commons-beanutils v1.8.0 dependency is not security compliant. See CVE-2014-0114:Apache Commons BeanUtils, as distributed in lib/commons-beanutils-1.8.0.jar in Apache Struts 1.x through 1.3.10 and in other products requiring commons-beanutils through 1.9.2, does not suppress the class property, which allows remote attackers to "manipulate" the ClassLoader and execute arbitrary code via the class parameter, as demonstrated by the passing of this parameter to the getClass method of the ActionForm object in Struts 1.Note that current version commons-beanutils 1.9.2 in turn has a CVE in its dependency commons-collections (CVE-2015-6420, see BEANUTILS-488), which is fixed in 1.9.3.We should upgrade commons-beanutils from 1.8.3 to 1.9.3</description>
      <version>1.4.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-30 01:00:00" id="8173" opendate="2017-11-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix input unboxing and support Avro Utf8 in Table API</summary>
      <description>It is a stream of Avro objects, simply select a String field and trying to print out val query = "SELECT nd_key FROM table1" val result = tableEnv.sql(query) tableEnv.toAppendStream&amp;#91;org.apache.avro.util.Utf8&amp;#93;(result).print()11/29/2017 16:07:36 Source: Custom Source -&gt; from: (accepted_cohort_id, admin_id, after_submission, amount_paid, anonymous_id, application_id, atom_key, bd_group_key, biz_geo, braavos_purchase_id, category, cohort_id, concept_key, concept_rank, context, context_campaign, context_experiment, coupon_code, course_key, course_rank, cta_destination, cta_location, cta_message, cta_type, currency, decision_group_id, device_browser, device_os, device_os_version, device_type, duration, evaluation_id, event_type, fin_geo, in_collaboration_with, lab_id, lab_rank, label, lesson_key, lesson_rank, locale, max_pause_duration, message, message_id, module_key, module_rank, nd_key, nd_unit_id, nd_unit_rank, new_cohort_id, notification_id, num_concepts_completed, num_interactions, num_lessons_completed, old_cohort_id, part_key, part_rank, pause_duration, pause_reason, payment_plan, payment_provider, points_earned, points_possible, price, price_sheet, product_key, product_type, provider_charge_id, provider_refund_id, quiz_type, referrer, refund_amount, requested_cohort_id, results, scholarship_group_key, search_term, skill_level, subscription_id, suspension_length, suspension_reason, technology, timestamp, total_concepts, total_lessons, total_time_sec, type, unenroll_reason, user_id, user_locale, user_response, variant, version, workspace_id, workspace_session, workspace_type) -&gt; select: (nd_key) -&gt; to: Utf8 -&gt; Sink: Unnamed(5/8) switched to FAILED org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.codegen.Compiler$class.compile(Compiler.scala:36) at org.apache.flink.table.runtime.CRowOutputMapRunner.compile(CRowOutputMapRunner.scala:33) at org.apache.flink.table.runtime.CRowOutputMapRunner.open(CRowOutputMapRunner.scala:48) at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36) at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:111) at org.apache.flink.streaming.runtime.tasks.StreamTask.openAllOperators(StreamTask.java:376) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:253) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:702) at java.lang.Thread.run(Thread.java:748)Caused by: org.codehaus.commons.compiler.CompileException: Line 790, Column 15: Assignment conversion not possible from type "java.lang.CharSequence" to type "org.apache.avro.util.Utf8" at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:11672) at org.codehaus.janino.UnitCompiler.assignmentConversion(UnitCompiler.java:10528) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2534) at org.codehaus.janino.UnitCompiler.access$2600(UnitCompiler.java:212) at org.codehaus.janino.UnitCompiler$6.visitLocalVariableDeclarationStatement(UnitCompiler.java:1459) at org.codehaus.janino.UnitCompiler$6.visitLocalVariableDeclarationStatement(UnitCompiler.java:1443) at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:3348) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1443) at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1523) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3052) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1313) at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1286) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:785) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:436) at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:212) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:390) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:385) at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1405) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:385) at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:357) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:234) at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:446) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:213) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:204) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) at org.apache.flink.table.codegen.Compiler$class.compile(Compiler.scala:33) ... 8 more</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.JoinHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-12-4 01:00:00" id="8194" opendate="2017-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable akka.actor.warn-about-java-serializer-usage to suppress akka warnings when using the Java serializer</summary>
      <description>With Akka 2.4, Akka is logging warnings when using the Java serializer for message serialization. We should turn this off via akka.actor.warn-about-java-serializer-usage since we used Java serialization before and it is only cluttering the logs making the users worry.</description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-4 01:00:00" id="8196" opendate="2017-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Hadoop Servlet Dependency Exclusion</summary>
      <description>We currently exclude the `javax.servlet` API dependency, which is unfortunately needed as a core dependency by Hadoop 2.7.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-1-6 01:00:00" id="8209" opendate="2017-12-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not relay on specific method names in LocalBufferPoolTest</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.taskmanager.TaskCancelAsyncProducerConsumerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.LocalBufferPoolDestroyTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-7 01:00:00" id="8222" opendate="2017-12-7 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Update Scala version</summary>
      <description>Update Scala to version 2.11.12. I don't believe this affects the Flink distribution but rather anyone who is compiling Flink or a Flink-quickstart-derived program on a shared system."A privilege escalation vulnerability (CVE-2017-15288) has been identified in the Scala compilation daemon."https://www.scala-lang.org/news/security-update-nov17.html</description>
      <version>1.4.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-11 01:00:00" id="8235" opendate="2017-12-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot run spotbugs for single module</summary>
      <description>When running the spotbugs plugin (-Dspotbugs) in a sub-module of Flink the build will fail because it cannot find the exclusion file.[ERROR] Could not find resource 'tools/maven/spotbugs-exclude.xml'. -&gt; [Help 1]The problem is that the configured relative path is resolved against the sub-module directory, and not the parent one.</description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-1-14 01:00:00" id="8258" opendate="2017-12-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable query configuration for batch queries</summary>
      <description>Query configuration holds some parameters to configure the behavior of batch queries. However, since there was nothing to set for batch queries before, the configuration was not really passed. Due to FLINK-8236, we need to enable it now.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetWindowAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetValues.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetSort.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetSingleRowJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetMinus.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetIntersect.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetDistinct.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetCorrelate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetCalc.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.BatchTableSourceScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.TableConversions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.BatchTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.java.BatchTableEnvironment.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.BatchTableEnvironment.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-14 01:00:00" id="8260" opendate="2017-12-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document API of Kafka 0.11 Producer</summary>
      <description>The API of the Flink Kafka Producer changed for Kafka 0.11, for example there is no writeToKafkaWithTimestamps method anymore.This needs to be added to the Kafka connector documentation, i.e., a new tab with a code snippet needs to be added for Kafka 0.11.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.kafka.md</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer010.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-14 01:00:00" id="8261" opendate="2017-12-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typos in the shading exclusion for jsr305 in the quickstarts</summary>
      <description>This affects both the Java and the Scala quickstarts.The typo is findbgs instead of findbugs.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-14 01:00:00" id="8263" opendate="2017-12-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong packaging of flink-core in scala quickstarty</summary>
      <description>The scala quickstart currently does not set flink-core to "provided" in the "build-jar" profile.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-1-14 01:00:00" id="8265" opendate="2017-12-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing jackson dependency for flink-mesos</summary>
      <description>The Jackson library that is required by Fenzo is missing from the Flink distribution jar-file.This manifests as an exception in certain circumstances when a hard constraint is configured ("mesos.constraints.hard.hostattribute").NoClassDefFoundError: org/apache/flink/mesos/shaded/com/fasterxml/jackson/databind/ObjectMapper at com.netflix.fenzo.ConstraintFailure.&lt;clinit&gt;(ConstraintFailure.java:35) at com.netflix.fenzo.AssignableVirtualMachine.findFailedHardConstraints(AssignableVirtualMachine.java:784) at com.netflix.fenzo.AssignableVirtualMachine.tryRequest(AssignableVirtualMachine.java:581) at com.netflix.fenzo.TaskScheduler.evalAssignments(TaskScheduler.java:796) at com.netflix.fenzo.TaskScheduler.access$1500(TaskScheduler.java:70)</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-17 01:00:00" id="8271" opendate="2017-12-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade from deprecated classes to AmazonKinesis</summary>
      <description/>
      <version>1.4.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualExactlyOnceWithStreamReshardingTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.test.java.org.apache.flink.streaming.connectors.kinesis.manualtests.ManualExactlyOnceTest.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-18 01:00:00" id="8276" opendate="2017-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Annotation for Kafka connector</summary>
      <description>See parent issue.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.ExceptionProxy.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.TypeInformationKeyValueSerializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.KeyedSerializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.KeyedDeserializationSchemaWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.JsonRowSerializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.JsonRowDeserializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.JSONKeyValueDeserializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.JSONDeserializationSchema.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.partitioner.KafkaPartitioner.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.partitioner.FlinkKafkaPartitioner.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.partitioner.FlinkKafkaDelegatePartitioner.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.partitioner.FlinkFixedPartitioner.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaJsonTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaJsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaAvroTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.metrics.KafkaMetricWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicsDescriptor.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionStateWithPunctuatedWatermarks.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionStateWithPeriodicWatermarks.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionStateSentinel.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionLeader.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionAssigner.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaCommitCallback.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer010.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer010.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka010Fetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka010PartitionDiscoverer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerCallBridge010.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010AvroTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010JsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010JsonTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafka011ErrorCode.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafka011Exception.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaProducer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.metrics.KafkaMetricMuttableWrapper.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.TransactionalIdsGenerator.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka011AvroTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka011JsonTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka011TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer08.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer08.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.Kafka08Fetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.Kafka08PartitionDiscoverer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KillerWatchDog.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.PartitionInfoFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.PeriodicOffsetCommitter.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.SimpleConsumerThread.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.ZookeeperOffsetHandler.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08AvroTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer09.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Handover.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.Kafka09PartitionDiscoverer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09AvroTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.config.OffsetCommitMode.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.config.OffsetCommitModes.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.config.StartupMode.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractPartitionDiscoverer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.ClosableBlockingQueue.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-18 01:00:00" id="8278" opendate="2017-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala examples in Metric documentation do not compile</summary>
      <description>The Scala examples in the Metrics documentation do not compile.The line @transient private var counter: Counterneeds to be extended to@transient private var counter: Counter = _</description>
      <version>1.3.2,1.4.0,1.5.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.monitoring.metrics.md</file>
      <file type="M">docs.dev.stream.state.state.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-18 01:00:00" id="8286" opendate="2017-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Flink-Yarn-Kerberos integration for FLIP-6</summary>
      <description>The current Flink-Yarn-Kerberos in Flip-6 is broken.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.YarnTaskExecutorRunner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-18 01:00:00" id="8287" opendate="2017-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink Kafka Producer docs should clearly state what partitioner is used by default</summary>
      <description>See original discussion in ML: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/FlinkKafkaProducerXX-td16951.htmlIt is worth mentioning what partitioning scheme is used by the FlinkKafkaProducer by default when writing to Kafka, as it seems user are often surprised by the default FlinkFixedPartitioner.</description>
      <version>None</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer09.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer08.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer010.java</file>
      <file type="M">docs.dev.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-19 01:00:00" id="8295" opendate="2017-12-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Netty shading does not work properly</summary>
      <description>Multiple users complained that the Cassandra connector is not usable in Flink 1.4.0 due to wrong/insufficient shading of Netty.See:http://mail-archives.apache.org/mod_mbox/flink-user/201712.mbox/%3Cb1f584b918c8aaf98b744c168407b0f5%40dbruhn.de%3Ehttp://mail-archives.apache.org/mod_mbox/flink-user/201712.mbox/%3CCACk7FTgMPR03bPBoKzmeVKCqS%2BumTR1u1X%2BKdPtHRgbnUZiO3A%40mail.gmail.com%3E</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-21 01:00:00" id="8301" opendate="2017-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Unicode in codegen for SQL &amp;&amp; TableAPI</summary>
      <description>The current code generation do not support Unicode, "\u0001" will be generated to "u0001", function call like concat(str, "\u0001") will lead to wrong result.This issue intend to handle char/varchar literal correctly, some examples followed as below.literal: '\u0001abc' -&gt; codegen: "\u0001abc"literal: '\u0022\' -&gt; codegen: "\""</description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.userDefinedScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.literals.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.ExpressionReducer.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-21 01:00:00" id="8303" opendate="2017-12-21 00:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Update Savepoint Compatibility Table for 1.4</summary>
      <description>The savepoint compatibility table of the upgrading applications documentation needs to be extended for 1.4.Also, the whole page should be double checked.</description>
      <version>1.4.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.build.docs.sh</file>
      <file type="M">docs.Gemfile.lock</file>
      <file type="M">docs.Gemfile</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-22 01:00:00" id="8308" opendate="2017-12-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update yajl-ruby dependency to 1.3.1 or higher</summary>
      <description>We got notified that yajl-ruby &lt; 1.3.1, a dependency which is used to build the Flink website, has a security vulnerability of high severity.We should update yajl-ruby to 1.3.1 or higher.Since the website is built offline and served as static HTML, I don't think this is a super critical issue (please correct me if I'm wrong), but we should resolve this soon.</description>
      <version>None</version>
      <fixedVersion>1.4.2,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..plugins.highlightCode.rb</file>
      <file type="M">docs..layouts.plain.html</file>
      <file type="M">docs..config.yml</file>
      <file type="M">docs.ruby2.Gemfile.lock</file>
      <file type="M">docs.ruby2.Gemfile</file>
      <file type="M">docs.Gemfile.lock</file>
      <file type="M">docs.Gemfile</file>
      <file type="M">docs.build.docs.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-23 01:00:00" id="8312" opendate="2017-12-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ScalarFunction varargs length exceeds 254 for SQL</summary>
      <description>With Varargs, TableAPI can handle scalar function call with parameters exceeds 254 correctly.This issue is intend to support long parameters for SQL</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.ScalarSqlFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-26 01:00:00" id="8320" opendate="2017-12-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink cluster does not work on Java 9</summary>
      <description>Recently got a new macbook and figured it was a good time to install java 9 and try it out. I didn't realize that Java 9 was such a breaking update (eg: https://blog.codefx.org/java/java-9-migration-guide/) and took the Flink documentation at face value and assumed that Java 7+ or higher would be fine.Here's is what happens after starting a local cluster and attempting to run the sample WordCount program under Java 9:flink-1.4.0 $ export JAVA_HOME=$(/usr/libexec/java_home -v 9)cru@lappy:flink-1.4.0 $ java -versionjava version "9.0.1"Java(TM) SE Runtime Environment (build 9.0.1+11)Java HotSpot(TM) 64-Bit Server VM (build 9.0.1+11, mixed mode)cru@lappy:flink-1.4.0 $ bin/start-cluster.shStarting cluster.Starting jobmanager daemon on host lappy.local.Starting taskmanager daemon on host lappy.local.cru@lappy:flink-1.4.0 $ bin/flink run examples/streaming/WordCount.jarCluster configuration: Standalone cluster with JobManager at localhost/127.0.0.1:6123Using address localhost:6123 to connect to JobManager.JobManager web interface address http://localhost:8081Starting execution of programExecuting WordCount example with default input data set.Use --input to specify file input.Printing result to stdout. Use --output to specify output path.Submitting job with JobID: ee054ffeb4784848143b76b7d51d99c1. Waiting for job completion.------------------------------------------------------------ The program finished with the following exception:org.apache.flink.client.program.ProgramInvocationException: The program execution failed: Couldn't retrieve the JobExecutionResult from the JobManager. at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:492) at org.apache.flink.client.program.StandaloneClusterClient.submitJob(StandaloneClusterClient.java:105) at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:456) at org.apache.flink.streaming.api.environment.StreamContextEnvironment.execute(StreamContextEnvironment.java:66) at org.apache.flink.streaming.examples.wordcount.WordCount.main(WordCount.java:89) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:525) at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:417) at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:396) at org.apache.flink.client.CliFrontend.executeProgram(CliFrontend.java:802) at org.apache.flink.client.CliFrontend.run(CliFrontend.java:282) at org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:1054) at org.apache.flink.client.CliFrontend$1.call(CliFrontend.java:1101) at org.apache.flink.client.CliFrontend$1.call(CliFrontend.java:1098) at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30) at org.apache.flink.client.CliFrontend.main(CliFrontend.java:1098)Caused by: org.apache.flink.runtime.client.JobExecutionException: Couldn't retrieve the JobExecutionResult from the JobManager. at org.apache.flink.runtime.client.JobClient.awaitJobResult(JobClient.java:300) at org.apache.flink.runtime.client.JobClient.submitJobAndWait(JobClient.java:387) at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:481) ... 18 moreCaused by: org.apache.flink.runtime.client.JobClientActorConnectionTimeoutException: Lost connection to the JobManager. at org.apache.flink.runtime.client.JobClientActor.handleMessage(JobClientActor.java:219) at org.apache.flink.runtime.akka.FlinkUntypedActor.handleLeaderSessionID(FlinkUntypedActor.java:104) at org.apache.flink.runtime.akka.FlinkUntypedActor.onReceive(FlinkUntypedActor.java:71) at akka.actor.UntypedActor$$anonfun$receive$1.applyOrElse(UntypedActor.scala:165) at akka.actor.Actor$class.aroundReceive(Actor.scala:502) at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526) at akka.actor.ActorCell.invoke(ActorCell.scala:495) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257) at akka.dispatch.Mailbox.run(Mailbox.scala:224) at akka.dispatch.Mailbox.exec(Mailbox.scala:234) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)WARNING: An illegal reflective access operation has occurredWARNING: Illegal reflective access by org.apache.flink.shaded.akka.org.jboss.netty.util.internal.ByteBufferUtil (file:/Users/cru/proj/flink/flink-1.4.0/lib/flink-dist_2.11-1.4.0.jar) to method java.nio.DirectByteBuffer.cleaner()WARNING: Please consider reporting this to the maintainers of org.apache.flink.shaded.akka.org.jboss.netty.util.internal.ByteBufferUtilWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operationsWARNING: All illegal access operations will be denied in a future releaseStrangely, the logs seemed to suggest that the JobManager was running fine before submitting the job, so figured this was just a problem with the client. The long timeout also made it seem like a low level network issue.Changing to Java 8 (without bouncing the local cluster) similarly times out as well, but gives a slightly different error. Including it here just for posterity in case someone doesn't bounce the server like I did (To be clear: in this case JobManager and TaskManager processes are still running under java 9.):$ java -versionjava version "1.8.0_151"Java(TM) SE Runtime Environment (build 1.8.0_151-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode)cru@lappy:flink-1.4.0 $ bin/flink run examples/streaming/WordCount.jarCluster configuration: Standalone cluster with JobManager at localhost/127.0.0.1:6123Using address localhost:6123 to connect to JobManager.JobManager web interface address http://localhost:8081Starting execution of programExecuting WordCount example with default input data set.Use --input to specify file input.Printing result to stdout. Use --output to specify output path.Submitting job with JobID: 6bd8fb1a904098473634a7290fbde812. Waiting for job completion.Connected to JobManager at Actor[akka.tcp://flink@localhost:6123/user/jobmanager#1031166856] with leader session id 00000000-0000-0000-0000-000000000000.------------------------------------------------------------ The program finished with the following exception:org.apache.flink.client.program.ProgramInvocationException: The program execution failed: Could not retrieve BlobServer address. at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:492) at org.apache.flink.client.program.StandaloneClusterClient.submitJob(StandaloneClusterClient.java:105) at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:456) at org.apache.flink.streaming.api.environment.StreamContextEnvironment.execute(StreamContextEnvironment.java:66) at org.apache.flink.streaming.examples.wordcount.WordCount.main(WordCount.java:89) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:525) at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:417) at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:396) at org.apache.flink.client.CliFrontend.executeProgram(CliFrontend.java:802) at org.apache.flink.client.CliFrontend.run(CliFrontend.java:282) at org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:1054) at org.apache.flink.client.CliFrontend$1.call(CliFrontend.java:1101) at org.apache.flink.client.CliFrontend$1.call(CliFrontend.java:1098) at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30) at org.apache.flink.client.CliFrontend.main(CliFrontend.java:1098)Caused by: org.apache.flink.runtime.client.JobSubmissionException: Could not retrieve BlobServer address. at org.apache.flink.runtime.client.JobSubmissionClientActor$1.call(JobSubmissionClientActor.java:166) at akka.dispatch.Futures$$anonfun$future$1.apply(Future.scala:97) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.util.concurrent.TimeoutException at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1771) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915) at org.apache.flink.runtime.client.JobSubmissionClientActor$1.call(JobSubmissionClientActor.java:160) ... 9 moreWhile I get that supporting Java 9 completely is probably a larger task, at the very least we should update documentation / prereqs to say that Flink is not yet compatible.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.quickstart.scala.api.quickstart.md</file>
      <file type="M">docs.quickstart.java.api.quickstart.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-1 01:00:00" id="8335" opendate="2018-1-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade hbase connector dependency to 1.4.3</summary>
      <description>hbase 1.4.3 has been released.1.4.0 shows speed improvement over previous 1.x releases.http://search-hadoop.com/m/HBase/YGbbBxedD1Mnm8t?subj=Re+VOTE+The+second+HBase+1+4+0+release+candidate+RC1+is+availableThis issue is to upgrade the dependency to 1.4.3</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-hbase.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-2 01:00:00" id="8346" opendate="2018-1-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add S3 signature v4 workaround to docs</summary>
      <description>As per https://lists.apache.org/thread.html/dd59f94d76ae809f83dc36958006974d0a13dc0798856d1d64bb7293@%3Cuser.flink.apache.org%3E, we should add a hint to enable signature v4 for older Hadoop versions to work with S3 (for the non-shaded S3 file systems and regions only accepting v4, e.g. eu-central-1)</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.deployment.aws.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-2-4 01:00:00" id="8362" opendate="2018-1-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shade Elasticsearch dependencies away</summary>
      <description>It would be nice to make the Elasticsearch connectors self-contained just like the s3 file system implementations and the cassandra connector.</description>
      <version>1.4.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.elasticsearch.md</file>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-1-5 01:00:00" id="8374" opendate="2018-1-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unstable Yarn tests due to Akka Shutdown Exception Logging</summary>
      <description>Akka may log the following in some cases during shutdown:java.util.concurrent.RejectedExecutionException: Worker has already been shutdownThe Yarn tests search the logs for unexpected exceptions and fail when encountering that exception. We should whitelist it, as it is not a problem, merely an Akka shutdown artifact.</description>
      <version>1.4.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.java.org.apache.flink.yarn.YarnTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-5 01:00:00" id="8381" opendate="2018-1-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document more flexible schema definition</summary>
      <description>FLINK-8203 implemented a more flexible schema definition for registering DataSet/DataStream as a table. Documentation should be added with examples.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-10 01:00:00" id="8407" opendate="2018-1-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setting the parallelism after a partitioning operation should be forbidden</summary>
      <description>Partitioning operations (shuffle, rescale, etc.) for a DataStream create new DataStreams, which allow the users to set parallelisms for them. However, the PartitionTransformations in these returned DataStreams will only add virtual nodes, whose parallelisms could not be specified, in the execution graph. We should forbid users to set the parallelism after a partitioning operation since they won't actually work. Also the corresponding documents should be updated.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.DataStreamTest.scala</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-1-18 01:00:00" id="8455" opendate="2018-1-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Hadoop to the parent-first loading patterns</summary>
      <description>Various users have reported issues (mainly in the BucketingSink) where they get ClassCastExceptions related to Hadoop classes.In all cases, users had Hadoop dependencies bundled into their application jar files. To make the experience better, I suggest to let Hadoop always load its classes parent-first. </description>
      <version>1.4.0</version>
      <fixedVersion>1.4.1,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-19 01:00:00" id="8458" opendate="2018-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the switch for keeping both the old mode and the new credit-based mode</summary>
      <description>After the whole feature of credit-based flow control is done, we should add a config parameter to switch on/off the new credit-based mode. To do so, we can roll back to the old network mode for any expected risks.The parameter is defined as taskmanager.network.credit-based-flow-control.enabled and the default value is true. This switch may be removed after next release.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs..includes.generated.netty.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-20 01:00:00" id="8468" opendate="2018-1-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make the connector to take advantage of AMQP features (routing key, exchange and message properties)</summary>
      <description>Make the connector to take advantage of AMQP features by adding a constructor and an interface to implement</description>
      <version>1.4.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSinkPublishOptions.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.test.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSink.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.README.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-22 01:00:00" id="8479" opendate="2018-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement time-bounded inner join of streams as a TwoInputStreamOperator</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.TestHarnessUtil.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2018-2-4 01:00:00" id="8555" opendate="2018-2-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix TableFunction varargs length exceeds 254 for SQL</summary>
      <description>With Varargs, TableAPI can handle table function call with parameters exceeds 254 correctly.This issue is intend to support long parameters for SQL</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.java.org.apache.flink.table.runtime.utils.JavaUserDefinedTableFunctions.java</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.TableSqlFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.ScalarSqlFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.AggSqlFunction.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-7-14 01:00:00" id="8650" opendate="2018-2-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests and documentation for WINDOW clause</summary>
      <description>We support queries with a WINDOW clause like:SELECT a, SUM(c) OVER w, MIN(c) OVER w FROM MyTable WINDOW w AS (PARTITION BY a ORDER BY proctime ROWS BETWEEN 4 PRECEDING AND CURRENT ROW)But this is neither documented nor tested.</description>
      <version>None</version>
      <fixedVersion>1.4.3,1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.OverWindowTest.scala</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-14 01:00:00" id="8657" opendate="2018-2-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix incorrect description for external checkpoint vs savepoint</summary>
      <description>I checked that external checkpoint also supported rescale both in code and practice. But in the doc it still note that "do not support Flink specific features like rescaling." I am afraid whether I have missed something, if so please just close this issue.</description>
      <version>1.4.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.state.backends.md</file>
      <file type="M">docs.ops.state.checkpoints.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-15 01:00:00" id="8661" opendate="2018-2-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Collections.EMPTY_MAP with Collections.emptyMap()</summary>
      <description>The use of Collections.EMPTY_SET and Collections.EMPTY_MAP often causes unchecked assignment. It should be replaced with Collections.emptySet() and Collections.emptyMap() .</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.UtilsTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobAccumulatorsInfoTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.types.ResourceProfileTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.types.ResourceProfile.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-2-21 01:00:00" id="8736" opendate="2018-2-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Memory segment offsets for slices of slices are wrong</summary>
      <description>FLINK-8588 introduced memory segment offsets but the offsets of slices of slices do not account for their parent's slice offset.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.ReadOnlySlicedBufferTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.buffer.ReadOnlySlicedNetworkBuffer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-21 01:00:00" id="8738" opendate="2018-2-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Converge runtime dependency versions for &amp;#39;scala-lang&amp;#39; and for &amp;#39;com.typesafe:config&amp;#39;</summary>
      <description>These dependencies are currently diverged:Dependency convergence error for com.typesafe:config:1.3.0 paths to dependency are:+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-actor_2.11:2.4.20 +-com.typesafe:config:1.3.0and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-stream_2.11:2.4.20 +-com.typesafe:ssl-config-core_2.11:0.2.1 +-com.typesafe:config:1.2.0andDependency convergence error for org.scala-lang:scala-library:2.11.12 paths to dependency are:+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-org.scala-lang:scala-library:2.11.12and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-actor_2.11:2.4.20 +-org.scala-lang:scala-library:2.11.11and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-actor_2.11:2.4.20 +-org.scala-lang.modules:scala-java8-compat_2.11:0.7.0 +-org.scala-lang:scala-library:2.11.7and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-stream_2.11:2.4.20 +-org.scala-lang:scala-library:2.11.11and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-stream_2.11:2.4.20 +-com.typesafe:ssl-config-core_2.11:0.2.1 +-org.scala-lang:scala-library:2.11.8and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-stream_2.11:2.4.20 +-com.typesafe:ssl-config-core_2.11:0.2.1 +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 +-org.scala-lang:scala-library:2.11.6and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-protobuf_2.11:2.4.20 +-org.scala-lang:scala-library:2.11.11and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.typesafe.akka:akka-slf4j_2.11:2.4.20 +-org.scala-lang:scala-library:2.11.11and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-org.clapper:grizzled-slf4j_2.11:1.0.2 +-org.scala-lang:scala-library:2.11.0and+-com.daplatform.flink:txn-api:1.0-SNAPSHOT +-org.apache.flink:flink-streaming-java_2.11:1.5-SNAPSHOT +-org.apache.flink:flink-runtime_2.11:1.5-SNAPSHOT +-com.twitter:chill_2.11:0.7.4 +-org.scala-lang:scala-library:2.11.7</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-11-8 01:00:00" id="8897" opendate="2018-3-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rowtime materialization causes "mismatched type" AssertionError</summary>
      <description>As raised in this thread, the query created by the following code will throw a calcite "mismatch type" (Timestamp(3) and TimeIndicator) exception.String sql1 = "select id, eventTs as t1, count(*) over (partition by id order by eventTs rows between 100 preceding and current row) as cnt1 from myTable1";String sql2 = "select distinct id as r_id, eventTs as t2, count(*) over (partition by id order by eventTs rows between 50 preceding and current row) as cnt2 from myTable2";Table left = tableEnv.sqlQuery(sql1);Table right = tableEnv.sqlQuery(sql2);left.join(right).where("id === r_id &amp;&amp; t1 === t2").select("id, t1").writeToSink(...)The logical plan is as follows.LogicalProject(id=[$0], t1=[$1]) LogicalFilter(condition=[AND(=($0, $3), =($1, $4))]) LogicalJoin(condition=[true], joinType=[inner]) LogicalAggregate(group=[{0, 1, 2}]) LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])]) LogicalProject(id=[$0], eventTs=[$3]) LogicalTableScan(table=[[_DataStreamTable_0]]) LogicalAggregate(group=[{0, 1, 2}]) LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])]) LogicalProject(id=[$0], eventTs=[$3]) LogicalTableScan(table=[[_DataStreamTable_0]])That is because the the rowtime field after an aggregation will be materialized while the RexInputRef type for the filter's operands (t1 === t2) is still TimeIndicator. We should make them unified.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.WindowJoinUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamWindowJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-28 01:00:00" id="9103" opendate="2018-3-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SSL verification on TaskManager when parallelism &gt; 1</summary>
      <description>In dynamic environments like Kubernetes, the SSL certificates can be generated to use only the DNS addresses for validation of the identity of servers, given that the IP can change eventually. In this cases when executing Jobs with Parallelism set to 1, the SSL validations are good and the Jobmanager can communicate with Task manager and vice versa. But with parallelism set to more than 1, SSL validation fails when Task Managers communicate to each other as it seems to try to validate against IP address:Caused by: java.security.cert.CertificateException: No subject alternative names matching IP address 172.xx.xxx.xxx found at sun.security.util.HostnameChecker.matchIP(HostnameChecker.java:168) at sun.security.util.HostnameChecker.match(HostnameChecker.java:94) at sun.security.ssl.X509TrustManagerImpl.checkIdentity(X509TrustManagerImpl.java:455) at sun.security.ssl.X509TrustManagerImpl.checkIdentity(X509TrustManagerImpl.java:436) at sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:252) at sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:136) at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1601) ... 21 more  From the logs, it seems the task managers register successfully its full address to Netty, but still the IP is used. Attached pertinent logs from JobManager and a TaskManager. </description>
      <version>1.4.0</version>
      <fixedVersion>1.4.3,1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-29 01:00:00" id="9107" opendate="2018-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document timer coalescing for ProcessFunctions</summary>
      <description>In a ProcessFunction, registering timers for each event via ctx.timerService().registerEventTimeTimer() using times like ctx.timestamp() + timeout will get a millisecond accuracy and may thus create one timer per millisecond which may lead to some overhead in the TimerService.This problem can be mitigated by using timer coalescing if the desired accuracy of the timer can be larger than 1ms. A timer firing at full seconds only, for example, can be realised like this:coalescedTime = ((ctx.timestamp() + timeout) / 1000) * 1000;ctx.timerService().registerEventTimeTimer(coalescedTime);As a result, only a single timer may exist for every second since we do not add timers for timestamps that are already there.This should be documented in the ProcessFunction docs.</description>
      <version>1.3.0,1.4.0,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.operators.process.function.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-29 01:00:00" id="9108" opendate="2018-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>invalid ProcessWindowFunction link in Document</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.side.output.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-30 01:00:00" id="9274" opendate="2018-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add thread name to Kafka Partition Discovery</summary>
      <description>For debugging, threads should have names to filter on and get a quick overview. The Kafka partition discovery thread(s) currently don't have any name assigned.</description>
      <version>1.4.0,1.4.1,1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2018-10-29 01:00:00" id="9990" opendate="2018-7-29 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add regexp_extract supported in TableAPI and SQL</summary>
      <description>regex_extract is a very useful function, it returns a string based on a regex pattern and a index.For example : regexp_extract('foothebar', 'foo(.*?)(bar)', 2) // returns 'bar.'It is provided as a UDF in Hive, more details please see&amp;#91;1&amp;#93;.&amp;#91;1&amp;#93;: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.functions.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.sql.ScalarSqlFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.functions.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-29 01:00:00" id="9991" opendate="2018-7-29 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add regexp_replace supported in TableAPI and SQL</summary>
      <description>regexp_replace is a very userful function to process String. For example :regexp_replace("foobar", "oo|ar", "") //returns 'fb.'It is supported as a UDF in Hive, more details please see&amp;#91;1&amp;#93;.&amp;#91;1&amp;#93;: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF </description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.functions.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.sql.ScalarSqlFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.functions.md</file>
    </fixedFiles>
  </bug>
</bugrepository>