<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2018-8-31 01:00:00" id="10001" opendate="2018-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Kubernetes documentation</summary>
      <description>We should update Flink's K8s documentation. This includes running it on MiniKube as well as on a K8s cluster.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.container-scripts.docker-compose.test.yml</file>
      <file type="M">flink-container.kubernetes.README.md</file>
      <file type="M">flink-container.docker.README.md</file>
      <file type="M">flink-container.docker.docker-compose.yml</file>
      <file type="M">docs.ops.deployment.kubernetes.md</file>
      <file type="M">docs.ops.deployment.docker.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-31 01:00:00" id="10005" opendate="2018-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>StreamingFileSink ignores checkpoint/processing time rolling policies</summary>
      <description>The StreamingFileSink supports different policies to determine whether a new part file should be created; on each checkpoint, once a certain size is reached or on processing time.This feature only works correctly for size thresholds, other policies are ignored.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.RollingPolicyTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.LocalStreamingFileSinkTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-31 01:00:00" id="10009" opendate="2018-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the casting problem for function TIMESTAMPADD in Table</summary>
      <description>There seems to be a bug in TIMESTAMPADD function. For example, TIMESTAMPADD(MINUTE, 1, DATE '2016-06-15') throws a ClassCastException ( java.lang.Integer cannot be cast to java.lang.Long). Actually, it tries to cast an integer date to a long timestamp in RexBuilder.java:1524 - return TimestampString.fromMillisSinceEpoch((Long) o).</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-2 01:00:00" id="10021" opendate="2018-8-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>All-round E2E tests query metric with wrong operator name</summary>
      <description>In FLINK-8994 the naming scheme for operators of the DataStreamAllroundTestProgram was modified, but the change wasn't propagated to other users of the test.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.externalized.checkpoints.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-2 01:00:00" id="10027" opendate="2018-8-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add logging to the StreamingFileSink</summary>
      <description/>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketState.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-2 01:00:00" id="10028" opendate="2018-8-2 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce ByteArrayData[Input|Output]View</summary>
      <description>In many place, I found that we require to a have a combination of ByteArray&amp;#91;Input/Output&amp;#93;StreamWithPos and the corresponding Data&amp;#91;Input|Output&amp;#93;ViewStreamWrapper, because we essentially want an adapter from byte[] to Data&amp;#91;Input|Output&amp;#93;View.Instead of handling a combination of two related objects all over the place, I suggest to introduce ByteArrayData&amp;#91;Input|Output&amp;#93;View that combine the features.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.KeyGroupPartitionedPriorityQueueWithRocksDBStoreTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBCachingPriorityQueueSet.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-2 01:00:00" id="10029" opendate="2018-8-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor Streaming File Sink for better separation of concerns.</summary>
      <description/>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.TestUtils.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.RollingPolicyTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.LocalStreamingFileSinkTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.BulkWriterTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketsTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketStateSerializerTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.RowWisePartWriter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.OnCheckpointRollingPolicy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.DefaultRollingPolicy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.PartFileInfo.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.DefaultBucketFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.BulkPartWriter.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketStateSerializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketState.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.BucketFactory.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.bucketers.SimpleVersionedStringSerializer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.bucketers.DateTimeBucketer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.bucketers.BasePathBucketer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Bucketer.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-6 01:00:00" id="10068" opendate="2018-8-6 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add documentation for async/RocksDB-based timers</summary>
      <description>Documentation how to activate RocksDB based timers, and update that snapshotting now works async, expect for heap-timers + rocks-incremental-snapshot).</description>
      <version>None</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.state.backends.md</file>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
      <file type="M">docs.dev.stream.operators.process.function.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-6 01:00:00" id="10069" opendate="2018-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add docs for updates SSL model</summary>
      <description>Add docs about the "internal" versus "external" connectivity and new configuration options.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.security-ssl.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-6 01:00:00" id="10070" opendate="2018-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink cannot be compiled with maven 3.0.x</summary>
      <description>In FLINK-9986 we bumped the version of the git-commit-id-plugin to 2.1.14 which is incompatible with various maven versions, like 3.0.X.We can either bump the version to 2.2.4 to support all versions, or downgrade to 2.1.9 and rework FLINK-9986 with property exclusions.Additionally we should setup a test that checks the compatibility of Flink with various maven versions.</description>
      <version>1.5.3,1.6.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-6 01:00:00" id="10071" opendate="2018-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document usage of INSERT INTO in SQL Client</summary>
      <description>Document the usage of INSERT INTO statements in SQL.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-3-6 01:00:00" id="10076" opendate="2018-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Calcite dependency to 1.18</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.QueryDecorrelationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.OverWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.OverWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.stringexpr.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.schema.TimeIndicatorRelDataType.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.functions.utils.AggSqlFunction.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.TableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.api.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.calcite.sql.validate.SqlValidatorImpl.java</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-table.flink-table-planner-blink.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-7 01:00:00" id="10085" opendate="2018-8-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update AbstractOperatorRestoreTestBase</summary>
      <description/>
      <version>1.6.0</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.unkeyed.AbstractNonKeyedOperatorRestoreTestBase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.state.operator.restore.keyed.AbstractKeyedOperatorRestoreTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-7 01:00:00" id="10094" opendate="2018-8-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Always backup default config for end-to-end tests</summary>
      <description>end-to-end tests frequently modify the configuration before running a test; to set state backends, # of task slots, reporters, etc.Currently it is the responsibility of the test to create a backup of the config before doing any modifications, which is then restored by the runner after the test.Instead we can have the test runner always create a backup before running the test.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.bucketing.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.stateful.stream.job.upgrade.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.savepoint.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.externalized.checkpoints.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.queryable.state.restart.tm.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.local.recovery.and.scheduling.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.high.parallelism.iterations.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.batch.allround.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-4-9 01:00:00" id="10107" opendate="2018-8-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Client end-to-end test fails for releases</summary>
      <description>It seems that SQL JARs for Kafka 0.10 and Kafka 0.9 have conflicts that only occur for releases and not SNAPSHOT builds. This might be due to their file name. Depending on the file name either 0.9 is loaded before 0.10 and vice versa.One of the following errors occured:2018-08-08 18:28:51,636 ERROR org.apache.flink.kafka09.shaded.org.apache.kafka.clients.ClientUtils - Failed to close coordinatorjava.lang.NoClassDefFoundError: org/apache/flink/kafka09/shaded/org/apache/kafka/common/requests/OffsetCommitResponse at org.apache.flink.kafka09.shaded.org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.sendOffsetCommitRequest(ConsumerCoordinator.java:473) at org.apache.flink.kafka09.shaded.org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:357) at org.apache.flink.kafka09.shaded.org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.maybeAutoCommitOffsetsSync(ConsumerCoordinator.java:439) at org.apache.flink.kafka09.shaded.org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.close(ConsumerCoordinator.java:319) at org.apache.flink.kafka09.shaded.org.apache.kafka.clients.ClientUtils.closeQuietly(ClientUtils.java:63) at org.apache.flink.kafka09.shaded.org.apache.kafka.clients.consumer.KafkaConsumer.close(KafkaConsumer.java:1277) at org.apache.flink.kafka09.shaded.org.apache.kafka.clients.consumer.KafkaConsumer.close(KafkaConsumer.java:1258) at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:286)Caused by: java.lang.ClassNotFoundException: org.apache.flink.kafka09.shaded.org.apache.kafka.common.requests.OffsetCommitResponse at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$ChildFirstClassLoader.loadClass(FlinkUserCodeClassLoaders.java:120) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 8 morejava.lang.NoSuchFieldError: producer at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer010.invoke(FlinkKafkaProducer010.java:369) at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:56) at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:579) at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:554) at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:534) at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:689) at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:667) at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:41) at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:579) at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:554) at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:534) at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:689) at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:667) at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51) at org.apache.flink.table.runtime.CRowWrappingCollector.collect(CRowWrappingCollector.scala:37) at org.apache.flink.table.runtime.CRowWrappingCollector.collect(CRowWrappingCollector.scala:28)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-9 01:00:00" id="10115" opendate="2018-8-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Content-length limit is also applied to FileUploads</summary>
      <description>Uploading jar files via WEB UI not working. After initializing upload... it only shows saving... and file never shows up on UI to be able to submit it</description>
      <version>1.5.1,1.6.0,1.7.0</version>
      <fixedVersion>1.5.4,1.6.1,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.MultipartUploadResource.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.FileUploadHandler.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-11-14 01:00:00" id="10142" opendate="2018-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce synchronization overhead for credit notifications</summary>
      <description>When credit-based flow control was introduced, we also added some checks and optimisations for uncommon code paths that make common code paths unnecessarily more expensive, e.g. checking whether a channel was released before forwarding a credit notification to Netty. Such checks would have to be confirmed by the Netty thread anyway and thus only add additional load for something that happens only once (per channel).</description>
      <version>1.5.0,1.5.1,1.5.2,1.5.3,1.6.0,1.7.0</version>
      <fixedVersion>1.5.4,1.6.1,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannelTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-14 01:00:00" id="10145" opendate="2018-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add replace supported in TableAPI and SQL</summary>
      <description>replace is an useful function for String. for example:select replace("Hello World", "World", "Flink") // return "Hello Flink"select replace("ababab", "abab", "z") // return "zab"It is supported as a UDF in Hive, more details please see&amp;#91;1&amp;#93;&amp;#91;1&amp;#93;: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-StringFunctions</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.stringExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.functions.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-15 01:00:00" id="10153" opendate="2018-8-15 00:00:00" resolution="Done">
    <buginformation>
      <summary>Add tutorial section to documentation</summary>
      <description>The current documentation does not feature a dedicated tutorials section and has a few issues that should be fix in order to help our (future) users getting started with Flink.I propose to add a single "Tutorials" section to the documentation where users find step-by-step guides. The tutorials section help users with different goals: Get a quick idea of the overall system Implement a DataStream/DataSet/Table API/SQL job Set up Flink on a local machine (or run a Docker container)There are already a few guides to get started but they are located at different places and should be moved into the Tutorials section. Moreover, some sections such as "Project Setup" contain content that addresses users with very different intentions.I propose to add a new Tutorials section and move all existing tutorials there (and later add new ones). move the "Quickstart" section to "Tutorials". remove the "Project Setup" section and move the pages to other sections (some pages will be split up or adjusted).</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.start.index.md</file>
      <file type="M">docs.start.flink.on.windows.md</file>
      <file type="M">docs.start.dependencies.md</file>
      <file type="M">docs.start.building.md</file>
      <file type="M">docs.redirects.scala.shell.md</file>
      <file type="M">docs.redirects.linking.with.optional.modules.md</file>
      <file type="M">docs.redirects.linking.with.flink.md</file>
      <file type="M">docs.redirects.java8.md</file>
      <file type="M">docs.quickstart.setup.quickstart.md</file>
      <file type="M">docs.quickstart.scala.api.quickstart.md</file>
      <file type="M">docs.quickstart.run.example.quickstart.md</file>
      <file type="M">docs.quickstart.java.api.quickstart.md</file>
      <file type="M">docs.ops.deployment.yarn.setup.md</file>
      <file type="M">docs.internals.index.md</file>
      <file type="M">docs.internals.ide.setup.md</file>
      <file type="M">docs.internals.components.md</file>
      <file type="M">docs.index.md</file>
      <file type="M">docs.examples.index.md</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
      <file type="M">docs.dev.stream.python.md</file>
      <file type="M">docs.dev.scala.shell.md</file>
      <file type="M">docs.dev.index.md</file>
      <file type="M">docs.dev.best.practices.md</file>
      <file type="M">docs.dev.batch.examples.md</file>
      <file type="M">docs.dev.api.concepts.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-15 01:00:00" id="10154" opendate="2018-8-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make sure we always read at least one record in KinesisConnector</summary>
      <description>It's possible in some cases to request zero records from Kinesis in the Kinesis connector.  This can happen when the "adpative reads" feature is enabled. </description>
      <version>1.6.0</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-16 01:00:00" id="10164" opendate="2018-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for resuming from savepoints to StandaloneJobClusterEntrypoint</summary>
      <description>The StandaloneJobClusterEntrypoint should support to resume from a savepoint/checkpoint. I suggest to introduce an optional command line parameter for specifying the savepoint/checkpoint path.</description>
      <version>1.6.0,1.7.0</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPointTest.java</file>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.StandaloneJobClusterConfigurationParserFactoryTest.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPoint.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.StandaloneJobClusterConfigurationParserFactory.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.StandaloneJobClusterConfiguration.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-17 01:00:00" id="10166" opendate="2018-8-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dependency problems when executing SQL query in sql-client</summary>
      <description>When tried to run query:select count(distinct name) from (Values ('a'), ('b')) AS NameTable(name)in sql-client.sh I got:[ERROR] Could not execute SQL statement. Reason:org.codehaus.commons.compiler.CompileException: Line 43, Column 10: Unknown variable or type "org.apache.commons.codec.binary.Base64"</description>
      <version>1.6.0</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.typeutils.TypeCheckUtilsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.typeutils.TypeCheckUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.functions.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.TreeNode.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.utils.UserDefinedFunctionUtils.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.UserDefinedFunction.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.literals.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.MatchCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.CodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.TableFunctionCallGen.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.ScalarFunctionCallGen.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.HashCalcCallGen.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.pom.xml</file>
      <file type="M">flink-libraries.flink-table-common.src.main.java.org.apache.flink.table.utils.EncodingUtils.java</file>
      <file type="M">flink-libraries.flink-table-common.src.main.java.org.apache.flink.table.descriptors.DescriptorProperties.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-20 01:00:00" id="10174" opendate="2018-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>getbytes with no charsets test error for hex and toBase64</summary>
      <description>Hex and toBase64 builtin method use str.getBytes() with no Charset. It maybe depend on local execution environment for special Unicode and maybe result in errors when test Hex for special Unicode</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.functions.ScalarFunctions.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-7-22 01:00:00" id="10195" opendate="2018-8-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>RabbitMQ Source With Checkpointing Doesn&amp;#39;t Backpressure Correctly</summary>
      <description>The connection between the RabbitMQ server and the client does not appropriately back pressure when auto acking is disabled. This becomes very problematic when a downstream process throttles the data processing to slower then RabbitMQ sends the data to the client.The difference in records ends up being stored in the flink's heap space, which grows indefinitely (or technically to "Integer Max" Deliveries). Looking at RabbitMQ's metrics the number of unacked messages looks like steadily rising saw tooth shape.Upon further invesitgation it looks like this is due to how the QueueingConsumer works, messages are added to the BlockingQueue faster then they are being removed and processed, resulting in the previously described behavior.This may be intended behavior, however this isn't explicitly obvious in the documentation or any of the examples I have seen.</description>
      <version>1.4.0,1.5.0,1.5.1,1.6.0</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.test.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.test.java.org.apache.flink.streaming.connectors.rabbitmq.common.RMQConnectionConfigTest.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.RMQSource.java</file>
      <file type="M">flink-connectors.flink-connector-rabbitmq.src.main.java.org.apache.flink.streaming.connectors.rabbitmq.common.RMQConnectionConfig.java</file>
      <file type="M">docs.dev.connectors.rabbitmq.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-10-29 01:00:00" id="10253" opendate="2018-8-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run MetricQueryService with lower priority</summary>
      <description>We should run the MetricQueryService with a lower priority than the main Flink components. An idea would be to start the underlying threads with a lower priority.</description>
      <version>1.5.3,1.6.0,1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.util.MetricUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.test.scala.org.apache.flink.runtime.akka.AkkaUtilsTest.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.akka.AkkaUtils.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.BootstrapTools.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.MetricOptions.java</file>
      <file type="M">docs..includes.generated.metric.configuration.html</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-1-31 01:00:00" id="10274" opendate="2018-8-31 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>The stop-cluster.sh cannot stop cluster properly when there are multiple clusters running</summary>
      <description>When you are preparing to do a Flink framework version upgrading by using the strategy shadow copy , you have to run multiple clusters concurrently,  however when you are ready to stop the old version cluster after upgrading, you would find the stop-cluster.sh wouldn't work as you expected, the following is the steps to duplicate the issue: There is already a running Flink 1.5.x cluster instance; Installing another Flink 1.6.x cluster instance at the same cluster machines; Migrating the jobs from Flink 1.5.x  to Flink 1.6.x ; go to the bin dir of the Flink 1.5.x cluster instance and run stop-cluster.sh ;You would expect the old Flink 1.5.x cluster instance be stopped ,right? Unfortunately the stopped cluster is the new installed Flink 1.6.x cluster instance instead!</description>
      <version>1.5.1,1.5.2,1.5.3,1.6.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.CoreOptions.java</file>
      <file type="M">docs..includes.generated.environment.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-5 01:00:00" id="10282" opendate="2018-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide separate thread-pool for REST endpoint</summary>
      <description>The REST endpoints currently share their thread-pools with the RPC system, which can cause the Dispatcher to become unresponsive if the REST parts are overloaded.</description>
      <version>1.5.1,1.6.0,1.7.0</version>
      <fixedVersion>1.5.5,1.6.2,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.ExecutorThreadFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.SessionRestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.JobRestEndpointFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.minicluster.MiniCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestOptions.java</file>
      <file type="M">docs..includes.generated.rest.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-6 01:00:00" id="10289" opendate="2018-9-6 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Classify Exceptions to different category for apply different failover strategy</summary>
      <description>We need to classify exceptions and treat them with different strategies. To do this, we propose to introduce the following Throwable Types, and the corresponding exceptions: NonRecoverable We shouldn’t retry if an exception was classified as NonRecoverable For example, NoResouceAvailiableException is a NonRecoverable Exception Introduce a new Exception UserCodeException to wrap all exceptions that throw from user code PartitionDataMissingError In certain scenarios producer data was transferred in blocking mode or data was saved in persistent store. If the partition was missing, we need to revoke/rerun the produce task to regenerate the data. Introduce a new exception PartitionDataMissingException to wrap all those kinds of issues. EnvironmentError It happened due to hardware, or software issues that were related to specific environments. The assumption is that a task will succeed if we run it in a different environment, and other task run in this bad environment will very likely fail. If multiple task failures in the same machine due to EnvironmentError, we need to consider adding the bad machine to blacklist, and avoiding schedule task on it. Introduce a new exception EnvironmentException to wrap all those kind of issues. Recoverable We assume other issues are recoverable.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.execution.SuppressRestartsException.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-6 01:00:00" id="10291" opendate="2018-9-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate JobGraph with fixed/configurable JobID in StandaloneJobClusterEntrypoint</summary>
      <description>The StandaloneJobClusterEntrypoint currently generates the JobGraph from the user code when being started. Due to the nature of how the JobGraph is generated, it will get a random JobID assigned. This is problematic in case of a failover because then, the JobMaster won't be able to detect the checkpoints. In order to solve this problem, we need to either fix the JobID assignment or make it configurable.</description>
      <version>1.6.0,1.7.0</version>
      <fixedVersion>1.6.2,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plan.StreamingPlan.java</file>
      <file type="M">flink-container.src.test.java.org.apache.flink.container.entrypoint.ClassPathJobGraphRetrieverTest.java</file>
      <file type="M">flink-container.src.main.java.org.apache.flink.container.entrypoint.ClassPathJobGraphRetriever.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.PackagedProgramUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-6 01:00:00" id="10295" opendate="2018-9-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tokenisation of Program Args resulting in unexpected results</summary>
      <description>We were upgrading from Flink 1.4 to 1.6. At present we have a jar which takes all the details to run the job as program args against a jarid, including sql query and kafka details. In version 1.5 the program args are tokenised as a result single quote (') and double quote(") are stripped from the arguments. This results in malformed args.Attached a sample request for reference.</description>
      <version>1.5.0,1.6.0,1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerRequestUtils.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarSubmissionITCase.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBodyTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandlerParameterTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.ProgramArgsQueryParameter.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunRequestBody.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunMessageParameters.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanMessageParameters.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHeaders.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler.java</file>
      <file type="M">docs..includes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-2-13 01:00:00" id="10342" opendate="2018-9-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka duplicate topic consumption when topic name is changed</summary>
      <description>In case of topic name is simply renamed for a KafkaConsumer Flink starts to consume from old and a new topic in the same time which can lead to unexpected behavior.Here is the PR with reproduce: https://github.com/apache/flink/pull/6691 </description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBaseMigrationTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicsDescriptor.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.internals.AbstractPartitionDiscoverer.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-11-17 01:00:00" id="10357" opendate="2018-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming File Sink end-to-end test failed with mismatch</summary>
      <description>The Streaming File Sink end-to-end test failed on an Amazon instance with the following result: FAIL File Streaming Sink: Output hash mismatch. Got f2000bbc18a889dc8ec4b6f2b47bf9f5, expected 6727342fdd3aae2129e61fc8f433fb6f.head hexdump of actual:0000000 0 \n 1 \n 2 \n 3 \n 4 \n 5 \n 6 \n 7 \n0000010 8 \n 9 \n0000014</description>
      <version>1.6.0,1.7.0</version>
      <fixedVersion>1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.file.sink.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-9-18 01:00:00" id="10362" opendate="2018-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bundled S3 connectors load wrong Hadoop config</summary>
      <description>The bundles S3 connectors internally build on Hadoop's file system support, but are packaged to not expose that at all - they are Hadoop-independent, from the user's perspective.Hence, they should not try to take external Hadoop configurations into account, but only the Flink configuration.</description>
      <version>1.6.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-hadoop-fs.src.main.java.org.apache.flink.runtime.fs.hdfs.HadoopConfigLoader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-18 01:00:00" id="10363" opendate="2018-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>S3 FileSystem factory prints secrets into logs</summary>
      <description>The file system factory logs all values it applies from the flink configuration.That frequently includes access keys, which should not leak into logs.The loader should only log the keys, not the values.</description>
      <version>None</version>
      <fixedVersion>1.6.2,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-hadoop-fs.src.main.java.org.apache.flink.runtime.fs.hdfs.HadoopConfigLoader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-18 01:00:00" id="10365" opendate="2018-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Consolidate shaded Hadoop classes for filesystems</summary>
      <description>We currently have have three bundled/shaded filesystem connectors that build on top of Hadoop's classes. More will probably come. Each of them re-builds the shaded Hadoop module, including creating the relocated config, adapting native code loading, etc.We should factor that out into a single base project to avoid duplicating work.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.maven.suppressions.xml</file>
      <file type="M">flink-filesystems.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-12-14 01:00:00" id="10543" opendate="2018-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Leverage efficient timer deletion in relational operators</summary>
      <description>FLINK-9423 added support for efficient timer deletions. This feature is available since Flink 1.6 and should be used by the relational operator of SQL and Table API.Currently, we use a few workarounds to handle situations when deleting timers would be the better solution.</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.operators.ProcessFunctionWithCleanupStateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.operators.KeyedProcessFunctionWithCleanupStateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.StateCleaningCountTriggerHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.JoinHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.triggers.StateCleaningCountTrigger.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.NonWindowOuterJoinWithNonEquiPredicates.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.NonWindowOuterJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.NonWindowLeftRightJoinWithNonEquiPredicates.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.NonWindowLeftRightJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.NonWindowJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.NonWindowInnerJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.NonWindowFullJoinWithNonEquiPredicates.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.NonWindowFullJoin.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.RowTimeUnboundedOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.RowTimeBoundedRowsOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.RowTimeBoundedRangeOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeUnboundedOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeBoundedRowsOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcTimeBoundedRangeOver.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.ProcessFunctionWithCleanupState.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.KeyedProcessFunctionWithCleanupState.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.GroupAggProcessFunction.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-9 01:00:00" id="5750" opendate="2017-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect translation of n-ary Union</summary>
      <description>Calcite's union operator is supports more than two input relations. However, Flink's translation rules only consider the first two relations because we assumed that Calcite's union is binary. This problem exists for batch and streaming queries.It seems that Calcite only generates non-binary Unions in rare cases ((SELECT * FROM t) UNION ALL (SELECT * FROM t) UNION ALL (SELECT * FROM t) results in two binary union operators) but the problem definitely needs to be fixed.The following query can be used to validate the problem. @Test public void testValuesWithCast() throws Exception { ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config()); String sqlQuery = "VALUES (1, cast(1 as BIGINT) )," + "(2, cast(2 as BIGINT))," + "(3, cast(3 as BIGINT))"; String sqlQuery2 = "VALUES (1,1)," + "(2, 2)," + "(3, 3)"; Table result = tableEnv.sql(sqlQuery); DataSet&lt;Row&gt; resultSet = tableEnv.toDataSet(result, Row.class); List&lt;Row&gt; results = resultSet.collect(); Table result2 = tableEnv.sql(sqlQuery2); DataSet&lt;Row&gt; resultSet2 = tableEnv.toDataSet(result2, Row.class); List&lt;Row&gt; results2 = resultSet2.collect(); String expected = "1,1\n2,2\n3,3"; compareResultAsText(results2, expected); compareResultAsText(results, expected); }AR for results variablejava.lang.AssertionError: Different elements in arrays: expected 3 elements and received 2 expected: [1,1, 2,2, 3,3] received: [1,1, 2,2] Expected :3Actual :2</description>
      <version>1.2.0,1.3.4,1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.3,1.6.0,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testUnionStream0.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testUnion1.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.resources.testUnion0.out</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.TableTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.TableSourceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.StreamTableEnvironmentTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.UnionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.ExternalCatalogTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.batch.sql.GroupingSetsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamUnionRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.dataSet.DataSetUnionRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.nodes.dataset.DataSetUnion.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.FlinkPlannerImpl.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-6 01:00:00" id="8563" opendate="2018-2-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support consecutive DOT operators</summary>
      <description>We added support for accessing fields of arrays of composite types in FLINK-7923. However, accessing another nested subfield is not supported by Calcite. See CALCITE-2162. We should fix this once we upgrade to Calcite 1.16.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.utils.CompositeTypeTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.CompositeAccessTest.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-27 01:00:00" id="8795" opendate="2018-2-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala shell broken for Flip6</summary>
      <description>I am trying to run the simple code below after building everything from Flink's github master branch for various reasons. I get an exception below and I wonder what runs on port 9065? and How to fix this exception?I followed the instructions from the Flink master branch so I did the following.git clone https://github.com/apache/flink.git cd flink mvn clean package -DskipTests cd build-target ./bin/start-scala-shell.sh localAnd Here is the code I ranval dataStream = senv.fromElements(1, 2, 3, 4)dataStream.countWindowAll(2).sum(0).print()senv.execute("My streaming program")And I finally get this exceptionCaused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph. at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$18(RestClusterClient.java:306) at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870) at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:852) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$222(RestClient.java:196) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:603) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:563) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:268) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:284) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137) at java.lang.Thread.run(Thread.java:745) Caused by: java.util.concurrent.CompletionException: java.net.ConnectException: Connection refused: localhost/127.0.0.1:9065 at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:943) at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926) ... 16 more Caused by: java.net.ConnectException: Connection refused: localhost/127.0.0.1:9065 at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:281) </description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellLocalStartupITCase.scala</file>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-28 01:00:00" id="8810" opendate="2018-2-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move end-to-end test scripts to end-to-end module</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">test-infra.end-to-end-test.test.streaming.python.wordcount.sh</file>
      <file type="M">test-infra.end-to-end-test.test.streaming.kafka010.sh</file>
      <file type="M">test-infra.end-to-end-test.test.streaming.classloader.sh</file>
      <file type="M">test-infra.end-to-end-test.test.shaded.presto.s3.sh</file>
      <file type="M">test-infra.end-to-end-test.test.shaded.hadoop.s3a.sh</file>
      <file type="M">test-infra.end-to-end-test.test.hadoop.free.sh</file>
      <file type="M">test-infra.end-to-end-test.test.batch.wordcount.sh</file>
      <file type="M">test-infra.end-to-end-test.test-data.words</file>
      <file type="M">test-infra.end-to-end-test.common.sh</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-3-2 01:00:00" id="8842" opendate="2018-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default REST port to 8081</summary>
      <description>In order to avoid confusion, we should set the default REST port to 8081.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterEntrypoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.entrypoint.ClusterConfiguration.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.RestOptions.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-11-8 01:00:00" id="8897" opendate="2018-3-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rowtime materialization causes "mismatched type" AssertionError</summary>
      <description>As raised in this thread, the query created by the following code will throw a calcite "mismatch type" (Timestamp(3) and TimeIndicator) exception.String sql1 = "select id, eventTs as t1, count(*) over (partition by id order by eventTs rows between 100 preceding and current row) as cnt1 from myTable1";String sql2 = "select distinct id as r_id, eventTs as t2, count(*) over (partition by id order by eventTs rows between 50 preceding and current row) as cnt2 from myTable2";Table left = tableEnv.sqlQuery(sql1);Table right = tableEnv.sqlQuery(sql2);left.join(right).where("id === r_id &amp;&amp; t1 === t2").select("id, t1").writeToSink(...)The logical plan is as follows.LogicalProject(id=[$0], t1=[$1]) LogicalFilter(condition=[AND(=($0, $3), =($1, $4))]) LogicalJoin(condition=[true], joinType=[inner]) LogicalAggregate(group=[{0, 1, 2}]) LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])]) LogicalProject(id=[$0], eventTs=[$3]) LogicalTableScan(table=[[_DataStreamTable_0]]) LogicalAggregate(group=[{0, 1, 2}]) LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])]) LogicalProject(id=[$0], eventTs=[$3]) LogicalTableScan(table=[[_DataStreamTable_0]])That is because the the rowtime field after an aggregation will be materialized while the RexInputRef type for the filter's operands (t1 === t2) is still TimeIndicator. We should make them unified.</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.TimeAttributesITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.plan.TimeIndicatorConversionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.JoinTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.join.WindowJoinUtil.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamWindowJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamJoinRule.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.calcite.RelTimeIndicatorConverter.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-3-11 01:00:00" id="8916" opendate="2018-3-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkpointing Mode is always shown to be "At Least Once" in Web UI</summary>
      <description>This only happens in flip6 mode. The CheckpointConfigHandler returns the checkpoint mode uppercased. For example:{"mode":"EXACTLY_ONCE","interval":5000,"timeout":600000,"min_pause":0,"max_concurrent":1,"externalization":{"enabled":false,"delete_on_cancellation":true}}However, the Web UI expects the value to be lower cased: &lt;tr&gt; &lt;td&gt;Checkpointing Mode&lt;/td&gt; &lt;td ng-if="checkpointConfig['mode'] == 'exactly_once'"&gt;Exactly Once&lt;/td&gt; &lt;td ng-if="checkpointConfig['mode'] != 'exactly_once'"&gt;At Least Once&lt;/td&gt; &lt;/tr&gt;</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointConfigInfo.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-4-22 01:00:00" id="9059" opendate="2018-3-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for unified table source and sink declaration in environment file</summary>
      <description>1) Add a common property called "type" with single value 'source'.2) in yaml file, replace "sources" with "tables".</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sources.TestTableSourceFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.sources.TableSourceFactoryServiceTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.TableSourceDescriptor.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-factory.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.ExecutionContext.java</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Environment.java</file>
      <file type="M">flink-libraries.flink-sql-client.conf.sql-client-defaults.yaml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-26 01:00:00" id="9091" opendate="2018-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failure while enforcing releasability in building flink-json module</summary>
      <description>Got the following when building flink-json module:[WARNING] Rule 0: org.apache.maven.plugins.enforcer.DependencyConvergence failed with message:Failed while enforcing releasability. See above detailed error message....[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.0.0-M1:enforce (dependency-convergence) on project flink-json: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -&gt; [Help 1]</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-27 01:00:00" id="9093" opendate="2018-3-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>If Google can&amp;#39;t be accessed,the document can&amp;#39;t be use</summary>
      <description>these links can't be visited.</description>
      <version>None</version>
      <fixedVersion>1.3.4,1.4.3,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.savepoint.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">docs..layouts.base.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-29 01:00:00" id="9107" opendate="2018-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document timer coalescing for ProcessFunctions</summary>
      <description>In a ProcessFunction, registering timers for each event via ctx.timerService().registerEventTimeTimer() using times like ctx.timestamp() + timeout will get a millisecond accuracy and may thus create one timer per millisecond which may lead to some overhead in the TimerService.This problem can be mitigated by using timer coalescing if the desired accuracy of the timer can be larger than 1ms. A timer firing at full seconds only, for example, can be realised like this:coalescedTime = ((ctx.timestamp() + timeout) / 1000) * 1000;ctx.timerService().registerEventTimeTimer(coalescedTime);As a result, only a single timer may exist for every second since we do not add timers for timestamps that are already there.This should be documented in the ProcessFunction docs.</description>
      <version>1.3.0,1.4.0,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.operators.process.function.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-29 01:00:00" id="9108" opendate="2018-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>invalid ProcessWindowFunction link in Document</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.side.output.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-6 01:00:00" id="9144" opendate="2018-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Spilling batch job hangs</summary>
      <description>A user on the mailing list reported that his batch job stops to run with Flink 1.5 RC1: https://lists.apache.org/thread.html/43721934405019e7255fda627afb7c9c4ed0d04fb47f1c8f346d4194@%3Cdev.flink.apache.org%3E</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartitionTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.buffer.BufferBuilderTestUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.SpillableSubpartition.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.batch.allround.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-12 01:00:00" id="9163" opendate="2018-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harden e2e tests&amp;#39; signal traps and config restoration during abort</summary>
      <description>Signal traps on certain systems, e.g. Linux, may be called concurrently when the trap is caught during its own execution. In that case, our cleanup may just be wrong and may also overly eagerly delete flink-conf.yaml.</description>
      <version>1.5.0,1.5.1,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.sql.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.kafka010.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.savepoint.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-30 01:00:00" id="9274" opendate="2018-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add thread name to Kafka Partition Discovery</summary>
      <description>For debugging, threads should have names to filter on and get a quick overview. The Kafka partition discovery thread(s) currently don't have any name assigned.</description>
      <version>1.4.0,1.4.1,1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-5-7 01:00:00" id="9309" opendate="2018-5-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Recommend HA setup on Production Readiness Checklist</summary>
      <description>It would be good to recommend the HA setup on the Production Readiness Checklist to ensure that users are aware of this feature.</description>
      <version>None</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.production.ready.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-14 01:00:00" id="9353" opendate="2018-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Kubernetes integration</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-pre-commit-tests.sh</file>
      <file type="M">flink-end-to-end-tests.README.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-14 01:00:00" id="9358" opendate="2018-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Closing of unestablished RM connections can cause NPE</summary>
      <description>When closing an unestablished RM connection, a NPE is thrown. The reason is that we try to unmonitor a non-existing heartbeat target.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-7-16 01:00:00" id="9380" opendate="2018-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failing end-to-end tests should not clean up logs</summary>
      <description>Some of the end-to-end tests clean up their logs also in the failure case. This makes debugging and understanding the problem extremely difficult. Ideally, the scripts says where it stored the respective logs.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test-runner-common.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-16 01:00:00" id="9383" opendate="2018-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend DistributedCache E2E test to cover directories</summary>
      <description/>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.distributed.cache.via.blob.sh</file>
      <file type="M">flink-end-to-end-tests.flink-distributed-cache-via-blob-test.src.main.java.org.apache.flink.streaming.tests.DistributedCacheViaBlobTestProgram.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-16 01:00:00" id="9386" opendate="2018-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove netty-router dependency</summary>
      <description>netty-router 1.10 blocks upgrade to 4.1, while netty-router 2.2.0 has broken compatibility in a way that it's unusable by us (it doesn't allow to sort router paths as in https://issues.apache.org/jira/browse/FLINK-8000 ). I propose to copy &amp; simplify &amp; modify netty-router code to suite our needs.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.handler.legacy.TaskManagerLogHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpoint.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.RouterHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.RedirectHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.TaskManagerLogHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.ConstantTextHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.AbstractHandler.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.RedirectHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandlerTest.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.utils.WebFrontendBootstrap.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.RuntimeMonitorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServer.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.util.MesosArtifactServer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-17 01:00:00" id="9392" opendate="2018-5-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add @FunctionalInterface annotations to all core functional interfaces</summary>
      <description>The @FunctionalInterface annotation should be added to all SAM interfaces in order to prevent accidentally breaking them (as non SAMs).We had a case of that before for the SinkFunction which was compatible through default methods, but incompatible for users that previously instantiated that interface through a lambda.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.ReduceFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.Partitioner.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.MapPartitionFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.MapFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.JoinFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.GroupReduceFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.GroupCombineFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.FoldFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.FlatMapFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.FlatJoinFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.FilterFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.CrossFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.CombineFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.CoGroupFunction.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.functions.BroadcastVariableInitializer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-22 01:00:00" id="9408" opendate="2018-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retry JM-RM connection in case of explicit disconnect</summary>
      <description>The JM should try to reconnect to the RM not only in the case of a heartbeat timeout but also in case of an explicit disconnect.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.JobMasterTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.JobMaster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.EstablishedResourceManagerConnection.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-23 01:00:00" id="9416" opendate="2018-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make job submission retriable operation in case of a ongoing leader election</summary>
      <description>When starting a session cluster, it can happen that the job submission fails if the REST server endpoint has already gained leadership but if the leadership election for the Dispatcher is still ongoing. In such a case, we receive a error response saying that the leader election is still ongoing and fail the job submission. I think it would be nicer to also make the submission step a retriable operation in order to avoid this race condition.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.program.rest.RestClusterClientTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.rest.RestClusterClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-6-16 01:00:00" id="942" opendate="2014-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Config key "env.java.opts" does not effect JVM args</summary>
      <description>Setting custom args for the JVM as in env.java.opts: -Dio.netty.leakDetectionLevel=paranoidhas no effect for the started JVMs.A fix is coming up.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">stratosphere-dist.src.main.stratosphere-bin.bin.taskmanager.sh</file>
      <file type="M">stratosphere-dist.src.main.stratosphere-bin.bin.jobmanager.sh</file>
      <file type="M">stratosphere-dist.src.main.stratosphere-bin.bin.config.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-23 01:00:00" id="9420" opendate="2018-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests for SQL IN sub-query operator in streaming</summary>
      <description>In FLINK-6094 we implemented non-windowed inner joins. The Table API &amp; SQL should now support the IN operator for sub-queries in streaming. Batch support has been added in FLINK-4565. We need to add unit tests, an IT case, and update the docs about that.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.validation.UnsupportedOpsValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.table.SetOperatorsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.subquery.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-23 01:00:00" id="9429" opendate="2018-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Quickstart E2E not working locally</summary>
      <description>The quickstart e2e test is not working locally. It seems as if the job does not produce anything into Elasticsearch. Furthermore, the test does not terminate with control-C.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-single-test.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-25 01:00:00" id="9438" opendate="2018-5-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for (Registry)AvroDeserializationSchema</summary>
      <description/>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-31 01:00:00" id="9483" opendate="2018-5-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Building Flink" doc doesn&amp;#39;t highlight quick build command</summary>
      <description/>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.start.building.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-3 01:00:00" id="9508" opendate="2018-6-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>General Spell Check on Flink Docs</summary>
      <description>Fixing Flink docs misspelling </description>
      <version>1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.upgrading.md</file>
      <file type="M">docs.ops.security-ssl.md</file>
      <file type="M">docs.ops.filesystems.md</file>
      <file type="M">docs.ops.cli.md</file>
      <file type="M">docs.dev.execution.configuration.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-4 01:00:00" id="9518" opendate="2018-6-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SSL setup Docs config example has wrong keys password</summary>
      <description>In creating keystores and turststore section password is set to password but in setup config section it is abc123</description>
      <version>1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.security-ssl.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-5 01:00:00" id="9526" opendate="2018-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>BucketingSink end-to-end test failed on Travis</summary>
      <description>https://travis-ci.org/zentol/flink-ci/builds/388130914</description>
      <version>1.6.0</version>
      <fixedVersion>1.8.2,1.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.bucketing.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-6 01:00:00" id="9539" opendate="2018-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate flink-shaded 4.0</summary>
      <description>With the recent release of flink-shaded 4.0 we should bump the versions for all dependencies (except netty which is handled in FLINK-3952).We can now remove the exclusions from the jackson dependencies as they are now properly hidden.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-7 01:00:00" id="9549" opendate="2018-6-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix FlickCEP Docs broken link and minor style changes</summary>
      <description/>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-7-15 01:00:00" id="9593" opendate="2018-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unify AfterMatch semantics with SQL MATCH_RECOGNIZE</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.UntilConditionITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.sharedbuffer.SharedBufferTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.SameElementITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFATestUtilities.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.NFAITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.GroupITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.compiler.NFACompilerTest.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.AfterMatchSkipITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.CEPITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.Pattern.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.pattern.GroupPattern.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.SelectTimeoutCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.SelectCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.FlatSelectTimeoutCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.FlatSelectCepOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.CEPOperatorUtils.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.operator.AbstractKeyedCEPPatternOperator.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.sharedbuffer.EventId.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.SharedBuffer.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.NFAStateSerializer.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.NFAState.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.NFA.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.MigrationUtils.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.DeweyNumber.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.ComputationState.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.compiler.NFACompiler.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.AfterMatchSkipStrategy.java</file>
      <file type="M">flink-libraries.flink-cep-scala.src.main.scala.org.apache.flink.cep.scala.pattern.Pattern.scala</file>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-15 01:00:00" id="9594" opendate="2018-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for e2e test changes introduced with FLINK-9257</summary>
      <description>There were some changes introduced with FLINK-9257 in how end-to-end tests are structured and how we handle triggering failure from a test case. This should be documented for future developers writing e2e tests</description>
      <version>None</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.README.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-15 01:00:00" id="9595" opendate="2018-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add instructions to docs about ceased support of KPL version used in Kinesis connector</summary>
      <description>The KPL version used in the Kinesis connector for FlinkKinesisProducer is not longer supported by AWS Kinesis Streams: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Flink-1-4-and-below-STOPS-writing-to-Kinesis-after-June-12th-td22687.html#a22701We should add a notice about this to the Kinesis connectors, and how to bypass it by specifying the KPL version when building the Kinesis connector.</description>
      <version>1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.1,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.connectors.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-21 01:00:00" id="962" opendate="2014-6-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move documentation from old website into source code</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-20 01:00:00" id="9622" opendate="2018-6-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DistributedCacheDfsTest failed on travis</summary>
      <description>DistributedCacheDfsTest#testDistributeFileViaDFS() failed flakey on travis.instance: https://api.travis-ci.org/v3/job/394399700/log.txt</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.util.FileUtilsTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-21 01:00:00" id="9629" opendate="2018-6-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Datadog metrics reporter does not have shaded dependencies</summary>
      <description>flink-metrics-datadog-1.5.0.jar does not contain shaded dependencies for okhttp3 and okio</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.1,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-datadog.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-21 01:00:00" id="9637" opendate="2018-6-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add public user documentation for TTL feature</summary>
      <description/>
      <version>1.6.0</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.stream.state.state.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-25 01:00:00" id="9655" opendate="2018-6-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Externalized checkpoint E2E test fails on travis</summary>
      <description>https://travis-ci.org/zentol/flink-ci/builds/396395491 ==============================================================================Running 'Resuming Externalized Checkpoint after terminal failure (file, sync) end-to-end test'==============================================================================Flink dist directory: /home/travis/build/zentol/flink-ci/flink/build-targetTEST_DATA_DIR: /home/travis/build/zentol/flink-ci/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-47420177246Starting cluster.Starting standalonesession daemon on host travis-job-b06f9adc-9d05-4569-b4bc-af11e6a0bc6e.Starting taskexecutor daemon on host travis-job-b06f9adc-9d05-4569-b4bc-af11e6a0bc6e.Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Waiting for dispatcher REST endpoint to come up...Dispatcher REST endpoint is up.Running externalized checkpoints test, with ORIGINAL_DOP=file NEW_DOP=false and STATE_BACKEND_TYPE=false STATE_BACKEND_FILE_ASYNC=true STATE_BACKEND_ROCKSDB_INCREMENTAL=false SIMULATE_FAILURE=false ...Job () is running.Waiting for job (1) to have at least completed checkpoints ...Waiting for job to process up to 200 records, current progress: 0 records ...Waiting for job to process up to 200 records, current progress: 0 records ...</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-1-27 01:00:00" id="9679" opendate="2018-6-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ConfluentRegistryAvroSerializationSchema</summary>
      <description>Implement AvroSerializationSchema using Confluent Schema Registry</description>
      <version>1.6.0</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.confluent.schema.registry.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.kafka-common.sh</file>
      <file type="M">flink-end-to-end-tests.flink-confluent-schema-registry.src.main.java.org.apache.flink.schema.registry.test.TestAvroConsumerConfluent.java</file>
      <file type="M">flink-formats.flink-avro.src.test.java.org.apache.flink.formats.avro.RegistryAvroDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-avro.src.main.java.org.apache.flink.formats.avro.SchemaCoder.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.src.main.java.org.apache.flink.formats.avro.registry.confluent.ConfluentSchemaRegistryCoder.java</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-27 01:00:00" id="9681" opendate="2018-6-27 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Make sure minRetentionTime not equal to maxRetentionTime</summary>
      <description>Currently, for a group by(or other operators), if minRetentionTime equals to maxRetentionTime, the group by operator will register a timer for each record coming at different time which cause performance problem. The reasoning for having two parameters is that we can avoid to register many timers if we have more freedom when to discard state. As min equals to max cause performance problem it is better to make sure these two parameters are not same.Any suggestions are welcome.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.operators.ProcessFunctionWithCleanupStateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.operators.KeyedProcessFunctionWithCleanupStateTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.StateCleaningCountTriggerHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.OverWindowHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.NonWindowHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.JoinHarnessTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.harness.HarnessTestBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.queryConfig.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Execution.java</file>
      <file type="M">docs.dev.table.streaming.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-7-6 01:00:00" id="9771" opendate="2018-7-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Show Plan" option under Submit New Job in WebUI not working</summary>
      <description>Show Plan button under Submit new job in WebUI not working.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.JobPlanInfoTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobPlanInfo.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-6 01:00:00" id="9772" opendate="2018-7-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation of Hadoop API outdated</summary>
      <description>It looks like the documentation of the Hadoop Compatibility is somewhat outdated? At least the text and examples in section Using Hadoop InputFormats mention methodsenv.readHadoopFile and env.createHadoopInputwhich do not exist anymore since 1.4.0.   </description>
      <version>1.4.2,1.5.0,1.6.0</version>
      <fixedVersion>1.4.3,1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.batch.index.md</file>
      <file type="M">docs.dev.batch.hadoop.compatibility.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-9 01:00:00" id="9781" opendate="2018-7-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>scala-maven-plugin fails on java 9</summary>
      <description>https://travis-ci.org/zentol/flink/jobs/40171125811:10:02.157 [INFO] --- scala-maven-plugin:3.2.2:compile (scala-compile-first) @ flink-runtime_2.11 ---11:10:04.861 [INFO] /home/travis/build/zentol/flink/flink-runtime/src/main/java:-1: info: compiling11:10:04.862 [INFO] /home/travis/build/zentol/flink/flink-runtime/src/main/scala:-1: info: compiling11:10:04.862 [INFO] Compiling 1486 source files to /home/travis/build/zentol/flink/flink-runtime/target/classes at 153113460486211:10:06.135 [ERROR] error: java.lang.NoClassDefFoundError: javax/tools/ToolProvider11:10:06.135 [INFO] at scala.reflect.io.JavaToolsPlatformArchive.iterator(ZipArchive.scala:301)11:10:06.135 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)11:10:06.135 [INFO] at scala.reflect.io.AbstractFile.foreach(AbstractFile.scala:92)11:10:06.135 [INFO] at scala.tools.nsc.util.DirectoryClassPath.traverse(ClassPath.scala:277)11:10:06.135 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15$lzycompute(ClassPath.scala:299)11:10:06.136 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15(ClassPath.scala:299)11:10:06.136 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages$lzycompute(ClassPath.scala:299)11:10:06.136 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:299)11:10:06.136 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:264)11:10:06.136 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)11:10:06.136 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)11:10:06.136 [INFO] at scala.collection.Iterator$class.foreach(Iterator.scala:891)11:10:06.136 [INFO] at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)11:10:06.136 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)11:10:06.136 [INFO] at scala.collection.AbstractIterable.foreach(Iterable.scala:54)11:10:06.136 [INFO] at scala.tools.nsc.util.MergedClassPath.packages$lzycompute(ClassPath.scala:358)11:10:06.136 [INFO] at scala.tools.nsc.util.MergedClassPath.packages(ClassPath.scala:353)11:10:06.136 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply$mcV$sp(SymbolLoaders.scala:269)11:10:06.136 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)11:10:06.136 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)11:10:06.136 [INFO] at scala.reflect.internal.SymbolTable.enteringPhase(SymbolTable.scala:235)11:10:06.136 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader.doComplete(SymbolLoaders.scala:260)11:10:06.136 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:211)11:10:06.136 [INFO] at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1535)11:10:06.136 [INFO] at scala.reflect.internal.Mirrors$RootsBase.init(Mirrors.scala:256)11:10:06.136 [INFO] at scala.tools.nsc.Global.rootMirror$lzycompute(Global.scala:73)11:10:06.136 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:71)11:10:06.136 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:39)11:10:06.136 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute(Definitions.scala:257)11:10:06.136 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass(Definitions.scala:257)11:10:06.136 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.init(Definitions.scala:1390)11:10:06.136 [INFO] at scala.tools.nsc.Global$Run.&lt;init&gt;(Global.scala:1242)11:10:06.136 [INFO] at scala.tools.nsc.Driver.doCompile(Driver.scala:31)11:10:06.136 [INFO] at scala.tools.nsc.MainClass.doCompile(Main.scala:23)11:10:06.136 [INFO] at scala.tools.nsc.Driver.process(Driver.scala:51)11:10:06.136 [INFO] at scala.tools.nsc.Driver.main(Driver.scala:64)11:10:06.136 [INFO] at scala.tools.nsc.Main.main(Main.scala)11:10:06.136 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)11:10:06.136 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)11:10:06.136 [INFO] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)11:10:06.137 [INFO] at java.base/java.lang.reflect.Method.invoke(Method.java:564)11:10:06.137 [INFO] at scala_maven_executions.MainHelper.runMain(MainHelper.java:164)11:10:06.137 [INFO] at scala_maven_executions.MainWithArgsInFile.main(MainWithArgsInFile.java:26)11:10:06.137 [INFO] java.lang.reflect.InvocationTargetException11:10:06.137 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)11:10:06.137 [INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)11:10:06.137 [INFO] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)11:10:06.137 [INFO] at java.base/java.lang.reflect.Method.invoke(Method.java:564)11:10:06.137 [INFO] at scala_maven_executions.MainHelper.runMain(MainHelper.java:164)11:10:06.137 [INFO] at scala_maven_executions.MainWithArgsInFile.main(MainWithArgsInFile.java:26)11:10:06.137 [ERROR] Caused by: java.lang.NoClassDefFoundError: javax/tools/ToolProvider11:10:06.137 [INFO] at scala.reflect.io.JavaToolsPlatformArchive.iterator(ZipArchive.scala:301)11:10:06.137 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)11:10:06.137 [INFO] at scala.reflect.io.AbstractFile.foreach(AbstractFile.scala:92)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.traverse(ClassPath.scala:277)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15$lzycompute(ClassPath.scala:299)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.x$15(ClassPath.scala:299)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages$lzycompute(ClassPath.scala:299)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:299)11:10:06.138 [INFO] at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:264)11:10:06.138 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)11:10:06.138 [INFO] at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)11:10:06.138 [INFO] at scala.collection.Iterator$class.foreach(Iterator.scala:891)11:10:06.138 [INFO] at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)11:10:06.138 [INFO] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)11:10:06.138 [INFO] at scala.collection.AbstractIterable.foreach(Iterable.scala:54)11:10:06.148 [INFO] at scala.tools.nsc.util.MergedClassPath.packages$lzycompute(ClassPath.scala:358)11:10:06.148 [INFO] at scala.tools.nsc.util.MergedClassPath.packages(ClassPath.scala:353)11:10:06.149 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply$mcV$sp(SymbolLoaders.scala:269)11:10:06.149 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)11:10:06.149 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)11:10:06.149 [INFO] at scala.reflect.internal.SymbolTable.enteringPhase(SymbolTable.scala:235)11:10:06.149 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader.doComplete(SymbolLoaders.scala:260)11:10:06.149 [INFO] at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:211)11:10:06.149 [INFO] at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1535)11:10:06.149 [INFO] at scala.reflect.internal.Mirrors$RootsBase.init(Mirrors.scala:256)11:10:06.149 [INFO] at scala.tools.nsc.Global.rootMirror$lzycompute(Global.scala:73)11:10:06.149 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:71)11:10:06.149 [INFO] at scala.tools.nsc.Global.rootMirror(Global.scala:39)11:10:06.151 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute(Definitions.scala:257)11:10:06.153 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass(Definitions.scala:257)11:10:06.153 [INFO] at scala.reflect.internal.Definitions$DefinitionsClass.init(Definitions.scala:1390)11:10:06.153 [INFO] at scala.tools.nsc.Global$Run.&lt;init&gt;(Global.scala:1242)11:10:06.153 [INFO] at scala.tools.nsc.Driver.doCompile(Driver.scala:31)11:10:06.153 [INFO] at scala.tools.nsc.MainClass.doCompile(Main.scala:23)11:10:06.172 [INFO] at scala.tools.nsc.Driver.process(Driver.scala:51)11:10:06.172 [INFO] at scala.tools.nsc.Driver.main(Driver.scala:64)11:10:06.172 [INFO] at scala.tools.nsc.Main.main(Main.scala)11:10:06.172 [INFO] ... 6 more11:10:06.196 [INFO] ------------------</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-8-10 01:00:00" id="9795" opendate="2018-7-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Mesos documentation for flip6</summary>
      <description>Mesos documentation would benefit from an overhaul after flip6 became the default cluster management model.</description>
      <version>1.5.0,1.6.0</version>
      <fixedVersion>1.6.1,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.java</file>
      <file type="M">flink-mesos.src.main.java.org.apache.flink.mesos.configuration.MesosOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HighAvailabilityOptions.java</file>
      <file type="M">docs..includes.generated.mesos.task.manager.configuration.html</file>
      <file type="M">docs..includes.generated.mesos.configuration.html</file>
      <file type="M">docs..includes.generated.high.availability.zookeeper.configuration.html</file>
      <file type="M">docs.ops.deployment.mesos.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-11 01:00:00" id="9801" opendate="2018-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flink-dist is missing dependency on flink-examples</summary>
      <description>For the assembly of flink-dist we copy various batch/streaming examples directly from the respective /target directory.Never mind that this is already a problem as is (see FLINK-9582), flink-dist defines no dependency on these modules.If you were to only compile flink-dist with the -am flag (to also build all dependencies) it thus may or may not happen that these modules are actually compiled, which could cause these examples to not be included in the final assembly.</description>
      <version>1.5.1,1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-13 01:00:00" id="9839" opendate="2018-7-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Streaming job with SSL</summary>
      <description>None of the existing e2e tests run with an SSL configuration but there should be such a test as well.</description>
      <version>1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.bucketing.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.batch.allround.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-7-13 01:00:00" id="9846" opendate="2018-7-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a Kafka table sink factory</summary>
      <description>FLINK-8866 implements a unified way of creating sinks and using the format discovery for searching for formats (FLINK-8858). It is now possible to add a Kafka table sink factory for streaming environment that uses the new interfaces.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.factories.utils.TestTableFormatFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.factories.utils.TestSerializationSchema.scala</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSinkTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.partitioner.FlinkFixedPartitioner.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaJsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09TableSourceFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09TableSourceFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka09JsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08TableSourceFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08TableSourceFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.8.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka08JsonTableSink.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka011TableSourceFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka011TableSourceFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka011TableSource.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010TableSourceFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.test.java.org.apache.flink.streaming.connectors.kafka.Kafka010JsonTableSinkTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010TableSourceFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.src.main.java.org.apache.flink.streaming.connectors.kafka.Kafka010JsonTableSink.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-15 01:00:00" id="9853" opendate="2018-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add hex support in table api and sql</summary>
      <description>like in mysql, HEX could take int or string arguments, For a integer argument N, it returns a hexadecimal string representation of the value of N. For a string argument str, it returns a hexadecimal string representation of str where each byte of each character in str is converted to two hexadecimal digits. Syntax:HEX(100) = 64HEX('This is a test String.') = '546869732069732061207465737420537472696e672e'See more: link MySQL</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.validate.FunctionCatalog.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.functions.ScalarFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.functions.sql.ScalarSqlFunctions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.expressions.mathExpressions.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.FunctionGenerator.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.calls.BuiltInMethods.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.functions.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-16 01:00:00" id="9857" opendate="2018-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Processing-time timers fire too early</summary>
      <description>The firing of processing-time timers is off by one. This leads to problems in edge cases, as discovered here (mailing list) when elements arrive at the timestamp that is the end of the window.The problem is here (github). For event-time, we fire timers when the watermark is &gt;= the timestamp, this is correct because a watermark T says that we will not see elements with a timestamp smaller or equal to T. For processing time, a time of T does not say that we won't see an element with timestamp T, which makes processing-time timers fire one ms too early.I think we can fix it by turning that &lt;= into a &lt;.</description>
      <version>1.3.4,1.4.2,1.5.1,1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-16 01:00:00" id="9858" opendate="2018-7-16 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>State TTL End-to-End Test</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-16 01:00:00" id="9860" opendate="2018-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Netty resource leak on receiver side</summary>
      <description>The Hadoop-free Wordcount end-to-end test fails with the following exception:ERROR org.apache.flink.shaded.netty4.io.netty.util.ResourceLeakDetector - LEAK: ByteBuf.release() was not called before it's garbage-collected. See http://netty.io/wiki/reference-counted-objects.html for more information.Recent access records: Created at: org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:331) org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:185) org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:176) org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBufAllocator.ioBuffer(AbstractByteBufAllocator.java:137) org.apache.flink.shaded.netty4.io.netty.channel.DefaultMaxMessagesRecvByteBufAllocator$MaxMessageHandle.allocate(DefaultMaxMessagesRecvByteBufAllocator.java:114) org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:147) org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645) org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459) org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884) org.apache.flink.shaded.netty4.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)We might have a resource leak on the receiving side of our network stack.https://api.travis-ci.org/v3/job/404225956/log.txt</description>
      <version>1.5.1,1.6.0</version>
      <fixedVersion>1.5.2,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.FileUploadHandlerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.FileUploadHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-16 01:00:00" id="9861" opendate="2018-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add end-to-end test for reworked BucketingSink</summary>
      <description>We should add a end-to-end test for the reworked BucketingSink to verify that the sink works with different FileSystems.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-16 01:00:00" id="9862" opendate="2018-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update end-to-end test to use RocksDB backed timers</summary>
      <description>We should add or modify an end-to-end test to use RocksDB backed timers.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.savepoint.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.SequenceGeneratorSource.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.SemanticsCheckMapper.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.java</file>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestJobFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-17 01:00:00" id="9870" opendate="2018-7-17 00:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Support field mapping and time attributes for table sinks</summary>
      <description>FLINK-7548 reworked the table source design and implemented the interfaces DefinedFieldMapping, DefinedProctimeAttribute, and DefinedRowtimeAttributes.However, these interfaces need to be implemented by table sinks as well in order to map a table back into a sink similar how source do it for reading input data.The current unified sink design assumes that this is possible.</description>
      <version>1.6.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.utils.InMemoryTableFactory.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.sinks.CsvTableSinkFactoryBase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.SchemaValidator.scala</file>
      <file type="M">flink-libraries.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.TestTableSourceFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-17 01:00:00" id="9871" opendate="2018-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Description class for ConfigOptions with rich formatting</summary>
      <description/>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.SecurityOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.AkkaOptions.java</file>
      <file type="M">docs..includes.generated.security.configuration.html</file>
      <file type="M">docs..includes.generated.akka.configuration.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-17 01:00:00" id="9874" opendate="2018-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>set_conf_ssl in E2E tests fails on macOS</summary>
      <description>Setting up a cluster with SSL support in the end-to-end tests with `set_conf_ssl` will fail under macOS because in the commandhostname -Iis used, but '-I' is not a supported parameter for the hostname command under macOS</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-18 01:00:00" id="9885" opendate="2018-7-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>End-to-end test: Elasticsearch 6.x connector</summary>
      <description>We have decided to try and merge the pending Elasticsearch 6.x PRs. This should also come with an end-to-end test that covers this.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.elasticsearch.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.elasticsearch-common.sh</file>
      <file type="M">flink-end-to-end-tests.flink-elasticsearch6-test.src.main.java.org.apache.flink.streaming.tests.Elasticsearch6SinkExample.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.Elasticsearch1ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.resources.log4j-test.properties</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.EmbeddedElasticsearchNodeEnvironmentImpl.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch6.examples.ElasticsearchSinkExample.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.test.java.org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.src.main.java.org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.test.java.org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.java.org.apache.flink.streaming.connectors.elasticsearch5.Elasticsearch5ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.test.java.org.apache.flink.streaming.connectors.elasticsearch2.ElasticsearchSinkITCase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.ElasticsearchSink.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.java.org.apache.flink.streaming.connectors.elasticsearch2.Elasticsearch2ApiCallBridge.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.test.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBaseTest.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.java</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch-base.src.main.java.org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchApiCallBridge.java</file>
      <file type="M">docs.dev.connectors.elasticsearch.md</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-18 01:00:00" id="9886" opendate="2018-7-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build SQL jars with every build</summary>
      <description>Currently, the shaded fat jars for SQL are only built in the -Prelease profile. However, end-to-end tests require those jars and should also be able to test them. E.g. existing META-INF entry and proper shading. We should build them with every release. If a build should happen quicker one can use the -Pfast profile.</description>
      <version>None</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.pom.xml</file>
      <file type="M">flink-formats.flink-avro.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kafka-0.10.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-19 01:00:00" id="9892" opendate="2018-7-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable local recovery in Jepsen tests</summary>
      <description>Until FLINK-9635 is fixed, local recovery should be disabled in the Jepsen tests.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.db.clj</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2018-8-24 01:00:00" id="9933" opendate="2018-7-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Simplify taskmanager memory default values</summary>
      <description>The default value for NETWORK_BUFFERS_MEMORY_MIN is currently defined is String.valueOf(64L &lt;&lt; 20), which in the documentation is represented as "67108864".Now that we have the MemorySize utility we can change the default to "64 mb".The same applies to NETWORK_BUFFERS_MEMORY_MAX.</description>
      <version>1.6.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.high.parallelism.iterations.sh</file>
      <file type="M">flink-dist.src.main.resources.flink-conf.yaml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs..includes.generated.task.manager.configuration.html</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-24 01:00:00" id="9934" opendate="2018-7-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka table source factory produces invalid field mapping</summary>
      <description>The Kafka table source factory produces an invalid field mapping when referencing a rowtime attribute from an input field. The check in TableSourceUtil#validateTableSource therefore can fail.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.descriptors.SchemaValidatorTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.descriptors.SchemaValidator.scala</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaJsonTableSourceFactoryTestBase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-24 01:00:00" id="9935" opendate="2018-7-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Batch Table API: grouping by window and attribute causes java.lang.ClassCastException:</summary>
      <description> Grouping by window AND some other attribute(s) seems broken. Test case attached:class BatchStatisticsIntegrationTest extends FlatSpec with Matchers { trait BatchContext { implicit lazy val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment implicit val tableEnv: BatchTableEnvironment = TableEnvironment.getTableEnvironment(env) val data = Seq( (1532424567000L, "id1", "location1"), (1532424567000L, "id2", "location1"), (1532424567000L, "id3", "location1"), (1532424568000L, "id1", "location2"), (1532424568000L, "id2", "location3") ) val rawDataSet: DataSet[(Long, String, String)] = env.fromCollection(data) val table: Table = tableEnv.fromDataSet(rawDataSet, 'rowtime, 'id, 'location) } it should "be possible to run Table API queries with grouping by tumble window and column(s) on batch data" in new BatchContext { val results = table .window(Tumble over 1.second on 'rowtime as 'w) .groupBy('w, 'location) .select( 'w.start.cast(Types.LONG), 'w.end.cast(Types.LONG), 'location, 'id.count ) .toDataSet[(Long, Long, String, Long)] .collect() results should contain theSameElementsAs Seq( (1532424567000L, 1532424568000L, "location1", 3L), (1532424568000L, 1532424569000L, "location2", 1L), (1532424568000L, 1532424569000L, "location3", 1L) ) }}It seems like during execution time, the 'rowtime attribute replaces 'location and that causes ClassCastException.[info] Cause: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.String[info] at org.apache.flink.api.common.typeutils.base.StringSerializer.serialize(StringSerializer.java:28)[info] at org.apache.flink.api.java.typeutils.runtime.RowSerializer.serialize(RowSerializer.java:160)[info] at org.apache.flink.api.java.typeutils.runtime.RowSerializer.serialize(RowSerializer.java:46)[info] at org.apache.flink.runtime.plugable.SerializationDelegate.write(SerializationDelegate.java:54)[info] at org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.addRecord(SpanningRecordSerializer.java:88)[info] at org.apache.flink.runtime.io.network.api.writer.RecordWriter.sendToTarget(RecordWriter.java:129)[info] at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:105)[info] at org.apache.flink.runtime.operators.shipping.OutputCollector.collect(OutputCollector.java:65)[info] at org.apache.flink.runtime.operators.util.metrics.CountingCollector.collect(CountingCollector.java:35)[info] at org.apache.flink.api.java.operators.translation.RichCombineToGroupCombineWrapper.combine(RichCombineToGroupCombineWrapper.java:52)Here is some debug information that I was able to get. So, field serializers don't match the type of Row fields:this.instance = {Row@68451} "1532424567000,(3),1532424567000" fields = {Object[3]@68461} 0 = {Long@68462} 1532424567000 1 = {CountAccumulator@68463} "(3)" 2 = {Long@68462} 1532424567000this.serializer = {RowSerializer@68452} fieldSerializers = {TypeSerializer[3]@68455} 0 = {StringSerializer@68458} 1 = {TupleSerializer@68459} 2 = {LongSerializer@68460} arity = 3 nullMask = {boolean[3]@68457}  </description>
      <version>1.4.2,1.5.1,1.6.0,1.7.0</version>
      <fixedVersion>1.4.3,1.5.3,1.6.0,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.batch.table.GroupWindowITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.runtime.aggregate.AggregateUtil.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-8-25 01:00:00" id="9946" opendate="2018-7-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Quickstart E2E test archetype version is hard-coded</summary>
      <description>mvn archetype:generate \ -DarchetypeGroupId=org.apache.flink \ -DarchetypeArtifactId=flink-quickstart-${TEST_TYPE} \ -DarchetypeVersion=1.6-SNAPSHOT \ -DgroupId=org.apache.flink.quickstart \ -DartifactId=${ARTIFACT_ID} \ -Dversion=${ARTIFACT_VERSION} \ -Dpackage=org.apache.flink.quickstart \ -DinteractiveMode=false</description>
      <version>1.6.0,1.7.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.quickstarts.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-25 01:00:00" id="9949" opendate="2018-7-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jepsen: Kill Flink processes when tearing down cluster</summary>
      <description>Because Flink processes are not killed at the end of the tests, it can happen that surviving instances create znodes in the ZooKeeper ensemble of the next test run. This creates ambiguity when we retrieve the address of the leading REST server from ZooKeeper.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.db.clj</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-25 01:00:00" id="9951" opendate="2018-7-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update scm developerConnection</summary>
      <description>The developer connection must be updated to point to the update remote.</description>
      <version>1.3.3,1.4.2,1.5.1,1.6.0,1.7.0</version>
      <fixedVersion>1.4.3,1.5.3,1.6.0,1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-2 01:00:00" id="996" opendate="2014-7-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NullPointerException while translating union node</summary>
      <description>The NepheleJobGraphGenerator throws a NullPointerException when translating a binary union operator. The BinaryUnionPlanNode is not replaced by a NAryUnionPlanNode and thus is still treated as a DualInputVertex. Accessing the driver code of the BinaryUnionPlanNode causes then the NullPointerException.</description>
      <version>None</version>
      <fixedVersion>0.6-incubating</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">stratosphere-compiler.src.main.java.eu.stratosphere.compiler.PactCompiler.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-10-26 01:00:00" id="9975" opendate="2018-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Netty dependency of Hadoop &gt;= 2.7 is not relocated</summary>
      <description>Previously, in flink-shaded-hadoop, we also relocate Netty (org.jboss.netty) to not conflict with user code. Since Hadoop 2.7 the Netty version they depend on has been upgraded and we missed relocating io.netty accordingly.</description>
      <version>1.4.2,1.5.0,1.5.1,1.6.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-27 01:00:00" id="9977" opendate="2018-7-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refine the docs for Table/SQL built-in functions</summary>
      <description>There exist some syntax errors or inconsistencies in documents and Scala docs of the Table/SQL built-in functions. This issue aims to make some improvements to them.Also, according to FLINK-10103, we should use single quotes to express strings in SQL. For example, CONCAT("AA", "BB", "CC") should be replaced with CONCAT('AA', 'BB', 'CC'). </description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.expressions.SqlExpressionTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.api.scala.expressionDsl.scala</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.sql.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-27 01:00:00" id="9981" opendate="2018-7-27 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Tune performance of RocksDB implementation</summary>
      <description>General performance tuning/polishing for the RocksDB implementation. We can figure out how caching/seeking can be improved.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.HeapInternalTimerServiceTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimeServiceManager.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.InternalTimerServiceSerializationProxy.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.HeapInternalTimerService.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackendConfigTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBOrderedSetStoreTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBAsyncSnapshotTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.KeyGroupPartitionedPriorityQueueWithRocksDBStoreTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.CachingInternalPriorityQueueSetWithRocksDBStoreTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBStateBackend.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOrderedSetStore.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.InternalPriorityQueueTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.TreeOrderedSetCacheTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.TestOrderedStore.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.SimpleCachingInternalPriorityQueueSetTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.OrderedSetCacheTestBase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.KeyGroupPartitionedPriorityQueueTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.heap.CachingInternalPriorityQueueSetTestBase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.TieBreakingPriorityComparator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.InternalPriorityQueue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.TreeOrderedSetCache.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.KeyGroupPartitionedPriorityQueue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapPriorityQueueSnapshotRestoreWrapper.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapPriorityQueueSet.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.HeapPriorityQueue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.heap.CachingInternalPriorityQueueSet.java</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.resume.savepoint.sh</file>
      <file type="M">flink-core.src.test.java.org.apache.flink.core.memory.ByteArrayInputStreamWithPosTest.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.memory.ByteArrayInputStreamWithPos.java</file>
      <file type="M">docs..includes.generated.rocks.db.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-27 01:00:00" id="9986" opendate="2018-7-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary information from .version.properties file</summary>
      <description>To log the revision flink-runtime creates a .version.properties file using the git-commit-id-plugin that is stored within the jar.Here's an example:git.commit.id.abbrev=1a9b648git.commit.user.email=chesnay@apache.orggit.commit.message.full=Commit for release 1.5.2\ngit.commit.id=1a9b6486a2d268d4fb8282c32d65fcc701d18e42git.commit.message.short=Commit for release 1.5.2git.commit.user.name=zentolgit.build.user.name=zentolgit.build.user.email=chesnay@apache.orggit.branch=1a9b6486a2d268d4fb8282c32d65fcc701d18e42git.commit.time=25.07.2018 @ 17\:10\:13 GMTgit.build.time=25.07.2018 @ 20\:47\:15 GMTgit.remote.origin.url=https\://github.com/zentol/flink.gitmost of this information isn't used, as flink-runtime only access git.commit.id.abbrev and git.commit.time.The build, remote and branch information should be removed as they are neither relevant, nor consistent, as releases can be created on any branch, under any git alias, against any remote.To exclude properties we have to bump the plugin version to 2.1.9.</description>
      <version>1.4.2,1.5.1,1.6.0</version>
      <fixedVersion>1.5.3,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-30 01:00:00" id="9995" opendate="2018-7-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jepsen: Clean up Mesos Logs and Working Directory</summary>
      <description>When tearing down Mesos, all files in the log and working directory should be cleaned up, or we risk running out of disk space after running enough tests in succession.</description>
      <version>1.6.0</version>
      <fixedVersion>1.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.mesos.clj</file>
    </fixedFiles>
  </bug>
</bugrepository>