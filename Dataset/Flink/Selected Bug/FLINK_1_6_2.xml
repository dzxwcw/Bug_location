<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2018-10-29 01:00:00" id="10465" opendate="2018-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jepsen: runit supervised sshd is stopped on tear down</summary>
      <description>When tearing down the DB, we tear down all services supervised by runit. However when running the tests in Docker, sshd is under supervision by runit. When sshd is stopped, the tests cannot be continued because the control node cannot interact with the DB nodes anymore.How to reproduceRun command below in control-node container:./docker/run-tests.sh 1 [...]/flink/flink-1.6.1/flink-1.6.1-bin-hadoop28-scala_2.11.tgzExpected behaviorsshd should never be stopped</description>
      <version>1.6.2,1.7.0</version>
      <fixedVersion>1.6.2,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.utils.clj</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-27 01:00:00" id="1072" opendate="2014-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend Travis testing to more Hadoop versions.</summary>
      <description>http://apache-flink-incubator-mailing-list-archive.1008284.n3.nabble.com/Extend-Travis-CI-build-matrix-td1516.html</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-11-2 01:00:00" id="10763" opendate="2018-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Interval join produces wrong result type in Scala API</summary>
      <description>When stream is a Scala case class, the TypeInformation will fall back to GenericType in the process function which result in bad performance when union another DataStream.In the union method of DataStream, the type is first checked for equality.Here is an example:object Test { def main(args: Array[String]): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment val orderA: DataStream[Order] = env.fromCollection(Seq( Order(1L, "beer", 3), Order(1L, "diaper", 4), Order(3L, "rubber", 2))) val orderB: DataStream[Order] = env.fromCollection(Seq( new Order(2L, "pen", 3), new Order(2L, "rubber", 3), new Order(4L, "beer", 1))) val orderC: DataStream[Order] = orderA.keyBy(_.user) .intervalJoin(orderB.keyBy(_.user)) .between(Time.seconds(0), Time.seconds(0)) .process(new ProcessJoinFunction[Order, Order, Order] { override def processElement(left: Order, right: Order, ctx: ProcessJoinFunction[Order, Order, Order]#Context, out: Collector[Order]): Unit = { out.collect(left) }}) println("C: " + orderC.dataType.toString) println("B: " + orderB.dataType.toString) orderC.union(orderB).print() env.execute() } case class Order(user: Long, product: String, amount: Int)}Here is the Exception:Exception in thread "main" java.lang.IllegalArgumentException: Cannot union streams of different types: GenericType&lt;com.manbuyun.awesome.flink.Test.Order&gt; and com.manbuyun.awesome.flink.Test$Order(user: Long, product: String, amount: Integer) at org.apache.flink.streaming.api.datastream.DataStream.union(DataStream.java:219) at org.apache.flink.streaming.api.scala.DataStream.union(DataStream.scala:357) at com.manbuyun.awesome.flink.Test$.main(Test.scala:38) at com.manbuyun.awesome.flink.Test.main(Test.scala) </description>
      <version>1.6.2</version>
      <fixedVersion>1.6.3,1.7.0,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-scala.src.test.scala.org.apache.flink.streaming.api.scala.IntervalJoinITCase.scala</file>
      <file type="M">flink-streaming-scala.src.main.scala.org.apache.flink.streaming.api.scala.KeyedStream.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-11-6 01:00:00" id="10799" opendate="2018-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set -Xms when starting JobManager in YARN mode</summary>
      <description>When start JobManager on Yarn mode, only set -Xmx parameter , add -Xms also to avoid high frequency full gc  at start up phase.</description>
      <version>1.3.3,1.4.2,1.5.5,1.6.2</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnClusterDescriptorTest.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.AbstractYarnClusterDescriptor.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-11-13 01:00:00" id="10863" opendate="2018-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Assign uids to all operators</summary>
      <description>We should assign uids to operators in the test so that we can also properly test removing operators.</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-datastream-allround-test.src.main.java.org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-12-13 01:00:00" id="10865" opendate="2018-11-13 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement Flink&amp;#39;s own Aliyun OSS filesystem</summary>
      <description>Aliyun OSS is widely used among China’s cloud users, and Hadoop supports Aliyun OSS since 2.9.1. Open this jira to wrap AliyunOSSFileSystem in flink(similar to s3 support), so that user can read from &amp; write to OSS more easily in flink.  </description>
      <version>1.6.2</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.opt.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-11-13 01:00:00" id="10869" opendate="2018-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update S3 testing settings</summary>
      <description>Currently S3 tests go against a bucket hosted by 'data Artisans'.As part of reworking the AWS permission setup, we need to adapt the credentials and buckets for these tests.Future tests should refer to the following environment variables for S3 tests: `IT_CASE_S3_BUCKET` `IT_CASE_S3_ACCESS_KEY` `IT_CASE_S3_SECRET_KEY`</description>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.travis.yml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils-junit.src.main.java.org.apache.flink.testutils.s3.S3Credentials.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.java.org.apache.flink.fs.s3presto.PrestoS3RecoverableWriterTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.java.org.apache.flink.fs.s3presto.PrestoS3FileSystemITCase.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.test.java.org.apache.flink.fs.s3presto.PrestoS3FileSystemBehaviorITCase.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionTest.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase.java</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.test.java.org.apache.flink.fs.s3hadoop.HadoopS3FileSystemBehaviorITCase.java</file>
      <file type="M">flink-yarn.src.test.java.org.apache.flink.yarn.YarnFileStageTestS3ITCase.java</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.file.sink.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.shaded.presto.s3.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.shaded.hadoop.s3a.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.s3.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-14 01:00:00" id="10872" opendate="2018-11-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend SQL client end-to-end to test KafkaTableSink for kafka connector 0.11</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-14 01:00:00" id="10880" opendate="2018-11-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failover strategies should not be applied to Batch Execution</summary>
      <description>When configuring a failover strategy other than "full", DataSet/Batch execution is currently not correct.This is expected, the failover region strategy is an experimental WIP feature for streaming that has not been extended to the DataSet API.We need to document this and prevent execution of DataSet features with other failover strategies than "full".</description>
      <version>1.6.2</version>
      <fixedVersion>1.5.6,1.6.3,1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.configuration.ConfigOptionsDocGeneratorTest.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-annotations.src.main.java.org.apache.flink.annotation.docs.Documentation.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.JobManagerOptions.java</file>
      <file type="M">docs..includes.generated.job.manager.configuration.html</file>
      <file type="M">docs.release-notes.flink-1.7.md</file>
      <file type="M">docs.release-notes.flink-1.6.md</file>
      <file type="M">docs.release-notes.flink-1.5.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-18 01:00:00" id="10917" opendate="2018-11-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump io.dropwizard:metrics-core to 3.2.6</summary>
      <description>I've experienced back pressure once in a while from a streaming pipeline of mine &amp;#91;1&amp;#93;.I strongly suspect SlidingTimeWindowReservoir from io.dropwizard.metrics:metrics-core:3.1.5.It is known to cause long GCs &amp;#91;2&amp;#93; so a new implementation called SlidingTimeWindowArrayReservoir is introduced in v3.2.3.So I suggest to bump up dropwizard's metrics-core to v3.2.3 or higher to use the new implementation to prevent back pressure which actually has nothing to do with Flink itself.I just tested compatibility very simply by importing io.dropwizard.metrics:metrics-core:4.0.3 in my own project in order to shadow v3.1.5 which is introduced by flink-metrics-dropwizard.It works without any incompatibility issues for me; there was no NoSuchMethodError or something.However, I'm not sure whether bumping up to 3.2.x or 4.x is okay for other users.&amp;#91;1&amp;#93; https://www.slideshare.net/ssuser6bb12d/realtime-driving-score-service-using-flink/30&amp;#91;2&amp;#93; https://github.com/dropwizard/metrics/pull/1139</description>
      <version>1.6.2</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-graphite.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-dropwizard.pom.xml</file>
      <file type="M">flink-metrics.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-graphite.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-11-22 01:00:00" id="10980" opendate="2018-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong statement in CEP Scala part</summary>
      <description>The CEP 'Selecting from patterns' part of Doc gives wrong code.{{ val startEvent = pattern.get("start").get.next }}val endEvent = pattern.get("end").get.nextshould be .get.head</description>
      <version>1.6.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.libs.cep.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-22 01:00:00" id="10992" opendate="2018-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jepsen: Do not use /tmp as HDFS Data Directory</summary>
      <description>dfs.name.dir and dfs.data.dir should not be located in /tmp. The directories might get deleted unintentionally, which can cause test failures.</description>
      <version>1.6.2,1.7.0,1.8.0</version>
      <fixedVersion>1.6.3,1.7.0,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-jepsen.src.jepsen.flink.utils.clj</file>
      <file type="M">flink-jepsen.src.jepsen.flink.hadoop.clj</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-12-23 01:00:00" id="10997" opendate="2018-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avro-confluent-registry does not bundle any dependency</summary>
      <description>The flink-avro-confluent-registry is not bundling any dependencies, yet defines a relocation for the transitive jackson dependency pulled in by kafka-schema-registry-client.It is like that the registry-client should be included in the jar.</description>
      <version>1.6.2,1.7.0</version>
      <fixedVersion>1.6.3,1.7.1,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-avro-confluent-registry.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-26 01:00:00" id="11003" opendate="2018-11-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document of Java Lambda Expressions has a mistake</summary>
      <description>Documentation of Java Lambda Expressions has a mistake which may cause confusion.In the last code block, it presents some way to solve type missing problem.In 15 line:public static class MyTuple2Mapper extends MapFunction&lt;Integer, Integer&gt; {    @Override    public Tuple2&lt;Integer, Integer&gt; map(Integer i){         return Tuple2.of(i, i);     }}The second generic type in MapFunction should be Tuple2&lt;Integer, Integer&gt;</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.java.lambdas.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-28 01:00:00" id="11017" opendate="2018-11-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Time interval for window aggregations in SQL is wrongly translated if specified with YEAR_MONTH resolution</summary>
      <description>If a time interval was specified with YEAR TO MONTH resolution like e.g.:SELECT * FROM MytableGROUP BY TUMBLE(rowtime, INTERVAL '1-2' YEAR TO MONTH)it will be wrongly translated to 14 milliseconds window. We should allow for only DAY TO SECOND resolution.</description>
      <version>1.6.2,1.7.0</version>
      <fixedVersion>1.6.3,1.7.1,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.api.stream.sql.validation.WindowAggregateValidationTest.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.plan.rules.datastream.DataStreamLogicalWindowAggregateRule.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-28 01:00:00" id="11023" opendate="2018-11-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update LICENSE and NOTICE files for flink-connectors</summary>
      <description>Similar to FLINK-10987 we should also update the LICENSE and NOTICE files for flink-connectors.</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka-0.9.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka-0.11.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka-0.10.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch5.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-elasticsearch2.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-1-5 01:00:00" id="11079" opendate="2018-12-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip deployment for flnk-storm-examples</summary>
      <description>Similar to FLINK-10987 we should also update the LICENSE and NOTICE for flink-storm-examples. This project creates several fat example jars that are deployed to maven central.Alternatively we could about dropping these examples.</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm-examples.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.ListSerializerSnapshot.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.ListSerializer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.base.CollectionSerializerConfigSnapshot.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-1-22 01:00:00" id="1112" opendate="2014-9-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add GroupSorting with KeySelectors</summary>
      <description>Group sorting is currently only supported for field-index keys and not for KeySelectors.This feature was requested.</description>
      <version>None</version>
      <fixedVersion>0.9,0.8.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.GroupReduceITCase.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.GroupReduceOperator.java</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.operators.translation.CustomPartitioningGroupingKeySelectorTest.scala</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.GroupReduceITCase.java</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.GroupedDataSet.scala</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.UnsortedGrouping.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.SortedGrouping.java</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.custompartition.GroupingKeySelectorTranslationTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-12-10 01:00:00" id="11120" opendate="2018-12-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TIMESTAMPADD function handles TIME incorrectly</summary>
      <description>The error occur when timestampadd(MINUTE, 1, time '01:00:00') is executed:java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Longat org.apache.calcite.rex.RexBuilder.clean(RexBuilder.java:1520)at org.apache.calcite.rex.RexBuilder.makeLiteral(RexBuilder.java:1318)at org.apache.flink.table.codegen.ExpressionReducer.reduce(ExpressionReducer.scala:135)at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressionsInternal(ReduceExpressionsRule.java:620)at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressions(ReduceExpressionsRule.java:540)at org.apache.calcite.rel.rules.ReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(ReduceExpressionsRule.java:288)I think it should meet the following conditions:expressionExpect the resulttimestampadd(MINUTE, -1, time '00:00:00')23:59:00timestampadd(MINUTE, 1, time '00:00:00')00:01:00timestampadd(MINUTE, 1, time '23:59:59')00:00:59timestampadd(SECOND, 1, time '23:59:59')00:00:00timestampadd(HOUR, 1, time '23:59:59')00:59:59This problem seems to be a bug in calcite. I have submitted isuse to calcite. The following is the link.CALCITE-2699</description>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.calls.ScalarOperators.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ScalarFunctionsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-12-12 01:00:00" id="11136" opendate="2018-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the logical of merge for DISTINCT aggregates</summary>
      <description>The logic of merge for DISTINCT aggregates has bug. For the following query:SELECT c, COUNT(DISTINCT b), SUM(DISTINCT b), SESSION_END(rowtime, INTERVAL '0.005' SECOND)FROM MyTableGROUP BY SESSION(rowtime, INTERVAL '0.005' SECOND), cthe following exception will be thrown:Caused by: java.lang.ClassCastException: org.apache.flink.types.Row cannot be cast to java.lang.Integerat scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:101)at scala.math.Numeric$IntIsIntegral$.plus(Numeric.scala:58)at org.apache.flink.table.functions.aggfunctions.SumAggFunction.accumulate(SumAggFunction.scala:50)at GroupingWindowAggregateHelper$18.mergeAccumulatorsPair(Unknown Source)at org.apache.flink.table.runtime.aggregate.AggregateAggFunction.merge(AggregateAggFunction.scala:66)at org.apache.flink.table.runtime.aggregate.AggregateAggFunction.merge(AggregateAggFunction.scala:33)at org.apache.flink.runtime.state.heap.HeapAggregatingState.mergeState(HeapAggregatingState.java:117)at org.apache.flink.runtime.state.heap.AbstractHeapMergingState$MergeTransformation.apply(AbstractHeapMergingState.java:102)at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.transform(CopyOnWriteStateTable.java:463)at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.transform(CopyOnWriteStateTable.java:341)at org.apache.flink.runtime.state.heap.AbstractHeapMergingState.mergeNamespaces(AbstractHeapMergingState.java:91)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator$2.merge(WindowOperator.java:341)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator$2.merge(WindowOperator.java:311)at org.apache.flink.streaming.runtime.operators.windowing.MergingWindowSet.addWindow(MergingWindowSet.java:212)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.processElement(WindowOperator.java:311)at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:202)at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:105)at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)at org.apache.flink.runtime.taskmanager.Task.run(Task.java:704)at java.lang.Thread.run(Thread.java:745)</description>
      <version>None</version>
      <fixedVersion>1.6.3,1.7.1,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-12-12 01:00:00" id="11142" opendate="2018-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Undefined behavior in the conversion from DataStream/DataSet to Table</summary>
      <description>When we try to convert DataStream/DataSet to Table. There are two ways of adding schema information, ByName or ByPosition.This feature first proposed in this pr.In ByPosition mode, the current code does not check if the number of fields less than its in  DataStream/DataSet. This may cause undefined behavior, e.g. make a projection in ByPosition mode.We can either fix it by adding some checking or regard this as a feature and just improve the doc to clarify it. In my opinion, the latter way seems better.twalthr Could you take a look at it when you free?</description>
      <version>1.5.5,1.6.2,1.7.0</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-1-20 01:00:00" id="11207" opendate="2018-12-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Apache commons-compress from 1.4.1 to 1.18</summary>
      <description>There is at least one security vulnerability in the current version that we should address by upgrading to 1.18+:https://app.snyk.io/vuln/SNYK-JAVA-ORGAPACHECOMMONS-32473</description>
      <version>1.5.5,1.6.2,1.7.0,1.8.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">NOTICE-binary</file>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2-uber.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-swift-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-29 01:00:00" id="11232" opendate="2018-12-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Empty Start Time of sub-task on web dashboard</summary>
      <description/>
      <version>1.5.5,1.6.2,1.6.3,1.7.0,1.7.1</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.JobVertexDetailsInfo.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-15 01:00:00" id="11336" opendate="2019-1-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink HA didn&amp;#39;t remove ZK metadata</summary>
      <description>Flink HA didn't remove ZK metadatasuch as go to zk cli  : ls /flinkone i suggest we should delete this metadata when the application  cancel or throw exception</description>
      <version>None</version>
      <fixedVersion>1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.leaderelection.ZooKeeperLeaderRetrievalTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServices.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-30 01:00:00" id="11469" opendate="2019-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix example in "Tuning Checkpoints and Large State" documentation</summary>
      <description>Sample code for subtitle Tuning RocksDB in Tuning Checkpoints and Large State is wrong  Affects Version：All versions after 1.1  </description>
      <version>1.6.2,1.6.3,1.6.4,1.7.0,1.7.1,1.7.2,1.8.0</version>
      <fixedVersion>1.6.4,1.7.2,1.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.state.large.state.tuning.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-13 01:00:00" id="1157" opendate="2014-10-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document TaskManager Slots</summary>
      <description>Slots are not explained in the documentation.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.streaming.guide.md</file>
      <file type="M">docs.setup.quickstart.md</file>
      <file type="M">docs.config.md</file>
      <file type="M">docs.cluster.setup.md</file>
      <file type="M">docs.cluster.execution.md</file>
    </fixedFiles>
  </bug>
</bugrepository>