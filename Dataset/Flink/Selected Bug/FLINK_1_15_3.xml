<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  
  
  
  <bug fixdate="2022-9-26 01:00:00" id="29112" opendate="2022-8-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Print the lookup join hint on the node `Correlate` in the origin RelNode tree for easier debuging</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.join.LookupJoinTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.RelTreeWriterImpl.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-1-8 01:00:00" id="29231" opendate="2022-9-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PyFlink UDAF produces different results in the same sliding window</summary>
      <description>It seems that PyFlink udtaf produces different results in the same sliding window. It can be reproduced with the given code and input. It is not always happening but the possibility is relatively high.The incorrect output is the following: We can see that the output contains different `val_sum` at `window_time` 2022-01-01 00:01:59.999.</description>
      <version>1.15.3</version>
      <fixedVersion>1.17.0,1.16.1,1.15.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonRowTimeBoundedRowsOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonRowTimeBoundedRangeOperatorTest.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperatorTest.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-11-16 01:00:00" id="29322" opendate="2022-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose savepoint format on Web UI</summary>
      <description>Savepoint format is not exposed on the Web UI, thus users should remember how they triggered it.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointingStatisticsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.checkpoints.CheckpointStatistics.java</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.job-checkpoints.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.checkpoints.detail.job-checkpoints-detail.component.html</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.interfaces.job-checkpoint.ts</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-1-20 01:00:00" id="29363" opendate="2022-9-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow web ui to fully redirect to other page</summary>
      <description>In a streaming platform system, web ui usually integrates with internal authentication and authorization system. Given the validation failed, the request needs to be redirected to a landing page. It does't work for AJAX request. It will be great to have the web ui configurable to allow auto full redirect.</description>
      <version>1.15.3</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.app.interceptor.ts</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-10-28 01:00:00" id="29455" opendate="2022-9-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add OperatorIdentifier</summary>
      <description>Add a class for identifying operators, that supports both uids and uidhashes, and integrate into the low-level APIs.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.WindowSavepointReader.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.SavepointWriter.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.SavepointReader.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.runtime.metadata.SavepointMetadataV2.java</file>
      <file type="M">flink-libraries.flink-state-processing-api.src.main.java.org.apache.flink.state.api.EvictingWindowSavepointReader.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-4-30 01:00:00" id="2946" opendate="2015-10-30 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add orderBy() to Table API</summary>
      <description>In order to implement a FLINK-2099 prototype that uses the Table APIs code generation facilities, the Table API needs a sorting feature.I would implement it the next days. Ideas how to implement such a sorting feature are very welcome. Is there any more efficient way instead of .sortPartition(...).setParallism(1)? Is it better to sort locally on the nodes first and finally sort on one node afterwards?</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils.src.main.java.org.apache.flink.test.util.TestBaseUtils.java</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.table.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.rules.FlinkRuleSets.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetRel.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.BatchScan.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.expressions.ExpressionParser.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.scala.table.expressionDsl.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-1-10 01:00:00" id="29558" opendate="2022-10-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use select count(*) from xxx; and get SQL syntax</summary>
      <description>Hi, I use flink sql to make kafka records to mysql.so I create these 2 tables in flink sql,here is the mysql ,and I created the table in mysql before I did the insert action in flink sql. CREATE TABLE mysql_MyUserTable ( id STRING, name STRING, age STRING, status STRING, PRIMARY KEY (id) NOT ENFORCED) WITH ( 'connector' = 'jdbc', 'url' = 'jdbc:mysql://10.19.29.170:3306/fromflink152', 'table-name' = 'users', 'username' = 'root', 'password' = '******');In mysql, I created database "fromflink152" then created the table like this way CREATE TABLE `users` ( `id` varchar(64) NOT NULL DEFAULT '', `name` varchar(255) DEFAULT NULL, `age` varchar(255) DEFAULT NULL, `status` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`)) After executed insert sql,I found 'select * from mysql_MyUserTable' can get correct result,but ’select count(&amp;#42;) from mysql_MyUserTable‘ or ’select count(id) from mysql_MyUserTable‘ ,the collect job in flink app keep restarting again and again.The exception is: So I wonder which config that I missed about the table in flink or mysql side</description>
      <version>1.15.3</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.TableScanTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.AggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.TableSourceTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.SortAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.HashAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRuleTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRule.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-10-10 01:00:00" id="29568" opendate="2022-10-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary whitespace in request/response blocks</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.sql.gateway.html</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-10-10 01:00:00" id="29569" opendate="2022-10-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace usages of deprecated expand shortcode</summary>
      <description>The expand shortcode is deprecated; use &lt;details&gt; instead.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.rest.RestAPIDocGenerator.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.sql.gateway.html</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
      <file type="M">docs.content.docs.dev.table.sql.queries.overview.md</file>
      <file type="M">docs.content.docs.dev.table.common.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.queries.overview.md</file>
      <file type="M">docs.assets..custom.scss</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-11-13 01:00:00" id="29626" opendate="2022-10-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Akka to 2.6.20</summary>
      <description>Update Akka to the latest 2.6 version that's still under Apache license</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-rpc.flink-rpc-akka.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-rpc.flink-rpc-akka.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-11-14 01:00:00" id="29644" opendate="2022-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reference Kubernetes operator from Flink Kubernetes deploy docs</summary>
      <description>Currently the Flink deployment/resource provider docs provide some information for the Standalone and Native Kubernetes integration without any reference to the operator. We should provide a bit more visibility and value to the users by directly proposing to use the operator when considering Flink on Kubernetes. We should make the point that for most users the easiest way to use Flink on Kubernetes is probably through the operator (where they can now benefit from both standalone and native integration under the hood). This should help us avoid cases where a new user completely misses the existence of the operator when starting out based on the Flink docs.</description>
      <version>1.16.0,1.17.0,1.15.3</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.resource-providers.standalone.kubernetes.md</file>
      <file type="M">docs.content.docs.deployment.resource-providers.native.kubernetes.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-11-31 01:00:00" id="29812" opendate="2022-10-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove deprecated Netty API usages</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.RestServerEndpointITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.netty.NettyConnectionManagerTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestServerEndpointConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClient.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.KeepAliveWrite.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.util.HandlerRedirectUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.router.RouterHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.FileUploadHandler.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyServer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyClient.java</file>
      <file type="M">flink-runtime-web.src.test.java.org.apache.flink.runtime.webmonitor.testutils.HttpTestClient.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.utils.WebFrontendBootstrap.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.PipelineErrorHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.HttpRequestHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandler.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-runtime.src.test.java.org.apache.flink.queryablestate.network.KvStateServerTest.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.Client.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.network.AbstractServerBase.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-7-6 01:00:00" id="2985" opendate="2015-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow different field names for unionAll() in Table API</summary>
      <description>The recently merged `unionAll` operator checks if the field names of the left and right side are equal. Actually, this is not necessary. The union operator in SQL checks only the types and uses the names of left side.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.util.serialization.JsonRowDeserializationSchema.java</file>
      <file type="M">flink-streaming-connectors.flink-connector-kafka-base.src.main.java.org.apache.flink.streaming.connectors.kafka.KafkaTableSource.java</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.stream.table.UnionITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.table.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.api.scala.batch.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.TypeConverter.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.typeutils.RowTypeInfo.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.schema.TableSourceTable.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.nodes.dataset.DataSetAggregate.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.plan.logical.operators.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.api.table.codegen.CodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-12-8 01:00:00" id="29925" opendate="2022-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>table ui of configure value is strange</summary>
      <description>As shown in the figure below, when the configure value is very large, the ui of the table is a bit strange</description>
      <version>1.15.3</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.configuration.job-configuration.component.less</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.pages.job.configuration.job-configuration.component.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-11-16 01:00:00" id="30041" opendate="2022-11-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setup conjars https mirror</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.google-mirror-settings.xml</file>
      <file type="M">tools.ci.alibaba-mirror-settings.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-12-15 01:00:00" id="30424" opendate="2022-12-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add source operator restore readerState log to distinguish split is from newPartitions or split state</summary>
      <description>When a job start firstly, we can find 'assignPartitions' from log。but if source recover from state, we can not distinguish the newPartitions is from timed discover thread or from reader task state.  We can add a helper log to distinguish and confirm the reader using split state in recover situation.  it's very useful for troubleshooting.  </description>
      <version>1.16.0,1.15.3,1.16.1</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperator.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2023-1-11 01:00:00" id="30631" opendate="2023-1-11 00:00:00" resolution="Done">
    <buginformation>
      <summary>Limit the max number of subpartitons consumed by each downstream task</summary>
      <description>In the current implementation(FLINK-25035), when the upstream vertex parallelism is much greater than the downstream vertex parallelism, it may lead to a large number of channels in the downstream tasks(for example, A -&gt; B, all to all edge, max parallelism is 1000. If parallelism of A is 1000, parallelism of B is decided to be 1, then the only subtask of B will consume 1000 * 1000 subpartitions), resulting in a large overhead for processing channels.In this ticket, we temporarily address this issue by limiting the max number of subpartitons consumed by each downstream task. The ultimate solution should be to support single channel consume multiple subpartitons.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismAndInputInfosDeciderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.DefaultVertexParallelismAndInputInfosDecider.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2023-2-1 01:00:00" id="30864" opendate="2023-2-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optional pattern at the start of a group pattern not working</summary>
      <description>The optional pattern at the start of a group pattern turns out be "not optional", e.g.Pattern.&lt;String&gt;begin("A").next(Pattern.&lt;String&gt;begin("B").optional().next("C")).next("D")cannot match sequence "a1 c1 d1".</description>
      <version>1.15.3,1.16.1</version>
      <fixedVersion>1.17.0,1.15.4,1.16.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-cep.src.test.java.org.apache.flink.cep.nfa.GroupITCase.java</file>
      <file type="M">flink-libraries.flink-cep.src.main.java.org.apache.flink.cep.nfa.compiler.NFACompiler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-2-6 01:00:00" id="30905" opendate="2023-2-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>doc generation fails with "concurrent map read and map write"</summary>
      <description>We experience a build failure in master (but since it looks like a Hugo issue, I added already released version to the affected versions as well) with a concurrent map read and map write within hugo:https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45707&amp;view=logs&amp;j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&amp;t=ddd6d61a-af16-5d03-2b9a-76a279badf98Start building sites … fatal error: concurrent map read and map writegoroutine 233 [running]:runtime.throw(0x23054e4, 0x21) /usr/local/go/src/runtime/panic.go:1116 +0x72 fp=0xc0016ea860 sp=0xc0016ea830 pc=0x4f5ff2runtime.mapaccess1_faststr(0x1f71280, 0xc000764a20, 0xc000aa60e1, 0x18, 0xcd) /usr/local/go/src/runtime/map_faststr.go:21 +0x465 fp=0xc0016ea8d0 sp=0xc0016ea860 pc=0x4d29c5[...]</description>
      <version>1.17.0,1.15.3,1.16.1</version>
      <fixedVersion>1.17.0,1.15.4,1.16.2</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.docs.sh</file>
      <file type="M">.github.workflows.docs.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2023-6-6 01:00:00" id="32267" opendate="2023-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update testcontainers dependency to v1.18.3</summary>
      <description>Among others there are Fixes the issue of missing root cause in container launch TimeoutException (e.g. SSLHandshakeException) Make sure we don't hide exceptions from waitUntilContainerStartedalso full list is at https://github.com/testcontainers/testcontainers-java/releases/tag/1.18.0https://github.com/testcontainers/testcontainers-java/releases/tag/1.18.1https://github.com/testcontainers/testcontainers-java/releases/tag/1.18.2https://github.com/testcontainers/testcontainers-java/releases/tag/1.18.3 </description>
      <version>None</version>
      <fixedVersion>elasticsearch-4.0.0,1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-7-15 01:00:00" id="32349" opendate="2023-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support atomic for CREATE TABLE AS SELECT(CTAS) statement</summary>
      <description>For detailed information, see FLIP-305https://cwiki.apache.org/confluence/display/FLINK/FLIP-305%3A+Support+atomic+for+CREATE+TABLE+AS+SELECT%28CTAS%29+statement</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.delegation.PlannerBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.delegation.DefaultExecutor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.test.java.org.apache.flink.table.utils.ExecutorMock.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.CreateTableASOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.delegation.Executor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.TableConfigOptions.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.executor.python.ChainingOptimizingExecutor.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.execution.JobStatusHook.java</file>
      <file type="M">docs.layouts.shortcodes.generated.table.config.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-6-15 01:00:00" id="32351" opendate="2023-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce base interfaces for call procedure</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.Catalog.java</file>
      <file type="M">flink-python.pyflink.table.catalog.py</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2023-7-3 01:00:00" id="32516" opendate="2023-7-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to parse [CREATE OR ] REPLACE TABLE AS statement</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2023-7-3 01:00:00" id="32517" opendate="2023-7-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to execute [CREATE OR] REPLACE TABLE AS statement</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.utils.OperationConverterUtils.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlCreateTableConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConverters.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ModifyOperationVisitor.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlReplaceTableAs.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2023-7-3 01:00:00" id="32519" opendate="2023-7-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add doc for [CREATE OR] REPLACE TABLE AS statement</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.create.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.create.md</file>
    </fixedFiles>
  </bug>
</bugrepository>