<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  
  
  <bug fixdate="2019-11-10 01:00:00" id="14693" opendate="2019-11-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>python tox checks fails on travis</summary>
      <description>ImportError: cannot import name 'ensure_is_path' from 'importlib_metadata._compat' (/home/travis/build/apache/flink/flink-python/dev/.conda/lib/python3.7/site-packages/importlib_metadata/_compat.py)============tox checks... &amp;#91;FAILED&amp;#93;============see: https://api.travis-ci.org/v3/job/609614353/log.txt</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-9 01:00:00" id="1496" opendate="2015-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Events at unitialized input channels are lost</summary>
      <description>If a program sends an event backwards to the producer task, it might happen that some of it input channels have not been initialized yet (UnknownInputChannel). In that case, the events are lost and will never be received at the producer.</description>
      <version>0.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.io.network.api.reader.BufferReaderTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.reader.BufferReader.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-2-11 01:00:00" id="1519" opendate="2015-2-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web frontend task state mismatch when cancelling</summary>
      <description>When cancelling a task via the web interface, the web interface immediately switches the tasks to the CANCELED state although they actually transition from CANCELLING to CANCELED.</description>
      <version>0.10.0</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.resources.web-docs-infoserver.js.jobmanagerFrontend.js</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-12-11 01:00:00" id="15192" opendate="2019-12-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split &amp;#39;SQL&amp;#39; page into multiple sub pages for better structure</summary>
      <description>with FLINK-15190, we are gonna add a bunch of ddl which makes the page too long and not really readable.I suggest split "SQL" page into sub pages of "SQL DDL", "SQL DML", "SQL DQL", and others if needed.assigned to Terry temporarily. cc Jark Wu Jingsong Lee As example, the SQL doc directory of Hive looks like below, which is a lot better that Flink's current oneCHILD PAGESPagesLanguageManualLanguageManual CliLanguageManual DDLLanguageManual DMLLanguageManual SelectLanguageManual JoinsLanguageManual LateralViewLanguageManual UnionLanguageManual SubQueriesLanguageManual SamplingLanguageManual ExplainLanguageManual VirtualColumnsConfiguration PropertiesLanguageManual ImportExportLanguageManual AuthorizationLanguageManual TypesLiteralsLanguageManual VariableSubstitutionLanguageManual ORCLanguageManual WindowingAndAnalyticsLanguageManual IndexingLanguageManual JoinOptimizationLanguageManual LZOLanguageManual CommandsParquetEnhanced Aggregation, Cube, Grouping and RollupFileFormatsHive HPL/SQL</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.types.zh.md</file>
      <file type="M">docs.dev.table.types.md</file>
      <file type="M">docs.dev.table.tuning.streaming.aggregation.optimization.zh.md</file>
      <file type="M">docs.dev.table.tuning.streaming.aggregation.optimization.md</file>
      <file type="M">docs.dev.table.tuning.index.zh.md</file>
      <file type="M">docs.dev.table.tuning.index.md</file>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.zh.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.md</file>
      <file type="M">docs.dev.table.streaming.query.configuration.zh.md</file>
      <file type="M">docs.dev.table.streaming.query.configuration.md</file>
      <file type="M">docs.dev.table.streaming.match.recognize.zh.md</file>
      <file type="M">docs.dev.table.streaming.match.recognize.md</file>
      <file type="M">docs.dev.table.streaming.joins.zh.md</file>
      <file type="M">docs.dev.table.streaming.joins.md</file>
      <file type="M">docs.dev.table.streaming.index.zh.md</file>
      <file type="M">docs.dev.table.streaming.index.md</file>
      <file type="M">docs.dev.table.streaming.dynamic.tables.zh.md</file>
      <file type="M">docs.dev.table.streaming.dynamic.tables.md</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.table.sql.zh.md</file>
      <file type="M">docs.dev.table.sql.md</file>
      <file type="M">docs.dev.table.modules.zh.md</file>
      <file type="M">docs.dev.table.modules.md</file>
      <file type="M">docs.dev.table.index.zh.md</file>
      <file type="M">docs.dev.table.index.md</file>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
      <file type="M">docs.dev.table.hive.hive.catalog.zh.md</file>
      <file type="M">docs.dev.table.hive.hive.catalog.md</file>
      <file type="M">docs.dev.table.functions.systemFunctions.zh.md</file>
      <file type="M">docs.dev.table.functions.systemFunctions.md</file>
      <file type="M">docs.dev.table.functions.index.zh.md</file>
      <file type="M">docs.dev.table.functions.index.md</file>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
      <file type="M">docs.dev.table.config.zh.md</file>
      <file type="M">docs.dev.table.config.md</file>
      <file type="M">docs.dev.table.common.zh.md</file>
      <file type="M">docs.dev.table.common.md</file>
      <file type="M">docs.dev.table.catalogs.zh.md</file>
      <file type="M">docs.dev.table.catalogs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-21 01:00:00" id="15700" opendate="2020-1-21 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Improve Python API Tutorial doc</summary>
      <description>Adds the content of preparing input data in the Python API Tutorial doc</description>
      <version>None</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.getting-started.walkthroughs.python.table.api.zh.md</file>
      <file type="M">docs.getting-started.walkthroughs.python.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-10 01:00:00" id="15970" opendate="2020-2-10 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Optimize the Python UDF execution to only serialize the value</summary>
      <description>Currently, the window/timestamp/pane info are also serialized and sent between the Java operator and the Python worker. These informations are useless and after bumping beam to 2.19.0(BEAM-7951), optimization is possible to not serialize these fields.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.AbstractPythonStatelessFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.python.AbstractPythonFunctionRunner.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-10 01:00:00" id="15972" opendate="2020-2-10 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add Python building blocks to make sure the basic functionality of Python TableFunction could work</summary>
      <description>We need to add a few Python building blocks such as TableFunctionOperation, TableFunctionRowCoder, etc for Python TableFunction execution. TableFunctionOperation is subclass of Operation in Beam and TableFunctionRowCoder, etc are subclasses of Coder in Beam. These classes will be registered into the Beam’s portability framework to make sure they take effects.This PR makes sure that a basic end to end Python UDTF could be executed.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.datastream.DataStreamPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.CommonPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecPythonCorrelate.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.nodes.common.CommonPythonCalc.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.util.python.PythonTableUtils.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.codegen.PythonFunctionCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.utils.python.PythonTableUtils.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.PythonFunctionCodeGenerator.scala</file>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.table.AbstractPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.AbstractPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.scalar.AbstractGeneralPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.AbstractPythonStatelessFunctionRunner.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-10 01:00:00" id="15974" opendate="2020-2-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to use the Python UDF directly in the Python Table API</summary>
      <description>Currently, a Python UDF has been registered before using in Python Table API, e.g.t_env.register_function("inc", inc)table.select("inc(id)") \ .insert_into("sink")It would be great if we could support to use Python UDF directly in the Python Table API, e.g.table.select(inc("id")) \ .insert_into("sink")</description>
      <version>None</version>
      <fixedVersion>1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.FlinkTypeFactory.scala</file>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udtf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udf.py</file>
      <file type="M">flink-python.pyflink.table.expressions.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-25 01:00:00" id="1610" opendate="2015-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Java docs do not build</summary>
      <description>Among a bunch of warnings, I get the following error which prevents the java doc generation from finishing:javadoc: error - com.sun.tools.doclets.internal.toolkit.util.DocletAbortException: com.sun.tools.javac.code.Symbol$CompletionFailure: class file for akka.testkit.TestKit not foundCommand line was: /System/Library/Frameworks/JavaVM.framework/Versions/CurrentJDK/Home/bin/javadoc -Xdoclint:none @options @packages at org.apache.maven.plugin.javadoc.AbstractJavadocMojo.executeJavadocCommandLine(AbstractJavadocMojo.java:5074) at org.apache.maven.plugin.javadoc.AbstractJavadocMojo.executeReport(AbstractJavadocMojo.java:1999) at org.apache.maven.plugin.javadoc.JavadocReport.generate(JavadocReport.java:130) at org.apache.maven.plugin.javadoc.JavadocReport.execute(JavadocReport.java:315) at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80) at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51) at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355) at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155) at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584) at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216) at org.apache.maven.cli.MavenCli.main(MavenCli.java:160) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:483) at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289) at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229) at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415) at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.9.2,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-3-16 01:00:00" id="1709" opendate="2015-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for programs with slot count exceeding parallelism</summary>
      <description>Currently, we can't run programs with higher parallelism than available slots.For example, if we currently have a map-reduce program and 4 task slots configured (e.g. 2 task managers with 2 slots per task manager), the map and reduce tasks will be scheduled with pipelined results and the same parallelism in shared slots. Setting the parallelism to more than available slots will result in a NoResourcesAvailableException.As a first step to support these kinds of programs, we can add initial support for this when running in batch mode (after https://github.com/apache/flink/pull/471 is merged).This is easier than the original pipelined scenario, because the map tasks can be deployed after each other to produce the blocking result. The blocking result can then be consumed after all map tasks produced their result. The mechanism in #471 to deploy result receivers can be used for this and should not need any modifications.</description>
      <version>0.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.ExecutionVertex.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.executiongraph.Execution.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.deployment.ResultPartitionDeploymentDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-15 01:00:00" id="17150" opendate="2020-4-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce Canal format to support reading canal changelogs</summary>
      <description>Introduce CanalFormatFactory and CanalRowDeserializationSchema to read canal changelogs.CREATE TABLE my_table ( ...) WITH ( 'connector'='...', -- e.g. 'kafka' 'format'='canal-json', 'canal-json.ignore-parse-errors'='true' -- default false);</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json.src.main.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-19 01:00:00" id="17244" opendate="2020-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Getting Started / Overview: training and python</summary>
      <description>The Getting Started page needs a bit of general editing, and should it also mention the Training section and the Python Table API walkthrough.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.getting-started.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-23 01:00:00" id="17340" opendate="2020-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update docs which related to default planner changes</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.index.zh.md</file>
      <file type="M">docs.dev.table.index.md</file>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
      <file type="M">docs.dev.table.common.zh.md</file>
      <file type="M">docs.dev.table.common.md</file>
      <file type="M">docs.dev.table.catalogs.zh.md</file>
      <file type="M">docs.dev.table.catalogs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-23 01:00:00" id="1770" opendate="2015-3-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename the variable &amp;#39;contentAdressable&amp;#39; to &amp;#39;contentAddressable&amp;#39;</summary>
      <description>Rename the variable 'contentAdressable' to 'contentAddressable' in order to better understanding.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobServerConnection.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-3-26 01:00:00" id="1790" opendate="2015-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the redundant import code</summary>
      <description>Remove the redundant import code</description>
      <version>0.10.0</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-31 01:00:00" id="1806" opendate="2015-3-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve S3 file system error message when no access/secret key provided</summary>
      <description>If no access/secret key has been configured for S3, the error message only states that "Cannot determine access key to Amazon S3".This should give a hint about how to configure it correctly as in FLINK-1646.</description>
      <version>0.10.0</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.fs.s3.S3FileSystem.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-1-13 01:00:00" id="1877" opendate="2015-4-13 00:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Stalled Flink on Tez build</summary>
      <description>https://travis-ci.org/uce/incubator-flink/jobs/57951373https://s3.amazonaws.com/archive.travis-ci.org/jobs/57951373/log.txt</description>
      <version>0.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-tez.src.test.java.org.apache.flink.tez.test.TezProgramTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-15 01:00:00" id="1891" opendate="2015-4-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add isEmpty check when the input dir</summary>
      <description>Add the input storageDirectory empty check, if input of storageDirectory is empty, we should use tmp as the base dir</description>
      <version>0.10.0</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.blob.BlobUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-8-13 01:00:00" id="18910" opendate="2020-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create the new document structure for Python documentation according to FLIP-133</summary>
      <description>Create the following catalog structure under the "Application Development" catalog:Application Development  - Python API        - Getting Started       - User Guide          - Table API</description>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.try-flink.python.table.api.zh.md</file>
      <file type="M">docs.try-flink.python.table.api.md</file>
      <file type="M">docs.ops.python.shell.zh.md</file>
      <file type="M">docs.ops.python.shell.md</file>
      <file type="M">docs.dev.table.types.md</file>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.sql.alter.zh.md</file>
      <file type="M">docs.dev.table.sql.alter.md</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
      <file type="M">docs.dev.table.python.vectorized.python.udfs.zh.md</file>
      <file type="M">docs.dev.table.python.vectorized.python.udfs.md</file>
      <file type="M">docs.dev.table.python.python.udfs.zh.md</file>
      <file type="M">docs.dev.table.python.python.udfs.md</file>
      <file type="M">docs.dev.table.python.python.types.zh.md</file>
      <file type="M">docs.dev.table.python.python.types.md</file>
      <file type="M">docs.dev.table.python.python.config.zh.md</file>
      <file type="M">docs.dev.table.python.python.config.md</file>
      <file type="M">docs.dev.table.python.metrics.zh.md</file>
      <file type="M">docs.dev.table.python.metrics.md</file>
      <file type="M">docs.dev.table.python.installation.zh.md</file>
      <file type="M">docs.dev.table.python.installation.md</file>
      <file type="M">docs.dev.table.python.index.zh.md</file>
      <file type="M">docs.dev.table.python.index.md</file>
      <file type="M">docs.dev.table.python.dependency.management.zh.md</file>
      <file type="M">docs.dev.table.python.dependency.management.md</file>
      <file type="M">docs.dev.table.python.conversion.of.pandas.zh.md</file>
      <file type="M">docs.dev.table.python.conversion.of.pandas.md</file>
      <file type="M">docs.dev.table.python.common.questions.zh.md</file>
      <file type="M">docs.dev.table.python.common.questions.md</file>
      <file type="M">docs.dev.table.functions.udfs.zh.md</file>
      <file type="M">docs.dev.table.functions.udfs.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-9-13 01:00:00" id="18913" opendate="2020-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a "TableEnvironment" document under the "Python API" -&gt; "User Guide" -&gt; "Table API" section</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.2,1.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.python.table-api-users-guide.intro.to.table.api.zh.md</file>
      <file type="M">docs.dev.python.table-api-users-guide.intro.to.table.api.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-29 01:00:00" id="1957" opendate="2015-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve error handling of transport failures</summary>
      <description>Currently, transport failures fail the receiver silently w/o proper error attribution to the transport.We want the following behaviour: Attributed to the receiver Receiver fails with an Exception Subpartition on the sender side stays sane Netty channel needs to be closed (as result of transport error) and data transfer aborted</description>
      <version>0.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-29 01:00:00" id="1959" opendate="2015-4-29 00:00:00" resolution="Pending Closed">
    <buginformation>
      <summary>Accumulators BROKEN after Partitioning</summary>
      <description>while running the Accumulator example in https://github.com/Elbehery/flink/blob/master/flink-examples/flink-java-examples/src/main/java/org/apache/flink/examples/java/relational/EmptyFieldsCountAccumulator.java, I tried to alter the data flow with "PartitionByHash" function before applying "Filter", and the resulted accumulator was NULL. By Debugging, I could see the accumulator in the RunTime Map. However, by retrieving the accumulator from the JobExecutionResult object, it was NULL. The line caused the problem is "file.partitionByHash(1).filter(new EmptyFieldFilter())" instead of "file.filter(new EmptyFieldFilter())"</description>
      <version>0.10.0</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.RegularPactTask.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.accumulators.AccumulatorHelper.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2021-5-22 01:00:00" id="21095" opendate="2021-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove legacy slotmanagement profile</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.test.controller.sh</file>
      <file type="M">tools.ci.stage.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.ReactiveModeITCase.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.AdaptiveSchedulerITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerSlotSharingITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerSimpleITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.ResourceManagerHATest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerFactoryTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactoryTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRuntimeServicesConfiguration.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.ResourceManagerRuntimeServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ClusterOptions.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-1 01:00:00" id="2123" opendate="2015-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix CLI client logging</summary>
      <description>The CLI client complains about missing log4j configuration and prints too much information.</description>
      <version>0.10.0</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-10-4 01:00:00" id="2156" opendate="2015-6-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala modules cannot create logging file</summary>
      <description>The Scala only modules flink-scala-shell and flink-ml use Maven's scalatest plugin to run their tests. The scalatest plugin has no forkNumber, though. Therefore, the logging fails to create the logging file as specified in the log4j-travis.properties file.We can fix this issue by giving these two modules different log4j.properties files which don't require a forkNumber. Or we fix the forkNumber to 1.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-scala-shell.pom.xml</file>
      <file type="M">flink-staging.flink-ml.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-10-14 01:00:00" id="2210" opendate="2015-6-14 00:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Table API aggregate by ignoring null values</summary>
      <description>Attempting to aggregate on columns which may have null values results in NullPointerException.</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-table.src.test.scala.org.apache.flink.api.scala.table.test.AggregationsITCase.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.runtime.ExpressionAggregateFunction.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.expressions.comparison.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.expressions.aggregations.scala</file>
      <file type="M">flink-staging.flink-table.src.main.scala.org.apache.flink.api.table.codegen.ExpressionCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-15 01:00:00" id="2216" opendate="2015-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Examples directory contains `flink-java-examples-0.9.0-javadoc.jar`</summary>
      <description>Example directory of the packaged code contains `flink-java-examples-0.9.0-javadoc.jar`.</description>
      <version>0.10.0</version>
      <fixedVersion>0.9,0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-15 01:00:00" id="2226" opendate="2015-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fail YARN application on failed single-job YARN cluster</summary>
      <description>Users find it confusing that jobs submitted in single-job YARN cluster mode leave the Flink YARN application in state SUCCEEDED after the job fails.</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMasterActor.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationClient.scala</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnCluster.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClient.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.yarn.AbstractFlinkYarnCluster.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.jobgraph.JobStatus.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.program.Client.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.FlinkYarnSessionCli.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-4-13 01:00:00" id="22260" opendate="2021-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Source schema in CREATE TABLE LIKE statements is not inferred correctly</summary>
      <description>When using a LIKE statement such as in the following (assume some_sink and some_source to be two tables with the same schema)CREATE TEMPORARY TABLE b LIKE some_sinkINSERT INTO b SELECT * FROM some_sourcethe source schema for the INSERT operation is not actually inferred correctly, causing the entire query to fail:org.apache.flink.table.api.ValidationException: Column types of query result and sink for registered table 'default.default.b' do not match.Cause: Different number of columns.Query schema: &amp;#91;name: STRING, ts: TIMESTAMP(3) *ROWTIME*&amp;#93;Sink schema:  []</description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.utils.OperationMatchers.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.java.org.apache.flink.table.planner.operations.SqlCreateTableConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-2-16 01:00:00" id="22320" opendate="2021-4-16 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add documentation for new introduced ALTER TABLE statements</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sql.alter.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sql.alter.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-19 01:00:00" id="2246" opendate="2015-6-19 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add chained combine driver strategy for ReduceFunction</summary>
      <description>Running the WordCount example with a text file input/output results and a manual reduce function (instead of the sum(1)) results in a combiner, which is not chained.Replace sum(1) with the following to reproduce and use a text file as input:fileOutput = true;textPath = "...";outputPath = "...";.reduce(new ReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() { @Override public Tuple2&lt;String, Integer&gt; reduce(Tuple2&lt;String, Integer&gt; value1, Tuple2&lt;String, Integer&gt; value2) throws Exception { return new Tuple2&lt;String, Integer&gt;(value1.f0, value1.f1 + value2.f1); } });</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.DriverStrategy.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-30 01:00:00" id="22540" opendate="2021-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove YAML environment file support in SQL Client</summary>
      <description>As discussed in https://cwiki.apache.org/confluence/display/FLINK/FLIP-163%3A+SQL+Client+Improvements, YAML environment file is deprecated in 1.13 version and should be removed in 1.14 version. Users are recommended to use SQL script to initialize session.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-python-functions.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-streaming.yaml</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptions.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliOptionsParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.ConfigUtil.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.CatalogEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.ConfigEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.ConfigurationEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.DeploymentEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.ExecutionEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.FunctionEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.ModuleEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.SinkTableEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.SourceSinkTableEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.SourceTableEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.TableEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.TemporalTableEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.ViewEntry.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.Environment.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.YamlConfigUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.context.DefaultContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.context.LegacyTableEnvironmentInitializer.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.context.SessionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalContextUtils.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.assembly.test-table-factories.xml</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.CliClientTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.config.YamlConfigUtilsTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.context.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.DependencyTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.EnvironmentTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.DummyTableSinkFactory.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.DummyTableSourceFactory.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.EnvironmentFileUtil.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.TestTableSinkFactoryBase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.TestTableSourceFactoryBase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.utils.UserDefinedFunctions.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.META-INF.services.org.apache.flink.table.factories.Factory</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.set.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-factory-services-file</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-catalogs.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-configuration.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-dialect.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-execution.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-factory.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-modules.yaml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-6-1 01:00:00" id="22831" opendate="2021-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop usages of legacy planner in Scala shell</summary>
      <description>Remove references to flink-table-planner in the Scala shell.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITCase.scala</file>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkILoop.scala</file>
      <file type="M">flink-scala-shell.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-1 01:00:00" id="22832" opendate="2021-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop usages of legacy planner in SQL Client</summary>
      <description>Drop legacy planner support for SQL Client</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-python-functions.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-modules.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-execution.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-dialect.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-defaults.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-configuration.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.test-sql-client-catalogs.yaml</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.EnvironmentTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.context.SessionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.context.ExecutionContextTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.config.YamlConfigUtilsTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.context.LegacyTableEnvironmentInitializer.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.context.ExecutionContext.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.config.entries.ExecutionEntry.java</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-3 01:00:00" id="22860" opendate="2021-6-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Supplement &amp;#39;HELP&amp;#39; command prompt message for SQL-Cli.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql.misc.q</file>
      <file type="M">flink-table.flink-sql-client.src.test.resources.sql-client-help-command.out</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-8-8 01:00:00" id="22914" opendate="2021-6-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Kafka new source in Table/SQL connector</summary>
      <description>Currently the Kafka Table/SQL connector is still using the legacy Kafka SourceFunction. In order to align DataStream and Table/SQL API, the new Kafka source should also be used in Table/SQL connector.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryTest.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSource.java</file>
      <file type="M">docs.content.docs.connectors.table.kafka.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.kafka.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-30 01:00:00" id="2298" opendate="2015-6-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow setting custom YARN application names through the CLI</summary>
      <description>A Flink user asked for adding an option to the YARN CLI frontend to provide a custom application name when starting Flink on YARN.</description>
      <version>0.10.0</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClient.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.yarn.AbstractFlinkYarnClient.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.FlinkYarnSessionCli.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
      <file type="M">docs.setup.yarn.setup.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-23 01:00:00" id="23112" opendate="2021-6-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Linting should be enforced for Flink UI</summary>
      <description>There is some formatting infrastructure in place, however this currently doesn't seem to work or at least be enforced in the CI.We should make sure that formatting works "out of the box" as much as possible, and definitely ensure that the CI enforces consistent formatting.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package.json</file>
      <file type="M">flink-runtime-web.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-9-23 01:00:00" id="23114" opendate="2021-6-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Address vulnerabilities in Flink UI</summary>
      <description>We should at least run npm audit and address any current, open vulnerabilities.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package.json</file>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
      <file type="M">flink-runtime-web.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-8 01:00:00" id="2327" opendate="2015-7-8 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Log the limit of open file handles at startup</summary>
      <description/>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.EnvironmentInformationTest.java</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.util.EnvironmentInformation.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-8-9 01:00:00" id="23330" opendate="2021-7-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecate toAppendStream and toRetractStream</summary>
      <description>The new toDataStream and toChangelogStream should be stable by now. They will not be marked experimental in 1.14. So we can deprecate the old methods and remove them in a couple of releases.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.bridge.scala.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.bridge.java.StreamTableEnvironment.java</file>
      <file type="M">docs.content.docs.deployment.repls.scala.shell.md</file>
      <file type="M">docs.content.zh.docs.deployment.repls.scala.shell.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-9 01:00:00" id="2339" opendate="2015-7-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent asynchronous checkpoint calls from overtaking each other</summary>
      <description>Currently, when checkpoint state materialization takes very long, and the checkpoint interval is low, the asynchronous calls to trigger checkpoints (on the sources) could overtake prior calls.We can fix that by making sure that all calls are dispatched in order by the same thread, rather than spawning a new thread for each call.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.MemoryLogger.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.api.TaskEventHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-7-15 01:00:00" id="23400" opendate="2021-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support to decode a single record for the Python coder</summary>
      <description>Currently, the Python coder always output a Python generator. This makes it impossible to separate the timers and the data into different channels.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamTableAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamGroupWindowAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughStreamAggregatePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonTableFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonScalarFunctionRunner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.runtime.utils.PassThroughPythonAggregateFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.runners.python.beam.BeamTablePythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.table.AbstractPythonTableFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.RowDataPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.arrow.RowDataArrowPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.AbstractRowDataPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.scalar.AbstractPythonScalarFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.PythonStreamGroupWindowAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.AbstractStreamArrowPythonOverWindowAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonOverWindowAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupWindowAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.AbstractBatchArrowPythonAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.arrow.AbstractArrowPythonAggregateFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.AbstractPythonStreamGroupAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.aggregate.AbstractPythonStreamAggregateOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.utils.PythonOperatorUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.runners.python.beam.BeamDataStreamPythonFunctionRunner.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.TwoInputPythonFunctionOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonTimestampsAndWatermarksOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonPartitionCustomOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonMapOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonKeyedProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonKeyedCoProcessOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonFlatMapOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonCoMapOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.PythonCoFlatMapOperator.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.operators.python.OneInputPythonFunctionOperator.java</file>
      <file type="M">flink-python.pyflink.proto.flink-fn-execution.proto</file>
      <file type="M">flink-python.pyflink.fn.execution.flink.fn.execution.pb2.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.operations.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.slow.py</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.fast.pyx</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coder.impl.fast.pxd</file>
      <file type="M">flink-python.pyflink.fn.execution.beam.beam.coders.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-10 01:00:00" id="2343" opendate="2015-7-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default garbage collector in streaming environments</summary>
      <description>When starting Flink, we don't pass any particular GC related JVM flags to the system. That means, it uses the default garbage collectors, which are the bulk parallel GCs for both old gen and new gen.For streaming applications, this results in vastly fluctuating latencies. Latencies are much more constant with either the CMS or G1 GC.I propose to make the CMS the default GC for streaming setups.G1 may become the GC of choice in the future, but fro various articles I found, it is still somewhat in "beta" status (see for example here: http://jaxenter.com/kirk-pepperdine-on-the-g1-for-java-9-118190.html )</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.taskmanager.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-8-16 01:00:00" id="23800" opendate="2021-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose newly added RocksDB native metrics</summary>
      <description>Since Flink bumped RocksDB version, we could expose more newly added RocksDB native metrics.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBProperty.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBNativeMetricOptions.java</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.native.metric.configuration.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-20 01:00:00" id="2382" opendate="2015-7-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Live Metric Reporting Does Not Work for Two-Input StreamTasks</summary>
      <description>Also, there are no tests for the live metrics in streaming.</description>
      <version>0.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.TwoInputStreamTask.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2021-9-13 01:00:00" id="24271" opendate="2021-9-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for special char in JSON_VALUE</summary>
      <description>If user has a json string:{"fields": {"system.process": &amp;#91;0.998&amp;#93;}}It is hard to write a valid json path to get 0.998.The correct json path should be '$.fields.&amp;#91;&amp;#39;&amp;#39;system.process&amp;#39;&amp;#39;&amp;#93;&amp;#91;0&amp;#93;'</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.BaseExpressions.java</file>
      <file type="M">flink-python.pyflink.table.expression.py</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-2-1 01:00:00" id="24441" opendate="2021-10-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Block SourceReader when watermarks are out of alignment</summary>
      <description>SourceReader should become unavailable once it's latest watermark is too far into the future</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.source.TestingSourceOperator.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.source.CollectingDataOutput.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.operators.SourceOperatorTestContext.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.source.WatermarkToDataOutput.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.source.TimestampsAndWatermarks.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.source.ProgressiveTimestampsAndWatermarks.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.source.NoOpTimestampsAndWatermarks.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.SourceOperator.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.source.coordinator.SourceCoordinator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-30 01:00:00" id="2445" opendate="2015-7-30 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add tests for HadoopOutputFormats</summary>
      <description>The HadoopOutputFormats and HadoopOutputFormatBase classes are not sufficiently covered by unit tests.We need tests that ensure that the methods of the wrapped Hadoop OutputFormats are correctly called.</description>
      <version>0.9.1,0.10.0</version>
      <fixedVersion>1.0.1,1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.src.test.java.org.apache.flink.api.java.hadoop.mapreduce.HadoopOutputFormatTest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-8-1 01:00:00" id="2456" opendate="2015-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The flink-hbase module dependencies hadoop-2 specifies a repository ID</summary>
      <description>In some special network environment, we can only use maven mirrors to download dependencies for mvn package.It cannot complete construction using one mirror.Error like:&amp;#91;ERROR&amp;#93; Failed to execute goal on project flink-hbase: Could not resolve dependencies for project org.apache.flink:flink-hbase:jar:0.10-SNAPSHOT: Failure to find org.apache.hbase:hbase-server:jar:0.98.11-hadoop2 in http://mirrors.ibiblio.org/pub/mirrors/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of ibiblio.org has elapsed or updates are forced -&gt; &amp;#91;Help 1&amp;#93;We need to specify a repository ID to hadoop-2 plugin so that it can be downloaded over another mirror.This will not affect the normal network environment.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-hbase.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-11-4 01:00:00" id="24771" opendate="2021-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump test dependencies mariadb-java-client to 2.7.4 and mysql-connector-java to 8.0.27</summary>
      <description>The JDBC test dependencies are still a bit outdated: org.mariadb.jdbc:mariadb-java-client can be bumped from 2.5.4 to 2.7.4 mysql:mysql-connector-java can be bumped from 8.0.20 to 8.0.27</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2021-11-16 01:00:00" id="24922" opendate="2021-11-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix spelling errors in the word "parallism"</summary>
      <description>Fix the spelling error of "parallism" in the document of SQL client.</description>
      <version>None</version>
      <fixedVersion>1.13.6,1.14.3,1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.dev.table.sqlClient.md</file>
      <file type="M">docs.content.zh.docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-7 01:00:00" id="2499" opendate="2015-8-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>start-cluster.sh can start multiple TaskManager on the same node</summary>
      <description>11562 JobHistoryServer3251 Main10596 Jps17934 RunJar6879 Main8837 Main19215 RunJar28902 DataNode6627 TaskManager642 NodeManager10408 RunJar10210 TaskManager5067 TaskManager357 ApplicationHistoryServer3540 RunJar28501 ResourceManager28572 SecondaryNameNode17630 QuorumPeerMain9069 TaskManagerIf we keep execute the start-cluster.sh, it may generate infinite TaskManagers in a single system.And the "nohup" command in the start-cluster.sh can generate nohup.out file that disturb any other nohup processes in the system.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-dist.src.main.flink-bin.bin.flink-daemon.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-22 01:00:00" id="24990" opendate="2021-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LookupJoinITCase fails on Java 17</summary>
      <description>The UserDefinedFunctionHelper validates that a given function is public.The InMemory&amp;#91;Async&amp;#93;LookupFunction classes are not public however. Probably some Java&lt;-&gt;Scala interplay that causes this to not be detected at this time.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.utils.InMemoryLookupableTableSource.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-22 01:00:00" id="24996" opendate="2021-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update CI image to contain Java 17</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-8 01:00:00" id="2500" opendate="2015-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some reviews to improve code quality</summary>
      <description>I reviewed the Streaming module and there are some suggestions to improve the code quality(.i.e reduce the complexity of the loop, fix memory leak...).</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.DataStream.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-6-22 01:00:00" id="25000" opendate="2021-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala 2.12.7 doesn&amp;#39;t compile on Java 17</summary>
      <description>Fails with "fails with /packages cannot be represented as URI" during compilation.2.12.15 was working fine.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.migration.ScalaSerializersMigrationTest.scala</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-22 01:00:00" id="25004" opendate="2021-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable spotless on Java 17</summary>
      <description>The current spotless version doesn't on Java 17. An upgrade is at this time not possible because compatible versions no longer run on Java 8.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-22 01:00:00" id="25005" opendate="2021-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add java17-target profile</summary>
      <description>Add a new profile analogous to the java11-target profile.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-11-25 01:00:00" id="25063" opendate="2021-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow calls to @VisibleForTesting from enclosing class</summary>
      <description>Pretty much FLINK-25042, just in reverse.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-architecture-tests.violations.e5126cae-f3fe-48aa-b6fb-60ae6cc3fcd5</file>
      <file type="M">flink-architecture-tests.src.test.java.org.apache.flink.architecture.rules.ApiAnnotationRules.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-11 01:00:00" id="2509" opendate="2015-8-11 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Improve error messages when user code classes are not found</summary>
      <description>When a job fails because the user code classes are not found, we should add some information about the class loader and class path into the exception message.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.ClassLoaderUtilsTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-6 01:00:00" id="25190" opendate="2021-12-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The number of TaskManagers whose status is Pending should be reported</summary>
      <description>The number of TaskManagers whose status is Pending should be reported to allow the outside world to perceive the lack of resources.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.MetricNames.java</file>
      <file type="M">docs.content.docs.ops.metrics.md</file>
      <file type="M">docs.content.zh.docs.ops.metrics.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-1-6 01:00:00" id="25193" opendate="2021-12-6 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Document claim &amp; no-claim mode</summary>
      <description>We should describe how the different restore modes work. It is important to go through the FLIP and include all NOTES in the written documentation</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.state.savepoints.md</file>
      <file type="M">docs.content.docs.ops.state.checkpoints.md</file>
      <file type="M">docs.content.docs.deployment.cli.md</file>
      <file type="M">docs.content.zh.docs.ops.state.checkpoints.md</file>
      <file type="M">docs.content.zh.docs.deployment.cli.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2021-1-6 01:00:00" id="25194" opendate="2021-12-6 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement an API for duplicating artefacts</summary>
      <description>We should implement methods that let us duplicate artefacts in a DFS. We can later on use it for cheaply duplicating shared snapshots artefacts instead of reuploading them.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.UnalignedCheckpointFailureHandlingITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.sorted.state.NonCheckpointingStorageAccess.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.util.BlockerCheckpointStreamFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ttl.mock.MockCheckpointStorage.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.testutils.TestCheckpointStreamFactory.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.testutils.BackendForTestStream.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestingCheckpointStorageAccessCoordinatorView.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.TestCheckpointStorageWorkerView.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStorageAccessTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.ChannelPersistenceITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.MemoryBackendCheckpointStorageAccess.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.memory.MemCheckpointStreamFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.filesystem.FsCheckpointStorageAccess.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.CheckpointStreamFactory.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.CheckpointStorageWorkerView.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.core.fs.EntropyInjector.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-14 01:00:00" id="2523" opendate="2015-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make task canceling interrupt interval configurable</summary>
      <description>When a task is canceled, the cancellation calls periodically "interrupt()" on the task thread, if the task thread does not cancel with a certain time.Currently, this value is hard coded to 10 seconds. We should make that time configurable.Until then, I would like to increase the value to 30 seconds, as many tasks (here I am observing it for Kafka consumers) can take longer then 10 seconds for proper cleanup.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.javaApiOperators.MapITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.taskmanager.Task.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.ExecutionConfig.java</file>
      <file type="M">docs.apis.common.index.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-7-9 01:00:00" id="25231" opendate="2021-12-9 00:00:00" resolution="Done">
    <buginformation>
      <summary>Update PyFlink to use the new type system</summary>
      <description>Currently, there are a lot of places in PyFlink Table API still using the legacy type system. We need to revisit this and migrate them to the new type system(DataType/LogicalType).</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.python.PythonTableFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.python.PythonScalarFunction.java</file>
      <file type="M">flink-python.src.test.resources.META-INF.services.org.apache.flink.table.factories.TableFactory</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.TestUpsertSink.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.TestRetractSink.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.TestCollectionTableFactory.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.TestAppendSink.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.TableFunc1.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.RowSink.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.RowCollector.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.RichFunc0.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.MaxAccumulator.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.CustomExtractor.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.CustomAssigner.java</file>
      <file type="M">flink-python.src.test.java.org.apache.flink.table.legacyutils.ByteMaxAggFunction.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.utils.python.PythonTableUtils.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.table.utils.python.PythonInputFormatTableSource.java</file>
      <file type="M">flink-python.src.main.java.org.apache.flink.streaming.api.utils.PythonTypeUtils.java</file>
      <file type="M">flink-python.pyflink.testing.source.sink.utils.py</file>
      <file type="M">flink-python.pyflink.table.udf.py</file>
      <file type="M">flink-python.pyflink.table.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udtf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.udaf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.types.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.sql.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.row.based.operation.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.udaf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.pandas.conversion.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.descriptor.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.dependency.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.correlate.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.calc.py</file>
      <file type="M">flink-python.pyflink.table.table.schema.py</file>
      <file type="M">flink-python.pyflink.table.table.result.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.table.sources.py</file>
      <file type="M">flink-python.pyflink.table.sinks.py</file>
      <file type="M">flink-python.pyflink.table.expressions.py</file>
      <file type="M">flink-python.pyflink.table.descriptors.py</file>
      <file type="M">flink-python.pyflink.datastream.stream.execution.environment.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2021-1-23 01:00:00" id="25431" opendate="2021-12-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement file-based JobResultStore</summary>
      <description>The implementation should comply to what's described in FLIP-194</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedJobResultStoreTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.zookeeper.AbstractZooKeeperHaServices.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedJobResultStore.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderElectionHaServices.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.highavailability.KubernetesHaServices.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">flink-annotations.src.main.java.org.apache.flink.annotation.docs.Documentation.java</file>
      <file type="M">docs.content.docs.deployment.config.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-1-5 01:00:00" id="25520" opendate="2022-1-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement "ALTER TABLE ... COMPACT" SQL</summary>
      <description>FLINK-25176 Introduce "ALTER TABLE ... COMPACT" syntax. In this ticket, we should implement "ALTER TABLE ... COMPACT" SQL</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.connector.source.TestManagedTableSource.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.connector.source.TestManagedSource.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.connector.sink.TestManagedTableSink.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.connector.sink.TestManagedSink.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.utils.TableTestBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.calcite.FlinkRelBuilder.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.QueryOperationConverter.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlToOperationConverter.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.TestManagedTableFactory.java</file>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.factories.FactoryUtilTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.factories.ManagedTableFactory.java</file>
      <file type="M">flink-table.flink-table-common.pom.xml</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.SourceQueryOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.operations.ddl.AlterTableCompactOperation.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.ManagedTableListener.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.CatalogManager.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-4-6 01:00:00" id="25541" opendate="2022-1-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-test-utils</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-test-utils-parent.flink-test-utils.src.test.java.org.apache.flink.metric.testutils.MetricListenerTest.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2022-3-6 01:00:00" id="25549" opendate="2022-1-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-dstl</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.state.changelog.inmemory.StateChangelogStorageTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.TestingStateChangeUploader.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.RetryingExecutorTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.FsStateChangelogWriterTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.FsStateChangelogWriterSqnTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.FsStateChangelogStorageTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.ChangelogStorageMetricsTest.java</file>
      <file type="M">flink-dstl.flink-dstl-dfs.src.test.java.org.apache.flink.changelog.fs.BatchingStateChangeUploadSchedulerTest.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-9-25 01:00:00" id="2570" opendate="2015-8-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a Triangle Count Library Method</summary>
      <description>The Gather-Sum-Apply-Scatter version of this algorithm receives an undirected graph as input and outputs the total number of triangles formed by the graph's edges. The implementation consists of three phases:1). Select neighbours with id greater than the current vertex id.Gather: no-opSum: create a set out of these neighboursApply: attach the computed values to the vertices2). Propagate each received value to neighbours with higher id (again using GSA)3). Compute the number of Triangles by verifying if the final vertex contains the sender's id in its list.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.utils.PageRankData.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.utils.MusicProfilesData.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.utils.LabelPropagationData.java</file>
      <file type="M">flink-staging.flink-gelly.src.main.java.org.apache.flink.graph.example.utils.CommunityDetectionData.java</file>
      <file type="M">docs.libs.gelly.guide.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-26 01:00:00" id="2575" opendate="2015-8-26 00:00:00" resolution="Done">
    <buginformation>
      <summary>Don&amp;#39;t activate hash table bloom filter optimisation by default</summary>
      <description>I would like to set the default value of taskmanager.runtime.hashjoin-bloom-filters to false until FLINK-2545 is fixed. This issue is to keep track of this for the upcoming release. After FLINK-2545 is fixed, this issue becomes obsolete.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-8-27 01:00:00" id="2584" opendate="2015-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ASM dependency is not shaded away</summary>
      <description>ASM is not correctly shaded away. If you build the quick start against the snapshot version, you will see the following dependencies. Robert is fixing this.[INFO] +- org.apache.flink:flink-java:jar:0.9.1:compile[INFO] | +- org.apache.flink:flink-core:jar:0.9.1:compile[INFO] | | \- commons-collections:commons-collections:jar:3.2.1:compile[INFO] | +- org.apache.flink:flink-shaded-include-yarn:jar:0.9.1:compile[INFO] | +- org.apache.avro:avro:jar:1.7.6:compile[INFO] | | +- org.codehaus.jackson:jackson-core-asl:jar:1.9.13:compile[INFO] | | +- org.codehaus.jackson:jackson-mapper-asl:jar:1.9.13:compile[INFO] | | +- com.thoughtworks.paranamer:paranamer:jar:2.3:compile[INFO] | | +- org.xerial.snappy:snappy-java:jar:1.0.5:compile[INFO] | | \- org.apache.commons:commons-compress:jar:1.4.1:compile[INFO] | | \- org.tukaani:xz:jar:1.0:compile[INFO] | +- com.esotericsoftware.kryo:kryo:jar:2.24.0:compile[INFO] | | +- com.esotericsoftware.minlog:minlog:jar:1.2:compile[INFO] | | \- org.objenesis:objenesis:jar:2.1:compile[INFO] | +- com.twitter:chill_2.10:jar:0.5.2:compile[INFO] | | +- org.scala-lang:scala-library:jar:2.10.4:compile[INFO] | | \- com.twitter:chill-java:jar:0.5.2:compile[INFO] | +- com.twitter:chill-avro_2.10:jar:0.5.2:compile[INFO] | | +- com.twitter:chill-bijection_2.10:jar:0.5.2:compile[INFO] | | | \- com.twitter:bijection-core_2.10:jar:0.7.2:compile[INFO] | | \- com.twitter:bijection-avro_2.10:jar:0.7.2:compile[INFO] | +- de.javakaffee:kryo-serializers:jar:0.36:compile[INFO] | | +- com.esotericsoftware:kryo:jar:3.0.3:compile[INFO] | | | +- com.esotericsoftware:reflectasm:jar:1.10.1:compile[INFO] | | | | \- org.ow2.asm:asm:jar:5.0.3:compile[INFO] | | | \- com.esotericsoftware:minlog:jar:1.3.0:compile[INFO] | | \- com.google.protobuf:protobuf-java:jar:2.6.1:compile</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">tools.travis.mvn.watchdog.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.flink-connector-kafka.pom.xml</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.flink-connector-elasticsearch.pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-contrib.flink-storm-compatibility.flink-storm-compatibility-core.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-4-28 01:00:00" id="25867" opendate="2022-1-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[ZH] Add ChangelogBackend documentation</summary>
      <description>In FLINK-25024, documentation for Changelog was added.Chinese version is a copy of English one and needs translation.</description>
      <version>None</version>
      <fixedVersion>1.15.0,1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.ops.state.state.backends.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-9-28 01:00:00" id="25868" opendate="2022-1-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable japicmp for all modules</summary>
      <description>While we have many modules with stability annotations, the japicmp plugin is only run on very few (essentially just dataset/-stream + metrics-core), essentially voiding the purpose of the annotations.We should look into enabling the plugin for all modules.</description>
      <version>None</version>
      <fixedVersion>1.16.0,1.15.3,elasticsearch-3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.java-ci-tools.pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-loader-bundle.pom.xml</file>
      <file type="M">flink-formats.flink-sql-protobuf.pom.xml</file>
      <file type="M">flink-formats.flink-sql-parquet.pom.xml</file>
      <file type="M">flink-formats.flink-sql-orc.pom.xml</file>
      <file type="M">flink-formats.flink-sql-json.pom.xml</file>
      <file type="M">flink-formats.flink-sql-csv.pom.xml</file>
      <file type="M">flink-formats.flink-sql-avro.pom.xml</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.pom.xml</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.pom.xml</file>
      <file type="M">flink-examples.pom.xml</file>
      <file type="M">flink-end-to-end-tests.pom.xml</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-dist-scala.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-rabbitmq.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kinesis.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.2.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.9.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch7.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-elasticsearch6.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-aws-kinesis-streams.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-aws-kinesis-firehose.pom.xml</file>
      <file type="M">flink-streaming-scala.pom.xml</file>
      <file type="M">flink-streaming-java.pom.xml</file>
      <file type="M">flink-scala.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-core.pom.xml</file>
      <file type="M">flink-java.pom.xml</file>
      <file type="M">flink-core.pom.xml</file>
      <file type="M">flink-connectors.flink-hadoop-compatibility.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-8 01:00:00" id="26011" opendate="2022-2-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Develop ArchUnit test (infra) for formats test code</summary>
      <description>ArchUnit test (infra) should be developed for formats submodules after the ArchUnit infra for test code has been built.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.flink-json-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-csv.pom.xml</file>
      <file type="M">flink-formats.flink-orc.pom.xml</file>
      <file type="M">flink-formats.flink-avro-glue-schema-registry.pom.xml</file>
      <file type="M">flink-formats.flink-orc-nohive.pom.xml</file>
      <file type="M">flink-formats.flink-parquet.pom.xml</file>
      <file type="M">flink-formats.flink-json.pom.xml</file>
      <file type="M">flink-formats.flink-compress.pom.xml</file>
      <file type="M">flink-formats.flink-avro.pom.xml</file>
      <file type="M">flink-formats.flink-sequence-file.pom.xml</file>
      <file type="M">flink-formats.flink-hadoop-bulk.pom.xml</file>
      <file type="M">flink-formats.flink-avro-confluent-registry.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-10-2 01:00:00" id="2607" opendate="2015-9-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shade plugin copies signature files from original jar into fat jar</summary>
      <description>The Quickstart project contains a Maven configuration that builds a fat jar using the Maven Shade plugin. It copies the META-INF folder of the original jar into the fat jar as well. That can lead to a SecurityException when submitting jobs to the cluster because the signature file contained in the original jar is not suitable for the fat jar. java.lang.SecurityException: Invalid signature file digest for Manifest main attributesThe solution is to change the configuration of the Shade plugin to ignore the signature files in the META-INF folder when copying the dependencies to the fat jar. See also:http://zhentao-li.blogspot.ch/2012/06/maven-shade-plugin-invalid-signature.htmlhttp://stackoverflow.com/questions/999489/invalid-signature-file-when-attempting-to-run-a-jar</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.9.2,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-quickstart.flink-quickstart-scala.src.main.resources.archetype-resources.pom.xml</file>
      <file type="M">flink-quickstart.flink-quickstart-java.src.main.resources.archetype-resources.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-2-11 01:00:00" id="26090" opendate="2022-2-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove pre FLIP-84 methods</summary>
      <description>See https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=134745878 and https://issues.apache.org/jira/browse/FLINK-16364.</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.FileSystemITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentITCase.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.bridge.scala.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.TableEnvironment.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.Table.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableImpl.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentInternal.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.internal.TableEnvironmentImpl.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.bridge.java.StreamTableEnvironment.java</file>
      <file type="M">flink-python.pyflink.table.tests.test.udf.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.sql.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.table.table.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-9-3 01:00:00" id="2614" opendate="2015-9-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scala Shell&amp;#39;s default local execution mode is broken</summary>
      <description>With recent changes on the master, the FlinkMiniCluster does not start automatically anymore. That's why the Scala Shell doesn't startup anymore:./bin/start-scala-shell.shStarting Flink Shell:Creating new local server</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2022-2-23 01:00:00" id="26320" opendate="2022-2-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update hive doc for 1 m-&gt;1 min</summary>
      <description>Time interval unit label 'm' does not match any of the recognized units: DAYS: (d | day | days), HOURS: (h | hour | hours), MINUTES: (min | minute | minutes), SECONDS: (s | sec | secs | second | seconds), MILLISECONDS: (ms | milli | millis | millisecond | milliseconds), MICROSECONDS: (µs | micro | micros | microsecond | microseconds), NANOSECONDS: (ns | nano | nanos | nanosecond | nanoseconds)‘1 m’ is misleading when we used, which is not correct. </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.connectors.table.hive.hive.read.write.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.read.write.md</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2015-9-8 01:00:00" id="2639" opendate="2015-9-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Building Flink for specific HDP versions fails</summary>
      <description>Building Flink for the latest Hadoop version in HDP 2.2 fails{{ mvn clean install -DskipTests -Dhadoop.version=2.6.0.2.2.6.0-2800 -Pvendor-repos}}fails with[ERROR] Failed to execute goal on project flink-shaded-include-yarn-tests: Could not resolve dependencies for project org.apache.flink:flink-shaded-include-yarn-tests:jar:0.10-SNAPSHOT: The following artifacts could not be resolved: org.mortbay.jetty:jetty:jar:6.1.26.hwx, org.mortbay.jetty:jetty-util:jar:6.1.26.hwx: Could not find artifact org.mortbay.jetty:jetty:jar:6.1.26.hwx in cloudera-releases (https://repository.cloudera.com/artifactory/cloudera-repos) -&gt; [Help 1]</description>
      <version>0.10.0</version>
      <fixedVersion>0.9.2,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-8 01:00:00" id="2640" opendate="2015-9-8 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Integrate the off-heap configurations with YARN runner</summary>
      <description>The YARN runner needs to adjust the -Xmx, -Xms, and -XX:MaxDirectMemorySize parameters according to the off-heap memory settings.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMasterActor.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMaster.scala</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-8-1 01:00:00" id="26412" opendate="2022-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive dialect supports "CREATE FUNCTION USING xxx.jar"</summary>
      <description>In Hive, it's supported to use such sql likeCREATE FUNCTION USING xxx.jarto create udf.It's also need to be supported using Hive dialect in Flink</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.catalog.hive.HiveCatalogTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-7-1 01:00:00" id="26414" opendate="2022-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive dialect supports "MACRO"</summary>
      <description>In Hive, it's support to using such sql like:CREATE TEMPORARY MACRO macro_name([col_name col_type, ...]) expression;which enable use to define function using sql statement.It's also need to be supported using Hive dialect in Flink.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.planner.delegation.hive.HiveASTParserTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.planner.delegation.hive.HiveParser.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.functions.hive.HiveFunctionWrapper.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-10-10 01:00:00" id="2651" opendate="2015-9-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failing NettyServerLowAndHighWatermarkTest</summary>
      <description>https://travis-ci.org/aljoscha/flink/jobs/79610050</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-3-9 01:00:00" id="26551" opendate="2022-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make the legacy behavior disabled by default</summary>
      <description>Followup of https://issues.apache.org/jira/browse/FLINK-25111For the discussion, see https://lists.apache.org/thread/r13y3plwwyg3sngh8cz47flogq621txv</description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.runtime.stream.jsonplan.CorrelateJsonPlanITCase.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.api.config.ExecutionConfigOptions.java</file>
      <file type="M">flink-examples.flink-examples-table.src.test.java.org.apache.flink.table.examples.java.functions.AdvancedFunctionsExampleITCase.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.catalog.PostgresCatalogITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerITCase.java</file>
      <file type="M">docs.layouts.shortcodes.generated.execution.config.configuration.html</file>
      <file type="M">docs.content.docs.dev.table.types.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-3-17 01:00:00" id="26700" opendate="2022-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update chinese documentation regarding restore modes</summary>
      <description>Translate FLINK-25193</description>
      <version>None</version>
      <fixedVersion>1.15.0,1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.ops.state.savepoints.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-9-15 01:00:00" id="2677" opendate="2015-9-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a general-purpose keyed-window operator</summary>
      <description>This operator should support: Customizable triggers Eviction on triggers: all / none / custom Discard by time (expiry of state) Event time time window assignmentThis set of requirements is effectively a mix between the current trigger/evict model and the Cloud Dataflow window definition model.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.PolicyToOperator.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.graph.StreamGraph.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.KeyedDataStream.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-3-24 01:00:00" id="26851" opendate="2022-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Properly model reporter options as ConfigOptions</summary>
      <description>The generic reporter options are currently not properly modeled as config options. Some don't have a config option at all, others are just for documentation purposes and not actually used in the code.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-metrics.flink-metrics-prometheus.src.main.java.org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporterOptions.java</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.main.java.org.apache.flink.metrics.influxdb.InfluxdbReporterOptions.java</file>
      <file type="M">flink-docs.src.test.java.org.apache.flink.docs.configuration.ConfigOptionsDocsCompletenessITCase.java</file>
      <file type="M">flink-docs.src.main.java.org.apache.flink.docs.configuration.ConfigOptionsDocGenerator.java</file>
      <file type="M">docs.layouts.shortcodes.generated.prometheus.push.gateway.reporter.configuration.html</file>
      <file type="M">docs.layouts.shortcodes.generated.influxdb.reporter.configuration.html</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.testutils.InMemoryReporter.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.ReporterSetupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.MetricRegistryImplTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.MetricGroupTest.java</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.metrics.groups.AbstractMetricGroupTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.metrics.ReporterSetup.java</file>
      <file type="M">flink-metrics.flink-metrics-jmx.src.test.java.org.apache.flink.runtime.jobmanager.JMXJobManagerMetricTest.java</file>
      <file type="M">flink-end-to-end-tests.flink-metrics-reporter-prometheus-test.src.test.java.org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.MetricOptions.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.streaming.connectors.kafka.KafkaTestBase.java</file>
      <file type="M">flink-annotations.src.main.java.org.apache.flink.annotation.docs.Documentation.java</file>
      <file type="M">docs.layouts.shortcodes.generated.metric.configuration.html</file>
      <file type="M">docs.content.docs.deployment.metric.reporters.md</file>
      <file type="M">docs.content.zh.docs.deployment.metric.reporters.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-15 01:00:00" id="2686" opendate="2015-9-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend JSON plan dump to contain channel info</summary>
      <description>The execution plan JSON (via env.getExecutionPlan()) contains no information about the data exchange mode of the used channels.I think this would be super helpful while debugging problems like FLINK-2685. The channel types should also be considered in the plan rendering, but that's a separate issue.</description>
      <version>0.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plandump.PlanJSONDumpGenerator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-9-16 01:00:00" id="2688" opendate="2015-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about monitoring api</summary>
      <description>The documentation should list what requests are available and how to extend the API.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.navbar.html</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.messages.GenericMessageTester.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.CurrentJobsOverviewHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.CurrentJobIdsHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.ClusterOverviewHandler.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.ExecutionGraphHolder.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plantranslate.JsonMapper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-16 01:00:00" id="2689" opendate="2015-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reusing null object for joins with SolutionSet</summary>
      <description>Joins and CoGroups with a solution set have outer join semantics because a certain key might not have been inserted into the solution set yet. When probing a non-existing key, the CompactingHashTable will return null.In object reuse mode, this null value is used as reuse object when the next key is probed.</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.9,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.JoinWithSolutionSetSecondDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.JoinWithSolutionSetFirstDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.CoGroupWithSolutionSetSecondDriver.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.CoGroupWithSolutionSetFirstDriver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-18 01:00:00" id="2702" opendate="2015-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an implementation of distributed copying utility using Flink</summary>
      <description>Add the DistCP example proposed in this pull request: https://github.com/apache/flink/pull/1090</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.distcp.FileCopyTaskInputSplit.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.distcp.FileCopyTaskInputFormat.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.distcp.FileCopyTask.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.distcp.DistCp.java</file>
      <file type="M">flink-examples.flink-java-examples.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-18 01:00:00" id="2703" opendate="2015-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove log4j classes from fat jar / document how to use Flink with logback</summary>
      <description>Flink is using slf4j as the logging interface and log4j as the logging backend.I got requests from users which want to use a different logging backend (logback) with Flink. Currently, its quite hard for them, because they have to do a custom Flink build with log4j excluded.The purpose of this JIRA is to ship a Flink build that is prepared for users to use a different logging backend.I'm also going to document how to use logback with Flink.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClient.java</file>
      <file type="M">flink-yarn-tests.src.main.resources.log4j-test.properties</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YarnTestBase.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.java</file>
      <file type="M">flink-shaded-hadoop.pom.xml</file>
      <file type="M">flink-dist.src.main.assemblies.bin.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">docs.apis.best.practices.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-4-5 01:00:00" id="27063" opendate="2022-4-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Hive 2.3 connector to version 2.3.6</summary>
      <description>As discussed on the Dev mailing list we should update our Hive 2.3 connector to Hive 2.3.6 (the latest supported version of the Hive 2.3.* release)Discussion can be found in https://lists.apache.org/thread/2w046dwl46tf2wy750gzmt0qrcz17z8t</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.java-ci-tools.src.main.resources.modules-defining-excess-dependencies.modulelist</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-connectors.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.src.main.resources.META-INF.licenses.LICENSE.slf4j-api</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.src.main.resources.META-INF.licenses.LICENSE.reflectasm</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.src.main.resources.META-INF.licenses.LICENSE.protobuf</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.src.main.resources.META-INF.licenses.LICENSE.minlog</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.src.main.resources.META-INF.licenses.LICENSE.kryo</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.src.main.resources.META-INF.licenses.LICENSE.jodd</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.src.main.resources.META-INF.licenses.LICENSE.javolution</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.src.main.resources.META-INF.licenses.LICENSE.antlr</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.6.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveRunnerShimLoader.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">docs.content.docs.connectors.table.hive.overview.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.overview.md</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2022-6-14 01:00:00" id="27240" opendate="2022-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support ADD PARTITION statement for partitioned table</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlDdlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConverters.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlAddPartitions.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-6-14 01:00:00" id="27241" opendate="2022-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support DROP PARTITION statement for partitioned table</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.TableEnvironmentTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.operations.SqlDdlToOperationConverterTest.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.operations.converters.SqlNodeConverters.java</file>
      <file type="M">flink-table.flink-sql-parser.src.test.java.org.apache.flink.sql.parser.FlinkSqlParserImplTest.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.java.org.apache.flink.sql.parser.ddl.SqlDropPartitions.java</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.includes.parserImpls.ftl</file>
      <file type="M">flink-table.flink-sql-parser.src.main.codegen.data.Parser.tdd</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-7-14 01:00:00" id="27244" opendate="2022-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support subdirectories with Hive tables</summary>
      <description>Hive support to read recursive directory by setting the property 'set mapred.input.dir.recursive=true', and Spark also support [such behavior|https://stackoverflow.com/questions/42026043/how-to-recursively-read-hadoop-files-from-directory-using-spark].For normal case, it won't happed for reading recursive directory. But it may happen in the following case:I have a paritioned table `fact_tz` with partition day/hourCREATE TABLE fact_tz(x int) PARTITIONED BY (ds STRING, hr STRING) Then I want to create an external table `fact_daily` refering to  `fact_tz`, but with a coarse-grained partition day. create external table fact_daily(x int) PARTITIONED BY (ds STRING) location 'fact_tz_localtion' ;ALTER TABLE fact_daily ADD PARTITION (ds='1') location 'fact_tz_localtion/ds=1'But it wll throw exception "Not a file: fact_tz_localtion/ds=1" when try to query this table `fact_daily` for it's the first level of the origin partition and is actually a directory .  </description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveDialectITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveOptions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveDynamicTableFactory.java</file>
      <file type="M">docs.content.docs.connectors.table.hive.hive.read.write.md</file>
      <file type="M">docs.content.zh.docs.connectors.table.hive.hive.read.write.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-4-14 01:00:00" id="27250" opendate="2022-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove custom surefire config from sql-parser[-hive]</summary>
      <description>These modules disable fork reuse and set the fork count to 1. After a quick test I see now reason why that is required, so we get rid of this special case.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-parser.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-9-22 01:00:00" id="2729" opendate="2015-9-22 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add TaskManager overview</summary>
      <description>The dashboard needs a task manager overview, similar to the old web frontend.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.index.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.css.index.css</file>
      <file type="M">flink-runtime-web.web-dashboard.app.styles.job.styl</file>
      <file type="M">flink-runtime-web.web-dashboard.app.styles.index.styl</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.index.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.common.filters.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.index.jade</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-22 01:00:00" id="2730" opendate="2015-9-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CPU/Network utilization graphs to new web dashboard</summary>
      <description>The charts rendered in the previous dashboard should be added to the new web dashboard.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0,1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.taskmanager.taskmanager.metrics.html</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.taskmanager.taskmanager.dir.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanager.taskmanager.metrics.jade</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.messages.JobManagerMessages.scala</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.taskmanagers.metrics.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.taskmanagers.index.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.vendor.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.index.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.css.vendor.css</file>
      <file type="M">flink-runtime-web.web-dashboard.web.css.index.css</file>
      <file type="M">flink-runtime-web.web-dashboard.bower.json</file>
      <file type="M">flink-runtime-web.web-dashboard.app.styles.index.styl</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.taskmanagers.taskmanagers.svc.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.taskmanagers.taskmanagers.ctrl.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.index.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanagers.metrics.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanagers.index.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.index.jade</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.handlers.TaskManagersHandler.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-9-22 01:00:00" id="2734" opendate="2015-9-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ArrayKeySelector returns wrong positions (or fails)</summary>
      <description>The ArrayKeySelector is broken and returns wrong values in all cases except for &amp;#91;0&amp;#93; as a single only key position.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.util.keys.KeySelectorUtil.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.java</file>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.datastream.KeyedDataStream.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-4-25 01:00:00" id="27390" opendate="2022-4-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unused flink-tests dependencies</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-pulsar.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-cassandra.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-23 01:00:00" id="2744" opendate="2015-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce number of concurrent test forks to 1 for the Kafka connector project</summary>
      <description>Since the Kafka connector tests are heavyweight with many Mini Clusters, their stability would benefit from not having multiple builds competing for resources.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.flink-connector-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-5-2 01:00:00" id="27476" opendate="2022-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build new import option that only focus on maven main classes</summary>
      <description>ImportOption.DoNotIncludeTests.class used currently has some issue when running test with testContainer. It would be good to define the target class path precisely.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-architecture-tests.flink-architecture-tests-production.src.main.java.org.apache.flink.architecture.rules.ApiAnnotationRules.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-base.src.main.java.org.apache.flink.architecture.common.SourcePredicates.java</file>
      <file type="M">flink-architecture-tests.flink-architecture-tests-base.src.main.java.org.apache.flink.architecture.common.ImportOptions.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-10-23 01:00:00" id="2748" opendate="2015-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Accumulator fetch failure leads to duplicate job result response</summary>
      <description>On JobStatusChanged message and a failure to catch the accumulator result the client will receive a JobResultFailure and JobResultSuccess responsenewJobStatus match { case JobStatus.FINISHED =&gt; val accumulatorResults: java.util.Map[String, SerializedValue[AnyRef]] = try { executionGraph.getAccumulatorsSerialized() } catch { case e: Exception =&gt; log.error(s"Cannot fetch final accumulators for job $jobID", e) val exception = new JobExecutionException(jobID, "Failed to retrieve accumulator results.", e) jobInfo.client ! decorateMessage(JobResultFailure( new SerializedThrowable(exception))) Collections.emptyMap() &lt;&lt;&lt; HERE } val result = new SerializedJobExecutionResult( jobID, jobInfo.duration, accumulatorResults) jobInfo.client ! decorateMessage(JobResultSuccess(result)) &lt;&lt;&lt; HEREFurthermore the indentation is off.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.jobmanager.JobManager.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-9-23 01:00:00" id="2751" opendate="2015-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Quickstart is in documentation but only linked through the Flink homepage</summary>
      <description>The Quickstart docs contained in docs/quickstart should also be included in the documentation menu. Basically, we could copy over the Quickstart menu from the Flink homepage.</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.9,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.navbar.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-23 01:00:00" id="2752" opendate="2015-9-23 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Documentation is not easily differentiable from the Flink homepage</summary>
      <description>When users go to the documentation, either via the homepage's quickstart menu or the documentation menu, the transition to the documentation is not easily noticeable; the layout of both pages are pretty much the same. There should be a hint in the documentation page "Flink documentation version X. Click here to go back to the homepage" or something similar.</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>0.9,1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.navbar.html</file>
      <file type="M">docs.internals.how.to.contribute.md</file>
      <file type="M">docs.internals.coding.guidelines.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-5-6 01:00:00" id="27532" opendate="2022-5-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop flink-clients test-jar</summary>
      <description>The test-jar is actually unused and could be removed entirely.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">flink-clients.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-24 01:00:00" id="2761" opendate="2015-9-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent instantiation of new ExecutionEnvironments in the Scala Shell</summary>
      <description>When someone mistakenly creates a new ExecutionEnvironment in the Scala Shell, the programs don't work. The Scala Shell should prevent new ExecutionEnvironment instantiations.That can be done by setting a context environment factory that throws an error when attempting to create a new environment.See here for a user with that problem:http://stackoverflow.com/questions/32763052/flink-datasources-outputs-caused-an-error-could-not-read-the-user-code-wrappe/32765236#32765236</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITSuite.scala</file>
      <file type="M">flink-staging.flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkILoop.scala</file>
      <file type="M">flink-staging.flink-scala-shell.src.main.java.org.apache.flink.api.java.ScalaShellRemoteEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-16 01:00:00" id="27620" opendate="2022-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support percent_rank</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.OverAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.utils.AggFunctionFactory.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable.java</file>
      <file type="M">docs.data.sql.functions.zh.yml</file>
      <file type="M">docs.data.sql.functions.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-25 01:00:00" id="2767" opendate="2015-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support Scala 2.11 to Scala shell</summary>
      <description>Since FLINK-2200 is resolved, the Flink community provides JARs for Scala 2.11. But currently, there is no Scala shell with Scala 2.11. If we add support Scala 2.11 to Scala shell, the user with Scala 2.11 could use Flink easily.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-staging.pom.xml</file>
      <file type="M">flink-staging.flink-scala-shell.src.test.scala.org.apache.flink.api.scala.ScalaShellITSuite.scala</file>
      <file type="M">flink-staging.flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
      <file type="M">flink-staging.flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkILoop.scala</file>
      <file type="M">flink-staging.flink-scala-shell.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-5-17 01:00:00" id="27673" opendate="2022-5-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JUnit5 Migration] Module: flink-table-api-scala</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-scala.src.test.scala.org.apache.flink.table.types.extraction.TypeInferenceExtractorScalaTest.scala</file>
      <file type="M">flink-table.flink-table-api-scala.src.test.scala.org.apache.flink.table.types.extraction.DataTypeExtractorScalaTest.scala</file>
      <file type="M">flink-table.flink-table-api-scala.src.test.scala.org.apache.flink.table.expressions.ObjectToExpressionScalaTest.scala</file>
      <file type="M">flink-table.flink-table-api-scala.src.test.scala.org.apache.flink.table.api.ImplicitConversionsTest.scala</file>
      <file type="M">flink-table.flink-table-api-scala.src.test.scala.org.apache.flink.table.api.ExpressionsConsistencyCheckTest.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-25 01:00:00" id="2768" opendate="2015-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong Java version requirements in "Quickstart: Scala API" page</summary>
      <description>Since Flink 0.10, we dropped Java 6 support. But "Quickstart: Scala API" page says that Java 6 is one of minimum requirement.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.setup.local.setup.md</file>
      <file type="M">docs.quickstart.setup.quickstart.md</file>
      <file type="M">docs.quickstart.scala.api.quickstart.md</file>
      <file type="M">docs.quickstart.java.api.quickstart.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-11-1 01:00:00" id="2797" opendate="2015-10-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CLI: Missing option to submit jobs in detached mode</summary>
      <description>Jobs can only be submitted in detached mode using YARN but not on a standalone installation. This has been requested by users who want to submit a job, get the job id, and later query its status.</description>
      <version>0.9,0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-clients.src.test.java.org.apache.flink.client.CliFrontendRunTest.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.ProgramOptions.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.cli.CliFrontendParser.java</file>
      <file type="M">flink-clients.src.main.java.org.apache.flink.client.CliFrontend.java</file>
      <file type="M">docs.apis.cli.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-6-10 01:00:00" id="27985" opendate="2022-6-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce FlinkRecomputeStatisticsProgram to compute statistics after filter push and partition pruning</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.formats.testcsv.TestCsvFormatFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkBatchProgram.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoSourceScanRuleBase.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.PartitionPushDownSpec.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.abilities.source.FilterPushDownSpec.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.Date.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.stats.CatalogTableStatistics.java</file>
      <file type="M">flink-connectors.flink-connector-files.src.main.java.org.apache.flink.connector.file.table.FileSystemTableSource.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-7-10 01:00:00" id="27989" opendate="2022-6-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CSV format supports reporting statistics</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.formats.testcsv.TestCsvFormatFactory.java</file>
      <file type="M">flink-formats.flink-csv.src.main.java.org.apache.flink.formats.csv.CsvFileFormatFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-12 01:00:00" id="28011" opendate="2022-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize getAllPartitions in HiveSource</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.util.HivePartitionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-2 01:00:00" id="2811" opendate="2015-10-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add page with configuration overview</summary>
      <description>The old web interface contained a page to view the configuration of the JobManager.This issue is about adding the page again.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.index.html</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.index.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.index.jade</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.WebRuntimeMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-20 01:00:00" id="28121" opendate="2022-6-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate "Extension Points" and "Full Stack Example" in "User-defined Sources &amp; Sinks" page</summary>
      <description>The links are https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sourcessinks/#extension-pointsand https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sourcessinks/#full-stack-example</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.dev.table.sourcesSinks.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-9-20 01:00:00" id="28122" opendate="2022-6-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Translate "Overview " and "Project Configuration" in "User-defined Sources &amp; Sinks" page</summary>
      <description>The links arehttps://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sourcessinks/#overviewand https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sourcessinks/#project-configuration</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.zh.docs.dev.table.sourcesSinks.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-4 01:00:00" id="2817" opendate="2015-10-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>FileMonitoring function throws NPE when location is empty</summary>
      <description>StreamExecutionEnvironment.readFileStream() does not handle a missing location properly. I would suggest to log that the location is empty and continue running the job.A test covering the correct behavior is also needed.</description>
      <version>0.9.1,0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.api.functions.source.FileMonitoringFunction.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-7-22 01:00:00" id="28201" opendate="2022-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generalize utils around dependency-plugin</summary>
      <description>We'll be adding another safeguard against developer mistakes which also parses the output of the dependency plugin, like the scala suffix checker.We should generalize this parsing such that both checks can use the same code.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.java-ci-tools.src.main.java.org.apache.flink.tools.ci.suffixcheck.ScalaSuffixChecker.java</file>
      <file type="M">tools.ci.java-ci-tools.src.main.java.org.apache.flink.tools.ci.licensecheck.NoticeFileChecker.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-11-22 01:00:00" id="28202" opendate="2022-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generalize utils around shade-plugin</summary>
      <description>We'll be adding another safeguard against developer mistakes which also parses the output of the shade plugin, like the license checker.We should generalize this parsing such that both checks can use the same code.</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.ci.flink-ci-tools.src.test.java.org.apache.flink.tools.ci.licensecheck.NoticeFileCheckerTest.java</file>
      <file type="M">tools.ci.flink-ci-tools.src.main.java.org.apache.flink.tools.ci.licensecheck.NoticeFileChecker.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-5-22 01:00:00" id="28203" opendate="2022-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mark all bundled dependencies as optional</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.flink-sql-client-test.pom.xml</file>
      <file type="M">tools.ci.flink-ci-tools.src.main.java.org.apache.flink.tools.ci.utils.shared.DependencyTree.java</file>
      <file type="M">tools.ci.compile.sh</file>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-table-runtime.pom.xml</file>
      <file type="M">flink-table.flink-table-planner.pom.xml</file>
      <file type="M">flink-table.flink-table-planner-loader-bundle.pom.xml</file>
      <file type="M">flink-table.flink-table-code-splitter.pom.xml</file>
      <file type="M">flink-table.flink-table-api-java-uber.pom.xml</file>
      <file type="M">flink-table.flink-sql-jdbc-driver-bundle.pom.xml</file>
      <file type="M">flink-table.flink-sql-gateway.pom.xml</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-runtime.pom.xml</file>
      <file type="M">flink-rpc.flink-rpc-akka.pom.xml</file>
      <file type="M">flink-python.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-prometheus.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-graphite.pom.xml</file>
      <file type="M">flink-metrics.flink-metrics-datadog.pom.xml</file>
      <file type="M">flink-kubernetes.pom.xml</file>
      <file type="M">flink-formats.flink-sql-protobuf.pom.xml</file>
      <file type="M">flink-formats.flink-sql-parquet.pom.xml</file>
      <file type="M">flink-formats.flink-sql-orc.pom.xml</file>
      <file type="M">flink-formats.flink-sql-json.pom.xml</file>
      <file type="M">flink-formats.flink-sql-csv.pom.xml</file>
      <file type="M">flink-formats.flink-sql-avro.pom.xml</file>
      <file type="M">flink-formats.flink-sql-avro-confluent-registry.pom.xml</file>
      <file type="M">flink-formats.flink-json.pom.xml</file>
      <file type="M">flink-formats.flink-csv.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-oss-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-gs-fs-hadoop.pom.xml</file>
      <file type="M">flink-filesystems.flink-fs-hadoop-shaded.pom.xml</file>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.pom.xml</file>
      <file type="M">flink-examples.flink-examples-build-helper.flink-examples-streaming-state-machine.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-dist-scala.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-kafka.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-3.1.3.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hive-2.3.9.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-2.2.pom.xml</file>
      <file type="M">flink-connectors.flink-sql-connector-hbase-1.4.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-files.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-7-30 01:00:00" id="28311" opendate="2022-6-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce REST APIs for the environmental information</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-7-30 01:00:00" id="28312" opendate="2022-6-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce REST APIs for log URL retrieval</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.webmonitor.WebMonitorEndpoint.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime-web.src.main.java.org.apache.flink.runtime.webmonitor.history.HistoryServer.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.HistoryServerOptions.java</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
      <file type="M">docs.layouts.shortcodes.generated.history.server.configuration.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-8-4 01:00:00" id="28380" opendate="2022-7-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Produce one intermediate dataset for multiple consumers consuming the same data</summary>
      <description>Currently, if one output of an upstream job vertex is consumed by multiple downstream job vertices, the upstream vertex will produce multiple dataset. For blocking shuffle, it means serialize and persist the same data multiple times. This ticket aims to optimize this behavior and make the upstream job vertex produce one dataset which will be read by multiple downstream vertex.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime.src.main.java.org.apache.flink.table.runtime.partitioner.BinaryHashPartitioner.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.util.MockStreamConfig.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskMailboxTestHarnessBuilder.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamConfigChainer.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.partitioner.ForwardForUnspecifiedPartitionerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.partitioner.ForwardForConsecutiveHashPartitionerTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGeneratorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.OperatorChain.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.partitioner.CustomPartitionerWrapper.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.graph.StreamConfig.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.ResultPartitionType.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-8-6 01:00:00" id="28420" opendate="2022-7-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support partial lookup caching in lookup join runners</summary>
      <description>Support partial lookup caching in lookup join runners, including LookupJoinRunner, LookupJoinWithCalcRunner, AsyncLookupJoinRunner and AsyncLookupJoinWithCalcRunner</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AsyncLookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.LookupJoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestValuesTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner.src.test.java.org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.LookupJoinCodeGenerator.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.utils.LookupJoinUtil.java</file>
      <file type="M">flink-table.flink-table-planner.src.main.java.org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLookupJoin.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.LookupFunction.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.AsyncLookupFunction.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-9 01:00:00" id="2843" opendate="2015-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for DataSet outer joins</summary>
      <description>Outer joins are not included in the documentation yet.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.programming.guide.md</file>
      <file type="M">docs.apis.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-10-9 01:00:00" id="2846" opendate="2015-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Emit checkpoint barriers earlier, before drawing the state snapshot</summary>
      <description>State snapshot drawing and downstream barrier emitting occur in an atomic scope. Currently, the barriers are emitted after the state snapshot has been drawn.There is no reason why the barriers could not be emitted at the beginning of the atomic scope, and it would reduce the short stalls in the streaming pipeline when drawing the state snapshot of an operator checkpoint.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-core.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-7-14 01:00:00" id="28551" opendate="2022-7-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Store the number of bytes instead of the number of buffers in index entry for sort-shuffle</summary>
      <description>Currently, in each index entry of sort-shuffle index file, one filed is the number of buffers in the current data region. The problem is that it is hard to know the data boundary before reading the file, to solve the problem, we can store the number of bytes instead of the number of buffers in index entry. Based on this change, we can do some optimization, for example, read larger size of data than a buffer for better sequential IO like what's mentioned in FLINK-28373.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFileWriter.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFileReader.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.partition.PartitionedFile.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-16 01:00:00" id="2861" opendate="2015-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fields grouping on split streams fails</summary>
      <description>Using split streams works for shuffle grouping, but not for Fields grouping.The reason is that the KeySelector expects an array, and the given type is the SplitStreamType.</description>
      <version>0.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.WrapperSetupHelperTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.test.java.org.apache.flink.storm.wrappers.BoltWrapperTest.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.WrapperSetupHelper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.SpoutWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.wrappers.BoltWrapper.java</file>
      <file type="M">flink-contrib.flink-storm.src.main.java.org.apache.flink.storm.api.FlinkTopologyBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-7-19 01:00:00" id="28610" opendate="2022-7-19 00:00:00" resolution="Done">
    <buginformation>
      <summary>Enable speculative execution of sources</summary>
      <description>Currently speculative execution of sources is disabled. It can be enabled with the improvement done to support InputFormat sources and new sources to work correctly with speculative execution.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.scheduling.SpeculativeSchedulerITCase.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.adaptivebatch.SpeculativeScheduler.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-10-16 01:00:00" id="2863" opendate="2015-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka producer does not fail in case of write failure</summary>
      <description>The async producer used in the Kafka connector only logs errors, but does not fail the program in case of an error.I will change it such that it fails by default on error and add a flag for the "lenient" mode that only logs failures.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-streaming.flink-streaming-connectors.flink-connector-kafka.src.main.java.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-21 01:00:00" id="28630" opendate="2022-7-21 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Allow to GetSchemas in the HiveServer2 Endpoint</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.test.java.org.apache.flink.table.gateway.api.utils.MockedSqlGatewayService.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.SqlGatewayService.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.operation.OperationType.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.util.ThriftObjectConversions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.util.StringRowDataUtils.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Schemas.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-21 01:00:00" id="28631" opendate="2022-7-21 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Allow to GetFunctions in the HiveServer2 Endpoint</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.functions.FunctionIdentifier.java</file>
      <file type="M">flink-table.flink-table-api-java.src.main.java.org.apache.flink.table.catalog.FunctionCatalog.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.context.SessionContext.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.test.java.org.apache.flink.table.gateway.api.utils.MockedSqlGatewayService.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.SqlGatewayService.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.util.OperationExecutorFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Schemas.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-21 01:00:00" id="28632" opendate="2022-7-21 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Allow to GetColumns/GetTableTypes/GetPrimaryKeys in the HiveServer2 Endpoint</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.catalog.ResolvedCatalogBaseTable.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.test.java.org.apache.flink.table.gateway.api.utils.MockedSqlGatewayService.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.SqlGatewayService.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.util.OperationExecutorFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Schemas.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-21 01:00:00" id="28633" opendate="2022-7-21 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Allow to GetTables in the HiveServer2 Endpoint</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationExecutor.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.test.java.org.apache.flink.table.gateway.api.utils.MockedSqlGatewayService.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.SqlGatewayService.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.operation.OperationType.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.util.ThriftObjectConversionsTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.util.ThriftObjectConversions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.util.OperationExecutorFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Schemas.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-7-21 01:00:00" id="28634" opendate="2022-7-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a simple Json (De) SerializationSchema</summary>
      <description>Add a basic schema to read/write JSON.This is so common that users shouldn't have to implement that themselves.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-kafka.src.test.java.org.apache.flink.connector.kafka.source.reader.deserializer.KafkaRecordDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.test.java.org.apache.flink.formats.json.JsonNodeDeserializationSchemaTest.java</file>
      <file type="M">flink-formats.flink-json.src.main.java.org.apache.flink.formats.json.JsonNodeDeserializationSchema.java</file>
      <file type="M">flink-formats.flink-json.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2022-8-27 01:00:00" id="28713" opendate="2022-7-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unused curator-test dependency from flink-test-utils</summary>
      <description>Remove an unused dependency that also pulls in log4j1 into user projects.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-test-utils.pom.xml</file>
      <file type="M">flink-table.flink-sql-client.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-elasticsearch.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-10-20 01:00:00" id="2876" opendate="2015-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minutiae</summary>
      <description>A collection of small documentation and grammar updates.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.main.scala.org.apache.flink.runtime.taskmanager.TaskManager.scala</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.sort.PartialOrderPriorityQueue.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.hash.MutableHashTable.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.operators.BatchTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.iterative.task.IterationIntermediateTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.iomanager.AsynchronousFileIOChannel.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.client.JobClient.java</file>
      <file type="M">flink-optimizer.src.main.java.org.apache.flink.optimizer.plantranslate.JobGraphGenerator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.operators.translation.Tuple3UnwrappingIterator.java</file>
      <file type="M">flink-java.src.main.java.org.apache.flink.api.java.ExecutionEnvironment.java</file>
      <file type="M">flink-examples.flink-java-examples.src.main.java.org.apache.flink.examples.java.relational.EmptyFieldsCountAccumulator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.typeutils.TypeComparator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.accumulators.AverageAccumulator.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.common.accumulators.Accumulator.java</file>
      <file type="M">docs.apis.programming.guide.md</file>
      <file type="M">docs.apis.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-20 01:00:00" id="2883" opendate="2015-10-20 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add documentation to forbid key-modifying ReduceFunction</summary>
      <description>If one uses a combinable reduce operation which also changes the key value of the underlying data element, then the results of the reduce operation can become wrong. The reason is that after the combine phase, another reduce operator is executed which will then reduce the elements based on the new key values. This might be not so surprising if one explicitly defined ones GroupReduceOperation as combinable. However, the ReduceFunction conceals the fact that a combiner is used implicitly. Furthermore, the API does not prevent the user from changing the key fields which could solve the problem.The following example program illustrates the problemval env = ExecutionEnvironment.getExecutionEnvironmentenv.setParallelism(1)val input = env.fromElements((1,2), (1,3), (2,3), (3,3), (3,4))val result = input.groupBy(0).reduce{ (left, right) =&gt; (left._1 + right._1, left._2 + right._2)}result.output(new PrintingOutputFormat[Int]())env.execute()The expected output is (2, 5)(2, 3)(6, 7)However, the actual output is(4, 8)(6, 7)I think that the underlying problem is that associativity and commutativity is not sufficient for a combinable reduce operation. Additionally we also need to make sure that the key stays the same.</description>
      <version>0.10.0</version>
      <fixedVersion>1.2.1,1.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.batch.dataset.transformations.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-7 01:00:00" id="28851" opendate="2022-8-7 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Allow to GetTypeInfo in the HiveServer2 Endpoint</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-gateway.src.test.java.org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.java</file>
      <file type="M">flink-table.flink-sql-gateway.src.main.java.org.apache.flink.table.gateway.service.operation.OperationManager.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.test.java.org.apache.flink.table.gateway.api.utils.MockedSqlGatewayService.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.SqlGatewayService.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.results.OperationInfo.java</file>
      <file type="M">flink-table.flink-sql-gateway-api.src.main.java.org.apache.flink.table.gateway.api.operation.OperationType.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.util.ThriftObjectConversionsTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.util.ThriftObjectConversions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.util.OperationExecutorFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Schemas.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-10-22 01:00:00" id="2891" opendate="2015-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Key for Keyed State is not set upon Window Evaluation</summary>
      <description>In both the aligned and the general-purpose windows the key for the keyed operator state is not set when evaluating the windows. This silently leads to incorrect results.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AggregatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingAlignedProcessingTimeWindowOperatorTest.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AggregatingKeyedTimePanes.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AccumulatingKeyedTimePanes.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AbstractKeyedTimePanes.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.AbstractAlignedProcessingTimeWindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.operators.AbstractStreamOperator.java</file>
      <file type="M">flink-tests.src.test.java.org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.operators.windowing.NonKeyedWindowOperator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-8-14 01:00:00" id="28951" opendate="2022-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Header in janino generated java files can merge with line numbers</summary>
      <description>Since Line numbers are generated only for debug output it should not be a big issue.From the other side currently this behavior leads to not compiled code.The suggestion is usage of one-line comments for header to prevent this</description>
      <version>None</version>
      <fixedVersion>1.16.0,1.15.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.planner.codegen.CodeGeneratorContext.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-22 01:00:00" id="2898" opendate="2015-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invert Travis CI build order</summary>
      <description>The Travis CI builds generally perform fastest to slowest. When running additional, concurrent Travis CI builds it would be preferable to have the slowest tasks begin first.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.deploy.to.maven.sh</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-22 01:00:00" id="2900" opendate="2015-10-22 00:00:00" resolution="Done">
    <buginformation>
      <summary>Remove Record-API dependencies from Hadoop Compat module</summary>
      <description>The Hadoop Compat module includes wrappers for Hadoop Input/OutputFormat for the Record API classes and a corresponding test.These need to be removed before removing the Record API.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.test.java.org.apache.flink.test.hadoopcompatibility.mapred.record.HadoopRecordInputOutputITCase.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.HadoopRecordOutputFormat.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.HadoopRecordInputFormat.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.HadoopDataSource.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.HadoopDataSink.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.example.WordCountWithOutputFormat.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.example.WordCount.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.WritableWrapperConverter.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.WritableWrapper.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.WritableComparableWrapper.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.HadoopTypeConverter.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.HadoopFileOutputCommitter.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.FlinkTypeConverter.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.DefaultHadoopTypeConverter.java</file>
      <file type="M">flink-staging.flink-hadoop-compatibility.src.main.java.org.apache.flink.hadoopcompatibility.mapred.record.datatypes.DefaultFlinkTypeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-22 01:00:00" id="2902" opendate="2015-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web interface sort tasks newest first</summary>
      <description>Sort completed jobs in reverse order so the most recently finished are at the top of the list. With a long list of completed jobs the user must scroll down to view recently completed jobs.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0,1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.overview.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.running-jobs.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.completed-jobs.html</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.overview.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.running-jobs.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.completed-jobs.jade</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-22 01:00:00" id="2903" opendate="2015-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web interface numeric localization</summary>
      <description>It would be nice to localize numbers in the web interface as 10+ digits is difficult to parse without separators.</description>
      <version>0.10.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.plan.node.subtasks.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.plan.node-list.overview.html</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.node.subtasks.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.node-list.overview.jade</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-8-18 01:00:00" id="29030" opendate="2022-8-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Print a log message if a Pojo/Tuple contains a generic type</summary>
      <description>Users are encouraged to use POJO types, that will be serialized by the PojoSerializer which supports schema evolution.If a user does not use a POJO we print an info message, linking to the docs and citing potential performance issues.However, no such message is printed if a POJO contains a generic type.As a result there may be users out there believing to have optimal performance and support for schema evolution since, after all, they are able to use the POJO serializer, when this may not be the case.</description>
      <version>None</version>
      <fixedVersion>1.16.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.main.java.org.apache.flink.api.java.typeutils.TypeExtractor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-22 01:00:00" id="2904" opendate="2015-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web interface truncated task counts</summary>
      <description>Task counts have only three digits visible as the color square needs to dynamically expand.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.css.index.css</file>
      <file type="M">flink-runtime-web.web-dashboard.app.styles.job.styl</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-9-12 01:00:00" id="29260" opendate="2022-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Auto-wipe exclusion list after updating reference version</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0,1.15.3</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.releasing.update.japicmp.configuration.sh</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-9-12 01:00:00" id="29262" opendate="2022-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.16.0,1.15.3</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.ops.upgrading.md</file>
      <file type="M">docs.content.zh.docs.ops.upgrading.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-11-29 01:00:00" id="2938" opendate="2015-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming docs not in sync with latest state changes</summary>
      <description>The section about "Working with State" does not reflect the recent changes to access the key/value in the RuntimeContext. The code examples need to be updated.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.1,1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.streaming.guide.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-11-29 01:00:00" id="2942" opendate="2015-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dangling operators in web UI&amp;#39;s program visualization (non-deterministic)</summary>
      <description>When visualizing a program with three MapPartition operators that branch off from an OuterJoin operator, two of the three MapPartition operators are not connected to the OuterJoin operator and appear to have no input.The problem is present in FireFox as well as in Chrome. I'll attach a screenshot.The problem and be reproduced by executing the "Cascading for the impatient" TFIDF example program using the Cascading Flink Connector.Update: It appears that the problem is non-deterministic. I ran the same job again (same setup) and the previously missing connections were visualized. However, the UI showed only one input for a binary operator (OuterJoin). Running the job a third time resulted in a graph layout which was again different from both runs before. However, two of the MapPartition operators had not inputs just as in the first run.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.1,1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.js.vendor.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobs.jobs.dir.coffee</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-12-26 01:00:00" id="29420" opendate="2022-9-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Zookeeper to 3.7</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-test-utils-parent.flink-connector-test-utils.src.main.java.org.apache.flink.connector.testframe.container.FlinkTestcontainersConfigurator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-11-29 01:00:00" id="2943" opendate="2015-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Confusing Bytes/Records "read" and "write" labels in WebUI job view</summary>
      <description>The job detail view of the WebUI shows the amount of data and records received from and sent out by each individual operator. This information is very valuable, if correctly interpreted. However, the table headings for the corresponding columns ("Bytes read", "Records read", "Bytes written", and "Records written") are confusing in my opinion because they do not indicate that only incoming and outgoing data is tracked and disk IO is not considered at all. For example, the UI shows no "Bytes/Records read" and only "Bytes/Records written" for DataSources which I find counter-intuitive.I propose to rename these labels to either "X received" and "X sent" or "X incoming" and "X outgoing" to make clear that this information only affects incoming and outgoing data.Opinions?</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.taskmanager.taskmanager.metrics.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.plan.node.subtasks.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.plan.node-list.overview.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.css.index.css</file>
      <file type="M">flink-runtime-web.web-dashboard.app.styles.index.styl</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.taskmanager.taskmanager.metrics.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.node.subtasks.jade</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.plan.node-list.overview.jade</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-2 01:00:00" id="2954" opendate="2015-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Not able to pass custom environment variables in cluster to processes that spawning TaskManager</summary>
      <description>There are programs that rely on custom environment variables. In hadoop mapreduce job we can use -Dmapreduce.map.env and - Dmapreduce.reduce.env to do pass them. Similarly in Sparkwe can use --conf 'spark.executor.XXX=value for XXX'. There is no such feature yet in Flink.This has given Flink a serious disadvantage when customers need such feature.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.YarnJobManager.scala</file>
      <file type="M">flink-yarn.src.main.scala.org.apache.flink.yarn.ApplicationMasterBase.scala</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.Utils.java</file>
      <file type="M">flink-yarn.src.main.java.org.apache.flink.yarn.FlinkYarnClientBase.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.YARNSessionFIFOITCase.java</file>
      <file type="M">flink-yarn-tests.src.main.java.org.apache.flink.yarn.UtilsTest.java</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-dist.pom.xml</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.ConfigConstants.java</file>
      <file type="M">docs.setup.config.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-3 01:00:00" id="2957" opendate="2015-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make cancel button look less like a label</summary>
      <description>The cancel button looks like a label. Its visual appearance could be improved.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.web.partials.jobs.job.html</file>
      <file type="M">flink-runtime-web.web-dashboard.web.js.index.js</file>
      <file type="M">flink-runtime-web.web-dashboard.web.css.index.css</file>
      <file type="M">flink-runtime-web.web-dashboard.app.styles.job.styl</file>
      <file type="M">flink-runtime-web.web-dashboard.app.styles.index.styl</file>
      <file type="M">flink-runtime-web.web-dashboard.app.scripts.modules.jobs.jobs.ctrl.coffee</file>
      <file type="M">flink-runtime-web.web-dashboard.app.partials.jobs.job.jade</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-10-10 01:00:00" id="29570" opendate="2022-10-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump org.jsoup:jsoup to v1.15.3</summary>
      <description>Bump JSoup to avoid getting flagged for CVE-2022-36033 (which doesn't affect Flink directly)</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-docs.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2022-11-3 01:00:00" id="29862" opendate="2022-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to flink-shaded 16.1</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-table.flink-sql-gateway.src.test.resources.sql.gateway.rest.api.v1.snapshot</file>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.rest.compatibility.CompatibilityRoutines.java</file>
      <file type="M">flink-runtime-web.src.test.resources.rest.api.v1.snapshot</file>
      <file type="M">docs.static.generated.rest.v1.dispatcher.yml</file>
      <file type="M">docs.layouts.shortcodes.generated.rest.v1.dispatcher.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-9 01:00:00" id="2987" opendate="2015-11-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink 0.10 fails to start on YARN 2.6.0</summary>
      <description>While testing Flink for the release, I noticed that it does not start on YARN due to classloading issues.The error message is the followingrobert@hn0-apache:~/flink-0.10.0$ ./bin/yarn-session.sh -n 2Exception in thread "main" java.lang.RuntimeException: Could not instantiate type 'org.apache.flink.yarn.FlinkYarnClient' Most likely the constructor (or a member variable initialization) threw an exception: com/sun/jersey/api/client/config/ClientConfig at org.apache.flink.util.InstantiationUtil.instantiate(InstantiationUtil.java:152) at org.apache.flink.util.InstantiationUtil.instantiate(InstantiationUtil.java:118) at org.apache.flink.client.FlinkYarnSessionCli.getFlinkYarnClient(FlinkYarnSessionCli.java:262) at org.apache.flink.client.FlinkYarnSessionCli.createFlinkYarnClient(FlinkYarnSessionCli.java:107) at org.apache.flink.client.FlinkYarnSessionCli.run(FlinkYarnSessionCli.java:400) at org.apache.flink.client.FlinkYarnSessionCli.main(FlinkYarnSessionCli.java:351)Caused by: java.lang.NoClassDefFoundError: com/sun/jersey/api/client/config/ClientConfig at org.apache.hadoop.yarn.client.api.TimelineClient.createTimelineClient(TimelineClient.java:45) at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceInit(YarnClientImpl.java:163) at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163) at org.apache.flink.yarn.FlinkYarnClientBase.&lt;init&gt;(FlinkYarnClientBase.java:157) at org.apache.flink.yarn.FlinkYarnClient.&lt;init&gt;(FlinkYarnClient.java:23) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at java.lang.Class.newInstance(Class.java:379) at org.apache.flink.util.InstantiationUtil.instantiate(InstantiationUtil.java:139) ... 5 moreCaused by: java.lang.ClassNotFoundException: com.sun.jersey.api.client.config.ClientConfig at java.net.URLClassLoader$1.run(URLClassLoader.java:366) at java.net.URLClassLoader$1.run(URLClassLoader.java:355) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:354) at java.lang.ClassLoader.loadClass(ClassLoader.java:425) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) at java.lang.ClassLoader.loadClass(ClassLoader.java:358) ... 16 moreThe issue occurs with the following versions: flink-0.10.0-bin-hadoop26-scala_2.11.tgz flink-0.10.0-bin-hadoop26-scala_2.10.tgz flink-0.10.0-bin-hadoop27-scala_2.10.tgzIt works for: flink-0.10.0-bin-hadoop24-scala_2.10.tgz flink-0.9.1-bin-hadoop26.tgzInterestingly, the issue only occurs, when HADOOP_CONF_DIR is set. Otherwise, the YARN client is starting until its unable to connect to the RM.The missing class is in the jersey-client dependency, which we exclude from Hadoop, in flink-shaded-hadoop2.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.1,1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-shaded-hadoop.flink-shaded-hadoop2.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-11-11 01:00:00" id="3002" opendate="2015-11-11 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add an EitherType to the Java API</summary>
      <description>Either types are recurring patterns and should be serialized efficiently, so it makes sense to add them to the core Java API.Since Java does not have such a type as of Java 8, we would need to add our own version.The Scala API handles the Scala Either Type already efficiently. I would not use the Scala Either Type in the Java API, since we are trying to get the flink-java project "Scala free" for people that don't use Scala and o not want to worry about Scala version matches and mismatches.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.apis.programming.guide.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-11-23 01:00:00" id="30170" opendate="2022-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Parquet version to 1.12.3</summary>
      <description>Flink currently uses version 1.12.2, which should be upgraded to 1.12.3</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-formats.pom.xml</file>
      <file type="M">flink-formats.flink-sql-parquet.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-11-23 01:00:00" id="30171" opendate="2022-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump socket.io-parser from 4.0.4 to 4.0.5</summary>
      <description>Dependabot created PR: https://github.com/apache/flink/pull/21279</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-11-23 01:00:00" id="30172" opendate="2022-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump loader-utils from 2.0.2 to 2.0.4</summary>
      <description>Dependabot created PR: https://github.com/apache/flink/pull/21327</description>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-4-23 01:00:00" id="30174" opendate="2022-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump engine.io from 6.2.0 to 6.2.1</summary>
      <description>Bump engine.io from 6.2.0 to 6.2.1 to avoid false flag for CVE-2022-41940</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.package-lock.json</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-12-23 01:00:00" id="30175" opendate="2022-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump snakeyaml from 1.31 to 1.33</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.17.0,pulsar-4.0.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-kubernetes.src.main.resources.META-INF.NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2022-11-23 01:00:00" id="30177" opendate="2022-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update pgjdbc to fix CVE-2022-41946</summary>
      <description>There is CVE-2022-41946 fixed in 42.5.1, 42.4.3 42.3.8, 42.2.27.jre7.Also mentioned in released noteshttps://jdbc.postgresql.org/changelogs/2022-11-23-42.5.1-release/</description>
      <version>None</version>
      <fixedVersion>jdbc-3.1.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2022-1-24 01:00:00" id="30191" opendate="2022-11-24 00:00:00" resolution="Done">
    <buginformation>
      <summary>Update py4j from 0.10.9.3 to 0.10.9.7</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.17.0</fixedVersion>
      <type>Technical Debt</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">NOTICE</file>
      <file type="M">flink-python.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-python.setup.py</file>
      <file type="M">flink-python.README.md</file>
      <file type="M">flink-python.pyflink.table.table.result.py</file>
      <file type="M">flink-python.lib.py4j-0.10.9.3-src.zip</file>
      <file type="M">flink-python.dev.dev-requirements.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-2-17 01:00:00" id="3025" opendate="2015-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flink Kafka consumer may get stuck due to Kafka/Zookeeper client bug</summary>
      <description>In some cases the Flink kafka consumer might fail due to https://issues.apache.org/jira/browse/KAFKA-824.Subsequently it can happen that the sources gets stuck in a Zookeeper client call (zookeeper bug).A proposed fix would be bumping the zookeeper dependency to a version that includes the fix for this bug.</description>
      <version>0.10.0,1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.flink-connector-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-11-18 01:00:00" id="3040" opendate="2015-11-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add docs describing how to configure State Backends</summary>
      <description/>
      <version>0.10.0</version>
      <fixedVersion>0.10.1,1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.navbar.html</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2015-1-22 01:00:00" id="3057" opendate="2015-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Provide a way to pass information back to the plan process</summary>
      <description/>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Environment.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.connection.Iterator.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.connection.Connection.py</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.StreamPrinter.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.Sender.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.Receiver.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.streaming.PythonStreamer.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonPlanBinder.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonOperationInfo.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.functions.PythonMapPartition.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.functions.PythonCombineIdentity.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.functions.PythonCoGroup.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-23 01:00:00" id="3063" opendate="2015-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[py] Remove combiner</summary>
      <description>The current combiner implementation in the PythonAPI is quite a mess. It adds a lot of unreadable clutter, is inefficient at times, and can straight up break in some edge cases.I will revisit this feature after FLINK-2501 is resolved. Several changes for that issue will make the reimplementation easier.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-python.src.test.python.org.apache.flink.python.api.test.main.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.OperationInfo.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.Environment.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.plan.DataSet.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.functions.ReduceFunction.py</file>
      <file type="M">flink-libraries.flink-python.src.main.python.org.apache.flink.python.api.flink.functions.GroupReduceFunction.py</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonPlanBinder.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.PythonOperationInfo.java</file>
      <file type="M">flink-libraries.flink-python.src.main.java.org.apache.flink.python.api.functions.PythonCombineIdentity.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-12-24 01:00:00" id="3071" opendate="2015-11-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add asynchronous materialization thread</summary>
      <description>Add a thread to the stream task that handles background materialization and acknowledges the checkpoint ones the materialization is complete.</description>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-java.src.test.java.org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.TimerException.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.runtime.tasks.StreamTask.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.state.AsynchronousStateHandle.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-26 01:00:00" id="3083" opendate="2015-11-26 00:00:00" resolution="Done">
    <buginformation>
      <summary>Add docs how to configure streaming fault tolerance</summary>
      <description/>
      <version>0.10.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..includes.navbar.html</file>
      <file type="M">docs.apis.streaming.guide.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-3-4 01:00:00" id="3115" opendate="2015-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Elasticsearch connector to 2.X</summary>
      <description>The Elasticsearch connector is not up to date anymore. In version 2.X the API changed. The code needs to be adapted. Probably it makes sense to have a new class ElasticsearchSink2.</description>
      <version>0.10.0,0.10.1,1.0.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-streaming-connectors.pom.xml</file>
      <file type="M">docs.apis.streaming.connectors.index.md</file>
    </fixedFiles>
  </bug>
  
  
  
</bugrepository>