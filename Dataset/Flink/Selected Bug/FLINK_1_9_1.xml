<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="FLINK">
  <bug fixdate="2018-12-12 01:00:00" id="11136" opendate="2018-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the logical of merge for DISTINCT aggregates</summary>
      <description>The logic of merge for DISTINCT aggregates has bug. For the following query:SELECT c, COUNT(DISTINCT b), SUM(DISTINCT b), SESSION_END(rowtime, INTERVAL '0.005' SECOND)FROM MyTableGROUP BY SESSION(rowtime, INTERVAL '0.005' SECOND), cthe following exception will be thrown:Caused by: java.lang.ClassCastException: org.apache.flink.types.Row cannot be cast to java.lang.Integerat scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:101)at scala.math.Numeric$IntIsIntegral$.plus(Numeric.scala:58)at org.apache.flink.table.functions.aggfunctions.SumAggFunction.accumulate(SumAggFunction.scala:50)at GroupingWindowAggregateHelper$18.mergeAccumulatorsPair(Unknown Source)at org.apache.flink.table.runtime.aggregate.AggregateAggFunction.merge(AggregateAggFunction.scala:66)at org.apache.flink.table.runtime.aggregate.AggregateAggFunction.merge(AggregateAggFunction.scala:33)at org.apache.flink.runtime.state.heap.HeapAggregatingState.mergeState(HeapAggregatingState.java:117)at org.apache.flink.runtime.state.heap.AbstractHeapMergingState$MergeTransformation.apply(AbstractHeapMergingState.java:102)at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.transform(CopyOnWriteStateTable.java:463)at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.transform(CopyOnWriteStateTable.java:341)at org.apache.flink.runtime.state.heap.AbstractHeapMergingState.mergeNamespaces(AbstractHeapMergingState.java:91)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator$2.merge(WindowOperator.java:341)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator$2.merge(WindowOperator.java:311)at org.apache.flink.streaming.runtime.operators.windowing.MergingWindowSet.addWindow(MergingWindowSet.java:212)at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.processElement(WindowOperator.java:311)at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:202)at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:105)at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)at org.apache.flink.runtime.taskmanager.Task.run(Task.java:704)at java.lang.Thread.run(Thread.java:745)</description>
      <version>None</version>
      <fixedVersion>1.6.3,1.7.1,1.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-libraries.flink-table.src.test.scala.org.apache.flink.table.runtime.stream.sql.SqlITCase.scala</file>
      <file type="M">flink-libraries.flink-table.src.main.scala.org.apache.flink.table.codegen.AggregationCodeGenerator.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-7 01:00:00" id="13999" opendate="2019-9-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct the documentation of MATCH_RECOGNIZE</summary>
      <description>Regarding to the following example in the doc:SELECT *FROM Ticker MATCH_RECOGNIZE ( PARTITION BY symbol ORDER BY rowtime MEASURES FIRST(A.rowtime) AS start_tstamp, LAST(A.rowtime) AS end_tstamp, AVG(A.price) AS avgPrice ONE ROW PER MATCH AFTER MATCH SKIP TO FIRST B PATTERN (A+ B) DEFINE A AS AVG(A.price) &lt; 15 ) MR;Given the inputs shown in the doc, it should be: symbol start_tstamp end_tstamp avgPrice========= ================== ================== ============ACME 01-APR-11 10:00:00 01-APR-11 10:00:03 14.5instead of: symbol start_tstamp end_tstamp avgPrice========= ================== ================== ============ACME 01-APR-11 10:00:00 01-APR-11 10:00:03 14.5ACME 01-APR-11 10:00:04 01-APR-11 10:00:09 13.5</description>
      <version>1.8.2,1.9.1</version>
      <fixedVersion>1.8.3,1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.streaming.match.recognize.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-19 01:00:00" id="14126" opendate="2019-9-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elasticsearch Xpack Machine Learning doesn&amp;#39;t support ARM</summary>
      <description>Elasticsearch Xpack Machine Learning function is enabled by default if the version is &gt;=6.0. But This feature doesn't support ARM arch. So that in some e2e tests, Elasticsearch is failed to start.We should disable ML feature in this case on ARM.</description>
      <version>1.9.1</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.quickstarts.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.streaming.elasticsearch.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.sql.client.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.elasticsearch-common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-19 01:00:00" id="14459" opendate="2019-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Python module build hangs</summary>
      <description>The build of python module hangs when installing conda. See travis log: https://api.travis-ci.org/v3/job/599704570/log.txtCan't reproduce it neither on my local mac nor on my repo with travis.</description>
      <version>1.9.0,1.9.1</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.dev.lint-python.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-11-22 01:00:00" id="14481" opendate="2019-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify the Flink valid socket port check to 0 to 65535.</summary>
      <description>In Flink, I found that Flink's socket port check is 'port &gt;= 0 &amp;&amp; port &lt;= 65536.checkArgument(serverPort &gt;= 0 &amp;&amp; serverPort &lt;= 65536, "Invalid port number.");But in the process of binding the port, the valid port is 0 to 65535(A port number of zero will let the System pick up anephemeral port in a bin operation). Although the 65536 port will fail due to the port out of range when actually binding, Flink has already done a valid range check on the port, which seems to be very confusing. Should we modify Flink's port check to 0 to 65535?</description>
      <version>1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.RestClient.java</file>
      <file type="M">flink-queryable-state.flink-queryable-state-client-java.src.main.java.org.apache.flink.queryablestate.client.QueryableStateClient.java</file>
      <file type="M">flink-metrics.flink-metrics-influxdb.src.main.java.org.apache.flink.metrics.influxdb.InfluxdbReporter.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.util.NetUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.network.netty.NettyConfig.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.source.SocketTextStreamFunction.java</file>
      <file type="M">flink-streaming-java.src.main.java.org.apache.flink.streaming.api.functions.sink.SocketClientSink.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-22 01:00:00" id="14482" opendate="2019-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump up rocksdb version</summary>
      <description>This JIRA aims at rebasing frocksdb to newer version of official RocksDB.</description>
      <version>None</version>
      <fixedVersion>1.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainerTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.test.java.org.apache.flink.contrib.streaming.state.RocksDBInitTest.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksIteratorWrapper.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBResourceContainer.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.src.main.java.org.apache.flink.contrib.streaming.state.RocksDBConfigurableOptions.java</file>
      <file type="M">flink-state-backends.flink-statebackend-rocksdb.pom.xml</file>
      <file type="M">flink-dist.src.main.resources.META-INF.NOTICE</file>
      <file type="M">docs.layouts.shortcodes.generated.rocksdb.configurable.configuration.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-11-22 01:00:00" id="14488" opendate="2019-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update python table API with temporary tables &amp; views methods</summary>
      <description>Update python table API with new methods introduced in Java/Scala APIThis should cover the scope of FLINK-14490</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.table.tests.test.table.environment.api.py</file>
      <file type="M">flink-python.pyflink.table.tests.test.environment.completeness.py</file>
      <file type="M">flink-python.pyflink.table.table.environment.py</file>
      <file type="M">flink-python.pyflink.table.descriptors.py</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-10-26 01:00:00" id="14535" opendate="2019-10-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cast exception is thrown when count distinct on decimal fields</summary>
      <description>DecimalType in DistinctInfo bridged to wrong external BigDecimal type, which causes failures count distinct on decimal type.</description>
      <version>1.9.1</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.utils.DateTimeTestUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.SplitAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.AggregateUtil.scala</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-1-29 01:00:00" id="1460" opendate="2015-1-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo fixes</summary>
      <description>Fix some typos. Also fix some inconsistent uses of partition operator and partitioning operator in the codebase.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.compiler.PartitionOperatorTranslationTest.scala</file>
      <file type="M">flink-examples.flink-scala-examples.src.main.scala.org.apache.flink.examples.scala.ml.LinearRegression.scala</file>
      <file type="M">flink-compiler.src.test.java.org.apache.flink.compiler.java.PartitioningOperatorTest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-11-5 01:00:00" id="14605" opendate="2019-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Hive-1.1.0 as the profile to test against 1.1.x</summary>
      <description>Hive-1.1.1 has the issue that it can't properly handle stored as file_format syntax. So let's test against 1.1.0.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-12-5 01:00:00" id="14610" opendate="2019-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for how to define time attribute in DDL</summary>
      <description>Add documentation for how to use watermark syntax and computed column in DDL to define processing time attribute and event time attribute.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.streaming.time.attributes.zh.md</file>
      <file type="M">docs.dev.table.streaming.time.attributes.md</file>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.sourceSinks.zh.md</file>
      <file type="M">docs.dev.table.sourceSinks.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-6 01:00:00" id="14636" opendate="2019-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle schedule mode LAZY_FROM_SOURCES_WITH_BATCH_SLOT_REQUEST correctly in DefaultScheduler</summary>
      <description>It should be possible to schedule a job with ScheduleMode.LAZY_FROM_SOURCES_WITH_BATCH_SLOT_REQUEST.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-6 01:00:00" id="14637" opendate="2019-11-6 00:00:00" resolution="Done">
    <buginformation>
      <summary>Introduce framework off heap memory config</summary>
      <description>At the moment after FLINK-13982, when we do not account for adhoc direct memory allocations for Flink framework (except network buffers) or done by some libraries used in Flink. In general, we expect this allocations to stay under a certain reasonably low limit but we have to have some margin for them so that JVM direct memory limit is not exactly equal to network buffers and does not fail. We can address it by introducing framework off heap memory config option.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.test.java.org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtilsTest.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtils.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.clusterframework.TaskExecutorResourceSpec.java</file>
      <file type="M">flink-core.src.main.java.org.apache.flink.configuration.TaskManagerOptions.java</file>
      <file type="M">docs..includes.generated.task.manager.memory.configuration.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-11-12 01:00:00" id="14724" opendate="2019-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Join condition could be simplified in logical phase</summary>
      <description>currently the plan of tpcds q38.sql contains NestedLoopJoin, because it's join condition is CAST(AND(IS NOT DISTINCT FROM($2, $3), IS NOT DISTINCT FROM($1, $4), IS NOT DISTINCT FROM($0, $5))):BOOLEAN, and planner can't find equal join keys from the condition by Join#analyzeCondition.SimplifyJoinConditionRule could solve this.</description>
      <version>1.9.0,1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.SetOperatorsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-13 01:00:00" id="14727" opendate="2019-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update doc of supported Hive versions</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-13 01:00:00" id="14728" opendate="2019-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add reminder for users of potential thread safety issues of hive built-in function</summary>
      <description>remind users of https://issues.apache.org/jira/browse/HIVE-16183</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.hive.functions.zh.md</file>
      <file type="M">docs.dev.table.hive.hive.functions.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-2-4 01:00:00" id="1475" opendate="2015-2-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minimize log output of yarn test cases</summary>
      <description>The new yarn test cases are quite verbose. Maybe we could increase the log level for these tests.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-yarn-tests.src.test.resources.log4j-test.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-15 01:00:00" id="14800" opendate="2019-11-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce parallelism inference for HiveSource</summary>
      <description>Because new source api is not ready, we can not finish FLINK-14676Let's introduce parallelism inference for HiveSource first.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.HiveTableSourceTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableSource.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveTableFactory.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.connectors.hive.HiveOptions.java</file>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-11-16 01:00:00" id="14830" opendate="2019-11-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct the link for chinese version stream_checkpointing page</summary>
      <description>Currently, in Chinese version of stream_checkpointing page, there are some links not correct set to the Chinese version, but set to the English version.Such as {{site.baseurl }}/dev/stream/state/index.html &amp;#91;state backend&amp;#93;({{ site.baseurl }}/ops/state/state_backends.html).&amp;#91;State Backends&amp;#93;({{ site.baseurl }}/ops/state/state_backends.html)&amp;#91;Restart Strategies&amp;#93;({{ site.baseurl }}/dev/restart_strategies.html)  This issue wants to fix the problem.</description>
      <version>1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.internals.stream.checkpointing.zh.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-12-19 01:00:00" id="14847" opendate="2019-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support retrieving Hive PK constraints</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.src.test.java.org.apache.flink.connectors.hive.TableEnvHiveConnectorTest.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.util.HiveTableUtil.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.HiveCatalog.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV210.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShimV100.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveShim.java</file>
      <file type="M">flink-connectors.flink-connector-hive.src.main.java.org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-19 01:00:00" id="14849" opendate="2019-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix documentation about Hive dependencies</summary>
      <description>With:&lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt;&lt;/dependency&gt;Caused by: java.lang.ClassCastException: org.codehaus.janino.CompilerFactory cannot be cast to org.codehaus.commons.compiler.ICompilerFactory at org.codehaus.commons.compiler.CompilerFactoryFactory.getCompilerFactory(CompilerFactoryFactory.java:129) at org.codehaus.commons.compiler.CompilerFactoryFactory.getDefaultCompilerFactory(CompilerFactoryFactory.java:79) at org.apache.calcite.rel.metadata.JaninoRelMetadataProvider.compile(JaninoRelMetadataProvider.java:432) ... 68 moreAfter https://issues.apache.org/jira/browse/FLINK-13749 , flink-client will use default child-first resolve-order.If user jar has some conflict dependents, there will be some problem.Maybe we should update document to add some exclusions to hive dependents.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-11-20 01:00:00" id="14874" opendate="2019-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add local aggregate to solve data skew for ROLLUP/CUBE case</summary>
      <description>Many tpc-ds queries have rollup keyword, which will be translated to multiple groups. for example: group by rollup (channel, id) is equivalent group by (channel, id) + group by (channel) + group by (). All data on empty group will be shuffled to a single node, It is a typical data skew case. If there is a local aggregate, the data size shuffled to the single node will be greatly reduced. However, currently the cost mode can't estimate the local aggregate's cost, and the plan with local aggregate may be chose even the query has rollup keyword.we could add a rule based phase (after physical phase) to enforce local aggregate if it's input has empty group.</description>
      <version>1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.GroupingSetsTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.AggregateReduceGroupingTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecSortAggRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecHashAggRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecAggRuleBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.FlinkBatchRuleSets.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.optimize.program.FlinkBatchProgram.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-5-20 01:00:00" id="14881" opendate="2019-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade AWS SDK to support "IAM Roles for Service Accounts" in AWS EKS</summary>
      <description>In order to use IAM Roles for Service Accounts in AWS EKS, the minimum required version of the AWS SDK  is 1.11.625.https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts-minimum-sdk.html</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-s3-fs-presto.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-base.pom.xml</file>
      <file type="M">flink-filesystems.flink-s3-fs-presto.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-filesystems.flink-s3-fs-hadoop.pom.xml</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.resources.META-INF.NOTICE</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.src.main.java.org.apache.flink.streaming.connectors.kinesis.config.AWSConfigConstants.java</file>
      <file type="M">flink-connectors.flink-connector-kinesis.pom.xml</file>
      <file type="M">docs.dev.connectors.kinesis.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-21 01:00:00" id="14899" opendate="2019-11-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>can not be translated to StreamExecDeduplicate when PROCTIME() is defined in query</summary>
      <description>CREATE TABLE user_log ( user_id VARCHAR, item_id VARCHAR, category_id VARCHAR, behavior VARCHAR, ts TIMESTAMP) WITH ( 'connector.type' = 'kafka', 'connector.version' = 'universal', 'connector.topic' = 'user_behavior', 'connector.startup-mode' = 'earliest-offset', 'connector.properties.0.key' = 'zookeeper.connect', 'connector.properties.0.value' = 'localhost:2181', 'connector.properties.1.key' = 'bootstrap.servers', 'connector.properties.1.value' = 'localhost:9092', 'update-mode' = 'append', 'format.type' = 'json', 'format.derive-schema' = 'true');CREATE TABLE user_dist ( dt VARCHAR, user_id VARCHAR, behavior VARCHAR) WITH ( 'connector.type' = 'jdbc', 'connector.url' = 'jdbc:mysql://localhost:3306/flink-test', 'connector.table' = 'user_behavior_dup', 'connector.username' = 'root', 'connector.password' = ‘******', 'connector.write.flush.max-rows' = '1');INSERT INTO user_distSELECT dt, user_id, behaviorFROM ( SELECT dt, user_id, behavior, ROW_NUMBER() OVER (PARTITION BY dt, user_id, behavior ORDER BY proc asc ) AS rownum FROM (select DATE_FORMAT(ts, 'yyyy-MM-dd HH:00') as dt,user_id,behavior,PROCTIME() as proc from user_log) )WHERE rownum = 1;Exception in thread "main" org.apache.flink.table.api.TableException: UpsertStreamTableSink requires that Table has a full primary keys if it is updated.at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:114)at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:50)at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:54)at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlan(StreamExecSink.scala:50)at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:61)at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:60)at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)at scala.collection.Iterator$class.foreach(Iterator.scala:891)at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)at scala.collection.AbstractIterable.foreach(Iterable.scala:54)at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)at scala.collection.AbstractTraversable.map(Traversable.scala:104)at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:60)at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:149)at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:439)at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:348)</description>
      <version>1.9.0,1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.OverWindowITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.DeduplicateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.OverAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.RankTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.OverAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.calcite.RelTimeIndicatorConverter.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-1-26 01:00:00" id="14954" opendate="2019-11-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Publish OpenAPI specification of REST Monitoring API</summary>
      <description>Hello,Flink provides a very helpful REST Monitoring API.OpenAPI is convenient standard to generate clients in a variety of language for REST API documented according to their specification. In this case, clients would be helpful to automate management of Flink clusters.Currently, there is no "official" OpenAPI specification of Flink REST Monitoring API. Some have written by users, but their consistency across Flink releases is uncertain.I think it would be beneficial to have an OpenAPI specification provided and maintained by the Flink project. Kind regards, </description>
      <version>None</version>
      <fixedVersion>1.15.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.messages.json.SerializedThrowableSerializer.java</file>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.rest.handler.async.AsynchronousOperationResult.java</file>
      <file type="M">flink-docs.README.md</file>
      <file type="M">flink-docs.pom.xml</file>
      <file type="M">docs.content.docs.ops.rest.api.md</file>
      <file type="M">docs.content.zh.docs.ops.rest.api.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-12-27 01:00:00" id="14978" opendate="2019-11-27 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Introduce constraint class hierarchy required for primary keys</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-common.src.test.java.org.apache.flink.table.api.TableSchemaTest.java</file>
      <file type="M">flink-table.flink-table-common.src.main.java.org.apache.flink.table.api.TableSchema.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-12-2 01:00:00" id="15001" opendate="2019-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The digest of sub-plan reuse should contain retraction traits for stream physical nodes</summary>
      <description>This bug is found in FLINK-14946:The plan for the given sql in FLINK-14946 is however, the plan after sub-plan reuse is: in the first picture, we could find that the accMode of two joins are different, but the two joins are reused in the second picture. The reason is the digest of sub-plan reuse does not contain retraction traits for stream physical nodes now.</description>
      <version>1.9.0,1.9.1</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.RelDigestUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtilTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.DagOptimizationTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.digest.testGetDigestWithDynamicFunctionView.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.digest.testGetDigestWithDynamicFunction.out</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelTreeWriterImpl.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.RelDigestUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.reuse.SubplanReuser.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-2 01:00:00" id="15006" opendate="2019-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to close shuffle when dynamic partition inserting</summary>
      <description>When partition values are rare or have skew, if we shuffle by dynamic partitions, will break the performance.We can have an option to close shuffle in such cases:‘connector.sink.shuffle-by-partition.enable’ = ...</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.filesystem.FileSystemTableFactory.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.PartitionableSinkITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.PartitionableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.PartitionableSinkTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.PartitionableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.PartitionableSinkTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.stream.StreamExecSinkRule.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecSinkRule.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-2 01:00:00" id="15010" opendate="2019-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Temp directories flink-netty-shuffle-* are not cleaned up</summary>
      <description>Starting a Flink cluster with 2 TMs and stopping it again will leave 2 temporary directories (and not delete them): flink-netty-shuffle-&lt;uid&gt;</description>
      <version>1.9.0,1.9.1,1.9.2</version>
      <fixedVersion>1.9.3,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime.src.main.java.org.apache.flink.runtime.io.disk.FileChannelManagerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-3 01:00:00" id="15026" opendate="2019-12-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support database DDLs in SQL CLI</summary>
      <description>Support DDL as following:CREATE DATABASE [ IF NOT EXISTS ] [ catalogName.] dataBaseName [ COMMENT database_comment ] [WITH ( name=value [, name=value]*)]DROP DATABASE [ IF EXISTS ] [ catalogName.] dataBaseName [ (RESTRICT|CASCADE)]ALTER DATABASE [ catalogName.] dataBaseName SET ( name=value [, name=value]*)USE [ catalogName.] dataBaseName</description>
      <version>1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.gateway.local.LocalExecutor.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-3 01:00:00" id="15027" opendate="2019-12-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support alter table DDLs in SQL CLI</summary>
      <description>Support syntax as following:ALTER TABLE [[catalogName.] dataBasesName].tableName RENAME TO newTableNameALTER TABLE [[catalogName.] dataBasesName].tableName SET ( name=value [, name=value]*)</description>
      <version>1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.cli.SqlCommandParserTest.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.SqlCommandParser.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliStrings.java</file>
      <file type="M">flink-table.flink-sql-client.src.main.java.org.apache.flink.table.client.cli.CliClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-4 01:00:00" id="15052" opendate="2019-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test transformation.clear() in sqlClient</summary>
      <description>when executing multiple commands from sql client, the later job graph will include all job graphs which already executed. </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.src.test.java.org.apache.flink.table.client.gateway.local.LocalExecutorITCase.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2019-12-9 01:00:00" id="15135" opendate="2019-12-9 00:00:00" resolution="Done">
    <buginformation>
      <summary>Adding e2e tests for Flink&amp;#39;s Mesos integration</summary>
      <description>Currently, there is no end to end test or IT case for Mesos deployment. We want to add Mesos end-to-end tests which will benefit both Mesos users and contributors.More discussion could be found here.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.yarn.docker.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
      <file type="M">tools.travis.splits.split.container.sh</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-12-9 01:00:00" id="15153" opendate="2019-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Service selector needs to contain jobmanager component label</summary>
      <description>The jobmanager label needs to be added to service selector. Otherwise, it may select the wrong backend pods(taskmanager).The internal service is used for taskmanager talking to jobmanager. If it does not have correct backend pods, the taskmanager may fail to register.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-kubernetes.src.test.java.org.apache.flink.kubernetes.kubeclient.Fabric8ClientTest.java</file>
      <file type="M">flink-kubernetes.src.main.java.org.apache.flink.kubernetes.kubeclient.decorators.ServiceDecorator.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-12-9 01:00:00" id="15157" opendate="2019-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make ScalaShell ensureYarnConfig() and fetchConnectionInfo() public</summary>
      <description>This allows users of the Scala Shell, such as Zeppelin to work better with the shell.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-scala-shell.src.main.scala.org.apache.flink.api.scala.FlinkShell.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-9 01:00:00" id="15159" opendate="2019-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the mapping of JSON schema string type to Flink SQL STRING type</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.connect.zh.md</file>
      <file type="M">docs.dev.table.connect.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-10 01:00:00" id="15181" opendate="2019-12-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix minor typos and mistakes in table documentation</summary>
      <description>Minor documentation corrections.</description>
      <version>1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.streaming.dynamic.tables.zh.md</file>
      <file type="M">docs.dev.table.streaming.dynamic.tables.md</file>
      <file type="M">docs.dev.table.sql.create.zh.md</file>
      <file type="M">docs.dev.table.sql.create.md</file>
      <file type="M">docs.dev.table.common.zh.md</file>
      <file type="M">docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-11 01:00:00" id="15185" opendate="2019-12-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive sink can not run in standalone mode</summary>
      <description>Now in hive HadoopFileSystemFactory, we use org.apache.flink.runtime.fs.hdfs.HadoopFileSystem to get FileSystem.But it should not work after we setting default child first class loader. Because in standalone mode, the cluster has no hadoop dependency. So the solution is: Add `flink-hadoop-fs` dependency to hive module, not work, because classes with "org.apache.flink" prefix will always be loaded by parent class loader  User add hadoop dependency to standalone cluster, it breaks out-of-the-box. Shade hadoop FileSystem in hive module, not complex, good.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-connector-hive.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-12 01:00:00" id="15214" opendate="2019-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding multiple submission e2e test for Flink&amp;#39;s Mesos integration</summary>
      <description>As discussed, we need a e2e test to verify the user's flow of submitting multiple jobs, in which the second job should reuse the slots of the first job.More detail could be found in ML.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.travis.splits.split.container.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-mesos-cluster.docker-compose.yml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.mesos.docker.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-16 01:00:00" id="15266" opendate="2019-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE in blink planner code gen</summary>
      <description>`cast` function in blink planner and old planner are different:in legacy planner:cast('' as int) -&gt; throw NumberFormatExceptioncast(null as int) -&gt; throw NullPointerExceptioncast('abc' as int) -&gt; throw NumberFormatExceptionbut in blink planner:cast('' as int) -&gt; return nullcast(null as int) -&gt; return nullcast('abc' as int) -&gt; return nullA step forward:```create table source { age int, id varchar};select case when age &lt; 20 then cast(id as bigint) else 0 end from source;```queries like above will throw NPE because we will try assign a `null` to a `long` field when the input satisfy `age &lt; 20`.</description>
      <version>1.9.1</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.ScalarOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-22 01:00:00" id="15360" opendate="2019-12-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn e2e test is broken with building docker image</summary>
      <description>Yarn e2e test is broken with building docker image. This is because this change https://github.com/apache/flink/commit/cce1cef50d993aba5060ea5ac597174525ae895e. Shell function retry_times do not support passing a command as multiple parts. For example, retry_times 5 0 docker build image could not work. cc karmagyz</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.common.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-1-27 01:00:00" id="15420" opendate="2019-12-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cast string to timestamp will loose precision</summary>
      <description>cast('2010-10-14 12:22:22.123456' as timestamp(9))Will produce "2010-10-14 12:22:22.123" in blink planner, this should not happen.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-runtime-blink.src.main.java.org.apache.flink.table.runtime.functions.SqlDateTimeUtils.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.expressions.TemporalTypesTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-1-6 01:00:00" id="15485" opendate="2020-1-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reopen tests when blocking issue has been resolved</summary>
      <description>Sometimes we  close test and left comment like 'TODO/when FLINK-xx is closed/when FLINK-xx is merged' for various reasons and ready to reopen  it after they are really fixed.Unfortunately we missed some of them. This issue aims to reopen tests that close by FLINK-12088   FLINK-13740  CALCITE-1860  </description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.runtime.batch.sql.SetOperatorsITCase.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.plan.ExpressionReductionRulesTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.SetOperatorsTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.TableAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.GroupWindowTableAggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.MiscITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.join.JoinITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CorrelateITCase2.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.ModifiedMonotonicityTest.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-6 01:00:00" id="15489" opendate="2020-1-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebUI log refresh not working</summary>
      <description>There is no way to query the latest state of logs of jobmanager/taskmanager.The Web UI show only the first version that was ever displayed.How to reproduce: (not sure if necessary) configure logback as described here: https://ci.apache.org/projects/flink/flink-docs-stable/dev/best_practices.html#use-logback-when-running-flink-on-a-cluster start a cluster show jobmanager logs in the Web UI run example job check again the jobmanager logs, there is no trace of the job. Clicking the refresh button does not help</description>
      <version>1.9.1,1.10.0</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.task-manager.service.ts</file>
      <file type="M">flink-runtime-web.web-dashboard.src.app.services.job-manager.service.ts</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-7 01:00:00" id="15495" opendate="2020-1-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set default planner for SQL Client to Blink planner</summary>
      <description>As discussed in the mailing list &amp;#91;1&amp;#93;, we will change the default planner to Blink planner for SQL CLI. &amp;#91;1&amp;#93;: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Set-default-planner-for-SQL-Client-to-Blink-planner-in-1-10-release-td36379.html</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-sql-client.conf.sql-client-defaults.yaml</file>
      <file type="M">docs.dev.table.sqlClient.zh.md</file>
      <file type="M">docs.dev.table.sqlClient.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-1-8 01:00:00" id="15515" opendate="2020-1-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document that Hive connector should be used with blink planner</summary>
      <description>HiveCatalog works with both old and blink planner. But read/write Hive tables only works with blink planner.</description>
      <version>None</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.hive.scala.shell.hive.zh.md</file>
      <file type="M">docs.dev.table.hive.scala.shell.hive.md</file>
      <file type="M">docs.dev.table.hive.read.write.hive.zh.md</file>
      <file type="M">docs.dev.table.hive.read.write.hive.md</file>
      <file type="M">docs.dev.table.hive.index.zh.md</file>
      <file type="M">docs.dev.table.hive.index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-9 01:00:00" id="15537" opendate="2020-1-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Type of keys should be `BinaryRow` when manipulating map state with `BaseRow` as key type.</summary>
      <description>`BaseRow` is serialized and deserialized as `BinaryRow` by default, so when the key type of the map state is `BaseRow`, we should construct map keys with `BinaryRow` as type to get value from map state, otherwise, you would  always get Null...Try it with following SQL:// (b: Int, c: String)SELECT b, listagg(DISTINCT c, '#')FROM MyTableGROUP BY b </description>
      <version>1.9.1</version>
      <fixedVersion>1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.table.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.IncrementalAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.DistinctAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.rules.logical.SplitAggregateRuleTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.java.org.apache.flink.table.planner.functions.aggfunctions.ListAggWsWithRetractAggFunctionTest.java</file>
      <file type="M">flink-table.flink-table-planner-blink.src.main.scala.org.apache.flink.table.planner.codegen.agg.DistinctAggCodeGen.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-13 01:00:00" id="15575" opendate="2020-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Azure Filesystem Shades Wrong Package "httpcomponents"</summary>
      <description>Instead of shading "org.apache.httpcomponents" (this package does not exist) the azure filesystem should shade "org.apache.http". This e.g. causes problems when the azure filesystem and elasticsearch6 connector are both on the classpath.</description>
      <version>1.9.1,1.10.0</version>
      <fixedVersion>1.9.3,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-filesystems.flink-azure-fs-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-13 01:00:00" id="15577" opendate="2020-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WindowAggregate RelNodes missing Window specs in digest</summary>
      <description>The RelNode's digest (AbstractRelNode.getDigest()), along with its RowType, is used by the Calcite HepPlanner to avoid adding duplicate Vertices to the graph. If an equivalent vertex is already present in the graph, then that vertex is used in place of the newly generated one: https://github.com/apache/calcite/blob/branch-1.21/core/src/main/java/org/apache/calcite/plan/hep/HepPlanner.java#L828This means that the digest needs to contain all the information necessary to identify a vertex and distinguish it from similar - but not equivalent - vertices.In the case of `LogicalWindowAggregation` and `FlinkLogicalWindowAggregation`, the window specs are currently not in the digest, meaning that two aggregations with the same signatures and expressions but different windows are considered equivalent by the planner, which is not correct and will lead to an invalid Physical Plan.For instance, the following query would give an invalid plan:WITH window_1h AS ( SELECT HOP_ROWTIME(`timestamp`, INTERVAL '1' HOUR, INTERVAL '1' HOUR) as `timestamp` FROM my_table GROUP BY HOP(`timestamp`, INTERVAL '1' HOUR, INTERVAL '1' HOUR)),window_2h AS ( SELECT HOP_ROWTIME(`timestamp`, INTERVAL '1' HOUR, INTERVAL '2' HOUR) as `timestamp` FROM my_table GROUP BY HOP(`timestamp`, INTERVAL '1' HOUR, INTERVAL '2' HOUR))(SELECT * FROM window_1h)UNION ALL(SELECT * FROM window_2h)The invalid plan generated by the planner is the following (Please note the windows in the two DataStreamGroupWindowAggregates nodes being the same when they should be different):DataStreamUnion(all=[true], union all=[timestamp]): rowcount = 200.0, cumulative cost = {800.0 rows, 802.0 cpu, 0.0 io}, id = 176 DataStreamCalc(select=[w$rowtime AS timestamp]): rowcount = 100.0, cumulative cost = {300.0 rows, 301.0 cpu, 0.0 io}, id = 173 DataStreamGroupWindowAggregate(window=[SlidingGroupWindow('w$, 'timestamp, 7200000.millis, 3600000.millis)], select=[start('w$) AS w$start, end('w$) AS w$end, rowtime('w$) AS w$rowtime, proctime('w$) AS w$proctime]): rowcount = 100.0, cumulative cost = {200.0 rows, 201.0 cpu, 0.0 io}, id = 172 DataStreamScan(id=[1], fields=[timestamp]): rowcount = 100.0, cumulative cost = {100.0 rows, 101.0 cpu, 0.0 io}, id = 171 DataStreamCalc(select=[w$rowtime AS timestamp]): rowcount = 100.0, cumulative cost = {300.0 rows, 301.0 cpu, 0.0 io}, id = 175 DataStreamGroupWindowAggregate(window=[SlidingGroupWindow('w$, 'timestamp, 7200000.millis, 3600000.millis)], select=[start('w$) AS w$start, end('w$) AS w$end, rowtime('w$) AS w$rowtime, proctime('w$) AS w$proctime]): rowcount = 100.0, cumulative cost = {200.0 rows, 201.0 cpu, 0.0 io}, id = 174 DataStreamScan(id=[1], fields=[timestamp]): rowcount = 100.0, cumulative cost = {100.0 rows, 101.0 cpu, 0.0 io}, id = 171</description>
      <version>1.9.1</version>
      <fixedVersion>1.9.2,1.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.table.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.scala.org.apache.flink.table.planner.plan.batch.sql.agg.WindowAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.table.GroupWindowTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner-blink.src.test.resources.org.apache.flink.table.planner.plan.batch.sql.agg.WindowAggregateTest.xml</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.table.GroupWindowTableAggregateTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.stream.sql.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.GroupWindowTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalWindowTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.nodes.logical.FlinkLogicalWindowAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalWindowTableAggregate.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.plan.logical.rel.LogicalWindowAggregate.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-13 01:00:00" id="15578" opendate="2020-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement exactly-once JDBC sink</summary>
      <description>As per discussion in the dev mailing list, there are two options: Write-ahead log Two-phase commit (XA)the latter being preferable. </description>
      <version>None</version>
      <fixedVersion>1.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-core.src.test.java.org.apache.flink.testutils.migration.MigrationVersion.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcTestFixture.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.JdbcITCase.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.test.java.org.apache.flink.connector.jdbc.DbMetadata.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.src.main.java.org.apache.flink.connector.jdbc.JdbcSink.java</file>
      <file type="M">flink-connectors.flink-connector-jdbc.pom.xml</file>
      <file type="M">docs.dev.connectors.jdbc.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-14 01:00:00" id="15584" opendate="2020-1-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Give nested data type of ROWs in ValidationException</summary>
      <description>In INSERT INTO baz_sinkSELECT  a, ROW(b, c)FROM foo_sourceSchema mismatch mistakes will not get proper detail level, yielding the following:Caused by: org.apache.flink.table.api.ValidationException: Field types of query result and registered TableSink &amp;#91;baz_sink&amp;#93; do not match. Query result schema: &amp;#91;a: Integer, EXPR$2: Row&amp;#93; TableSink schema: &amp;#91;a: Integer, payload: Row&amp;#93;Leaving the user with an opaque 'Row' type to debug. </description>
      <version>1.9.1,1.10.0,1.11.0</version>
      <fixedVersion>1.10.1,1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-planner.src.test.scala.org.apache.flink.table.api.batch.sql.validation.InsertIntoValidationTest.scala</file>
      <file type="M">flink-table.flink-table-planner.src.main.scala.org.apache.flink.table.sinks.TableSinkUtils.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-20 01:00:00" id="15694" opendate="2020-1-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove HDFS Section of Configuration Page</summary>
      <description>The section "HDFS" is outdated (and flagged as such).</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.ops.config.zh.md</file>
      <file type="M">docs.ops.config.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-7-23 01:00:00" id="15736" opendate="2020-1-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Java 17 (LTS)</summary>
      <description>Long-term issue for preparing Flink for Java 17.</description>
      <version>None</version>
      <fixedVersion>1.18.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.content.docs.deployment.memory..index.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-31 01:00:00" id="15833" opendate="2020-1-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix &amp;#39;Kerberized YARN on Docker&amp;#39; e2e test on Azure Pipelines</summary>
      <description>AZP support has been introduced in FLINK-13978.The YARN on Docker tests are disabled in the PR introducing Azure support.The issue is likely occurring due to different resources available on the Azure machines, compared to Travis.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-end-to-end-tests.test-scripts.test.yarn.kerberos.docker.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.yarn.docker.sh</file>
      <file type="M">flink-end-to-end-tests.run-nightly-tests.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-31 01:00:00" id="15834" opendate="2020-1-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set up nightly cron jobs on Azure Pipelines build</summary>
      <description>FLINK-13978 introduced support for Azure Pipelines, however limited to building pull requests and pushes.The scope of this issue is to add the cron jobs available in travis also to the Azure setup.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">tools.azure-pipelines.prepare.precommit.sh</file>
      <file type="M">tools.azure-pipelines.build-apache-repo.yml</file>
      <file type="M">azure-pipelines.yml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.test.yarn.kerberos.docker.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.docker-hadoop-secure-cluster.config.yarn-site.xml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.yarn.docker.sh</file>
      <file type="M">tools.azure.controller.sh</file>
      <file type="M">tools.azure-pipelines.jobs-template.yml</file>
      <file type="M">flink-end-to-end-tests.test-scripts.queryable.state.base.sh</file>
      <file type="M">flink-end-to-end-tests.test-scripts.common.kubernetes.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-13 01:00:00" id="16034" opendate="2020-2-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the string based expressions to the java dsl in documentation</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.dev.table.tableApi.zh.md</file>
      <file type="M">docs.dev.table.tableApi.md</file>
      <file type="M">docs.dev.table.streaming.time.attributes.zh.md</file>
      <file type="M">docs.dev.table.streaming.time.attributes.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.zh.md</file>
      <file type="M">docs.dev.table.streaming.temporal.tables.md</file>
      <file type="M">docs.dev.table.sql.queries.zh.md</file>
      <file type="M">docs.dev.table.sql.queries.md</file>
      <file type="M">docs.dev.table.common.zh.md</file>
      <file type="M">docs.dev.table.common.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-13 01:00:00" id="16036" opendate="2020-2-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecate String based Expression DSL in TableEnvironments</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.StreamTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-api-scala-bridge.src.main.scala.org.apache.flink.table.api.scala.BatchTableEnvironment.scala</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.java.StreamTableEnvironment.java</file>
      <file type="M">flink-table.flink-table-api-java-bridge.src.main.java.org.apache.flink.table.api.java.BatchTableEnvironment.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-13 01:00:00" id="16037" opendate="2020-2-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>maven-dependency-plugin not fully compatible with Java 11</summary>
      <description>The maven-dependency-plugin 3.1.1 is not fully compatible with Java 11; dependency analysis and listing of dependencies is currently failing.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">flink-yarn-tests.pom.xml</file>
      <file type="M">flink-table.flink-sql-parser.pom.xml</file>
      <file type="M">flink-examples.flink-examples-streaming.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-tpch-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-tpcds-test.pom.xml</file>
      <file type="M">flink-end-to-end-tests.flink-end-to-end-tests-common-kafka.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-3-10 01:00:00" id="16524" opendate="2020-3-10 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Optimize the execution of Python UDF to use generator to eliminate unnecessary function calls</summary>
      <description>Optimize the result of FlattenRowCoder and ArrowCoder to generator to eliminate unnecessary function calls.</description>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-python.pyflink.fn.execution.tests.coders.test.common.py</file>
      <file type="M">flink-python.pyflink.fn.execution.operations.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coder.impl.py</file>
      <file type="M">flink-python.pyflink.fn.execution.coders.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-27 01:00:00" id="16820" opendate="2020-3-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>support reading timestamp, data, and time in JDBCTableSource</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.JdbcTypeUtil.java</file>
      <file type="M">flink-connectors.flink-jdbc.src.main.java.org.apache.flink.api.java.io.jdbc.dialect.JDBCDialects.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-1-15 01:00:00" id="2023" opendate="2015-5-15 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>TypeExtractor does not work for (some) Scala Classes</summary>
      <description>vanaepi discovered some problems while working on the Scala Gelly API where, for example, a Scala MapFunction can not be correctly analyzed by the type extractor. For example, generic types will not be correctly detected.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">flink-tests.src.test.scala.org.apache.flink.api.scala.types.TypeInformationGenTest.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.codegen.TypeInformationGen.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.codegen.TypeDescriptors.scala</file>
      <file type="M">flink-scala.src.main.scala.org.apache.flink.api.scala.codegen.TypeAnalyzer.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-22 01:00:00" id="2083" opendate="2015-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure high quality docs for FlinkML in 0.9</summary>
      <description>As defined in our vision for FlinkML, providing high-quality documentation is a primary goal for us.This issue concerns the docs that will be included in 0.9, and will track improvements and additions for the release.</description>
      <version>None</version>
      <fixedVersion>0.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs..layouts.base.html</file>
      <file type="M">docs..includes.latex.commands.html</file>
      <file type="M">docs.libs.ml.optimization.md</file>
      <file type="M">docs.libs.ml.multiple.linear.regression.md</file>
      <file type="M">docs.libs.ml.distance.metrics.md</file>
      <file type="M">docs.libs.ml.als.md</file>
    </fixedFiles>
  </bug>
  
</bugrepository>